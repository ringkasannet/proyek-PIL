{"edecac102986f220cd30fdb104e3ccb1911255d2": {"paper_id": "edecac102986f220cd30fdb104e3ccb1911255d2", "abstract": "Enterprise systems are complex and expensive and create dramatic organizational change. Implementing an enterprise system can be the \"corporate equivalent of a root canal,\" a meaningful analogy given that an ES with its single database replaces myriad special-purpose legacy systems that once operated in isolation. An ES, or enterprise resource planning system, has the Herculean task of seamlessly supporting and integrating a full range of business processes, uniting functional islands and making their data visible across the organization in real time. The authors offer guidelines based on five years of observing ES implementations that can help managers circumvent obstacles and control the tensions during and after the project.", "title": "A roadmap for enterprise system implementation"}, "5696e39ce34dc10f19fee2e688364573de9715c4": {"paper_id": "5696e39ce34dc10f19fee2e688364573de9715c4", "abstract": "The topic of cyber security has been subject to more attention and interest outside the community of computer security experts. Cyber security is not a single problem, but rather it is a group of highly different problems involving different sets of threats. Fuzzy Rule based system for cyber security is a system that consists of a rule depository and a mechanism for accessing and running the rules. The depository is usually constructed with a collection of related rule sets. The aim of this study is to develop a fuzzy rule based technical indicator for cyber security with the use of an expert system which is named FRBCES (Fuzzy Rule Based Cyber Expert System). Rule based systems employ fuzzy rule to automate complex processes. Common cyber threats assumed for cyber experts are used as linguistic variables in this paper. KeywordsCyber security, cyber terrorism, fuzzy logic, fuzzy rules, Fuzzy Rule Based Cyber Expert System (FRBCES).", "title": "Designing a Fuzzy Rule Based Expert System for Cyber Security"}, "eeb1a1e0cab8d809b5789d04418dc247dca956cc": {"paper_id": "eeb1a1e0cab8d809b5789d04418dc247dca956cc", "abstract": "Lee, Stolfo, and Mok have previously reported the use of association rules and frequency episodes for mining audit data to gain knowledge for intrusion detection. The integration of association rules and frequency episodes with fuzzy logic can produce more abstract and flexible patterns for intrusion detection, since many quantitative features are involved in intrusion detection and security itself is fuzzy. We present a modification of a previously reported algorithm for mining fuzzy association rules, define the concept of fuzzy frequency episodes, and present an original algorithm for mining fuzzy frequency episodes. We add a normalization step to the procedure for mining fuzzy association rules in order to prevent one data instance from contributing more than others. We also modify the procedure for mining frequency episodes to learn fuzzy frequency episodes. Experimental results show the utility of fuzzy association rules and fuzzy frequency episodes in intrusion detection. Draft: Updated version published in the International Journal of Intelligent Systems, Volume 15, No. I, August 2000 3", "title": "Mining fuzzy association rules and fuzzy frequency episodes for intrusion detection"}, "9f712bd6b2118758152aeace0a0e7a6205325f53": {"paper_id": "9f712bd6b2118758152aeace0a0e7a6205325f53", "abstract": "In this paper, we present a possible application of neural networks as a component of an intrusion detection system. Neural network algorithms are emerging nowadays as a new arnficial intelligence techm\u201dquethat can be applied to real-l~e problems. We present an approach of user behavior modeling that takes advantage of the properties of neural algorithms and display results obtained on preliminary testing of our approach.", "title": "A neural network component for an intrusion detection system"}, "780e2631adae2fb3fa43965bdeddc0f3b885e20d": {"paper_id": "780e2631adae2fb3fa43965bdeddc0f3b885e20d", "abstract": "We introduce the problem of mining association rules in large relational tables containing both quantitative and categorical attributes. An example of such an association might be \"10% of married people between age 50 and 60 have at least 2 cars\". We deal with quantitative attributes by fine-partitioning the values of the attribute and then combining adjacent partitions as necessary. We introduce measures of partial completeness which quantify the information lost due to partitioning. A direct application of this technique can generate too many similar rules. We tackle this problem by using a \"greater-than-expected-value\" interest measure to identify the interesting rules in the output. We give an algorithm for mining such quantitative association rules. Finally, we describe the results of using this approach on a real-life dataset.", "title": "Mining Quantitative Association Rules in Large Relational Tables"}, "3a921534c39415ac41e87c3ef343c60f51dd928f": {"paper_id": "3a921534c39415ac41e87c3ef343c60f51dd928f", "abstract": "Today, every application uses software libraries. Yet, while a lot of research exists w.r.t. analyzing applications, research that targets the analysis of libraries independent of any application is scarce. This is unfortunate, because, for developers of libraries, such as the Java Development Kit (JDK), it is crucial to ensure that the library behaves as intended regardless of how it is used. To fill this gap, we discuss the construction of call graphs for libraries that abstract over all potential library usages. Call graphs are particularly relevant as they are a precursor of many advanced analyses, such as inter-procedural data-flow analyses. \n We show that the current practice of using call graph algorithms designed for applications to analyze libraries leads to call graphs that, at the same time, lack relevant call edges and contain unnecessary edges. This motivates the need for call graph construction algorithms dedicated to libraries. Unlike algorithms for applications, call graph construction algorithms for libraries must take into consideration the goals of subsequent analyses. Specifically, we show that it is essential to distinguish between the scenario of an analysis for potential exploitable vulnerabilities from the scenario of an analysis for general software quality attributes, e.g., dead methods or unused fields. This distinction affects the decision about what constitutes the library-private implementation, which therefore, needs special treatment. Thus, building one call graph that satisfies all needs is not sensical. Overall, we observed that the proposed call graph algorithms reduce the number of call edges up to 30% when compared to existing approaches.", "title": "Call graph construction for Java libraries"}, "046f6abc4f5d738d08fe9b6453951cd5f4efa3bf": {"paper_id": "046f6abc4f5d738d08fe9b6453951cd5f4efa3bf", "abstract": "A major limitation of Brain-Computer Interfaces (BCI) is their long calibration time, as much data from the user must be collected in order to tune the BCI for this target user. In this paper, we propose a new method to reduce this calibration time by using data from other subjects. More precisely, we propose an algorithm to regularize the Common Spatial Patterns (CSP) and Linear Discriminant Analysis (LDA) algorithms based on the data from a subset of automatically selected subjects. An evaluation of our approach showed that our method significantly outperformed the standard BCI design especially when the amount of data from the target user is small. Thus, our approach helps in reducing the amount of data needed to achieve a given performance level.", "title": "Learning from other subjects helps reducing Brain-Computer Interface calibration time"}, "23d8219db1aff006b41007effc696fca6fbcabcf": {"paper_id": "23d8219db1aff006b41007effc696fca6fbcabcf", "abstract": "------------------------------------------------------------Many economic problems require a covariance matrix estimator that is not only invertible, but also well-conditioned (that is, inverting it does not amplify estimation error). For largedimensional covariance matrices, the usual estimator -the sample covariance matrixis typically not well-conditioned and may not even be invertible. This paper introduces an estimator that is both well-conditioned and more accurate than the sample covariance matrix asymptotically. This estimator is distribution-free and has a simple explicit formula that is easy to compute and interpret. It is the asymptotically optimal convex linear combination of the sample covariance matrix with the identity matrix. Optimality is meant with respect to a quadratic loss function, asymptotically as the number of observations and the number of variables go to infinity together. Extensive Monte-Carlo confirm that the asymptotic results tend to hold well in finite sample.", "title": "A WELL-CONDITIONED ESTIMATOR FOR LARGE DIMENSIONAL COVARIANCE MATRICES"}, "1e2d6aa9262e5b7db4e7d3b7e2a0e96186141b74": {"paper_id": "1e2d6aa9262e5b7db4e7d3b7e2a0e96186141b74", "abstract": "In this paper we review classification algorithms used to design brain-computer interface (BCI) systems based on electroencephalography (EEG). We briefly present the commonly employed algorithms and describe their critical properties. Based on the literature, we compare them in terms of performance and provide guidelines to choose the suitable classification algorithm(s) for a specific BCI.", "title": "A review of classification algorithms for EEG-based brain-computer interfaces."}, "509bf8d7ac8ff4334c5aee5771183da0fd9c7a42": {"paper_id": "509bf8d7ac8ff4334c5aee5771183da0fd9c7a42", "abstract": "This paper introduces stacked generalization, a scheme for minimizing the generalization error rate of one or more generalizers. Stacked generalization works by deducing the biases of the generalizer(s) with respect to a provided learning set. This deduction proceeds by generalizing in a second space whose inputs are (for example) the guesses of the original generalizers when taught with part of the learning set and trying to guess the rest of it, and whose output is (for example) the correct guess. When used with multiple generalizers, stacked generalization can be seen as a more sophisticated version of cross-validation, exploiting a strategy more sophisticated than cross-vali-dation's crude winner-takes-all for combining the individual generalizers. When used with a single generalizer, stacked generalization is a scheme for estimating (and then correcting for) the error of a generalizer which has been trained on a particular learning set and then asked a particular question. After introducing stacked generalization and justifying its use, this paper presents two numerical experiments. The first demonstrates how stacked generalization improves upon a set of separate generalizers for the NETtalk task of translating text to phonemes. The second demonstrates how stacked generalization improves the performance of a single surface-fitter. With the other experimental evidence in the literature, the usual arguments supporting cross-validation, and the abstract justifications presented in this paper, the conclusion is that for almost any real-world generalization problem one should use some version of stacked generalization to minimize the generalization error rate. This paper ends by discussing some of the variations of stacked generalization, and how it touches on other fields like chaos theory.", "title": "Stacked generalization"}, "4f4e38c0257c86c28a8d134af91daf35bbfb61ae": {"paper_id": "4f4e38c0257c86c28a8d134af91daf35bbfb61ae", "abstract": "The Berlin brain-computer interface (BBCI) project develops a noninvasive BCI system whose key features are: 1) the use of well-established motor competences as control paradigms; 2) high-dimensional features from multichannel EEG; and 3) advanced machine-learning techniques. Spatio-spectral changes of sensorimotor rhythms are used to discriminate imagined movements (left hand, right hand, and foot). A previous feedback study [M. Krauledat, K.-R. Muller, and G. Curio. (2007) The non-invasive Berlin brain-computer Interface: Fast acquisition of effective performance in untrained subjects. NeuroImage. [Online]. 37(2), pp. 539--550. Available: http://dx.doi.org/10.1016/j.neuroimage.2007.01.051] with ten subjects provided preliminary evidence that the BBCI system can be operated at high accuracy for subjects with less than five prior BCI exposures. Here, we demonstrate in a group of 14 fully BCI-naive subjects that 8 out of 14 BCI novices can perform at >84% accuracy in their very first BCI session, and a further four subjects at >70%. Thus, 12 out of 14 BCI-novices had significant above-chance level performances without any subject training even in the first session, as based on an optimized EEG analysis by advanced machine-learning algorithms.", "title": "The Berlin Brain-Computer Interface: Accurate performance from first-session in BCI-naive subjects"}, "b54c5dd03ca2c0b9b17b9fc1d4ebe4c2b6478982": {"paper_id": "b54c5dd03ca2c0b9b17b9fc1d4ebe4c2b6478982", "abstract": "This review discusses machine learning methods and their application to Brain-Computer Interfacing. A particular focus is placed on feature selection. We also point out common flaws when validating machine learning methods in the context of BCI. Finally we provide a brief overview on the Berlin-Brain Computer Interface (BBCI).", "title": "MACHINE LEARNING TECHNIQUES FOR BRAIN-COMPUTER INTERFACES"}, "2eb2ca05a79d1d81033237aad416ad4a1ce90a70": {"paper_id": "2eb2ca05a79d1d81033237aad416ad4a1ce90a70", "abstract": "This paper provides an introduction to support vector machines, kernel Fisher discriminant analysis, and kernel principal component analysis, as examples for successful kernel-based learning methods. We first give a short background about Vapnik-Chervonenkis theory and kernel feature spaces and then proceed to kernel based learning in supervised and unsupervised scenarios including practical and algorithmic considerations. We illustrate the usefulness of kernel algorithms by discussing applications such as optical character recognition and DNA analysis.", "title": "An introduction to kernel-based learning algorithms"}, "0307ef4f7b9b401ffa079b528e6235509307ef5f": {"paper_id": "0307ef4f7b9b401ffa079b528e6235509307ef5f", "abstract": "Driven by the progress in the field of single-trial analysis of EEG, there is a growing interest in brain computer interfaces (BCIs), i.e., systems that enable human subjects to control a computer only by means of their brain signals. In a pseudo-online simulation our BCI detects upcoming finger movements in a natural keyboard typing condition and predicts their laterality. This can be done on average 100\u2013230ms before the respective key is actually pressed, i.e., long before the onset of EMG. Our approach is appealing for its short response time and high classification accuracy (>96%) in a binary decision where no human training is involved. We compare discriminative classifiers like Support Vector Machines (SVMs) and different variants of Fisher Discriminant that possess favorable regularization properties for dealing with high noise cases (inter-trial variablity).", "title": "Classifying Single Trial EEG: Towards Brain Computer Interfacing"}, "40c3869fe8522aae6a6a23c6f3b59f1e3fb30651": {"paper_id": "40c3869fe8522aae6a6a23c6f3b59f1e3fb30651", "abstract": "In this paper, we address two different types of noise in information extraction models: noise from distant supervision and noise from pipeline input features. Our target tasks are entity typing and relation extraction. For the first noise type, we introduce multi-instance multi-label learning algorithms using neural network models, and apply them to fine-grained entity typing for the first time. Our model outperforms the state-of-the-art supervised approach which uses global embeddings of entities. For the second noise type, we propose ways to improve the integration of noisy entity type predictions into relation extraction. Our experiments show that probabilistic predictions are more robust than discrete predictions and that joint training of the two tasks performs best.", "title": "Noise Mitigation for Neural Entity Typing and Relation Extraction"}, "2538e3eb24d26f31482c479d95d2e26c0e79b990": {"paper_id": "2538e3eb24d26f31482c479d95d2e26c0e79b990", "abstract": "We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.", "title": "Natural Language Processing (almost) from Scratch"}, "2c5135a0531bc5ad7dd890f018e67a40529f5bcb": {"paper_id": "2c5135a0531bc5ad7dd890f018e67a40529f5bcb", "abstract": "One of the most important issues in machine learning is whether one can improve the performance of a supervised learning algorithm by including unlabeled data. Methods that use both labeled and unlabeled data are generally referred to as semi-supervised learning. Although a number of such methods are proposed, at the current stage, we still don\u2019t have a complete understanding of their effectiveness. This paper investigates a closely related problem, which leads to a novel approach to semi-supervised learning. Specifically we consider learning predictive structures on hypothesis spaces (that is, what kind of classifiers have good predictive power) from multiple learning tasks. We present a general framework in which the structural learning problem can be formulated and analyzed theoretically, and relate it to learning with unlabeled data. Under this framework, algorithms for structural learning will be proposed, and computational issues will be investigated. Experiments will be given to demonstrate the effectiveness of the proposed algorithms in the semi-supervised learning setting.", "title": "A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data"}, "31274eabb84407e3bc2c1d14b804cb7bcc068111": {"paper_id": "31274eabb84407e3bc2c1d14b804cb7bcc068111", "abstract": "We provide an experimental study of the role of syntactic parsing in semantic role labeling. Our conclusions demonstrate that syntactic parse information is clearly most relevant in the very first stage \u2013 the pruning stage. In addition, the quality of the pruning stage cannot be determined solely based on its recall and precision. Instead it depends on the characteristics of the output candidates that make downstream problems easier or harder. Motivated by this observation, we suggest an effective and simple approach of combining different semantic role labeling systems through joint inference, which significantly improves the performance.", "title": "The Necessity of Syntactic Parsing for Semantic Role Labeling"}, "32a175b36ec7f2f08cb3dfac30ce141e144ec9e9": {"paper_id": "32a175b36ec7f2f08cb3dfac30ce141e144ec9e9", "abstract": "Statistical methods useful in automatic recognition of continuous speech are described. They concern modeling of a speaker and of an acoustic processor, extraction of the models' statistical parameters and hypothesis search procedures and likelihood computations of linguistic decoding. Experimental results are presented that indicate the power of the methods.", "title": "Continuous speech recognition by statistical methods"}, "2abe6b9ea1b13653b7384e9c8ef14b0d87e20cfc": {"paper_id": "2abe6b9ea1b13653b7384e9c8ef14b0d87e20cfc", "abstract": "Reuters Corpus Volume I (RCV1) is an archive of over 800,000 manually categorized newswire stories recently made available by Reuters, Ltd. for research purposes. Use of this data for research on text categorization requires a detailed understanding of the real world constraints under which the data was produced. Drawing on interviews with Reuters personnel and access to Reuters documentation, we describe the coding policy and quality control procedures used in producing the RCV1 data, the intended semantics of the hierarchical category taxonomies, and the corrections necessary to remove errorful data. We refer to the original data as RCV1-v1, and the corrected data as RCV1-v2. We benchmark several widely used supervised learning methods on RCV1-v2, illustrating the collection\u2019s properties, suggesting new directions for research, and providing baseline results for future studies. We make available detailed, per-category experimental results, as well as corrected versions of the category assignments and taxonomy structures, via online appendices.", "title": "RCV1: A New Benchmark Collection for Text Categorization Research"}, "2d2e53746269594ba3c6f677f6f2f19d6bf03dc5": {"paper_id": "2d2e53746269594ba3c6f677f6f2f19d6bf03dc5", "abstract": "Distantly supervised approaches have become popular in recent years as they allow training relation extractors without textbound annotation, using instead known relations from a knowledge base and a large textual corpus from an appropriate domain. While state of the art distant supervision approaches use off-theshelf named entity recognition and classification (NERC) systems to identify relation arguments, discrepancies in domain or genre between the data used for NERC training and the intended domain for the relation extractor can lead to low performance. This is particularly problematic for \u201cnon-standard\u201d named entities such as album which would fall into the MISC category. We propose to ameliorate this issue by jointly training the named entity classifier and the relation extractor using imitation learning which reduces structured prediction learning to classification learning. We further experiment with Web features different features and compare against using two off-the-shelf supervised NERC systems, Stanford NER and FIGER, for named entity classification. Our experiments show that imitation learning improves average precision by 4 points over an one-stage classification model, while removing Web features results in a 6 points reduction. Compared to using FIGER and Stanford NER, average precision is 10 points and 19 points higher with our imitation learning approach.", "title": "Extracting Relations between Non-Standard Entities using Distant Supervision and Imitation Learning"}, "559e47c03297bc423c4ee866d042790ea056365c": {"paper_id": "559e47c03297bc423c4ee866d042790ea056365c", "abstract": "Entity Recognition (ER) is a key component of relation extraction systems and many other natural-language processing applications. Unfortunately, most ER systems are restricted to produce labels from to a small set of entity classes, e.g., person, organization, location or miscellaneous. In order to intelligently understand text and extract a wide range of information, it is useful to more precisely determine the semantic classes of entities mentioned in unstructured text. This paper defines a fine-grained set of 112 tags, formulates the tagging problem as multi-class, multi-label classification, describes an unsupervised method for collecting training data, and presents the FIGER implementation. Experiments show that the system accurately predicts the tags for entities. Moreover, it provides useful information for a relation extraction system, increasing the F1 score by 93%. We make FIGER and its data available as a resource for future work.", "title": "Fine-Grained Entity Recognition"}, "8f8139b63a2fc0b3ae8413acaef47acd35a356e0": {"paper_id": "8f8139b63a2fc0b3ae8413acaef47acd35a356e0", "abstract": "Modern models of relation extraction for tasks like ACE are based on supervised learning of relations from small hand-labeled corpora. We investigate an alternative paradigm that does not require labeled corpora, avoiding the domain dependence of ACEstyle algorithms, and allowing the use of corpora of any size. Our experiments use Freebase, a large semantic database of several thousand relations, to provide distant supervision. For each pair of entities that appears in some Freebase relation, we find all sentences containing those entities in a large unlabeled corpus and extract textual features to train a relation classifier. Our algorithm combines the advantages of supervised IE (combining 400,000 noisy pattern features in a probabilistic classifier) and unsupervised IE (extracting large numbers of relations from large corpora of any domain). Our model is able to extract 10,000 instances of 102 relations at a precision of 67.6%. We also analyze feature performance, showing that syntactic parse features are particularly helpful for relations that are ambiguous or lexically distant in their expression.", "title": "Distant supervision for relation extraction without labeled data"}, "0d4fe56e306d2d3ed4fa272ecbc5fe018eeb850a": {"paper_id": "0d4fe56e306d2d3ed4fa272ecbc5fe018eeb850a", "abstract": "We present Searn, an algorithm for integrating search and learning to solve complex structured prediction problems such as those that occur in natural language, speech, computational biology, and vision. Searn is a meta-algorithm that transforms these complex problems into simple classification problems to which any binary classifier may be applied. Unlike current algorithms for structured learning that require decomposition of both the loss function and the feature functions over the predicted structure, Searn is able to learn prediction functions for any loss function and any class of features. Moreover, Searn comes with a strong, natural theoretical guarantee: good performance on the derived classification problems implies good performance on the structured prediction problem.", "title": "Search-based structured prediction"}, "ec49316a5b2ab3da7a493ae276d02bd0e4a0b50f": {"paper_id": "ec49316a5b2ab3da7a493ae276d02bd0e4a0b50f", "abstract": "In this paper, we extend distant supervision (DS) based on Wikipedia for Relation Extraction (RE) by considering (i) relations defined in external repositories, e.g. YAGO, and (ii) any subset of Wikipedia documents. We show that training data constituted by sentences containing pairs of named entities in target relations is enough to produce reliable supervision. Our experiments with state-of-the-art relation extraction models, trained on the above data, show a meaningful F1 of 74.29% on a manually annotated test set: this highly improves the state-of-art in RE using DS. Additionally, our end-to-end experiments demonstrated that our extractors can be applied to any general text document.", "title": "End-to-End Relation Extraction Using Distant Supervision from External Semantic Repositories"}, "4f410ab5c8b12b34b38421241366ee456bbebab9": {"paper_id": "4f410ab5c8b12b34b38421241366ee456bbebab9", "abstract": "Most current statistical natural language processing models use only local features so as to permit dynamic programming in inference, but this makes them unable to fully account for the long distance structure that is prevalent in language use. We show how to solve this dilemma with Gibbs sampling, a simple Monte Carlo method used to perform approximate inference in factored probabilistic models. By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs, CMMs, and CRFs, it is possible to incorporate non-local structure while preserving tractable inference. We use this technique to augment an existing CRF-based information extraction system with long-distance dependency models, enforcing label consistency and extraction template consistency constraints. This technique results in an error reduction of up to 9% over state-of-the-art systems on two established information extraction tasks.", "title": "Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling"}, "3081f5d97c9c9b4917aed99d19fb5cf1cde0b0d1": {"paper_id": "3081f5d97c9c9b4917aed99d19fb5cf1cde0b0d1", "abstract": "This purpose of this introductory paper is threefold. First, it introduces the Monte Carlo method with emphasis on probabilistic machine learning. Second, it reviews the main building blocks of modern Markov chain Monte Carlo simulation, thereby providing and introduction to the remaining papers of this special issue. Lastly, it discusses new interesting research horizons.", "title": "An Introduction to MCMC for Machine Learning"}, "2640913656089380ffdd697b141b838a8f214909": {"paper_id": "2640913656089380ffdd697b141b838a8f214909", "abstract": "This paper presents a maximum entropy-based named entity recognizer (NER). It differs from previous machine learning-based NERs in that it uses information from the whole document to classify each word, with just one classifier. Previous work that involves the gathering of information from the whole document often uses a secondary classifier, which corrects the mistakes of a primary sentencebased classifier. In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC-6 and MUC-7 test data.", "title": "Named Entity Recognition: A Maximum Entropy Approach Using Global Information"}, "4f5cd4c2d81db5c52f952589a8d52bba16962707": {"paper_id": "4f5cd4c2d81db5c52f952589a8d52bba16962707", "abstract": "This paper proposes a novel approach for relation extraction from free text which is trained to jointly use information from the text and from existing knowledge. Our model is based on two scoring functions that operate by learning low-dimensional embeddings of words and of entities and relationships from a knowledge base. We empirically show on New York Times articles aligned with Freebase relations that our approach is able to efficiently use the extra information provided by a large subset of Freebase data (4M entities, 23k relationships) to improve over existing methods that rely on text features alone.", "title": "Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction"}, "68a33a3afac65eb6e0fb3726c1f9c8b727f32a42": {"paper_id": "68a33a3afac65eb6e0fb3726c1f9c8b727f32a42", "abstract": "Relational learning is becoming increasingly important in many areas of application. Here, we present a novel approach to relational learning based on the factorization of a three-way tensor. We show that unlike other tensor approaches, our method is able to perform collective learning via the latent components of the model and provide an efficient algorithm to compute the factorization. We substantiate our theoretical considerations regarding the collective learning capabilities of our model by the means of experiments on both a new dataset and a dataset commonly used in entity resolution. Furthermore, we show on common benchmark datasets that our approach achieves better or on-par results, if compared to current state-of-the-art relational learning solutions, while it is significantly faster to compute.", "title": "A Three-Way Model for Collective Learning on Multi-Relational Data"}, "233d861338cfcd479b1d21897453fcc66418d5e1": {"paper_id": "233d861338cfcd479b1d21897453fcc66418d5e1", "abstract": "Information-extraction (IE) systems seek to distill semantic relations from naturallanguage text, but most systems use supervised learning of relation-specific examples and are thus limited by the availability of training data. Open IE systems such as TextRunner, on the other hand, aim to handle the unbounded number of relations found on the Web. But how well can these open systems perform? This paper presents WOE, an open IE system which improves dramatically on TextRunner\u2019s precision and recall. The key to WOE\u2019s performance is a novel form of self-supervised learning for open extractors \u2014 using heuristic matches between Wikipedia infobox attribute values and corresponding sentences to construct training data. Like TextRunner, WOE\u2019s extractor eschews lexicalized features and handles an unbounded set of semantic relations. WOE can operate in two modes: when restricted to POS tag features, it runs as quickly as TextRunner, but when set to use dependency-parse features its precision and recall rise even higher.", "title": "Open Information Extraction Using Wikipedia"}, "6e73703654b645c7dab15a977434e9ee43d92e77": {"paper_id": "6e73703654b645c7dab15a977434e9ee43d92e77", "abstract": "A central problem in grounded language acquisition is learning the correspondences between a rich world state and a stream of text which references that world state. To deal with the high degree of ambiguity present in this setting, we present a generative model that simultaneously segments the text into utterances and maps each utterance to a meaning representation grounded in the world state. We show that our model generalizes across three domains of increasing difficulty\u2014Robocup sportscasting, weather forecasts (a new domain), and NFL recaps.", "title": "Learning Semantic Correspondences with Less Supervision"}, "151ee8aedc97e7a388a8edd704ff13698a7af0b4": {"paper_id": "151ee8aedc97e7a388a8edd704ff13698a7af0b4", "abstract": "Distant supervision for relation extraction (RE) \u2013 gathering training data by aligning a database of facts with text \u2013 is an efficient approach to scale RE to thousands of different relations. However, this introduces a challenging learning scenario where the relation expressed by a pair of entities found in a sentence is unknown. For example, a sentence containing Balzac and France may express BornIn or Died, an unknown relation, or no relation at all. Because of this, traditional supervised learning, which assumes that each example is explicitly mapped to a label, is not appropriate. We propose a novel approach to multi-instance multi-label learning for RE, which jointly models all the instances of a pair of entities in text and all their labels using a graphical model with latent variables. Our model performs competitively on two difficult domains.", "title": "Multi-instance Multi-label Learning for Relation Extraction"}, "68cd1c7c0651b116a83abab8a7a46a29975d3b5f": {"paper_id": "68cd1c7c0651b116a83abab8a7a46a29975d3b5f", "abstract": "Extracting semantic relationships between entities is challenging. This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM. Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement. This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking. We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance. Evaluation on the ACE corpus shows that effective incorporation of diverse features enables our system outperform previously best-reported systems on the 24 ACE relation subtypes and significantly outperforms tree kernel-based systems by over 20 in F-measure on the 5 ACE relation types.", "title": "Exploring Various Knowledge in Relation Extraction"}, "c5b5957ffde9d865dabd0d54089c7f427520bc24": {"paper_id": "c5b5957ffde9d865dabd0d54089c7f427520bc24", "abstract": "This article describes three statistical models for natural language parsing. The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree. Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram lexical dependencies, wh-movement, and preferences for close attachment. All of these preferences are expressed by probabilities conditioned on lexical heads. The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature. To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies. We analyze various characteristics of the models through experiments on parsing accuracy, by collecting frequencies of various structures in the treebank, and through linguistically motivated examples. Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models.", "title": "Head-Driven Statistical Models for Natural Language Parsing"}, "57b219132b634d1b4c634c92e58b9a1c8b3f60ce": {"paper_id": "57b219132b634d1b4c634c92e58b9a1c8b3f60ce", "abstract": "The World Wide Web is a vast resource for information. At the same time it is extremely distributed. A particular type of data such as restaurant lists may be scattered across thousands of independent information sources in many di erent formats. In this paper, we consider the problem of extracting a relation for such a data type from all of these sources automatically. We present a technique which exploits the duality between sets of patterns and relations to grow the target relation starting from a small sample. To test our technique we use it to extract a relation of (author,title) pairs from the World Wide Web.", "title": "Extracting Patterns and Relations from the World Wide Web"}, "669b4d7cf907479f0697305b8e11cb3700cca86a": {"paper_id": "669b4d7cf907479f0697305b8e11cb3700cca86a", "abstract": "Text documents often contain valuable structured data that is hidden Yin regular English sentences. This data is best exploited infavailable as arelational table that we could use for answering precise queries or running data mining tasks.We explore a technique for extracting such tables from document collections that requires only a handful of training examples from users. These examples are used to generate extraction patterns, that in turn result in new tuples being extracted from the document collection.We build on this idea and present our Snowball system. Snowball introduces novel strategies for generating patterns and extracting tuples from plain-text documents.At each iteration of the extraction process, Snowball evaluates the quality of these patterns and tuples without human intervention,and keeps only the most reliable ones for the next iteration. In this paper we also develop a scalable evaluation methodology and metrics for our task, and present a thorough experimental evaluation of Snowball and comparable techniques over a collection of more than 300,000 newspaper documents.", "title": "Snowball: extracting relations from large plain-text collections"}, "3d01fa12d5d18ca671b81e36ec336881af462cc4": {"paper_id": "3d01fa12d5d18ca671b81e36ec336881af462cc4", "abstract": "We present a novel abstractive summarization framework that draws on the recent development of a treebank for the Abstract Meaning Representation (AMR). In this framework, the source text is parsed to a set of AMR graphs, the graphs are transformed into a summary graph, and then text is generated from the summary graph. We focus on the graph-tograph transformation that reduces the source semantic graph into a summary graph, making use of an existing AMR parser and assuming the eventual availability of an AMR-totext generator. The framework is data-driven, trainable, and not specifically designed for a particular domain. Experiments on goldstandard AMR annotations and system parses show promising results. Code is available at: https://github.com/summarization", "title": "Toward Abstractive Summarization Using Semantic Representations"}, "129c96e900c5b0f4d41f449a89305d9cf91a1d1c": {"paper_id": "129c96e900c5b0f4d41f449a89305d9cf91a1d1c", "abstract": "We present a novel graph-based summarization framework (Opinosis) that generates concise abstractive summaries of highly redundant opinions. Evaluation results on summarizing user reviews show that Opinosis summaries have better agreement with human summaries compared to the baseline extractive method. The summaries are readable, reasonably well-formed and are informative enough to convey the major opinions.", "title": "Opinosis: A Graph Based Approach to Abstractive Summarization of Highly Redundant Opinions"}, "1bad3e9f15df77f06ae449bba17f9e85a3bb9187": {"paper_id": "1bad3e9f15df77f06ae449bba17f9e85a3bb9187", "abstract": "We present a multi-document summarizer, called MEAD, which generates summaries using cluster centroids produced by a topic detection and tracking system. We also describe two new techniques, based on sentence utility and subsumption, which we have applied to the evaluation of both single and multiple document summaries. Finally, we describe two user studies that test our models of multi-document summarization.", "title": "Centroid-based summarization of multiple documents: sentence extraction utility-based evaluation, and user studies"}, "255e97d82f528b613dbe8883727abfd14f3f9f39": {"paper_id": "255e97d82f528b613dbe8883727abfd14f3f9f39", "abstract": "ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It includes measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans. The measures count the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans. This paper introduces four different ROUGE measures: ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S included in the ROUGE summarization evaluation package and their evaluations. Three of them have been used in the Document Understanding Conference (DUC) 2004, a large-scale summarization evaluation sponsored by NIST.", "title": "ROUGE: A Package For Automatic Evaluation Of Summaries"}, "6872792acbfcafc900c79d2a2c0fb66afaa5ee5e": {"paper_id": "6872792acbfcafc900c79d2a2c0fb66afaa5ee5e", "abstract": "We consider the task of summarizing a cluster of related sentences with a short sentence which we call multi-sentence compressionand present a simple approach based on shortest paths in word graphs. The advantage and the novelty of the proposed method is that it is syntaxlean and requires little more than a tokenizer and a tagger. Despite its simplicity, it is capable of generating grammatical and informative summaries as our experiments with English and Spanish data demonstrate.", "title": "Multi-Sentence Compression: Finding Shortest Paths in Word Graphs"}, "44fca068eecce2203d111213e3691647914a3945": {"paper_id": "44fca068eecce2203d111213e3691647914a3945", "abstract": "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents.", "title": "LexRank: Graph-based Lexical Centrality as Salience in Text Summarization"}, "2455fd3a758b562e8502e3e59e70fb30632319cd": {"paper_id": "2455fd3a758b562e8502e3e59e70fb30632319cd", "abstract": "We address the text-to-text generation problem of sentence-level paraphrasing \u2014 a phenomenon distinct from and more difficult than wordor phrase-level paraphrasing. Our approach applies multiple-sequence alignment to sentences gathered from unannotated comparable corpora: it learns a set of paraphrasing patterns represented by word lattice pairs and automatically determines how to apply these patterns to rewrite new sentences. The results of our evaluation experiments show that the system derives accurate paraphrases, outperforming baseline systems.", "title": "Learning to Paraphrase: An Unsupervised Approach Using Multiple-Sequence Alignment"}, "e6a4c9c918a4d8154b08bbdc3673da6a6eaeba3c": {"paper_id": "e6a4c9c918a4d8154b08bbdc3673da6a6eaeba3c", "abstract": "Multidocument extractive summarization relies on the concept of sentence centrality to identify the most important sentences in a document. Centrality is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We are now considering an approach for computing sentence importance based on the concept of eigenvector centrality (prestige) that we call LexPageRank. In this model, a sentence connectivity matrix is constructed based on cosine similarity. If the cosine similarity between two sentences exceeds a particular predefined threshold, a corresponding edge is added to the connectivity matrix. We provide an evaluation of our method on DUC 2004 data. The results show that our approach outperforms centroid-based summarization and is quite successful compared to other summarization systems.", "title": "LexPageRank: Prestige in Multi-Document Text Summarization"}, "723f4317f405a8c09d441cefe31ff719890b678c": {"paper_id": "723f4317f405a8c09d441cefe31ff719890b678c", "abstract": "We investigate one technique to produce a summary of an original text without requiring its full semantic interpretation, but instead relying on a model of the topic progression in the text derived from lexical chains. We present a new algorithm to compute lexical chains in a text, merging several robust knowledge sources: the WordNet thesaurus, a part-of-speech tagger, shallow parser for the identification of nominal groups, and a segmentation algorithm. Summarization proceeds in four steps: the original text is segmented, lexical chains are constructed, strong chains are identified and significant sentences are extracted. We present in this paper empirical results on the identification of strong chains and of significant sentences. Preliminary results indicate that quality indicative summaries are produced. Pending problems are identified. Plans to address these short-comings are briefly presented.", "title": "Using Lexical Chains For Text Summarization"}, "8e2cd5369db9242574740e0d2739c755f8f61c92": {"paper_id": "8e2cd5369db9242574740e0d2739c755f8f61c92", "abstract": "We present an Integer Linear Program for exact inference under a maximum coverage model for automatic summarization. We compare our model, which operates at the subsentence or \u201cconcept\u201d-level, to a sentencelevel model, previously solved with an ILP. Our model scales more efficiently to larger problems because it does not require a quadratic number of variables to address redundancy in pairs of selected sentences. We also show how to include sentence compression in the ILP formulation, which has the desirable property of performing compression and sentence selection simultaneously. The resulting system performs at least as well as the best systems participating in the recent Text Analysis Conference, as judged by a variety of automatic and manual content-based metrics.", "title": "A Scalable Global Model for Summarization"}, "b0df677063ba6e964c1a3a1aa733bd06737aec7f": {"paper_id": "b0df677063ba6e964c1a3a1aa733bd06737aec7f", "abstract": "Sentence compression holds promise for many applications ranging from summarization to subtitle generation. Our work views sentence compression as an optimization problem and uses integer linear programming (ILP) to infer globally optimal compressions in the presence of linguistically motivated constraints. We show how previous formulations of sentence compression can be recast as ILPs and extend these models with novel global constraints. Experimental results on written and spoken texts demonstrate improvements over state-of-the-art models.", "title": "Global Inference for Sentence Compression: An Integer Linear Programming Approach"}, "014ff1eac046518a57ba2f1a75b1aa50579cc594": {"paper_id": "014ff1eac046518a57ba2f1a75b1aa50579cc594", "abstract": "This paper discusses a text extraction approach to multidocument summarization that builds on single-document summarization methods by using additional, available in-, formation about the document set as a whole and the relationships between the documents. Multi-document summarization differs from single in that the issues of compression, speed, redundancy and passage selection are critical in the formation of useful summaries. Our approach addresses these issues by using domainindependent techniques based mainly on fast, statistical processing, a metric for reducing redundancy and maximizing diversity in the selected passages, and a modular framework to allow easy parameterization for different genres, corpora characteristics and user requirements.", "title": "Multi-Document Summarization By Sentence Extraction"}, "05aba481e8a221df5d8775a3bb749001e7f2525e": {"paper_id": "05aba481e8a221df5d8775a3bb749001e7f2525e", "abstract": "We present a new family of subgradient methods that dynamica lly incorporate knowledge of the geometry of the data observed in earlier iterations to perfo rm more informative gradient-based learning. Metaphorically, the adaptation allows us to find n eedles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems fro m recent advances in stochastic optimization and online learning which employ proximal funct ions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adap tively modifying the proximal function, which significantly simplifies setting a learning rate nd results in regret guarantees that are provably as good as the best proximal function that can be cho sen in hindsight. We give several efficient algorithms for empirical risk minimization probl ems with common and important regularization functions and domain constraints. We experimen tally study our theoretical analysis and show that adaptive subgradient methods outperform state-o f-the-art, yet non-adaptive, subgradient algorithms.", "title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization"}, "41d6966c926015b8e0d7b1a9de3ffab013091e15": {"paper_id": "41d6966c926015b8e0d7b1a9de3ffab013091e15", "abstract": "In an online convex optimization problem a decision-maker makes a sequence of decisions, i.e., chooses a sequence of points in Euclidean space, from a fixed feasible set. After each point is chosen, it encounters a sequence of (possibly unrelated) convex cost functions. Zinkevich (ICML 2003) introduced this framework, which models many natural repeated decision-making problems and generalizes many existing problems such as Prediction from Expert Advice and Cover\u2019s Universal Portfolios. Zinkevich showed that a simple online gradient descent algorithm achieves additive regret $O(\\sqrt{T})$ , for an arbitrary sequence of T convex cost functions (of bounded gradients), with respect to the best single decision in hindsight. In this paper, we give algorithms that achieve regret O(log\u2009(T)) for an arbitrary sequence of strictly convex functions (with bounded first and second derivatives). This mirrors what has been done for the special cases of prediction from expert advice by Kivinen and Warmuth (EuroCOLT 1999), and Universal Portfolios by Cover (Math. Finance 1:1\u201319, 1991). We propose several algorithms achieving logarithmic regret, which besides being more general are also much more efficient to implement. The main new ideas give rise to an efficient algorithm based on the Newton method for optimization, a new tool in the field. Our analysis shows a surprising connection between the natural follow-the-leader approach and the Newton method. We also analyze other algorithms, which tie together several different previous approaches including follow-the-leader, exponential weighting, Cover\u2019s algorithm and gradient descent.", "title": "Logarithmic regret algorithms for online convex optimization"}, "1bd43bc8ec308a1ef3e9496868d8c6baa02e4f5d": {"paper_id": "1bd43bc8ec308a1ef3e9496868d8c6baa02e4f5d", "abstract": "We present AROW, an online learning algorithm for binary and multiclass problems that combines large margin training, confidence weighting, and the capacity to handle non-separable data. AROW performs adaptive regularization of the prediction function upon seeing each new instance, allowing it to perform especially well in the presence of label noise. We derive mistake bounds for the binary and multiclass settings that are similar in form to the second order perceptron bound. Our bounds do not assume separability. We also relate our algorithm to recent confidence-weighted online learning techniques. Empirical evaluations show that AROW achieves state-of-the-art performance on a wide range of binary and multiclass tasks, as well as robustness in the face of non-separable data.", "title": "Adaptive regularization of weight vectors"}, "1621f05894ad5fd6a8fcb8827a8c7aca36c81775": {"paper_id": "1621f05894ad5fd6a8fcb8827a8c7aca36c81775", "abstract": "This paper considers an important class of convex programming (CP) problems, namely, the stochastic composite optimization (SCO), whose objective function is given by the summation of general nonsmooth and smooth stochastic components. Since SCO covers non-smooth, smooth and stochastic CP as certain special cases, a valid lower bound on the rate of convergence for solving these problems is known from the classic complexity theory of convex programming. Note however that the optimization algorithms that can achieve this lower bound had never been developed. In this paper, we show that the simple mirror-descent stochastic approximation method exhibits the best-known rate of convergence for solving these problems. Our major contribution is to introduce the accelerated stochastic approximation (AC-SA) algorithm based on Nesterov\u2019s optimal method for smooth CP [32,34], and show that the AC-SA algorithm can achieve the aforementioned lower bound on the rate of convergence for SCO. To the best of our knowledge, it is also the first universally optimal algorithm in the literature for solving non-smooth, smooth and stochastic CP problems. We illustrate the significant advantages of the AC-SA algorithm over existing methods in the context of solving a special but broad class of stochastic programming problems.", "title": "An optimal method for stochastic composite optimization"}, "603707c2f133db6879cba2429ae1130a88948943": {"paper_id": "603707c2f133db6879cba2429ae1130a88948943", "abstract": "Text summarization is one of the oldest problems in natural language processing. Popular approaches rely on extracting relevant sentences from the original documents. As a side effect, sentences that are too long but partly relevant are doomed to either not appear in the final summary, or prevent inclusion of other relevant sentences. Sentence compression is a recent framework that aims to select the shortest subsequence of words that yields an informative and grammatical sentence. This work proposes a one-step approach for document summarization that jointly performs sentence extraction and compression by solving an integer linear program. We report favorable experimental results on newswire data.", "title": "Summarization with a Joint Model for Sentence Extraction and Compression"}, "2c56d3a6e0c5260614ebd8bc843a29e18c4afbbc": {"paper_id": "2c56d3a6e0c5260614ebd8bc843a29e18c4afbbc", "abstract": "s by the selection of sentences. American Documentation, 12(2):139\u2013143, April 1961. [13] U. Reimer and U. Hahn. Text condensationas knowledge base abstraction. In IEEE Conf. on AI Applications, pages 338\u2013 344, 1988. [14] G. Salton, J. Alan, and C. Buckley. Approaches to passage retrieval in full text information systems. In Proceedings of SIGIR\u201993, pages 49\u201358, June 1993. [15] G. Salton, J. Allan, C. Buckley, and A. Singhal. Automatic analysis, theme generation, and summarization of machinereadable texts. Science, 264(3):1421\u20131426, June 1994. [16] C. Schwarz. Content based text handling. Information Processing & Management, 26(2):219\u2013226, 1990. [17] E. F. Skorokhod\u2019ko. Adaptive method of automatic abstracting and indexing. In IFIP Congress,Ljubljana, Yugoslavia 71, pages 1179\u20131182. North Holland, 1972. [18] J. I. Tait. Generating summaries using a script-based language analyzer. In L. Steels and J.A. Campbell, editors, Progress in Artificial Intelligence, pages 312\u2013318. Ellis Horwood, 1985. [19] L. C. Tong and S. L. Tan. A statistical approach to automatic text extraction. Asian Library Journal. 9 Appendix 9.0.1 Direct Match If a summary sentence is identical to a sentence in the original, or has essentially the same content, the match is defined as a direct match. An example match that is not exact but considered to convey the same content is shown below: Manual: This paper identifies the desirable features of an ideal multisensor gas monitor and lists the different models currently available. Original: The present part lists the desirable features and the different models of portable, multisensor gas monitors currently available. 9.0.2 Direct Join If the content of the manual sentence is represented by two or more sentences in the original, the latter sentences are noted as joins. For example: Manual: In California, Caltrans has a rolling pavement management program, with continuouscollection of data with the aim of identifying roads that require more monitoring and repair. Original (1): Rather than conducting biennial surveys, Caltrans now has a rolling pavement-management program, with data collected continuously. Original (2): The idea is to pinpoint the roads that may need more or less monitoring and repair. 9.0.3 Incomplete Matches A sentence in the original document is labelled as an incomplete match if it only partially covers the content of a manual summary sentence, or if a direct match is not clear. It can occur in the context of a single sentence or a join. The following exemplifies an incomplete single sentence match: Manual: Intergranular fracture of polycrystalline Ni3Alwas studied at 77K. Original: Before discussing the observeddeformation and fracture behavior of polycrystalline Ni3Al at 77K in terms of the kinetics of the proposed environmental embrittlement mechanism, we should ask whether the low temperature by itself significantly affects the brittleness of Ni3Al.", "title": "A Trainable Document Summarizer"}, "480a3f5ff15485601d0346c17adc6334de430d63": {"paper_id": "480a3f5ff15485601d0346c17adc6334de430d63", "abstract": "In this paper we generalise the sentence compression task. Rather than simply shorten a sentence by deleting words or constituents, as in previous work, we rewrite it using additional operations such as substitution, reordering, and insertion. We present a new corpus that is suited to our task and a discriminative tree-totree transduction model that can naturally account for structural and lexical mismatches. The model incorporates a novel grammar extraction method, uses a language model for coherent output, and can be easily tuned to a wide range of compression specific loss functions.", "title": "Sentence Compression Beyond Word Deletion"}, "3b8e7c8220d3883d54960d896a73045f3c70ac17": {"paper_id": "3b8e7c8220d3883d54960d896a73045f3c70ac17", "abstract": "We describe new algorithms for training tagging models, as an alternative to maximum-entropy models or conditional random elds (CRFs). The algorithms rely on Viterbi decoding of training examples, combined with simple additive updates. We describe theory justifying the algorithms through a modi cation of the proof of convergence of the perceptron algorithm for classi cation problems. We give experimental results on part-of-speech tagging and base noun phrase chunking, in both cases showing improvements over results for a maximum-entropy tagger.", "title": "Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms"}, "0b44fcbeea9415d400c5f5789d6b892b6f98daff": {"paper_id": "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "abstract": "In this paper, we review our experience with constructing one such large annotated corpus--the Penn Treebank, a corpus consisting of over 4.5 million words of American English. During the first three-year phase of the Penn Treebank Project (1989-1992), this corpus has been annotated for part-of-speech (POS) information. In addition, over half of it has been annotated for skeletal syntactic structure. Comments University of Pennsylvania Department of Computer and Information Science Technical Report No. MSCIS-93-87. This technical report is available at ScholarlyCommons: http://repository.upenn.edu/cis_reports/237 Building A Large Annotated Corpus of English: The Penn Treebank MS-CIS-93-87 LINC LAB 260 Mitchell P. Marcus Beatrice Santorini Mary Ann Marcinkiewicz University of Pennsylvania School of Engineering and Applied Science Computer and Information Science Department Philadelphia, PA 19104-6389", "title": "Building a Large Annotated Corpus of English: The Penn Treebank"}, "167e21b3932ac9f18e49899b7c5b0347395fd9f9": {"paper_id": "167e21b3932ac9f18e49899b7c5b0347395fd9f9", "abstract": "Hidden Markov models (HMMs) are a powerful probabilistic tool for modeling sequential data, and have been applied with success to many text-related tasks, such as part-of-speech tagging, text segmentation and information extraction. In these cases, the observations are usually modeled as multinomial distributions over a discrete vocabulary, and the HMM parameters are set to maximize the likelihood of the observations. This paper presents a new Markovian sequence model, closely related to HMMs, that allows observations to be represented as arbitrary overlapping features (such as word, capitalization, formatting, part-of-speech), and defines the conditional probability of state sequences given observation sequences. It does this by using the maximum entropy framework to fit a set of exponential models that represent the probability of a state given an observation and the previous state. We present positive experimental results on the segmentation of FAQ\u2019s.", "title": "Maximum Entropy Markov Models for Information Extraction and Segmentation"}, "16c3eaad79a51a9e1a408d768a5096ca7675d4fd": {"paper_id": "16c3eaad79a51a9e1a408d768a5096ca7675d4fd", "abstract": "We describe the application of kernel methods to Natural Language Processing (NLP) problems. In many NLP tasks the objects being modeled are strings, trees, graphs or other discrete structures which require some mechanism to convert them into feature vectors. We describe kernels for various natural language structures, allowing rich, high dimensional representations of these structures. We show how a kernel over trees can be applied to parsing using the voted perceptron algorithm, and we give experimental results on the ATIS corpus of parse trees.", "title": "Convolution Kernels for Natural Language"}, "98b940cc7e06f078a921ca1b835eee19ae6395ec": {"paper_id": "98b940cc7e06f078a921ca1b835eee19ae6395ec", "abstract": "A frequency-reconfigurable microstrip slot antenna is proposed. The antenna is capable of frequency switching at six different frequency bands between 2.2 and 4.75 GHz. Five RF p-i-n diode switches are positioned in the slot to achieve frequency reconfigurability. The feed line and the slot are bended to reduce 33% of the original size of the antenna. The biasing circuit is integrated into the ground plane to minimize the parasitic effects toward the performance of the antenna. Simulated and measured results are used to demonstrate the performance of the antenna. The simulated and measured return losses, together with the radiation patterns, are presented and compared.", "title": "A Compact Frequency-Reconfigurable Narrowband Microstrip Slot Antenna"}, "ec248f96e4d4359fe5a74b9d252c8b3988b55515": {"paper_id": "ec248f96e4d4359fe5a74b9d252c8b3988b55515", "abstract": "A polarization reconfigurable slot antenna with a novel coplanar waveguide (CPW)-to-slotline transition for wireless local area networks (WLANs) is proposed and tested. The antenna consists of a square slot, a reconfigurable CPW-to-slotline transition, and two p-i-n diodes. No extra matching structure is needed for modes transiting, which makes it much more compact than all reference designs. The -10 dB bandwidths of an antenna with an implemented bias circuit are 610 (25.4%) and 680 MHz (28.3%) for vertical and horizontal polarizations, respectively. The radiation pattern and gain of the proposed antenna are also tested, and the radiation pattern data were compared to simulation results.", "title": "Polarization Reconfigurable Slot Antenna With a Novel Compact CPW-to-Slotline Transition for WLAN Application"}, "59483664dfb38a7ce66e7dc279ac2d0d8456dbb6": {"paper_id": "59483664dfb38a7ce66e7dc279ac2d0d8456dbb6", "abstract": "We propose a slotted bow-tie antenna with pattern reconfigurability. The antenna consists of a coplanar waveguide (CPW) input, a pair of reconfigurable CPW-to-slotline transitions, a pair of Vivaldi-shaped radiating tapered slots, and four PIN diodes for reconfigurability. With suitable arrangement of the bias network, the proposed antenna demonstrates reconfigurable radiation patterns in the frequency range from 3.5 to 6.5 GHz in three states: a broadside radiation with fairly omnidirectional pattern and two end-fire radiations whose main beams are directed to exactly opposite directions. The proposed antenna is investigated comprehensively with the help of the radiation patterns in the two principal cuts and also the antenna gain responses versus frequencies. The simulation and measurement results reveal fairly good agreement and hence sustain the reconfigurability of the proposed design.", "title": "A Wideband Slotted Bow-Tie Antenna With Reconfigurable CPW-to-Slotline Transition for Pattern Diversity"}, "b30c869640e36113276655ab7a78da9df0d852d7": {"paper_id": "b30c869640e36113276655ab7a78da9df0d852d7", "abstract": "An ultrawide-band coplanar waveguide (CPW) fed slot antenna is presented. A rectangular slot antenna is excited by a 50-/spl Omega/ CPW with a U-shaped tuning stub. The impedance bandwidth, from both measurement and simulation, is about 110% (S11<-10 dB). The antenna radiates bi-directionally. The radiation patterns obtained from simulations are found to be stable across the matching band and experimental verification is provided at the high end of the band.", "title": "Ultrawide-band coplanar waveguide-fed rectangular slot antenna"}, "0f42befba8435c7e7aad8ea3d150504304eb3695": {"paper_id": "0f42befba8435c7e7aad8ea3d150504304eb3695", "abstract": "A simple and compact slot antenna with a very wide tuning range is proposed. A 25 mm (roughly equal to \u03bbH/8, where \u03bbH corresponds to the highest frequency of the tuning range) open slot is etched at the edge of the ground. To achieve the tunability, only two lumped elements, namely, a PIN diode and a varactor diode are used in the structure. By switching the PIN diode placed at the open end of the slot, the slot antenna can resonate as a standard slot (when the switch is on) or a half slot (when the switch is off). Continuous tuning over a wide frequency range in those two modes can be achieved by adjusting the reverse bias (giving different capacitances) of the varactor diode loaded in the slot. Through optimal design, the tuning bands of the two modes are stitched together to form a very wide tuning range. The fabricated prototype has a tuning frequency range from 0.42 GHz to 1.48 GHz with Sll better than -10 dB, giving a frequency ratio (fR = fu/fL) of 3.52:1. The measured full spherical radiation patterns show consistent radiation characteristics of the proposed antenna within the whole tuning range.", "title": "A Simple Compact Reconfigurable Slot Antenna With a Very Wide Tuning Range"}, "4388273959adb10b86d5e404fd005dfc1be44603": {"paper_id": "4388273959adb10b86d5e404fd005dfc1be44603", "abstract": "In this letter, we propose a tunable patch antenna made of a slotted rectangular patch loaded by a number of posts close to the patch edge. The posts are short circuited to the ground plane via a set of PIN diode switches. Simulations and measurements verify the possibility of tuning the antenna in subbands from 620 to 1150 MHz. Good matching has been achieved over most of the bands. Other performed designs show that more than one octave can be achieved using the proposed structure.", "title": "A Widely Tunable Compact Patch Antenna"}, "3bcf92bca8955a4c2bfa972b693423862776c77e": {"paper_id": "3bcf92bca8955a4c2bfa972b693423862776c77e", "abstract": "Aim: To describe lower extremity injuries for badminton in New Zealand. Methods: Lower limb badminton injuries that resulted in claims accepted by the national insurance company Accident Compensation Corporation (ACC) in New Zealand between 2006 and 2011 were reviewed. Results: The estimated national injury incidence for badminton injuries in New Zealand from 2006 to 2011 was 0.66%. There were 1909 lower limb badminton injury claims which cost NZ$2,014,337 (NZ$ value over 2006 to 2011). The age-bands frequently injured were 10\u201319 (22%), 40\u201349 (22%), 30\u201339 (14%) and 50\u201359 (13%) years. Sixty five percent of lower limb injuries were knee ligament sprains/tears. Males sustained more cruciate ligament sprains than females (75 vs. 39). Movements involving turning, changing direction, shifting weight, pivoting or twisting were responsible for 34% of lower extremity injuries. Conclusion: The knee was most frequently OPEN ACCESS", "title": "A Retrospective Review from 2006 to 2011 of Lower Extremity Injuries in Badminton in New Zealand"}, "9c64d35a2da75283cd7fa8b2aaf01897dd5c3f46": {"paper_id": "9c64d35a2da75283cd7fa8b2aaf01897dd5c3f46", "abstract": "This paper studies the importance of identifying and categorizing scientific concepts as a way to achieve a deeper understanding of the research literature of a scientific community. To reach this goal, we propose an unsupervised bootstrapping algorithm for identifying and categorizing mentions of concepts. We then propose a new clustering algorithm that uses citations' context as a way to cluster the extracted mentions into coherent concepts. Our evaluation of the algorithms against gold standards shows significant improvement over state-of-the-art results. More importantly, we analyze the computational linguistic literature using the proposed algorithms and show four different ways to summarize and understand the research community which are difficult to obtain using existing techniques.", "title": "Concept-based analysis of scientific literature"}, "1de45d1563d6a36af3bc831ef54b9ebddc9dda39": {"paper_id": "1de45d1563d6a36af3bc831ef54b9ebddc9dda39", "abstract": "We introduce the ACL Anthology Network (AAN), a manually curated networked database of citations, collaborations, and summaries in the field of Computational Linguistics. We also present a number of statistics about the network including the most cited authors, the most central collaborators, as well as network statistics about the paper citation, author citation, and author collaboration networks.", "title": "The ACL anthology network corpus"}, "033b62167e7358c429738092109311af696e9137": {"paper_id": "033b62167e7358c429738092109311af696e9137", "abstract": "This paper presents a simple unsupervised learning algorithm for classifying reviews as recommended (thumbs up) or not recommended (thumbs down). The classification of a review is predicted by the average semantic orientation of the phrases in the review that contain adjectives or adverbs. A phrase has a positive semantic orientation when it has good associations (e.g., \u201csubtle nuances\u201d) and a negative semantic orientation when it has bad associations (e.g., \u201cvery cavalier\u201d). In this paper, the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word \u201cexcellent\u201d minus the mutual information between the given phrase and the word \u201cpoor\u201d. A review is classified as recommended if the average semantic orientation of its phrases is positive. The algorithm achieves an average accuracy of 74% when evaluated on 410 reviews from Epinions, sampled from four different domains (reviews of automobiles, banks, movies, and travel destinations). The accuracy ranges from 84% for automobile reviews to 66% for movie reviews.", "title": "Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews"}, "ad30dbedf02d6f99374e1931c49248096f955986": {"paper_id": "ad30dbedf02d6f99374e1931c49248096f955986", "abstract": "Sentiment analysis of citations in scientific papers and articles is a new and interesting problem due to the many linguistic differences between scientific texts and other genres. In this paper, we focus on the problem of automatic identification of positive and negative sentiment polarity in citations to scientific papers. Using a newly constructed annotated citation sentiment corpus, we explore the effectiveness of existing and novel features, including n-grams, specialised science-specific lexical features, dependency relations, sentence splitting and negation features. Our results show that 3-grams and dependencies perform best in this task; they outperform the sentence splitting, science lexicon and negation based features.", "title": "Sentiment Analysis of Citations using Sentence Structure-Based Features"}, "036c70cd07e4a2024eb71b2b1e0c1bc872cff107": {"paper_id": "036c70cd07e4a2024eb71b2b1e0c1bc872cff107", "abstract": "Quickly moving to a new area of research is painful for researchers due to the vast amount of scientific literature in each field of study. One possible way to overcome this problem is to summarize a scientific topic. In this paper, we propose a model of summarizing a single article, which can be further used to summarize an entire topic. Our model is based on analyzing others\u2019 viewpoint of the target article\u2019s contributions and the study of its citation summary network using a clustering approach.", "title": "Scientific Paper Summarization Using Citation Summary Networks"}, "1b255cda9c92a5fbaba319aeb4b4ec532693c2a4": {"paper_id": "1b255cda9c92a5fbaba319aeb4b4ec532693c2a4", "abstract": "A family of probabilistic time series models is developed to analyze the time evolution of topics in large document collections. The approach is to use state space models on the natural parameters of the multinomial distributions that represent the topics. Variational approximations based on Kalman filters and nonparametric wavelet regression are developed to carry out approximate posterior inference over the latent topics. In addition to giving quantitative, predictive models of a sequential corpus, dynamic topic models provide a qualitative window into the contents of a large document collection. The models are demonstrated by analyzing the OCR'ed archives of the journal Science from 1880 through 2000.", "title": "Dynamic topic models"}, "215aa495b4c860a1e6d87f2c36f34da464376cc4": {"paper_id": "215aa495b4c860a1e6d87f2c36f34da464376cc4", "abstract": "A first step in identifying the content of a document is determining which topics that document addresses. We describe a generative model for documents, introduced by Blei, Ng, and Jordan [Blei, D. M., Ng, A. Y. & Jordan, M. I. (2003) J. Machine Learn. Res. 3, 993-1022], in which each document is generated by choosing a distribution over topics and then choosing each word in the document from a topic selected according to this distribution. We then present a Markov chain Monte Carlo algorithm for inference in this model. We use this algorithm to analyze abstracts from PNAS by using Bayesian model selection to establish the number of topics. We show that the extracted topics capture meaningful structure in the data, consistent with the class designations provided by the authors of the articles, and outline further applications of this analysis, including identifying \"hot topics\" by examining temporal dynamics and tagging abstracts to illustrate semantic content.", "title": "Finding scientific topics."}, "29bae9472203546847ec1352a604566d0f602728": {"paper_id": "29bae9472203546847ec1352a604566d0f602728", "abstract": "Is perception of the whole based on perception of its parts? There is psychological and physiological evidence for parts-based representations in the brain, and certain computational theories of object recognition rely on such representations. But little is known about how brains or computers might learn the parts of objects. Here we demonstrate an algorithm for non-negative matrix factorization that is able to learn parts of faces and semantic features of text. This is in contrast to other methods, such as principal components analysis and vector quantization, that learn holistic, not parts-based, representations. Non-negative matrix factorization is distinguished from the other methods by its use of non-negativity constraints. These constraints lead to a parts-based representation because they allow only additive, not subtractive, combinations. When non-negative matrix factorization is implemented as a neural network, parts-based representations emerge by virtue of two properties: the firing rates of neurons are never negative and synaptic strengths do not change sign.", "title": "Learning the parts of objects by non-negative matrix factorization"}, "f0041b836d507b8d22367a6ef7faab583769de82": {"paper_id": "f0041b836d507b8d22367a6ef7faab583769de82", "abstract": "Probabilistic Latent Semantic Indexing is a novel approach to automated document indexing which is based on a statistical latent class model for factor analysis of count data. Fitted from a training corpus of text documents by a generalization of the Expectation Maximization algorithm, the utilized model is able to deal with domain{specific synonymy as well as with polysemous words. In contrast to standard Latent Semantic Indexing (LSI) by Singular Value Decomposition, the probabilistic variant has a solid statistical foundation and defines a proper generative data model. Retrieval experiments on a number of test collections indicate substantial performance gains over direct term matching methods as well as over LSI. In particular, the combination of models with different dimensionalities has proven to be advantageous.", "title": "Probabilistic Latent Semantic Indexing"}, "1a0a415178d20754418987e312681f3fb0b40257": {"paper_id": "1a0a415178d20754418987e312681f3fb0b40257", "abstract": "This paper discusses the use of unlabeled examples for the problem of named entity classification. A large number of rules is needed for coverage of the domain, suggesting that a fairly large number of labeled examples should be required to train a classifier. However, we show that the use of unlabeled data can reduce the requirements for supervision to just 7 simple \u201cseed\u201d rules. The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type. We present two algorithms. The first method uses a similar algorithm to that of (Yarowsky 95), with modifications motivated by (Blum and Mitchell 98). The second algorithm extends ideas from boosting algorithms, designed for supervised learning tasks, to the framework suggested by (Blum and Mitchell 98).", "title": "Unsupervised Models for Named Entity Classification"}, "3e0240b0db8395495d250927945cee3ac4ad03eb": {"paper_id": "3e0240b0db8395495d250927945cee3ac4ad03eb", "abstract": "This paper presents a statistical, learned approach to finding names and other nonrecursive entities in text (as per the MUC-6 definition of the NE task), using a variant of the standard hidden Markov model. We present our justification for the problem and our approach, a detailed discussion of the model itself and finally the successful results of this new approach.", "title": "Nymble: a High-Performance Learning Name-finder"}, "09ec387286f99ede233af307cc41e032279953ec": {"paper_id": "09ec387286f99ede233af307cc41e032279953ec", "abstract": "Semantic knowledge can be a great asset to natural language processing systems, but it is usually hand-coded for each application. Although some semantic information is available in general-purpose knowledge bases such as WordNet and Cyc, many applications require domain-specific lexicons that represent words and categories for a particular topic. In this paper, we present a corpus-based method that can be used to build semantic lexicons for specific categories. The input to the system is a small set of seed words for a category and a representative text corpus. The output is a ranked list of words that are associated with the category. A user then reviews the top-ranked words and decides which ones should be entered in the semantic lexicon. In experiments with five categories, users typically found about 60 words per category in 10-15 minutes to build a core semantic lexicon. 1 I n t r o d u c t i o n Semantic information can be helpful in almost all aspects of natural language understanding, including word sense disambiguation, selectional restrictions, attachment decisions, and discourse processing. Semantic knowledge can add a great deal of power and accuracy to natural language processing systems. But semantic information is difficult to obtain. In most cases, semantic knowledge is encoded manually for each application. There have been a few large-scale efforts to create broad semantic knowledge bases, such as WordNet (Miller, 1990) and Cyc (Lenat, Prakash, and Shepherd, 1986). While these efforts may be useful for some applications, we believe that they will never fully satisfy the need for semantic knowledge. Many domains are characterized by their own sublanguage containing terms and jargon specific to the field. Representing all sublanguages in a single knowledge base would be nearly impossible. Furthermore, domain-specific semantic lexicons are useful for minimizing ambiguity problems. Within the context of a restricted domain, many polysemous words have a strong preference for one word sense, so knowing the most probable word sense in a domain can strongly constrain the ambiguity. We have been experimenting with a corpusbased method for building semantic lexicons semiautomatically. Our system uses a text corpus and a small set of seed words for a category to identify other words that also belong to the category. The algorithm uses simple statistics and a bootstrapping mechanism to generate a ranked list of potential category words. A human then reviews the top words and selects the best ones for the dictionary. Our approach is geared toward fast semantic lexicon construction: given a handful of seed words for a category and a representative text corpus, one can build a semantic lexicon for a category in just a few minutes. In the first section, we describe the statistical bootstrapping algorithm for identifying candidate category words and ranking them. Next, we describe experimental results for five categories. Finally, we discuss our experiences with additional categories and seed word lists, and summarize our results. 2 G e n e r a t i n g a S e m a n t i c L e x i c o n Our work is based on the observation that category members are often surrounded by other category members in text, for example in conjunctions (lions and tigers and bears), lists (lions, tigers, bears...), appositives (the stallion, a white Arabian), and nominal compounds (Arabian stallion; tuna fish). Given a few category members, we wondered whether it", "title": "A Corpus-Based Approach for Building Semantic Lexicons"}, "01deebfc9e8ab895385a12cbe15545acb2186601": {"paper_id": "01deebfc9e8ab895385a12cbe15545acb2186601", "abstract": "Most topic models, such as latent Dirichlet allocation, rely on the bag-of-words assumption. However, word order and phrases are often critical to capturing the meaning of text in many text mining tasks. This paper presents topical n-grams, a topic model that discovers topics as well as topical phrases. The probabilistic model generates words in their textual order by, for each word, first sampling a topic, then sampling its status as a unigram or bigram, and then sampling the word from a topic-specific unigram or bigram distribution. Thus our model can model \"white house\" as a special meaning phrase in the 'politics' topic, but not in the 'real estate' topic. Successive bigrams form longer phrases. We present experiments showing meaningful phrases and more interpretable topics from the NIPS data and improved information retrieval performance on a TREC collection.", "title": "Topical N-Grams: Phrase and Topic Discovery, with an Application to Information Retrieval"}, "01fa57bd91f731522c861404d29e4604ba6ac6d3": {"paper_id": "01fa57bd91f731522c861404d29e4604ba6ac6d3", "abstract": "We discuss a hierarchical probabilistic model whose predictions are similar to those of the popular language modelling procedure known as 'smoothing'. A number of interesting differences from smoothing emerge. The insights gained from a probabilistic view of this problem point towards new directions for language modelling. The ideas of this paper are also applicable to other problems such as the modelling of triphomes in speech, and DNA and protein sequences in molecular biology. The new algorithm is compared with smoothing on a two million word corpus. The methods prove to be about equally accurate, with the hierarchical model using fewer computational resources.", "title": "A hierarchical Dirichlet language model"}, "9693056ac1d845a7a1537774fcd758bb14909694": {"paper_id": "9693056ac1d845a7a1537774fcd758bb14909694", "abstract": "Natural languages are full of collocations, recurrent combinations of words that co-occur more often than expected by chance and that correspond to arbitrary word usages. Recent work in lexicography indicates that collocations are pervasive in English; apparently, they are common in all types of writing, including both technical and nontechnical genres. Several approaches have been proposed to retrieve various types of collocations from the analysis of large samples of textual data. These techniques automatically produce large numbers of collocations along with statistical figures intended to reflect the relevance of the associations. However, noue of these techniques provides functional information along with the collocation. Also, the results produced often contained improper word associations reflecting some spurious aspect of the training corpus that did not stand for true collocations. In this paper, we describe a set of techniques based on statistical methods for retrieving and identifying collocations from large textual corpora. These techniques produce a wide range of collocations and are based on some original filtering methods that allow the production of richer and higher-precision output. These techniques have been implemented and resulted in a lexicographic tool, Xtract. The techniques are described and some results are presented on a 10 million-word corpus of stock market news reports. A lexicographic evaluation of Xtract as a collocation retrieval tool has been made, and the estimated precision of Xtract is 80%.", "title": "Retrieving Collocations from Text: Xtract"}, "0ef311acf523d4d0e2cc5f747a6508af2c89c5f7": {"paper_id": "0ef311acf523d4d0e2cc5f747a6508af2c89c5f7", "abstract": "Search algorithms incorporating some form of topic model have a long history in information retrieval. For example, cluster-based retrieval has been studied since the 60s and has recently produced good results in the language model framework. An approach to building topic models based on a formal generative model of documents, Latent Dirichlet Allocation (LDA), is heavily cited in the machine learning literature, but its feasibility and effectiveness in information retrieval is mostly unknown. In this paper, we study how to efficiently use LDA to improve ad-hoc retrieval. We propose an LDA-based document model within the language modeling framework, and evaluate it on several TREC collections. Gibbs sampling is employed to conduct approximate inference in LDA and the computational complexity is analyzed. We show that improvements over retrieval using cluster-based models can be obtained with reasonable efficiency.", "title": "LDA-based document models for ad-hoc retrieval"}, "1e56ed3d2c855f848ffd91baa90f661772a279e1": {"paper_id": "1e56ed3d2c855f848ffd91baa90f661772a279e1", "abstract": "We propose a generative model for text and other collections of discrete data that generalizes or improves on several previous models including naive Bayes/unigram, mixture of unigrams [6], and Hofmann's aspect model , also known as probabilistic latent semantic indexing (pLSI) [3]. In the context of text modeling, our model posits that each document is generated as a mixture of topics, where the continuous-valued mixture proportions are distributed as a latent Dirichlet random variable. Inference and learning are carried out efficiently via variational algorithms. We present empirical results on applications of this model to problems in text modeling, collaborative filtering, and text classification.", "title": "Latent Dirichlet Allocation"}, "119bb251cff0292cbf6bed27acdcad424ed9f9d0": {"paper_id": "119bb251cff0292cbf6bed27acdcad424ed9f9d0", "abstract": "This paper presents a tutorial introduction to the use of variational methods for inference and learning in graphical models (Bayesian networks and Markov random fields). We present a number of examples of graphical models, including the QMR-DT database, the sigmoid belief network, the Boltzmann machine, and several variants of hidden Markov models, in which it is infeasible to run exact inference algorithms. We then introduce variational methods, which exploit laws of large numbers to transform the original graphical model into a simplified graphical model in which inference is efficient. Inference in the simpified model provides bounds on probabilities of interest in the original model. We describe a general framework for generating variational transformations based on convex duality. Finally we return to the examples and demonstrate how variational algorithms can be formulated in each case.", "title": "An Introduction to Variational Methods for Graphical Models"}, "ad90445eb2b6615174981e3c36c9aa25754276b8": {"paper_id": "ad90445eb2b6615174981e3c36c9aa25754276b8", "abstract": "The Dirichlet distribution and its compound variant, the Dirichlet-multinomial, are two of the most basic models for proportional data, such as the mix of vocabulary words in a text document. Yet the maximum-likelihood estimate of these distributions is not available in closed-form. This paper describes simple and efficient iterative schemes for obtaining parameter estimates in these models. In each case, a fixed-point iteration and a Newton-Raphson (or generalized Newton-Raphson) iteration is provided. 1 The Dirichlet distribution The Dirichlet distribution is a model of how proportions vary. Let p denote a random vector whose elements sum to 1, so that pk represents the proportion of item k. Under the Dirichlet model with parameter vector \u03b1, the probability density at p is p(p) \u223c D(\u03b11, ..., \u03b1K) = \u0393( \u2211 k \u03b1k) \u220f k \u0393(\u03b1k) \u220f k pk k (1) where pk > 0 (2)", "title": "Estimating a Dirichlet distribution"}, "07e48fb058454006315696650a8246813cdc4a32": {"paper_id": "07e48fb058454006315696650a8246813cdc4a32", "abstract": "There are numerous text documents available in electronic form. More and more are becoming available every day. Such documents represent a massive amount of information that is easily accessible. Seeking value in this huge collection requires organization; much of the work of organizing documents can be automated through text classification. The accuracy and our understanding of such systems greatly influences their usefulness. In this paper, we seek 1) to advance the understanding of commonly used text classification techniques, and 2) through that understanding, improve the tools that are available for text classification. We begin by clarifying the assumptions made in the derivation of Naive Bayes, noting basic properties and proposing ways for its extension and improvement. Next, we investigate the quality of Naive Bayes parameter estimates and their impact on classification. Our analysis leads to a theorem which gives an explanation for the improvements that can be found in multiclass classification with Naive Bayes using Error-Correcting Output Codes. We use experimental evidence on two commonly-used data sets to exhibit an application of the theorem. Finally, we show fundamental flaws in a commonly-used feature selection algorithm and develop a statistics-based framework for text feature selection. Greater understanding of Naive Bayes and the properties of text allows us to make better use of it in text classification. Thesis Supervisor: Tommi Jaakkola Title: Assistant Professor of Electrical Engineering and Computer Science", "title": "Improving Multi-class Text Classification with Naive Bayes"}, "39f737f2e323a98d3505140339eabb53fdcda204": {"paper_id": "39f737f2e323a98d3505140339eabb53fdcda204", "abstract": "This paper proposes the use of maximum entropy techniques for text classification. Maximum entropy is a probability distribution estimation technique widely used for a variety of natural language tasks, such as language modeling, part-of-speech tagging, and text segmentation. The underlying principle of maximum entropy is that without external knowledge, one should prefer distributions that are uniform. Constraints on the distribution, derived from labeled training data, inform the technique where to be minimally non-uniform. The maximum entropy formulation has a unique solution which can be found by the improved iterative scaling algorithm. In this paper, maximum entropy is used for text classification by estimating the conditional distribution of the class variable given the document. In experiments on several text datasets we compare accuracy to naive Bayes and show that maximum entropy is sometimes significantly better, but also sometimes worse. Much future work remains, but the results indicate that maximum entropy is a promising technique for text classification.", "title": "Using Maximum Entropy for Text Classification"}, "01da903f93df749aa600c1aea50c86e611818e85": {"paper_id": "01da903f93df749aa600c1aea50c86e611818e85", "abstract": "As known, to finish this book, you may not need to get it at once in a day. Doing the activities along the day may make you feel so bored. If you try to force reading, you may prefer to do other entertaining activities. But, one of concepts we want you to have this book is that it will not make you feel bored. Feeling bored when reading will be only unless you don't like the book. statistical methods for speech recognition really offers what everybody wants.", "title": "Statistical Methods for Speech Recognition"}, "9105b910f44665d2cc5272fac56843fbded8a3cd": {"paper_id": "9105b910f44665d2cc5272fac56843fbded8a3cd", "abstract": "The fast filtering of massive point cloud data from light detection and ranging (LiDAR) systems is important for many applications, such as the automatic extraction of digital elevation models in urban areas. We propose a simple scan-line-based algorithm that detects local lowest points first and treats them as the seeds to grow into ground segments by using slope and elevation. The scan line segmentation algorithm can be naturally accelerated by parallel computing due to the independent processing of each line. Furthermore, modern graphics processing units (GPUs) can be used to speed up the parallel process significantly. Using a strip of a LiDAR point cloud, with up to 48 million points, we test the algorithm in terms of both error rate and time performance. The tests show that the method can produce satisfactory results in less than 0.6 s of processing time using the GPU acceleration.", "title": "Fast Filtering of LiDAR Point Cloud in Urban Areas Based on Scan Line Segmentation and GPU Acceleration"}, "7aa32daf5a7e9fb62fabff1dcc684de64972196f": {"paper_id": "7aa32daf5a7e9fb62fabff1dcc684de64972196f", "abstract": "FPGA devices have often found use as higher-performance alternatives to programmable processors for implementing a variety of computations. Applications successfully implemented on FPGAs have typically contained high levels of parallelism and have often used simple statically-scheduled control and modest arithmetic. Recently introduced computing devices such as coarse grain reconfigurable arrays, multi-core processors, and graphical processing units (GPUs) promise to significantly change the computational landscape for the implementation of high-speed real-time computing tasks. One reason for this is that these architectures take advantage of many of the same application characteristics that fit well on FPGAs. One real-time computing task, optical flow, is difficult to apply in robotic vision applications in practice because of its high computational and data rate requirements, and so is a good candidate for implementation on FPGAs and other custom computing architectures. In this paper, a tensor-based optical flow algorithm is implemented on both an FPGA and a GPU and the two implementations discussed. The two implementations had similar performance, but with the FPGA implementation requiring 12\u00d7 more development time. Other comparison data for these two technologies is then given for three additional applications taken from a MIMO digital communication system design, providing additional examples of the relative capabilities of these two technologies.", "title": "Real-Time Optical Flow Calculations on FPGA and GPU Architectures: A Comparison Study"}, "5358beed42cf607e43f9ad620d35278d7647759f": {"paper_id": "5358beed42cf607e43f9ad620d35278d7647759f", "abstract": "We describe a pipelined optical-flow processing system that works as a virtual motion sensor. It is based on a field-programmable gate array (FPGA) device enabling the easy change of configuring parameters to adapt the sensor to different speeds, light conditions and other environmental factors. We refer to it as a \"virtual sensor\" because it consists of a conventional camera as front-end supported by an FPGA processing device, which embeds the frame grabber, optical-flow algorithm implementation, output module, and some configuration and storage circuitry. To the best of our knowledge, this is the first study that presents a fully stand-alone optical-flow processing system to include measurements of the platform performance in terms of accuracy and speed.", "title": "FPGA-based real-time optical-flow system"}, "a51562d51c0d93d29e1afe53be5e94b6ac29c07c": {"paper_id": "a51562d51c0d93d29e1afe53be5e94b6ac29c07c", "abstract": "We demonstrate a use of deep neural networks (DNN) for OSNR monitoring with minimum prior knowledge. By using 5-layers DNN trained with 400,000 samples, the DNN successfully estimates OSNR in a 16-GBd DP-QPSK system.", "title": "OSNR monitoring by deep neural networks trained with asynchronously sampled data"}, "83174a52f38c80427e237446ccda79e2a9170742": {"paper_id": "83174a52f38c80427e237446ccda79e2a9170742", "abstract": "Rectifying neurons are more biologically plausible than logistic sigmoid neurons, which are themselves more biologically plausible than hyperbolic tangent neurons. However, the latter work better for training multi-layer neural networks than logistic sigmoid neurons. This paper shows that networks of rectifying neurons yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero and create sparse representations with true zeros which are remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extraunlabeled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised neural networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training.", "title": "Deep Sparse Rectifier Neural Networks"}, "5d90f06bb70a0a3dced62413346235c02b1aa086": {"paper_id": "5d90f06bb70a0a3dced62413346235c02b1aa086", "abstract": "Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it dicult to learn a good set of lters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels. The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of each of 100 non-overlapping classes. Using these labels, we show that object recognition is signicantly improved by pre-training a layer of features on a large set of unlabeled tiny images.", "title": "Learning Multiple Layers of Features from Tiny Images"}, "3b2bf65ebee91249d1045709200a51d157b0176e": {"paper_id": "3b2bf65ebee91249d1045709200a51d157b0176e", "abstract": "Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite.", "title": "Extracting and composing robust features with denoising autoencoders"}, "2337ff38e6cfb09e28c0958f07e2090c993ef6e8": {"paper_id": "2337ff38e6cfb09e28c0958f07e2090c993ef6e8", "abstract": "For many pattern recognition tasks, the ideal input feature would be invariant to multiple confounding properties (such as illumination and viewing angle, in computer vision applications). Recently, deep architectures trained in an unsupervised manner have been proposed as an automatic method for extracting useful features. However, it is difficult to evaluate the learned features by any means other than using them in a classifier. In this paper, we propose a number of empirical tests that directly measure the degree to which these learned features are invariant to different input transformations. We find that stacked autoencoders learn modestly increasingly invariant features with depth when trained on natural images. We find that convolutional deep belief networks learn substantially more invariant features in each layer. These results further justify the use of \u201cdeep\u201d vs. \u201cshallower\u201d representations, but suggest that mechanisms beyond merely stacking one autoencoder on top of another may be important for achieving invariance. Our evaluation metrics can also be used to evaluate future work in deep learning, and thus help the development of future algorithms.", "title": "Measuring Invariances in Deep Networks"}, "34760b63a2ae964a0b04d1850dc57002f561ddcb": {"paper_id": "34760b63a2ae964a0b04d1850dc57002f561ddcb", "abstract": "This paper considers a natural error correcting problem with real valued input/output. We wish to recover an input vector f/spl isin/R/sup n/ from corrupted measurements y=Af+e. Here, A is an m by n (coding) matrix and e is an arbitrary and unknown vector of errors. Is it possible to recover f exactly from the data y? We prove that under suitable conditions on the coding matrix A, the input f is the unique solution to the /spl lscr//sub 1/-minimization problem (/spl par/x/spl par//sub /spl lscr/1/:=/spl Sigma//sub i/|x/sub i/|) min(g/spl isin/R/sup n/) /spl par/y - Ag/spl par//sub /spl lscr/1/ provided that the support of the vector of errors is not too large, /spl par/e/spl par//sub /spl lscr/0/:=|{i:e/sub i/ /spl ne/ 0}|/spl les//spl rho//spl middot/m for some /spl rho/>0. In short, f can be recovered exactly by solving a simple convex optimization problem (which one can recast as a linear program). In addition, numerical experiments suggest that this recovery procedure works unreasonably well; f is recovered exactly even in situations where a significant fraction of the output is corrupted. This work is related to the problem of finding sparse solutions to vastly underdetermined systems of linear equations. There are also significant connections with the problem of recovering signals from highly incomplete measurements. In fact, the results introduced in this paper improve on our earlier work. Finally, underlying the success of /spl lscr//sub 1/ is a crucial property we call the uniform uncertainty principle that we shall describe in detail.", "title": "Decoding by linear programming"}, "15cf63f8d44179423b4100531db4bb84245aa6f1": {"paper_id": "15cf63f8d44179423b4100531db4bb84245aa6f1", "abstract": "Machine-learning technology powers many aspects of modern society: from web searches to content filtering on social networks to recommendations on e-commerce websites, and it is increasingly present in consumer products such as cameras and smartphones. Machine-learning systems are used to identify objects in images, transcribe speech into text, match news items, posts or products with users\u2019 interests, and select relevant results of search. Increasingly, these applications make use of a class of techniques called deep learning. Conventional machine-learning techniques were limited in their ability to process natural data in their raw form. For decades, constructing a pattern-recognition or machine-learning system required careful engineering and considerable domain expertise to design a feature extractor that transformed the raw data (such as the pixel values of an image) into a suitable internal representation or feature vector from which the learning subsystem, often a classifier, could detect or classify patterns in the input. Representation learning is a set of methods that allows a machine to be fed with raw data and to automatically discover the representations needed for detection or classification. Deep-learning methods are representation-learning methods with multiple levels of representation, obtained by composing simple but non-linear modules that each transform the representation at one level (starting with the raw input) into a representation at a higher, slightly more abstract level. With the composition of enough such transformations, very complex functions can be learned. For classification tasks, higher layers of representation amplify aspects of the input that are important for discrimination and suppress irrelevant variations. An image, for example, comes in the form of an array of pixel values, and the learned features in the first layer of representation typically represent the presence or absence of edges at particular orientations and locations in the image. The second layer typically detects motifs by spotting particular arrangements of edges, regardless of small variations in the edge positions. The third layer may assemble motifs into larger combinations that correspond to parts of familiar objects, and subsequent layers would detect objects as combinations of these parts. The key aspect of deep learning is that these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure. Deep learning is making major advances in solving problems that have resisted the best attempts of the artificial intelligence community for many years. It has turned out to be very good at discovering intricate structures in high-dimensional data and is therefore applicable to many domains of science, business and government. In addition to beating records in image recognition and speech recognition, it has beaten other machine-learning techniques at predicting the activity of potential drug molecules, analysing particle accelerator data, reconstructing brain circuits, and predicting the effects of mutations in non-coding DNA on gene expression and disease. Perhaps more surprisingly, deep learning has produced extremely promising results for various tasks in natural language understanding, particularly topic classification, sentiment analysis, question answering and language translation. We think that deep learning will have many more successes in the near future because it requires very little engineering by hand, so it can easily take advantage of increases in the amount of available computation and data. New learning algorithms and architectures that are currently being developed for deep neural networks will only accelerate this progress.", "title": "Deep Learning"}, "1224f4ed2a064a0a19ae8c7b9801064bda932c07": {"paper_id": "1224f4ed2a064a0a19ae8c7b9801064bda932c07", "abstract": "Gaussian mixture models are currently the dominant technique for modeling the emission distribution of hidden Markov models for speech recognition. We show that better phone recognition on the TIMIT dataset can be achieved by replacing Gaussian mixture models by deep neural networks that contain many layers of features and a very large number of parameters. These networks are first pre-trained as a multi-layer generative model of a window of spectral feature vectors without making use of any discriminative information. Once the generative pre-training has designed the features, we perform discriminative fine-tuning using backpropagation to adjust the features slightly to make them better at predicting a probability distribution over the states of monophone hidden Markov models.", "title": "Acoustic Modeling Using Deep Belief Networks"}, "b554da42487697cb0d01a4146858e966c1d2404f": {"paper_id": "b554da42487697cb0d01a4146858e966c1d2404f", "abstract": "In this paper we present a Time-Delay Neural Network (TDNN) approach to phoneme recognition which is characterized by two important properties. 1) Using a 3 layer arrangement of simple computing units, a hierarchy can be constructed that allows for the formation of arbitrary nonlinear decision surfaces. The TDNN learns these decision surfaces automatically using error backpropagation 111. 2) The time-delay arrangement enables the network to discover acoustic-phonetic features and the temporal relationships between them independent of position in time and hence not blurred by temporal shifts", "title": "Phoneme recognition using time-delay neural networks"}, "2d02cf53bc0f2d919b89bec8f9160b50916bb625": {"paper_id": "2d02cf53bc0f2d919b89bec8f9160b50916bb625", "abstract": "This paper proposes a new hybrid architecture that consists of a deep Convolutional Network and a Markov Random Field. We show how this architecture is successfully applied to the challenging problem of articulated human pose estimation in monocular images. The architecture can exploit structural domain constraints such as geometric relationships between body joint locations. We show that joint training of these two model paradigms improves performance and allows us to significantly outperform existing state-of-the-art techniques.", "title": "Joint Training of a Convolutional Network and a Graphical Model for Human Pose Estimation"}, "5115f3ff1ac45486a50ad3834a40490f9ca4bcea": {"paper_id": "5115f3ff1ac45486a50ad3834a40490f9ca4bcea", "abstract": "We study the complexity of functions computable by deep feedforward neural networks with piecewise linear activations in terms of the symmetries and the number of linear regions that they have. Deep networks are able to sequentially map portions of each layer\u2019s input-space to the same output. In this way, deep models compute functions that react equally to complicated patterns of different inputs. The compositional structure of these functions enables them to re-use pieces of computation exponentially often in terms of the network\u2019s depth. This paper investigates the complexity of such compositional maps and contributes new theoretical results regarding the advantage of depth for neural networks with piecewise linear activation functions. In particular, our analysis is not specific to a single family of models, and as an example, we employ it for rectifier and maxout networks. We improve complexity bounds from pre-existing work and investigate the behavior of units in higher layers.", "title": "On the Number of Linear Regions of Deep Neural Networks"}, "6b4fe4aa4d66fecc7b2869569002714d91d0b3f7": {"paper_id": "6b4fe4aa4d66fecc7b2869569002714d91d0b3f7", "abstract": "What chiefly distinguishes cerebral cortex from other parts of the central nervous system is the great diversity of its cell types and inter-connexions. It would be astonishing if such a structure did not profoundly modify the response patterns of fibres coming into it. In the cat's visual cortex, the receptive field arrangements of single cells suggest that there is indeed a degree of complexity far exceeding anything yet seen at lower levels in the visual system. In a previous paper we described receptive fields of single cortical cells, observing responses to spots of light shone on one or both retinas (Hubel & Wiesel, 1959). In the present work this method is used to examine receptive fields of a more complex type (Part I) and to make additional observations on binocular interaction (Part II). This approach is necessary in order to understand the behaviour of individual cells, but it fails to deal with the problem of the relationship of one cell to its neighbours. In the past, the technique of recording evoked slow waves has been used with great success in studies of functional anatomy. It was employed by Talbot & Marshall (1941) and by Thompson, Woolsey & Talbot (1950) for mapping out the visual cortex in the rabbit, cat, and monkey. Daniel & Whitteiidge (1959) have recently extended this work in the primate. Most of our present knowledge of retinotopic projections, binocular overlap, and the second visual area is based on these investigations. Yet the method of evoked potentials is valuable mainly for detecting behaviour common to large populations of neighbouring cells; it cannot differentiate functionally between areas of cortex smaller than about 1 mm2. To overcome this difficulty a method has in recent years been developed for studying cells separately or in small groups during long micro-electrode penetrations through nervous tissue. Responses are correlated with cell location by reconstructing the electrode tracks from histological material. These techniques have been applied to CAT VISUAL CORTEX 107 the somatic sensory cortex of the cat and monkey in a remarkable series of studies by Mountcastle (1957) and Powell & Mountcastle (1959). Their results show that the approach is a powerful one, capable of revealing systems of organization not hinted at by the known morphology. In Part III of the present paper we use this method in studying the functional architecture of the visual cortex. It helped us attempt to explain on anatomical \u2026", "title": "Receptive fields, binocular interaction and functional architecture in the cat's visual cortex."}, "fecd9820e886fd0ebe293799f62ad50524931064": {"paper_id": "fecd9820e886fd0ebe293799f62ad50524931064", "abstract": "As many as 50% of spontaneous preterm births are infection-associated. Intrauterine infection leads to a maternal and fetal inflammatory cascade, which produces uterine contractions and may also result in long-term adverse outcomes, such as cerebral palsy. This article addresses the prevalence, microbiology, and management of intrauterine infection in the setting of preterm labor with intact membranes. It also outlines antepartum treatment of infections for the purpose of preventing preterm birth.", "title": "Infection and preterm birth."}, "35c53e97d15aa2bf0debd17c9fb07c1b7de3ed67": {"paper_id": "35c53e97d15aa2bf0debd17c9fb07c1b7de3ed67", "abstract": "Animals typically have several navigational strategies available to them. Interactions between these strategies can reduce navigational errors and may lead to the emergence of new capacities.", "title": "Animal Navigation: Path Integration, Visual Landmarks and Cognitive Maps"}, "6bbbcd0a4b446e6d56fff8eb17476b9ad9f6ee91": {"paper_id": "6bbbcd0a4b446e6d56fff8eb17476b9ad9f6ee91", "abstract": "Image classification is one of classical problems of concern in image processing. There are various approaches for solving this problem. The aim of this paper is bring together two areas in which are Artificial Neural Network (ANN) and Support Vector Machine (SVM) applying for image classification. Firstly, we separate the image into many sub-images based on the features of images. Each sub-image is classified into the responsive class by an ANN. Finally, SVM has been compiled all the classify result of ANN. Our proposal classification model has brought together many ANN and one SVM. Let it denote ANN_SVM. ANN_SVM has been applied for Roman numerals recognition application and the precision rate is 86%. The experimental results show the feasibility of our proposal model.", "title": "Image Classification using Support Vector Machine and Artificial Neural Network"}, "8a131af3f9f27037f3e3612b84d1c508c5ac1823": {"paper_id": "8a131af3f9f27037f3e3612b84d1c508c5ac1823", "abstract": "This paper proposes a two level approach to solve the problem of real-time vision-based hand gesture classification. The lower level of the approach implements the posture recognition with Haar-like features and the AdaBoost learning algorithm. With this algorithm, real-time performance and high recognition accuracy can be obtained. The higher level implements the linguistic hand gesture recognition using a context-free grammar-based syntactic analysis. Given an input gesture, based on the extracted postures, the composite gestures can be parsed and recognized with a set of primitives and production rules.", "title": "Real-time Vision-based Hand Gesture Recognition Using Haar-like Features"}, "72e08cf12730135c5ccd7234036e04536218b6c1": {"paper_id": "72e08cf12730135c5ccd7234036e04536218b6c1", "abstract": "Recently Viola et al. [5] have introduced a rapid object detection scheme based on a boosted cascade of simple features. In this paper we introduce a novel set of rotated haar-like features, which significantly enrich this basic set of simple haar-like features and which can also be calculated very efficiently. At a given hit rate our sample face detector shows off on average a 10% lower false alarm rate by means of using these additional rotated features. We also present a novel post optimization procedure for a given boosted cascade improving on average the false alarm rate further by 12.5%. Using both enhancements the number of false detections is only 24 at a hit rate of 82.3% on the CMU face set [7].", "title": "An extended set of Haar-like features for rapid object detection"}, "0c4867f11c9758014d591381d8b397a1d38b04a7": {"paper_id": "0c4867f11c9758014d591381d8b397a1d38b04a7", "abstract": "The first \u20ac price and the \u00a3 and $ price are net prices, subject to local VAT. Prices indicated with * include VAT for books; the \u20ac(D) includes 7% for Germany, the \u20ac(A) includes 10% for Austria. Prices indicated with ** include VAT for electronic products; 19% for Germany, 20% for Austria. All prices exclusive of carriage charges. Prices and other details are subject to change without notice. All errors and omissions excepted. C. Bishop Pattern Recognition and Machine Learning", "title": "Pattern Recognition and Machine Learning"}, "1c01e44df70d6fde616de1ef90e485b23a3ea549": {"paper_id": "1c01e44df70d6fde616de1ef90e485b23a3ea549", "abstract": "We introduce a new class of upper bounds on the log partition function of a Markov random field (MRF). This quantity plays an important role in various contexts, including approximating marginal distributions, parameter estimation, combinatorial enumeration, statistical decision theory, and large-deviations bounds. Our derivation is based on concepts from convex duality and information geometry: in particular, it exploits mixtures of distributions in the exponential domain, and the Legendre mapping between exponential and mean parameters. In the special case of convex combinations of tree-structured distributions, we obtain a family of variational problems, similar to the Bethe variational problem, but distinguished by the following desirable properties: i) they are convex, and have a unique global optimum; and ii) the optimum gives an upper bound on the log partition function. This optimum is defined by stationary conditions very similar to those defining fixed points of the sum-product algorithm, or more generally, any local optimum of the Bethe variational problem. As with sum-product fixed points, the elements of the optimizing argument can be used as approximations to the marginals of the original model. The analysis extends naturally to convex combinations of hypertree-structured distributions, thereby establishing links to Kikuchi approximations and variants.", "title": "A new class of upper bounds on the log partition function"}, "6b81212ebb2d83c8c35b00016a7e9e36d99f62ac": {"paper_id": "6b81212ebb2d83c8c35b00016a7e9e36d99f62ac", "abstract": "Bayesian inference is now widely established as one of the pr inci al foundations for machine learning. In practice, exact inference is rarely possible, and so a variety of approximation techniques have been developed, one of the most widely used being a deter ministic framework called variational inference. In this paper we introduce Variational Me ssage Passing (VMP), a general purpose algorithm for applying variational inference to Bayesian N etworks. Like belief propagation, VMP proceeds by sending messages between nodes in the network an d updating posterior beliefs using local operations at each node. Each such update increase s a lower bound on the log evidence (unless already at a local maximum). In contrast to belief pr opagation, VMP can be applied to a very general class of conjugate-exponential models becaus it es a factorised variational approximation. Furthermore, by introducing additional variatio nal parameters, VMP can be applied to models containing non-conjugate distributions. The VMP fr amework also allows the lower bound to be evaluated, and this can be used both for model compariso n and for detection of convergence. Variational message passing has been implemented in the for m of a general purpose inference engine called VIBES (\u2018Variational Inference for BayEsian net workS\u2019) which allows models to be specified graphically and then solved variationally withou t recourse to coding.", "title": "Variational Message Passing"}, "5bd9374195809c73157ba876f463ea7c4ec9abb5": {"paper_id": "5bd9374195809c73157ba876f463ea7c4ec9abb5", "abstract": "Existing distributed graph processing frameworks, e.g., Pregel, Giraph, GPS and GraphLab, mainly exploit main memory to support flexible graph operations for efficiency. Due to the complexity of graph analytics, huge memory space is required especially for those graph analytics that spawn large intermediate results. Existing frameworks may terminate abnormally or degrade performance seriously when the memory is exhausted or the external storage has to be used. In this paper, we propose MOCgraph, a scalable distributed graph processing framework to reduce the memory footprint and improve the scalability, based on message online computing. MOCgraph consumes incoming messages in a streaming manner, so as to handle larger graphs or more complex analytics with the same memory capacity. MOCgraph also exploits message online computing with external storage to provide an efficient out-of-core support. We implement MOCgraph on top of Apache Giraph, and test it against several representative graph algorithms on large graph datasets. Experiments illustrate that MOCgraph is efficient and memory-saving, especially for graph analytics with large intermediate results.", "title": "MOCgraph: Scalable Distributed Graph Processing Using Message Online Computing"}, "0ad8e89091eed09217e66adc98136126addc2619": {"paper_id": "0ad8e89091eed09217e66adc98136126addc2619", "abstract": "Large-scale graph-structured computation is central to tasks ranging from targeted advertising to natural language processing and has led to the development of several graph-parallel abstractions including Pregel and GraphLab. However, the natural graphs commonly found in the real-world have highly skewed power-law degree distributions, which challenge the assumptions made by these abstractions, limiting performance and scalability. In this paper, we characterize the challenges of computation on natural graphs in the context of existing graphparallel abstractions. We then introduce the PowerGraph abstraction which exploits the internal structure of graph programs to address these challenges. Leveraging the PowerGraph abstraction we introduce a new approach to distributed graph placement and representation that exploits the structure of power-law graphs. We provide a detailed analysis and experimental evaluation comparing PowerGraph to two popular graph-parallel systems. Finally, we describe three different implementation strategies for PowerGraph and discuss their relative merits with empirical evaluations on large-scale real-world problems demonstrating order of magnitude gains.", "title": "PowerGraph: Distributed Graph-Parallel Computation on Natural Graphs"}, "29cc0a8802126d4e97f28109763df26ab91c6531": {"paper_id": "29cc0a8802126d4e97f28109763df26ab91c6531", "abstract": "How do real graphs evolve over time? What are normal growth patterns in social, technological, and information networks? Many studies have discovered patterns in static graphs, identifying properties in a single snapshot of a large network or in a very small number of snapshots; these include heavy tails for in- and out-degree distributions, communities, small-world phenomena, and others. However, given the lack of information about network evolution over long periods, it has been hard to convert these findings into statements about trends over time.\n Here we study a wide range of real graphs, and we observe some surprising phenomena. First, most of these graphs densify over time with the number of edges growing superlinearly in the number of nodes. Second, the average distance between nodes often shrinks over time in contrast to the conventional wisdom that such distance parameters should increase slowly as a function of the number of nodes (like O(log n) or O(log(log n)).\n Existing graph generation models do not exhibit these types of behavior even at a qualitative level. We provide a new graph generator, based on a forest fire spreading process that has a simple, intuitive justification, requires very few parameters (like the flammability of nodes), and produces graphs exhibiting the full range of properties observed both in prior work and in the present study.\n We also notice that the forest fire model exhibits a sharp transition between sparse graphs and graphs that are densifying. Graphs with decreasing distance between the nodes are generated around this transition point.\n Last, we analyze the connection between the temporal evolution of the degree distribution and densification of a graph. We find that the two are fundamentally related. We also observe that real networks exhibit this type of relation between densification and the degree distribution.", "title": "Graph evolution: Densification and shrinking diameters"}, "177c41a72cd1070c39366adb0a3bd88af010be81": {"paper_id": "177c41a72cd1070c39366adb0a3bd88af010be81", "abstract": "Large-scale graph analytics is a central tool in many fields, and exemplifies the size and complexity of Big Data applications. Recent distributed graph processing frameworks utilize the venerable Bulk Synchronous Parallel (BSP) model and promise scalability for large graph analytics. This has been made popular by Google's Pregel, which provides an architecture design for BSP graph processing. Public clouds offer democratized access to medium-sized compute infrastructure with the promise of rapid provisioning with no capital investment. Evaluating BSP graph frameworks on cloud platforms with their unique constraints is less explored. Here, we present optimizations and analyses for computationally complex graph analysis algorithms such as betweenness-centrality and all-pairs shortest paths on a native BSP framework we have developed for the Microsoft Azure Cloud, modeled on the Pregel graph processing model. We propose novel heuristics for scheduling graph vertex processing in swaths to maximize resource utilization on cloud VMs that lead to a 3.5x performance improvement. We explore the effects of graph partitioning in the context of BSP, and show that even a well partitioned graph may not lead to performance improvements due to BSP's barrier synchronization. We end with a discussion on leveraging cloud elasticity for dynamically scaling the number of BSP workers to achieve a better performance than a static deployment, and at a significantly lower cost.", "title": "Optimizations and Analysis of BSP Graph Processing Models on Public Clouds"}, "65227ddbbd12015ba8a45a81122b1fa540e79890": {"paper_id": "65227ddbbd12015ba8a45a81122b1fa540e79890", "abstract": "The importance of a Web page is an inherently subjective matter, which depends on the readers interests, knowledge and attitudes. But there is still much that can be said objectively about the relative importance of Web pages. This paper describes PageRank, a method for rating Web pages objectively and mechanically, e ectively measuring the human interest and attention devoted to them. We compare PageRank to an idealized random Web surfer. We show how to e ciently compute PageRank for large numbers of pages. And, we show how to apply PageRank to search and to user navigation.", "title": "The PageRank Citation Ranking: Bringing Order to the Web."}, "b78c04c7f29ddaeaeb208d4eae684ffccd71e04f": {"paper_id": "b78c04c7f29ddaeaeb208d4eae684ffccd71e04f", "abstract": "The success of the von Neumann model of sequential computation is attributable to the fact that it is an efficient bridge between software and hardware: high-level languages can be efficiently compiled on to this model; yet it can be effeciently implemented in hardware. The author argues that an analogous bridge between software and hardware in required for parallel computation if that is to become as widely used. This article introduces the bulk-synchronous parallel (BSP) model as a candidate for this role, and gives results quantifying its efficiency both in implementing high-level language features and algorithms, as well as in being implemented in hardware.", "title": "A Bridging Model for Parallel Computation"}, "0d0e3c418fa2dc31a021a6caa73c9efce5aa8b7b": {"paper_id": "0d0e3c418fa2dc31a021a6caa73c9efce5aa8b7b", "abstract": "Various scientific computations have become so complex, and thus computation tools play an important role. In this paper, we explore the state-of-the-art framework providing high-level matrix computation primitives with MapReduce through the case study approach, and demonstrate these primitives with different computation engines to show the performance and scalability. We believe the opportunity for using MapReduce in scientific computation is even more promising than the success to date in the parallel systems literature.", "title": "HAMA: An Efficient Matrix Computation with the MapReduce Framework"}, "0541d5338adc48276b3b8cd3a141d799e2d40150": {"paper_id": "0541d5338adc48276b3b8cd3a141d799e2d40150", "abstract": "MapReduce is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks. Users specify the computation in terms of a map and a reduce function, and the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks. Programmers find the system easy to use: more than ten thousand distinct MapReduce programs have been implemented internally at Google over the past four years, and an average of one hundred thousand MapReduce jobs are executed on Google's clusters every day, processing a total of more than twenty petabytes of data per day.", "title": "MapReduce: Simplified Data Processing on Large Clusters"}, "17a2d0feb8754ef81a945b9f5046c68605f59560": {"paper_id": "17a2d0feb8754ef81a945b9f5046c68605f59560", "abstract": "The introduction of Google\u2019s Pregel generated much interest in the field of large-scale graph data processing, inspiring the development of Pregel-like systems such as Apache Giraph, GPS, Mizan, and GraphLab, all of which have appeared in the past two years. To gain an understanding of how Pregel-like systems perform, we conduct a study to experimentally compare Giraph, GPS, Mizan, and GraphLab on equal ground by considering graph and algorithm agnostic optimizations and by using several metrics. The systems are compared with four different algorithms (PageRank, single source shortest path, weakly connected components, and distributed minimum spanning tree) on up to 128 Amazon EC2 machines. We find that the system optimizations present in Giraph and GraphLab allow them to perform well. Our evaluation also shows Giraph 1.0.0\u2019s considerable improvement since Giraph 0.1 and identifies areas of improvement for all systems.", "title": "An Experimental Comparison of Pregel-like Graph Processing Systems"}, "2b0cc03aa4625a09958c20dc721f4e0a52c13fd0": {"paper_id": "2b0cc03aa4625a09958c20dc721f4e0a52c13fd0", "abstract": "While high-level data parallel frameworks, like MapReduce, simplify the design and implementation of large-scale data processing systems, they do not naturally or efficiently support many important data mining and machine learning algorithms and can lead to inefficient learning systems. To help fill this critical void, we introduced the GraphLab abstraction which naturally expresses asynchronous, dynamic, graph-parallel computation while ensuring data consistency and achieving a high degree of parallel performance in the shared-memory setting. In this paper, we extend the GraphLab framework to the substantially more challenging distributed setting while preserving strong data consistency guarantees. We develop graph based extensions to pipelined locking and data versioning to reduce network congestion and mitigate the effect of network latency. We also introduce fault tolerance to the GraphLab abstraction using the classic Chandy-Lamport snapshot algorithm and demonstrate how it can be easily implemented by exploiting the GraphLab abstraction itself. Finally, we evaluate our distributed implementation of the GraphLab abstraction on a large Amazon EC2 deployment and show 1-2 orders of magnitude performance gains over Hadoop-based implementations.", "title": "Distributed GraphLab : A Framework for Machine Learning and Data Mining in the Cloud"}, "2bb1e444ca057597eb1d393457ca41e9897079c6": {"paper_id": "2bb1e444ca057597eb1d393457ca41e9897079c6", "abstract": "Security and performance are critical goals for distributed systems. The increased design complexity, incomplete expertise of developers, and limited functionality of existing testing tools often result in bugs and vulnerabilities that prevent implementations from achieving their design goals in practice. Many of these bugs, vulnerabilities, and misconfigurations manifest after the code has already been deployed making the debugging process difficult and costly. In this paper, we present Turret, a platform for automatically finding performance attacks in unmodified implementations of distributed systems. Turret does not require the user to provide any information about vulnerabilities and runs the implementation in the same operating system setup as the deployment, with an emulated network. Turret uses a new attack finding algorithm and several optimizations that allow it to find attacks in a matter of minutes. We ran Turret on 5 different distributed system implementations specifically designed to tolerate insider attacks, and found 30 performance attacks, 24 of which were not previously reported to the best of our knowledge.", "title": "Turret: A Platform for Automated Attack Finding in Unmodified Distributed System Implementations"}, "513bd1e5ec39f711f212d2105af3ee03dea4b53d": {"paper_id": "513bd1e5ec39f711f212d2105af3ee03dea4b53d", "abstract": "Many system errors do not emerge unless some intricate sequence of events occurs. In practice, this means that most systems have errors that only trigger after days or weeks of execution. Model checking [4] is an effective way to find such subtle errors. It takes a simplified description of the code and exhaustively tests it on all inputs, using techniques to explore vast state spaces efficiently. Unfortunately, while model checking systems code would be wonderful, it is almost never done in practice: building models is just too hard. It can take significantly more time to write a model than it did to write the code. Furthermore, by checking an abstraction of the code rather than the code itself, it is easy to miss errors.The paper's first contribution is a new model checker, CMC, which checks C and C++ implementations directly, eliminating the need for a separate abstract description of the system behavior. This has two major advantages: it reduces the effort to use model checking, and it reduces missed errors as well as time-wasting false error reports resulting from inconsistencies between the abstract description and the actual implementation. In addition, changes in the implementation can be checked immediately without updating a high-level description.The paper's second contribution is demonstrating that CMC works well on real code by applying it to three implementations of the Ad-hoc On-demand Distance Vector (AODV) networking protocol [7]. We found 34 distinct errors (roughly one bug per 328 lines of code), including a bug in the AODV specification itself. Given our experience building systems, it appears that the approach will work well in other contexts, and especially well for other networking protocols.", "title": "CMC: A Pragmatic Approach to Model Checking Real Code"}, "2f2dbb88c9266356dfda696ed907067c1e42902c": {"paper_id": "2f2dbb88c9266356dfda696ed907067c1e42902c", "abstract": "This paper introduces DIDUCE, a practical and effective tool that aids programmers in detecting complex program errors and identifying their root causes. By instrumenting a program and observing its behavior as it runs, DIDUCE dynamically formulates hypotheses of invariants obeyed by the program. DIDUCE hypothesizes the strictest invariants at the beginning, and gradually relaxes the hypothesis as violations are detected to allow for new behavior. The violations reported help users to catch software bugs as soon as they occur. They also give programmers new visibility into the behavior of the programs such as identifying rare corner cases in the program logic or even locating hidden errors that corrupt the program's results.We implemented the DIDUCE system for Java programs and applied it to four programs of significant size and complexity. DIDUCE succeeded in identifying the root causes of programming errors in each of the programs quickly and automatically. In particular, DIDUCE is effective in isolating a timing-dependent bug in a released JSSE (Java Secure Socket Extension) library, which would have taken an experienced programmer days to find. Our experience suggests that detecting and checking program invariants dynamically is a simple and effective methodology for debugging many different kinds of program errors across a wide variety of application domains.", "title": "Tracking down software bugs using automatic anomaly detection"}, "06160c998f3405bfbef7d1dd260c3831202b4e55": {"paper_id": "06160c998f3405bfbef7d1dd260c3831202b4e55", "abstract": "We propose a new design for highly concurrent Internet services, which we call the staged event-driven architecture (SEDA). SEDA is intended to support massive concurrency demands and simplify the construction of well-conditioned services. In SEDA, applications consist of a network of event-driven stages connected by explicit queues. This architecture allows services to be well-conditioned to load, preventing resources from being overcommitted when demand exceeds service capacity. SEDA makes use of a set of dynamic resource controllers to keep stages within their operating regime despite large fluctuations in load. We describe several control mechanisms for automatic tuning and load conditioning, including thread pool sizing, event batching, and adaptive load shedding. We present the SEDA design and an implementation of an Internet services platform based on this architecture. We evaluate the use of SEDA through two applications: a high-performance HTTP server and a packet router for the Gnutella peer-to-peer file sharing network. These results show that SEDA applications exhibit higher performance than traditional service designs, and are robust to huge variations in load.", "title": "SEDA: An Architecture for Well-Conditioned, Scalable Internet Services"}, "de494ca2ac7d8eeb507589ea8589ae6014ce4a66": {"paper_id": "de494ca2ac7d8eeb507589ea8589ae6014ce4a66", "abstract": "To develop a brief ataxia rating scale (BARS) for use by movement disorder specialists and general neurologists. Current ataxia rating scales are cumbersome and not designed for clinical practice. We first modified the International Cooperative Ataxia Rating Scale (ICARS) by adding seven ataxia tests (modified ICARS, or MICARS), and observed only minimally increased scores. We then used the statistics package R to find a five-test subset in MICARS that would correlate best with the total MICARS score. This was accomplished first without constraints and then with the clinical constraint requiring one test each of Gait, Kinetic Function-Arm, Kinetic Function-Leg, Speech, and Eye Movements. We validated these clinical constraints by factor analysis. We then validated the results in a second cohort of patients; evaluated inter-rater reliability in a third cohort; and used the same data set to compare BARS with the Scale for the Assessment and Rating of Ataxia (SARA). Correlation of ICARS with the seven additional tests that when added to ICARS form MICARS was 0.88. There were 31,481 five-test subtests (48% of possible combinations) that had a correlation with total MICARS score of > or =0.90. The strongest correlation of an unconstrained five-test subset was 0.963. The clinically constrained subtest validated by factor analysis, BARS, had a correlation with MICARS-minus-BARS of 0.952. Cronbach alpha for BARS and SARA was 0.90 and 0.92 respectively; and inter-rater reliability (intraclass correlation coefficient) was 0.91 and 0.93 respectively. BARS is valid, reliable, and sufficiently fast and accurate for clinical purposes.", "title": "Development of a brief ataxia rating scale (BARS) based on a modified form of the ICARS."}, "aa24b87e97d9467984ed7efccee93e61993afd63": {"paper_id": "aa24b87e97d9467984ed7efccee93e61993afd63", "abstract": "This paper proposes a novel attention model for semantic segmentation, which aggregates multi-scale and context features to refine prediction. Specifically, the skeleton convolutional neural network framework takes in multiple different scales inputs, by which means the CNN can get representations in different scales. The proposed attention model will handle the features from different scale streams respectively and integrate them. Then location attention branch of the model learns to softly weight the multi-scale features at each pixel location. Moreover, we add an recalibrating branch, parallel to where location attention comes out, to recalibrate the score map per class. We achieve quite competitive results on PASCAL VOC 2012 and ADE20K datasets, which surpass baseline and related works.", "title": "Attention to Refine Through Multi Scales for Semantic Segmentation"}, "420c46d7cafcb841309f02ad04cf51cb1f190a48": {"paper_id": "420c46d7cafcb841309f02ad04cf51cb1f190a48", "abstract": "State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction problems such as semantic segmentation are structurally different from image classification. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multiscale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy.", "title": "Multi-Scale Context Aggregation by Dilated Convolutions"}, "026e3363b7f76b51cc711886597a44d5f1fd1de2": {"paper_id": "026e3363b7f76b51cc711886597a44d5f1fd1de2", "abstract": "We present a novel dataset captured from a VW station wagon for use in mobile robotics and autonomous driving research. In total, we recorded 6 hours of traffic scenarios at 10-100 Hz using a variety of sensor modalities such as highresolution color and grayscale stereo cameras, a Velodyne 3D laser scanner and a high-precision GPS/IMU inertial navigation system. The scenarios are diverse, capturing real-world traffic situations and range from freeways over rural areas to innercity scenes with many static and dynamic objects. Our data is calibrated, synchronized and timestamped, and we provide the rectified and raw image sequences. Our dataset also contains object labels in the form of 3D tracklets and we provide online benchmarks for stereo, optical flow, object detection and other tasks. This paper describes our recording platform, the data format and the utilities that we provide.", "title": "Vision meets robotics: The KITTI dataset"}, "243e681e23e7d1744defd2ee0c83643b05f003d3": {"paper_id": "243e681e23e7d1744defd2ee0c83643b05f003d3", "abstract": "Scene labeling consists of labeling each pixel in an image with the category of the object it belongs to. We propose a method that uses a multiscale convolutional network trained from raw pixels to extract dense feature vectors that encode regions of multiple sizes centered on each pixel. The method alleviates the need for engineered features, and produces a powerful representation that captures texture, shape, and contextual information. We report results using multiple postprocessing methods to produce the final labeling. Among those, we propose a technique to automatically retrieve, from a pool of segmentation components, an optimal set of components that best explain the scene; these components are arbitrary, for example, they can be taken from a segmentation tree or from any family of oversegmentations. The system yields record accuracies on the SIFT Flow dataset (33 classes) and the Barcelona dataset (170 classes) and near-record accuracy on Stanford background dataset (eight classes), while being an order of magnitude faster than competing approaches, producing a 320\u00d7240 image labeling in less than a second, including feature extraction.", "title": "Learning Hierarchical Features for Scene Labeling"}, "6f9f143ec602aac743e07d092165b708fa8f1473": {"paper_id": "6f9f143ec602aac743e07d092165b708fa8f1473", "abstract": "We propose a novel deep architecture, SegNet, for semantic pixel wise image labelling 1. SegNet has several attractive properties; (i) it only requires forward evaluation of a fully learnt function to obtain smooth label predictions, (ii) with increasing depth, a larger context is considered for pixel labelling which improves accuracy, and (iii) it is easy to visualise the effect of feature activation(s) in the pixel label space at any depth. SegNet is composed of a stack of encoders followed by a corresponding decoder stack which feeds into a soft-max classification layer. The decoders help map low resolution feature maps at the output of the encoder stack to full input image size feature maps. This addresses an important drawback of recent deep learning approaches which have adopted networks designed for object categorization for pixel wise labelling. These methods lack a mechanism to map deep layer feature maps to input dimensions. They resort to ad hoc methods to upsample features, e.g. by replication. This results in noisy predictions and also restricts the number of pooling layers in order to avoid too much upsampling and thus reduces spatial context. SegNet overcomes these problems by learning to map encoder outputs to image pixel labels. We test the performance of SegNet on outdoor RGB scenes from CamVid, KITTI and indoor scenes from the NYU dataset. Our results show that SegNet achieves state-of-the-art performance even without use of additional cues such as depth, video frames or post-processing with CRF models.", "title": "SegNet: A Deep Convolutional Encoder-Decoder Architecture for Robust Semantic Pixel-Wise Labelling"}, "155f50770f43b7e52c85583a0a2d552f5b21cb81": {"paper_id": "155f50770f43b7e52c85583a0a2d552f5b21cb81", "abstract": "Please cite this article in press as: C. Galleguillos doi:10.1016/j.cviu.2010.02.004 The goal of object categorization is to locate and identify instances of an object category within an image. Recognizing an object in an image is difficult when images include occlusion, poor quality, noise or background clutter, and this task becomes even more challenging when many objects are present in the same scene. Several models for object categorization use appearance and context information from objects to improve recognition accuracy. Appearance information, based on visual cues, can successfully identify object classes up to a certain extent. Context information, based on the interaction among objects in the scene or global scene statistics, can help successfully disambiguate appearance inputs in recognition tasks. In this work we address the problem of incorporating different types of contextual information for robust object categorization in computer vision. We review different ways of using contextual information in the field of object categorization, considering the most common levels of extraction of context and the different levels of contextual interactions. We also examine common machine learning models that integrate context information into object recognition frameworks and discuss scalability, optimizations and possible future approaches. 2010 Elsevier Inc. All rights reserved.", "title": "Context based object categorization: A critical survey"}, "5e0f8c355a37a5a89351c02f174e7a5ddcb98683": {"paper_id": "5e0f8c355a37a5a89351c02f174e7a5ddcb98683", "abstract": "We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.", "title": "Microsoft COCO: Common Objects in Context"}, "15292f380f5996f539f4d5e93dba3082d53338fb": {"paper_id": "15292f380f5996f539f4d5e93dba3082d53338fb", "abstract": "We present an approach to long-range spatio-temporal regularization in semantic video segmentation. Temporal regularization in video is challenging because both the camera and the scene may be in motion. Thus Euclidean distance in the space-time volume is not a good proxy for correspondence. We optimize the mapping of pixels to a Euclidean feature space so as to minimize distances between corresponding points. Structured prediction is performed by a dense CRF that operates on the optimized features. Experimental results demonstrate that the presented approach increases the accuracy and temporal consistency of semantic video segmentation.", "title": "Feature Space Optimization for Semantic Video Segmentation"}, "480888bad59b314236f2d947ebf308ae146c98e4": {"paper_id": "480888bad59b314236f2d947ebf308ae146c98e4", "abstract": "Parsing articulated objects, e.g . humans and animals, into semantic parts (e.g . body, head and arms, etc.) from natural images is a challenging and fundamental problem for computer vision. A big difficulty is the large variability of scale and location for objects and their corresponding parts. Even limited mistakes in estimating scale and location will degrade the parsing output and cause errors in boundary details. To tackle these difficulties, we propose a \u201cHierarchical Auto-Zoom Net\u201d (HAZN) for object part parsing which adapts to the local scales of objects and parts. HAZN is a sequence of two \u201cAuto-Zoom Nets\u201d (AZNs), each employing fully convolutional networks that perform two tasks: (1) predict the locations and scales of object instances (the first AZN) or their parts (the second AZN); (2) estimate the part scores for predicted object instance or part regions. Our model can adaptively \u201czoom\u201d (resize) predicted image regions into their proper scales to refine the parsing. We conduct extensive experiments over the PASCAL part datasets on humans, horses, and cows. For humans, our approach significantly outperforms the state-of-the-arts by 5% mIOU and is especially better at segmenting small instances and small parts. We obtain similar improvements for parsing cows and horses over alternative methods. In summary, our strategy of first zooming into objects and then zooming into parts is very effective. It also enables us to process different regions of the image at different scales adaptively so that, for example, we do not need to waste computational resources scaling the entire image.", "title": "Zoom Better to See Clearer: Human and Object Parsing with Hierarchical Auto-Zoom Net"}, "09e15bb266da86d0a9525d2a94ac0b38f0b53b88": {"paper_id": "09e15bb266da86d0a9525d2a94ac0b38f0b53b88", "abstract": "Detecting objects becomes difficult when we need to deal with large shape deformation, occlusion and low resolution. We propose a novel approach to i) handle large deformations and partial occlusions in animals (as examples of highly deformable objects), ii) describe them in terms of body parts, and iii) detect them when their body parts are hard to detect (e.g., animals depicted at low resolution). We represent the holistic object and body parts separately and use a fully connected model to arrange templates for the holistic object and body parts. Our model automatically decouples the holistic object or body parts from the model when they are hard to detect. This enables us to represent a large number of holistic object and body part combinations to better deal with different \"detectability\" patterns caused by deformations, occlusion and/or low resolution. We apply our method to the six animal categories in the PASCAL VOC dataset and show that our method significantly improves state-of-the-art (by 4.1% AP) and provides a richer representation for objects. During training we use annotations for body parts (e.g., head, torso, etc.), making use of a new dataset of fully annotated object parts for PASCAL VOC 2010, which provides a mask for each part.", "title": "Detect What You Can: Detecting and Representing Objects Using Holistic Models and Body Parts"}, "21a1654b856cf0c64e60e58258669b374cb05539": {"paper_id": "21a1654b856cf0c64e60e58258669b374cb05539", "abstract": "We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.", "title": "You Only Look Once: Unified, Real-Time Object Detection"}, "9a29e4d43e94985c1d152e621764be93c286a78d": {"paper_id": "9a29e4d43e94985c1d152e621764be93c286a78d", "abstract": "In this paper, we propose an approach that exploits object segmentation in order to improve the accuracy of object detection. We frame the problem as inference in a Markov Random Field, in which each detection hypothesis scores object appearance as well as contextual information using Convolutional Neural Networks, and allows the hypothesis to choose and score a segment out of a large pool of accurate object segmentation proposals. This enables the detector to incorporate additional evidence when it is available and thus results in more accurate detections. Our experiments show an improvement of 4.1% in mAP over the R-CNN baseline on PASCAL VOC 2010, and 3.4% over the current state-of-the-art, demonstrating the power of our approach.", "title": "segDeepM: Exploiting segmentation and context in deep neural networks for object detection"}, "abe9f3b91fd26fa1b50cd685c0d20debfb372f73": {"paper_id": "abe9f3b91fd26fa1b50cd685c0d20debfb372f73", "abstract": "The Pascal Visual Object Classes (VOC) challenge consists of two components: (i)\u00a0a publicly available dataset of images together with ground truth annotation and standardised evaluation software; and (ii)\u00a0an annual competition and workshop. There are five challenges: classification, detection, segmentation, action classification, and person layout. In this paper we provide a review of the challenge from 2008\u20132012. The paper is intended for two audiences: algorithm designers, researchers who want to see what the state of the art is, as measured by performance on the VOC datasets, along with the limitations and weak points of the current generation of algorithms; and, challenge designers, who want to see what we as organisers have learnt from the process and our recommendations for the organisation of future challenges. To analyse the performance of submitted algorithms on the VOC datasets we introduce a number of novel evaluation methods: a bootstrapping method for determining whether differences in the performance of two algorithms are significant or not; a normalised average precision so that performance can be compared across classes with different proportions of positive instances; a clustering method for visualising the performance across multiple algorithms so that the hard and easy images can be identified; and the use of a joint classifier over the submitted algorithms in order to measure their complementarity and combined performance. We also analyse the community\u2019s progress through time using the methods of Hoiem et al. (Proceedings of European Conference on Computer Vision, 2012) to identify the types of occurring errors. We conclude the paper with an appraisal of the aspects of the challenge that worked well, and those that could be improved in future challenges.", "title": "The Pascal Visual Object Classes Challenge: A Retrospective"}, "51c765b8d872c206f6dd781ab26bd5a8c2feb81e": {"paper_id": "51c765b8d872c206f6dd781ab26bd5a8c2feb81e", "abstract": "This paper addresses semantic image segmentation by incorporating rich information into Markov Random Field (MRF), including high-order relations and mixture of label contexts. Unlike previous works that optimized MRFs using iterative algorithm, we solve MRF by proposing a Convolutional Neural Network (CNN), namely Deep Parsing Network (DPN), which enables deterministic end-to-end computation in a single forward pass. Specifically, DPN extends a contemporary CNN architecture to model unary terms and additional layers are carefully devised to approximate the mean field algorithm (MF) for pairwise terms. It has several appealing properties. First, different from the recent works that combined CNN and MRF, where many iterations of MF were required for each training image during back-propagation, DPN is able to achieve high performance by approximating one iteration of MF. Second, DPN represents various types of pairwise terms, making many existing works as its special cases. Third, DPN makes MF easier to be parallelized and speeded up in Graphical Processing Unit (GPU). DPN is thoroughly evaluated on the PASCAL VOC 2012 dataset, where a single DPN model yields a new state-of-the-art segmentation accuracy of 77.5%.", "title": "Semantic Image Segmentation via Deep Parsing Network"}, "28bdaf9b7fc5af73482e324d45acf91722f07340": {"paper_id": "28bdaf9b7fc5af73482e324d45acf91722f07340", "abstract": "Segmenting semantic objects from images and parsing them into their respective semantic parts are fundamental steps towards detailed object understanding in computer vision. In this paper, we propose a joint solution that tackles semantic object and part segmentation simultaneously, in which higher object-level context is provided to guide part segmentation, and more detailed part-level localization is utilized to refine object segmentation. Specifically, we first introduce the concept of semantic compositional parts (SCP) in which similar semantic parts are grouped and shared among different objects. A two-stream fully convolutional network (FCN) is then trained to provide the SCP and object potentials at each pixel. At the same time, a compact set of segments can also be obtained from the SCP predictions of the network. Given the potentials and the generated segments, in order to explore long-range context, we finally construct an efficient fully connected conditional random field (FCRF) to jointly predict the final object and part labels. Extensive evaluation on three different datasets shows that our approach can mutually enhance the performance of object and part segmentation, and outperforms the current state-of-the-art on both tasks.", "title": "Joint Object and Part Segmentation Using Deep Learned Potentials"}, "a3d96844dc4fc3b51f3b8361c72f156800794251": {"paper_id": "a3d96844dc4fc3b51f3b8361c72f156800794251", "abstract": "Recent leading approaches to semantic segmentation rely on deep convolutional networks trained with human-annotated, pixel-level segmentation masks. Such pixel-accurate supervision demands expensive labeling effort and limits the performance of deep networks that usually benefit from more training data. In this paper, we propose a method that achieves competitive accuracy but only requires easily obtained bounding box annotations. The basic idea is to iterate between automatically generating region proposals and training convolutional networks. These two steps gradually recover segmentation masks for improving the networks, and vise versa. Our method, called \"BoxSup\", produces competitive results (e.g., 62.0% mAP for validation) supervised by boxes only, on par with strong baselines (e.g., 63.8% mAP) fully supervised by masks under the same setting. By leveraging a large amount of bounding boxes, BoxSup further yields state-of-the-art results on PASCAL VOC 2012 and PASCAL-CONTEXT [26].", "title": "BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation"}, "3eb298bfcc33f6e40bfd2e8788b13b256d2c0391": {"paper_id": "3eb298bfcc33f6e40bfd2e8788b13b256d2c0391", "abstract": "We study the problem of human body configuration analysis, more specifically, human parsing and human pose estimation. These two tasks, ie identifying the semantic regions and body joints respectively over the human body image, are intrinsically highly correlated. However, previous works generally solve these two problems separately or iteratively. In this work, we propose a unified framework for simultaneous human parsing and pose estimation based on semantic parts. By utilizing Parselets and Mixture of Joint-Group Templates as the representations for these semantic parts, we seamlessly formulate the human parsing and pose estimation problem jointly within a unified framework via a tailored and-or graph. A novel Grid Layout Feature is then designed to effectively capture the spatial co-occurrence/occlusion information between/within the Parselets and MJGTs. Thus the mutually complementary nature of these two tasks can be harnessed to boost the performance of each other. The resultant unified model can be solved using the structure learning framework in a principled way. Comprehensive evaluations on two benchmark datasets for both tasks demonstrate the effectiveness of the proposed framework when compared with the state-of-the-art methods.", "title": "Towards Unified Human Parsing and Pose Estimation"}, "baddac96864c86538d3bd8bf495f00f818475a9e": {"paper_id": "baddac96864c86538d3bd8bf495f00f818475a9e", "abstract": "Image understanding requires not only individually estimating elements of the visual world but also capturing the interplay among them. In this paper, we provide a framework for placing local object detection in the context of the overall 3D scene by modeling the interdependence of objects, surface orientations, and camera viewpoint. Most object detection methods consider all scales and locations in the image as equally likely. We show that with probabilistic estimates of 3D geometry, both in terms of surfaces and world coordinates, we can put objects into perspective and model the scale and location variance in the image. Our approach reflects the cyclical nature of the problem by allowing probabilistic object hypotheses to refine geometry and vice-versa. Our framework allows painless substitution of almost any object detector and is easily extended to include other aspects of image understanding. Our results confirm the benefits of our integrated approach.", "title": "Putting Objects in Perspective"}, "146f6f6ed688c905fb6e346ad02332efd5464616": {"paper_id": "146f6f6ed688c905fb6e346ad02332efd5464616", "abstract": "Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-theart performance on three benchmark datasets: Flickr9k, Flickr30k and MS COCO.", "title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention"}, "26adb749fc5d80502a6d889966e50b31391560d3": {"paper_id": "26adb749fc5d80502a6d889966e50b31391560d3", "abstract": "Parameter set learned using all WMT12 data (Callison-Burch et al., 2012): \u2022 100,000 binary rankings covering 8 language directions. \u2022Restrict scoring for all languages to exact and paraphrase matching. Parameters encode human preferences that generalize across languages: \u2022Prefer recall over precision. \u2022Prefer word choice over word order. \u2022Prefer correct translations of content words over function words. \u2022Prefer exact matches over paraphrase matches, while still giving significant credit to paraphrases. Visualization", "title": "Meteor Universal: Language Specific Translation Evaluation for Any Target Language"}, "04483e2c56695b19f6912b061769eb8c175a5a7a": {"paper_id": "04483e2c56695b19f6912b061769eb8c175a5a7a", "abstract": "We propose to use the visual denotations of linguistic expressions (i.e. the set of images they describe) to define novel denotational similarity metrics, which we show to be at least as beneficial as distributional similarities for two tasks that require semantic inference. To compute these denotational similarities, we construct a denotation graph, i.e. a subsumption hierarchy over constituents and their denotations, based on a large corpus of 30K images and 150K descriptive captions.", "title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions"}, "18cc17c06e34baaa3e196db07e20facdbb17026d": {"paper_id": "18cc17c06e34baaa3e196db07e20facdbb17026d", "abstract": "Recent progress in using recurrent neural networks (RNNs) for image description has motivated the exploration of their application for video description. However, while images are static, working with videos requires modeling their dynamic temporal structure and then properly integrating that information into a natural language description model. In this context, we propose an approach that successfully takes into account both the local and global temporal structure of videos to produce descriptions. First, our approach incorporates a spatial temporal 3-D convolutional neural network (3-D CNN) representation of the short temporal dynamics. The 3-D CNN representation is trained on video action recognition tasks, so as to produce a representation that is tuned to human motion and behavior. Second we propose a temporal attention mechanism that allows to go beyond local temporal modeling and learns to automatically select the most relevant temporal segments given the text-generating RNN. Our approach exceeds the current state-of-art for both BLEU and METEOR metrics on the Youtube2Text dataset. We also present results on a new, larger and more challenging dataset of paired video and natural language descriptions.", "title": "Describing Videos by Exploiting Temporal Structure"}, "39dba6f22d72853561a4ed684be265e179a39e4f": {"paper_id": "39dba6f22d72853561a4ed684be265e179a39e4f", "abstract": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT\u201914 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM\u2019s BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM\u2019s performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.", "title": "Sequence to Sequence Learning with Neural Networks"}, "f4af49a1ead3c81cc5d023878cb67c5646dd8a04": {"paper_id": "f4af49a1ead3c81cc5d023878cb67c5646dd8a04", "abstract": "In this paper we explore the bi-directional mapping between images and their sentence-based descriptions. We propose learning this mapping using a recurrent neural network. Unlike previous approaches that map both sentences and images to a common embedding, we enable the generation of novel sentences given an image. Using the same model, we can also reconstruct the visual features associated with an image given its visual description. We use a novel recurrent visual memory that automatically learns to remember long-term visual concepts to aid in both sentence generation and visual feature reconstruction. We evaluate our approach on several tasks. These include sentence generation, sentence retrieval and image retrieval. State-ofthe-art results are shown for the task of generating novel image descriptions. When compared to human generated captions, our automatically generated captions are preferred by humans over 19.8% of the time. Results are better than or comparable to state-of-the-art results on the image and sentence retrieval tasks for methods using similar visual features.", "title": "Learning a Recurrent Visual Representation for Image Caption Generation"}, "0b544dfe355a5070b60986319a3f51fb45d1348e": {"paper_id": "0b544dfe355a5070b60986319a3f51fb45d1348e", "abstract": "In this paper, we propose a novel neural network model called RNN Encoder\u2013 Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder\u2013Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.", "title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation"}, "4b9b7eed30feee37db3452b74503d0db9f163074": {"paper_id": "4b9b7eed30feee37db3452b74503d0db9f163074", "abstract": "We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units. The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model. Through various experiments, we show first that our models obtain a perplexity with respect to gold translations that is > 43% lower than that of stateof-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and meaning of the source sentence despite lacking alignments. Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations.", "title": "Recurrent Continuous Translation Models"}, "cab372bc3824780cce20d9dd1c22d4df39ed081a": {"paper_id": "cab372bc3824780cce20d9dd1c22d4df39ed081a", "abstract": "In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or \u2018atrous\u00a0convolution\u2019, as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous\u00a0spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed \u201cDeepLab\u201d system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7 percent mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.", "title": "DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs"}, "32cde90437ab5a70cf003ea36f66f2de0e24b3ab": {"paper_id": "32cde90437ab5a70cf003ea36f66f2de0e24b3ab", "abstract": "Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations, 20 000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark.", "title": "The Cityscapes Dataset for Semantic Urban Scene Understanding"}, "165ef2b5f86b9b2c68b652391db5ece8c5a0bc7e": {"paper_id": "165ef2b5f86b9b2c68b652391db5ece8c5a0bc7e", "abstract": "Recent advances in semantic image segmentation have mostly been achieved by training deep convolutional neural networks (CNNs). We show how to improve semantic segmentation through the use of contextual information, specifically, we explore 'patch-patch' context between image regions, and 'patch-background' context. For learning from the patch-patch context, we formulate Conditional Random Fields (CRFs) with CNN-based pairwise potential functions to capture semantic correlations between neighboring patches. Efficient piecewise training of the proposed deep structured model is then applied to avoid repeated expensive CRF inference for back propagation. For capturing the patch-background context, we show that a network design with traditional multi-scale image input and sliding pyramid pooling is effective for improving performance. Our experimental results set new state-of-the-art performance on a number of popular semantic segmentation datasets, including NYUDv2, PASCAL VOC 2012, PASCAL-Context, and SIFT-flow. In particular, we achieve an intersection-overunion score of 78:0 on the challenging PASCAL VOC 2012 dataset.", "title": "Efficient Piecewise Training of Deep Structured Models for Semantic Segmentation"}, "752fc36f9813ebff3837cb12f790820d2f851c14": {"paper_id": "752fc36f9813ebff3837cb12f790820d2f851c14", "abstract": "CNN architectures have terrific recognition performance but rely on spatial pooling which makes it difficult to adapt them to tasks that require dense pixel-accurate labeling. This paper makes two contributions: (1) We demonstrate that while the apparent spatial resolution of convolutional feature maps is low, the high-dimensional feature representation contains significant sub-pixel localization information. (2) We describe a multi-resolution reconstruction architecture, akin to a Laplacian pyramid, that uses skip connections from higher resolution feature maps to successively refine segment boundaries reconstructed from lower resolution maps. This approach yields state-of-the-art semantic segmentation results on PASCAL without resorting to more complex CRF or detection driven architectures.", "title": "Laplacian Reconstruction and Refinement for Semantic Segmentation"}, "056892b7e573608e64c3c9130e8ce33353a94de2": {"paper_id": "056892b7e573608e64c3c9130e8ce33353a94de2", "abstract": "Deep convolutional neural networks (CNNs) are the backbone of state-of-art semantic image segmentation systems. Recent work has shown that complementing CNNs with fully-connected conditional random fields (CRFs) can significantly enhance their object localization accuracy, yet dense CRF inference is computationally expensive. We propose replacing the fully-connected CRF with domain transform (DT), a modern edge-preserving filtering method in which the amount of smoothing is controlled by a reference edge map. Domain transform filtering is several times faster than dense CRF inference and we show that it yields comparable semantic segmentation results, accurately capturing object boundaries. Importantly, our formulation allows learning the reference edge map from intermediate CNN features instead of using the image gradient magnitude as in standard DT filtering. This produces task-specific edges in an end-to-end trainable system optimizing the target semantic segmentation quality.", "title": "Semantic Image Segmentation with Task-Specific Edge Detection Using CNNs and a Discriminatively Trained Domain Transform"}, "13ccdfb9d2c2a77192c74ae6352bd684e312d0f8": {"paper_id": "13ccdfb9d2c2a77192c74ae6352bd684e312d0f8", "abstract": "Recognition algorithms based on convolutional networks (CNNs) typically use the output of the last layer as a feature representation. However, the information in this layer may be too coarse spatially to allow precise localization. On the contrary, earlier layers may be precise in localization but will not capture semantics. To get the best of both worlds, we define the hypercolumn at a pixel as the vector of activations of all CNN units above that pixel. Using hypercolumns as pixel descriptors, we show results on three fine-grained localization tasks: simultaneous detection and segmentation [22], where we improve state-of-the-art from 49.7 mean APr [22] to 60.0, keypoint localization, where we get a 3.3 point boost over [20], and part labeling, where we show a 6.6 point gain over a strong baseline.", "title": "Hypercolumns for object segmentation and fine-grained localization"}, "2a5667702b0f1ff77dde8fb3e2e10d4e05e8de9d": {"paper_id": "2a5667702b0f1ff77dde8fb3e2e10d4e05e8de9d", "abstract": "Scene parsing, or recognizing and segmenting objects and stuff in an image, is one of the key problems in computer vision. Despite the communitys efforts in data collection, there are still few image datasets covering a wide range of scenes and object categories with dense and detailed annotations for scene parsing. In this paper, we introduce and analyze the ADE20K dataset, spanning diverse annotations of scenes, objects, parts of objects, and in some cases even parts of parts. A scene parsing benchmark is built upon the ADE20K with 150 object and stuff classes included. Several segmentation baseline models are evaluated on the benchmark. A novel network design called Cascade Segmentation Module is proposed to parse a scene into stuff, objects, and object parts in a cascade and improve over the baselines. We further show that the trained scene parsing networks can lead to applications such as image content removal and scene synthesis1.", "title": "Scene Parsing through ADE20K Dataset"}, "41d8d35fd8ff2d82c7b32f2d59c0c1ba64ba699a": {"paper_id": "41d8d35fd8ff2d82c7b32f2d59c0c1ba64ba699a", "abstract": "This paper presents a database containing \u2018ground truth\u2019 segmentations produced by humans for images of a wide variety of natural scenes. We define an error measure which quantifies the consistency between segmentations of differing granularities and find that different human segmentations of the same image are highly consistent. Use of this dataset is demonstrated in two applications: (1) evaluating the performance of segmentation algorithms and (2) measuring probability distributions associated with Gestalt grouping factors as well as statistics of image region prop-", "title": "A Database of Human Segmented Natural Images and its Application to Evaluating Segmentation Algorithms and Measuring Ecological Statistics"}, "55bc43bc2b34acf3ab0cf0a4ef901ef5b786baf1": {"paper_id": "55bc43bc2b34acf3ab0cf0a4ef901ef5b786baf1", "abstract": "Today, visual recognition systems are still rarely employed in robotics applications. Perhaps one of the main reasons for this is the lack of demanding benchmarks that mimic such scenarios. In this paper, we take advantage of our autonomous driving platform to develop novel challenging benchmarks for the tasks of stereo, optical flow, visual odometry/SLAM and 3D object detection. Our recording platform is equipped with four high resolution video cameras, a Velodyne laser scanner and a state-of-the-art localization system. Our benchmarks comprise 389 stereo and optical flow image pairs, stereo visual odometry sequences of 39.2 km length, and more than 200k 3D object annotations captured in cluttered scenarios (up to 15 cars and 30 pedestrians are visible per image). Results from state-of-the-art algorithms reveal that methods ranking high on established datasets such as Middlebury perform below average when being moved outside the laboratory to the real world. Our goal is to reduce this bias by providing challenging benchmarks with novel difficulties to the computer vision community. Our benchmarks are available online at: www.cvlibs.net/datasets/kitti.", "title": "Are we ready for autonomous driving? The KITTI vision benchmark suite"}, "1550caab8d12c3f0ea19faaaa6bab3bdd092bafd": {"paper_id": "1550caab8d12c3f0ea19faaaa6bab3bdd092bafd", "abstract": "The goal of the scene labeling task is to assign a class label to each pixel in an image. To ensure a good visual coherence and a high class accuracy, it is essential for a model to capture long range (pixel) label dependencies in images. In a feed-forward architecture, this can be achieved simply by considering a sufficiently large input context patch, around each pixel to be labeled. We propose an approach that consists of a recurrent convolutional neural network which allows us to consider a large input context while limiting the capacity of the model. Contrary to most standard approaches, our method does not rely on any segmentation technique nor any taskspecific features. The system is trained in an end-to-end manner over raw pixels, and models complex spatial dependencies with low inference cost. As the context size increases with the built-in recurrence, the system identifies and corrects its own errors. Our approach yields state-ofthe-art performance on both the Stanford Background Dataset and the SIFT Flow Dataset, while remaining very fast at test time.", "title": "Recurrent Convolutional Neural Networks for Scene Labeling"}, "16174bcefcf38493c90576d0e3fed46537ef54fd": {"paper_id": "16174bcefcf38493c90576d0e3fed46537ef54fd", "abstract": "Deep neural networks (DNNs) have demonstrated state-of-the-art results on many pattern recognition tasks, especially vision classification problems. Understanding the inner workings of such computational brains is both fascinating basic science that is interesting in its own right\u2014similar to why we study the human brain\u2014and will enable researchers to further improve DNNs. One path to understanding how a neural network functions internally is to study what each of its neurons has learned to detect. One such method is called activation maximization (AM), which synthesizes an input (e.g. an image) that highly activates a neuron. Here we dramatically improve the qualitative state of the art of activation maximization by harnessing a powerful, learned prior: a deep generator network (DGN). The algorithm (1) generates qualitatively state-of-the-art synthetic images that look almost real, (2) reveals the features learned by each neuron in an interpretable way, (3) generalizes well to new datasets and somewhat well to different network architectures without requiring the prior to be relearned, and (4) can be considered as a high-quality generative method (in this case, by generating novel, creative, interesting, recognizable images).", "title": "Synthesizing the preferred inputs for neurons in neural networks via deep generator networks"}, "c504c88dbea0c1fe9383710646c8180ef44b9bc9": {"paper_id": "c504c88dbea0c1fe9383710646c8180ef44b9bc9", "abstract": "Image segmentation remains one of the major challenges in image analysis. In medical applications, skilled operators are usually employed to extract the desired regions that may be anatomically separate but statistically indistinguishable. Such manual processing is subject to operator errors and biases, is extremely time consuming, and has poor reproducibility. We propose a robust algorithm for the segmentation of three-dimensional (3-D) image data based on a novel combination of adaptive K-mean clustering and knowledge-based morphological operations. The proposed adaptive K-mean clustering algorithm is capable of segmenting the regions of smoothly varying intensity distributions. Spatial constraints are incorporated in the clustering algorithm through the modeling of the regions by Gibbs random fields. Knowledge-based morphological operations are then applied to the segmented regions to identify the desired regions according to the a priori anatomical knowledge of the region-of-interest. This proposed technique has been successfully applied to a sequence of cardiac CT volumetric images to generate the volumes of left ventricle chambers at 16 consecutive temporal frames. Our final segmentation results compare favorably with the results obtained using manual outlining. Extensions of this approach to other applications can be readily made when a priori knowledge of a given object is available.", "title": "Image segmentation via adaptive K-mean clustering and knowledge-based morphological operations with biomedical applications"}, "50ed18cf637831d51109cb95fdafc7f7df7ab356": {"paper_id": "50ed18cf637831d51109cb95fdafc7f7df7ab356", "abstract": "Image retrieval can be considered as a classification problem. Classification is usually based on some image features. In the feature extraction image segmentation is commonly used. In this paper we introduce a new feature for image classification for retrieval purposes. This feature is based on the gray level histogram of the image. The feature is called binary histogram and it can be used for image classification without segmentation. Binary histogram can be used for image retrieval as such by using similarity calculation. Another approach is to extract some features from it. In both cases indexing and retrieval do not require much computational time. We test the similarity measurement and the feature-based retrieval by making classification experiments. The proposed features are tested using a set of paper defect images, which are acquired from an industrial imaging application.", "title": "Binary Histogram in Image Classification for Retrieval Purposes"}, "e80bf7f22328c35eeddf59addca98510f7e927e0": {"paper_id": "e80bf7f22328c35eeddf59addca98510f7e927e0", "abstract": "Purpose \u2013 The purpose of this paper is to present a novel swarm intelligence optimizer \u2014 pigeoninspired optimization (PIO) \u2014 and describe how this algorithm was applied to solve air robot path planning problems. Design/methodology/approach \u2013 The formulation of threat resources and objective function in air robot path planning is given. The mathematical model and detailed implementation process of PIO is presented. Comparative experiments with standard differential evolution (DE) algorithm are also conducted. Findings \u2013 The feasibility, effectiveness and robustness of the proposed PIO algorithm are shown by a series of comparative experiments with standard DE algorithm. The computational results also show that the proposed PIO algorithm can effectively improve the convergence speed, and the superiority of global search is also verified in various cases. Originality/value \u2013 In this paper, the authors first presented a PIO algorithm. In this newly presented algorithm, map and compass operator model is presented based on magnetic field and sun, while landmark operator model is designed based on landmarks. The authors also applied this newly proposed PIO algorithm for solving air robot path planning problems.", "title": "Pigeon-inspired optimization: a new swarm intelligence optimizer for air robot path planning"}, "053346fbfa9630eb5a57822def5821d9d61b16b9": {"paper_id": "053346fbfa9630eb5a57822def5821d9d61b16b9", "abstract": "Human being is the most intelligent animal in this world. Intuitively, optimization algorithm inspired by human being creative problem solving process should be superior to the optimization algorithms inspired by collective behavior of insects like ants, bee, etc. In this paper, we introduce a novel brain storm optimization algorithm, which was inspired by the human brainstorming process. Two benchmark functions were tested to validate the effectiveness and usefulness of the proposed algorithm.", "title": "Brain Storm Optimization Algorithm"}, "10fa778e675d0b6951a12b3d8160420317950608": {"paper_id": "10fa778e675d0b6951a12b3d8160420317950608", "abstract": "An analogy with the way ant colonies function has suggested the definition of a new computational paradigm, which we call ant system (AS). We propose it as a viable new approach to stochastic combinatorial optimization. The main characteristics of this model are positive feedback, distributed computation, and the use of a constructive greedy heuristic. Positive feedback accounts for rapid discovery of good solutions, distributed computation avoids premature convergence, and the greedy heuristic helps find acceptable solutions in the early stages of the search process. We apply the proposed methodology to the classical traveling salesman problem (TSP), and report simulation results. We also discuss parameter selection and the early setups of the model, and compare it with tabu search and simulated annealing using TSP. To demonstrate the robustness of the approach, we show how the ant system (AS) can be applied to other optimization problems like the asymmetric traveling salesman, the quadratic assignment and the job-shop scheduling. Finally we discuss the salient characteristics-global data structure revision, distributed communication and probabilistic transitions of the AS.", "title": "Ant system: optimization by a colony of cooperating agents"}, "506172b0e0dd4269bdcfe96dda9ea9d8602bbfb6": {"paper_id": "506172b0e0dd4269bdcfe96dda9ea9d8602bbfb6", "abstract": "In this paper, we introduce a new parameter, called inertia weight, into the original particle swarm optimizer. Simulations have been done to illustrate the signilicant and effective impact of this new parameter on the particle swarm optimizer.", "title": "A modified particle swarm optimizer"}, "8263efcbc5e6b3c7c022b1131038b888babc8548": {"paper_id": "8263efcbc5e6b3c7c022b1131038b888babc8548", "abstract": "The optimization of nonlinear functions using particle swarm methodology is described. Implementations of two paradigms are discussed and compared, including a recently developed locally oriented paradigm. Benchmark testing of both paradigms is described, and applications, including neural network training and robot task learning, are proposed. Relationships between particle swarm optimization and both artificial life and evolutionary computation are reviewed.", "title": "A new optimizer using particle swarm theory"}, "3d3d51142c4245482729e8d6faf10ac2051c8dfe": {"paper_id": "3d3d51142c4245482729e8d6faf10ac2051c8dfe", "abstract": "This paper introduces particle systems--a method for modeling fuzzy objects such as fire, clouds, and water. Particle systems model an object as a cloud of primitive particles that define its volume. Over a period of time, particles are generated into the system, move and change form within the system, and die from the system. The resulting model is able to represent motion, changes of form, and dynamics that are not possible with classical surface-based representations. The particles can easily be motion blurred, and therefore do not exhibit temporal aliasing or strobing. Stochastic processes are used to generate and control the many particles within a particle system. The application of particle systems to the wall of fire element from the Genesis Demo sequence of the film Star Trek II: The Wrath of Khan [10] is presented.", "title": "Particle Systems - a Technique for Modeling a Class of Fuzzy Objects"}, "54acdb67ca083326c34eabdeb59bfdc01c748df0": {"paper_id": "54acdb67ca083326c34eabdeb59bfdc01c748df0", "abstract": "In this paper, the effect of Genetic algorithm (GA) on the enhancement of the texture and quality of plants visual L. Davis, The handbook of Genetic Algorithms. Genetic Algorithm is one of many methods that can be used to create a schedule. This method Davis, L.: Handbook on Genetic Algorithms. Van Nostrand. In this research, genetic algorithm approach has been applied for solving exam (18) Davis L., Handbook of Genetic Algorithms, New York, Van Nostrand.", "title": "Handbook of genetic algorithms"}, "b2c91f6708eb345fd3b39f28fa36334c2ad4f611": {"paper_id": "b2c91f6708eb345fd3b39f28fa36334c2ad4f611", "abstract": "The initial state of an Unmanned Aerial Vehicle (UAV) system and the relative state of the system, the continuous inputs of each flight unit are piecewise linear by a Control Parameterization and Time Discretization (CPTD) method. The approximation piecewise linearization control inputs are used to substitute for the continuous inputs. In this way, the multi-UAV formation reconfiguration problem can be formulated as an optimal control problem with dynamical and algebraic constraints. With strict constraints and mutual interference, the multi-UAV formation reconfiguration in 3-D space is a complicated problem. The recent boom of bio-inspired algorithms has attracted many researchers to the field of applying such intelligent approaches to complicated optimization problems in multi-UAVs. In this paper, a Hybrid Particle Swarm Optimization and Genetic Algorithm (HPSOGA) is proposed to solve the multi-UAV formation reconfiguration problem, which is modeled as a parameter optimization problem. This new approach combines the advantages of Particle Swarm Optimization (PSO) and Genetic Algorithm (GA), which can find the time-optimal solutions simultaneously. The proposed HPSOGA will also be compared with basic PSO algorithm and the series of experimental results will show that our HPSOGA outperforms PSO in solving multi-UAV formation reconfiguration problem under complicated environments.", "title": "?Hybrid Particle Swarm Optimization and Genetic Algorithm for Multi-UAV Formation Reconfiguration"}, "cf20e34a1402a115523910d2a4243929f6704db1": {"paper_id": "cf20e34a1402a115523910d2a4243929f6704db1", "abstract": null, "title": "AN IDEA BASED ON HONEY BEE SWARM FOR NUMERICAL OPTIMIZATION"}, "e330e2e49c0a744256c5de5b510a64b0b7529dbd": {"paper_id": "e330e2e49c0a744256c5de5b510a64b0b7529dbd", "abstract": "Resilient transportation systems enable quick evacuation, rescue, distribution of relief supplies, and other activities for reducing the impact of natural disasters and for accelerating the recovery from them. The resilience of a transportation system largely relies on the decisions made during a natural disaster. We developed an agent-based traffic simulator for predicting the results of potential actions taken with respect to the transportation system to quickly make appropriate decisions. For realistic simulation, we govern the behavior of individual drivers of vehicles with foundational principles learned from probe-car data. For example, we used the probe-car data to estimate the personality of individual drivers of vehicles in selecting their routes, taking into account various metrics of routes such as travel time, travel distance, and the number of turns. This behavioral model, which was constructed from actual data, constitutes a special feature of our simulator. We built this simulator using the X10 language, which enables massively parallel execution for simulating traffic in a large metropolitan area. We report the use cases of the simulator in three major cities in the context of disaster recovery and resilient transportation.", "title": "Toward simulating entire cities with behavioral models of traffic"}, "685ee67cdfbbb8c9d6c5163849c0399ed4936149": {"paper_id": "685ee67cdfbbb8c9d6c5163849c0399ed4936149", "abstract": "SUMO is an open source traffic simulation package including net import and demand modeling components. We describe the current state of the package as well as future developments and extensions. SUMO helps to investigate several research topics e.g. route choice and traffic light algorithm or simulating vehicular communication. Therefore the framework is used in different projects to simulate automatic driving or traffic management strategies. Keywordsmicroscopic traffic simulation, software, open", "title": "SUMO \u2013 Simulation of Urban MObility An Overview"}, "c6943db4843e938d8180a31181eae3bce7aa0372": {"paper_id": "c6943db4843e938d8180a31181eae3bce7aa0372", "abstract": "Contemporary theories of metaphor differ in many dimensions, including the discipline they originate from (e.g., linguistics, psychology, philosophy), and whether they are developed primarily within a cognitive or pragmatic theoretical framework. This article evaluates two directions of metaphor research within linguistics, cognitive linguistics and relevance theory, which both aim to capture essential aspects of the reason for metaphor, and how people ordinarily use and understand metaphor in daily life. We argue, contrary to most received opinion, that cognitive linguistics and relevance theory provide complementary perspectives on metaphor. Both theories offer important insights into the role of metaphor in cognition and language use, and suggest detailed hypotheses on metaphor understanding that surely are part of a comprehensive theory of metaphor. # 2008 Elsevier B.V. All rights reserved.", "title": "Complementary perspectives on metaphor : Cognitive linguistics and relevance theory"}, "e13853606fa9ccad26476c82ae88d6c1e2522464": {"paper_id": "e13853606fa9ccad26476c82ae88d6c1e2522464", "abstract": "Psycholinguistic research has shown that people\u2019s tacit knowledge of conceptual metaphors, such as ANGER IS HEATED FLUID IN A CONTAINER, partly motivates how they make sense of idiomatic phrases like blow your stack and flip your lid. But do people quickly access conceptual metaphors each time an idiom is encountered in discourse? The present studies used a priming method to examine the role of conceptual metaphors in immediate idiom comprehension. Experiment 1 showed that people access conceptual metaphors when understanding idioms, but significantly less so when processing literal paraphrases of idioms. Experiment 2 demonstrated that people access the appropriate conceptual metaphors, such as ANGER IS HEAT, when processing some idioms, such as blow your stack, but not when they read idioms, such as jump down your throat, which have similar figurative meanings that are motivated by different conceptual metaphors (e.g., ANGER IS ANIMAL BEHAVIOR). The findings from these studies provide important evidence on the constraining role that common patterns of metaphoric thought have in figurative language understanding. q 1997 Academic Press", "title": "Metaphor in Idiom Comprehension"}, "33b62c898c4ca9233c4a466145bd611fa72ddf3c": {"paper_id": "33b62c898c4ca9233c4a466145bd611fa72ddf3c", "abstract": "What should you think more? Time to get this [PDF? It is easy then. You can only sit and stay in your place to get this book. Why? It is on-line book store that provide so many collections of the referred books. So, just with internet connection, you can enjoy downloading this book and numbers of books that are searched for now. By visiting the link page download that we have provided, the book that you refer so much can be found. Just save the requested book downloaded and then you can enjoy the book to read every time and place you want.", "title": "Metaphors We Live By"}, "3d5a34451f45c57b4ed99e06b1395a5291566b72": {"paper_id": "3d5a34451f45c57b4ed99e06b1395a5291566b72", "abstract": "inferences as metaphorical spatial inferences Spatial inferences are characterized by the topological structure of image-schemas. We have seen cases such as CATEGORIES ARE CONTAINERS and LINEAR SCALES ARE PATHS where image-schema structure is preserved by metaphor and where abstract inferences about categories and linear scales are metaphorical versions of spatial inferences about containers and paths. The Invariance Principle hypothesizes that imageschema structure is always preserved by metaphor. The Invariance Principle raises the possibility that a great many, if not all, abstract inferences are actually metaphorical versions of spatial inferences that are inherent in the topological structure of imageschemas. What I will do now is turn to other cases of basic, but abstract, concepts to see what evidence there is for the claim that such concepts are fundamentally characterized by metaphor.", "title": "The Contemporary Theory of Metaphor"}, "9ac81b0563e5a2ac40ce41b3197bc6e7d768ddfe": {"paper_id": "9ac81b0563e5a2ac40ce41b3197bc6e7d768ddfe", "abstract": "We have previously reported that bilateral amygdala damage in humans compromises the recognition of fear in facial expressions while leaving intact recognition of face identity (Adolphs et al., 1994). The present study aims at examining questions motivated by this finding. We addressed the possibility that unilateral amygdala damage might be sufficient to impair recognition of emotional expressions. We also obtained further data on our subject with bilateral amygdala damage, in order to elucidate possible mechanisms that could account for the impaired recognition of expressions of fear. The results show that bilateral, but not unilateral, damage to the human amygdala impairs the processing of fearful facial expressions. This impairment appears to result from an insensitivity to the intensity of fear expressed by faces. We also confirmed a double dissociation between the recognition of facial expressions of fear, and the recognition of identity of a face: these two processes can be impaired independently, lending support to the idea that they are subserved in part by anatomically separate neural systems. Based on our data, and on what is known about the amygdala's connectivity, we propose that the amygdala is required to link visual representations of facial expressions, on the one hand, with representations that constitute the concept of fear, on the other. Preliminary data suggest the amygdala's role extends to both recognition and recall of fearful facial expressions.", "title": "Fear and the human amygdala."}, "7aaea6ec841f1a9fe1e4793aa4318edaa8945b59": {"paper_id": "7aaea6ec841f1a9fe1e4793aa4318edaa8945b59", "abstract": "Organizations are attempting to leverage their knowledge resources by employing knowledge management (KM) systems, a key form of which are electronic knowledge repositories (EKRs). A large number of KM initiatives fail due to reluctance of employees to share knowledge through these systems. Motivated by such concerns, this study formulates and tests a theoretical model to explain EKR usage by knowledge contributors. The model employs social exchange theory to identify cost and benefit factors affecting EKR usage, and social capital theory to account for the moderating influence of contextual factors. The model is validated through a large-scale survey of public sector organizations. The results reveal that knowledge self-efficacy and enjoyment in helping others significantly impact EKR usage by knowledge contributors. Contextual factors (generalized trust, pro-sharing norms, and identification) moderate the impact of codification effort, reciprocity, and organizational reward on EKR usage, respectively. It can be seen that extrinsic benefits (reciprocity and organizational reward) impact EKR usage contingent on particular contextual factors whereas the effects of intrinsic benefits (knowledge self-efficacy and enjoyment in helping others) on EKR usage are not moderated by contextual factors. The loss of knowledge power and image do not appear to impact EKR usage by knowledge contributors. Besides contributing to theory building in KM, the results of this study inform KM practice.", "title": "Contributing Knowledge to Electronic Knowledge Repositories: An Empirical Investigation"}, "1eefc51ff886acd6f2c317f8f25a3f42fe984ddb": {"paper_id": "1eefc51ff886acd6f2c317f8f25a3f42fe984ddb", "abstract": "Previous research on organizational commitment has typically not focused on the underlying dimensions of psychological attachment to the organization. Results of two studies using university employees (N = 82) and students (N = 162) suggest that psychological attachment may be predicated on compliance, identification, and internalization (e.g., Kelman, 19S8). Identification and internalization are positively related to prosocial behaviors and negatively related to turnover. Internalization is predictive of financial donations to a fund-raising campaign. Overall, the results suggest the importance of clearly specifying the underlying dimensions of commitment using notions of psychological attachment and the various forms such attachment can take.", "title": "Organizational Commitment and Psychological Attachment : The Effects of Compliance , Identification , and Internalization on Prosocial Behavior"}, "a567feb4c93cdb416fda9fd1797533df39a7e1e4": {"paper_id": "a567feb4c93cdb416fda9fd1797533df39a7e1e4", "abstract": "Many CEOs and managers understand the importance of knowledge sharing among their employees and are eager to introduce the knowledge management paradigm in their organizations. However little is known about the determinants of the individual\u0092s knowledge sharing behavior. The purpose of this study is to develop an understanding of the factors affecting the individual\u0092s knowledge sharing behavior in the organizational context. The research model includes various constructs based on social exchange theory, self-efficacy, and theory of reasoned action. Research results from the field survey of 467 employees of four large, public organizations show that expected associations and contribution are the major determinants of the individual\u0092s attitude toward knowledge sharing. Expected rewards, believed by many as the most important motivating factor for knowledge sharing, are not significantly related to the attitude toward knowledge sharing. As expected, positive attitude toward knowledge sharing is found to lead to positive intention to share knowledge and, finally, to actual knowledge sharing behaviors.", "title": "Breaking the Myths of Rewards: An Exploratory Study of Attitudes about Knowledge Sharing"}, "ba0644aa7569f33194090ade9f8f91fa51968b18": {"paper_id": "ba0644aa7569f33194090ade9f8f91fa51968b18", "abstract": "Computer systems cannot improve organizational performance if they aren't used. Unfortunately, resistance to end-user systems by managers and professionals is a widespread problem. To better predict, explain, and increase user acceptance, we need to better understand why people accept or reject computers. This research addresses the ability to predict peoples' computer acceptance from a measure oftheir intentions, and the ability to explain their intentions in terms oftheir attitudes, subjective norms, perceived usefulness, perceived ease of use, and related variables. In a longitudinal study of 107 users, intentions to use a specific system, measured after a onehour introduction to the system, were correlated 0.35 with system use 14 weeks later. The intentionusage correlation was 0.63 at the end of this time period. Perceived usefulness strongly influenced peoples' intentions, explaining more than half of the variance in intentions at the end of 14 weeks. Perceived ease of use had a small but significant effect on intentions as well, although this effect subsided over time. Attitudes only partially mediated the effects of these beliefs on intentions. Subjective norms had no effect on intentions. These results suggest the possibility of simple but powerful models ofthe determinants of user acceptance, with practical value for evaluating systems and guiding managerial interventions aimed at reducing the problem of underutilized computer technology. (INFORMATION TECHNOLOGY; USER ACCEPTANCE; INTENTION MODELS)", "title": "USER ACCEPTANCE OF COMPUTER TECHNOLOGY : A COMPARISON OF TWO THEORETICAL MODELS *"}, "295af62fc8daff93dea71e2d02f0cc0713977e11": {"paper_id": "295af62fc8daff93dea71e2d02f0cc0713977e11", "abstract": "We trace in pragmatic terms some of what we know about knowledge, information technology, knowledge management practice and research, and provide two complementary frameworks that highlight potential opportunities for building a research agenda in this area. The papers in this special issue are then discussed.", "title": "General Perspectives on Knowledge Management: Fostering a Research Agenda"}, "c9a4858855d01933acea5eda42d9ebe12394ad46": {"paper_id": "c9a4858855d01933acea5eda42d9ebe12394ad46", "abstract": "Correspondence: Bruce Kogut, INSEAD, Boulevard de Constance, 77305 Fontainebleau Cedex, France. Tel: +33 1607242 05 Fax: +33 1607455 00/01 E-mail: Bruce.KOGUT@insead.edu Abstract Firms are social communities that specialize in the creation and internal transfer of knowledge. The multinational corporation arises not out of the failure of markets for the buying and selling of knowledge, but out of its superior efficiency as an organizational vehicle by which to transfer this knowledge across borders. We test the claim that firms specialize in the internal transfer of tacit knowledge by empirically examining the decision to transfer the capability to manufacture new products to wholly owned subsidiaries or to other parties. The empirical results show that the less codifiable and the harder to teach is the technology, the more likely the transfer will be to wholly owned operations. This result implies that the choice of transfer mode is determined by the efficiency of the multinational corporation in transferring knowledge relative to other firms, not relative to an abstract market transaction. The notion of the firm as specializing in the transfer and recombination of knowledge is the foundation to an evolutionary theory of the multinational corporation Journal of International Business Studies (2003) 34, 516\u2013529. doi:10.1057/palgrave. jibs.8400058", "title": "Knowledge of the firm and the evolutionary theory of the multinational corporation"}, "676680a4812cee810341f9153c15ae93fefd6675": {"paper_id": "676680a4812cee810341f9153c15ae93fefd6675", "abstract": "Machine-to-machine communication, M2M, will make up a large portion of the new types of services and use cases that the fifth generation (5G) systems will address. On the one hand, 5G will connect a large number of low-cost and low-energy devices in the context of the Internet of things; on the other hand it will enable critical machine type communication use cases, such as smart factory, automotive, energy, and e-health \u2013 which require communication with very high reliability and availability, as well as very low end-to-end latency. In this paper, we will discuss the requirements, enablers and challenges to support these emerging mission-critical 5G use cases. Keywords\u2014 5G, NR, M2M, MTC, URLLC, Tactile Internet, Reliability, Latency.", "title": "Ultra-Reliable and Low-Latency 5 G Communication"}, "67b72d2d419ee183f3e4e9810de403d7b85d68c2": {"paper_id": "67b72d2d419ee183f3e4e9810de403d7b85d68c2", "abstract": "Fifth generation wireless networks are currently being developed to handle a wide range of new use cases. One important emerging area is ultra-reliable communication with guaranteed low latencies well beyond what current wireless technologies can provide. In this paper, we explore the viability of using wireless communication for low-latency, high-reliability communication in an example scenario of factory automation, and outline important design choices for such a system. We show that it is possible to achieve very low error rates and latencies over a radio channel, also when considering fast fading signal and interference, channel estimation errors, and antenna correlation. The most important tool to ensure high reliability is diversity, and low latency is achieved by using short transmission intervals without retransmissions, which, however, introduces a natural restriction on coverage area.", "title": "Radio access for ultra-reliable and low-latency 5G communications"}, "d8cc9fd8ff09c266bd44766051c8aefe87a6b4f3": {"paper_id": "d8cc9fd8ff09c266bd44766051c8aefe87a6b4f3", "abstract": "Cellular technology has dramatically changed our society and the way we communicate. First it impacted voice telephony, and then has been making inroads into data access, applications, and services. However, today potential capabilities of the Internet have not yet been fully exploited by cellular systems. With the advent of 5G we will have the opportunity to leapfrog beyond current Internet capabilities.", "title": "5G: Personal mobile internet beyond what cellular did to telephony"}, "8c410cf942b91ae71b36a73669534b13598de48c": {"paper_id": "8c410cf942b91ae71b36a73669534b13598de48c", "abstract": "Cognitive radio technology, a revolutionary communication paradigm that can utilize the existing wireless spectrum resources more efficiently, has been receiving a growing attention in recent years. As network users need to adapt their operating parameters to the dynamic environment, who may pursue different goals, traditional spectrum sharing approaches based on a fully cooperative, static, and centralized network environment are no longer applicable. Instead, game theory has been recognized as an important tool in studying, modeling, and analyzing the cognitive interaction process. In this tutorial survey, we introduce the most fundamental concepts of game theory, and explain in detail how these concepts can be leveraged in designing spectrum sharing protocols, with an emphasis on state-of-the-art research contributions in cognitive radio networking. Research challenges and future directions in game theoretic modeling approaches are also outlined. This tutorial survey provides a comprehensive treatment of game theory with important applications in cognitive radio networks, and will aid the design of efficient, self-enforcing, and distributed spectrum sharing schemes in future wireless networks. 2010 Elsevier B.V. All rights reserved.", "title": "Game theory for cognitive radio networks: An overview"}, "af2625ceb82fc88d850bce19f2f73d6d37a17bfa": {"paper_id": "af2625ceb82fc88d850bce19f2f73d6d37a17bfa", "abstract": "We address the problem of spectrum pricing in a cognitive radio network where multiple primary service providers compete with each other to offer spectrum access opportunities to the secondary users. By using an equilibrium pricing scheme, each of the primary service providers aims to maximize its profit under quality of service (QoS) constraint for primary users. We formulate this situation as an oligopoly market consisting of a few firms and a consumer. The QoS degradation of the primary services is considered as the cost in offering spectrum access to the secondary users. For the secondary users, we adopt a utility function to obtain the demand function. With a Bertrand game model, we analyze the impacts of several system parameters such as spectrum substitutability and channel quality on the Nash equilibrium (i.e., equilibrium pricing adopted by the primary services). We present distributed algorithms to obtain the solution for this dynamic game. The stability of the proposed dynamic game algorithms in terms of convergence to the Nash equilibrium is studied. However, the Nash equilibrium is not efficient in the sense that the total profit of the primary service providers is not maximized. An optimal solution to gain the highest total profit can be obtained. A collusion can be established among the primary services so that they gain higher profit than that for the Nash equilibrium. However, since one or more of the primary service providers may deviate from the optimal solution, a punishment mechanism may be applied to the deviating primary service provider. A repeated game among primary service providers is formulated to show that the collusion can be maintained if all of the primary service providers are aware of this punishment mechanism, and therefore, properly weight their profits to be obtained in the future.", "title": "Competitive Pricing for Spectrum Sharing in Cognitive Radio Networks: Dynamic Game, Inefficiency of Nash Equilibrium, and Collusion"}, "504939c784878906c8f9fbd41783e3bbce4e8e4b": {"paper_id": "504939c784878906c8f9fbd41783e3bbce4e8e4b", "abstract": "Cognitive radio is viewed as a novel approach for improving the utilization of a precious natural resource: the radio electromagnetic spectrum. The cognitive radio, built on a software-defined radio, is defined as an intelligent wireless communication system that is aware of its environment and uses the methodology of understanding-by-building to learn from the environment and adapt to statistical variations in the input stimuli, with two primary objectives in mind: /spl middot/ highly reliable communication whenever and wherever needed; /spl middot/ efficient utilization of the radio spectrum. Following the discussion of interference temperature as a new metric for the quantification and management of interference, the paper addresses three fundamental cognitive tasks. 1) Radio-scene analysis. 2) Channel-state estimation and predictive modeling. 3) Transmit-power control and dynamic spectrum management. This work also discusses the emergent behavior of cognitive radio.", "title": "Cognitive radio: brain-empowered wireless communications"}, "44477c870a0e2255af99b5c00a76c44665a19c55": {"paper_id": "44477c870a0e2255af99b5c00a76c44665a19c55", "abstract": "In this work, we propose a game theoretic framework to analyze the behavior of cognitive radios for distributed adaptive channel allocation. We define two different objective functions for the spectrum sharing games, which capture the utility of selfish users and cooperative users, respectively. Based on the utility definition for cooperative users, we show that the channel allocation problem can be formulated as a potential game, and thus converges to a deterministic channel allocation Nash equilibrium point. Alternatively, a no-regret learning implementation is proposed for both scenarios and it is shown to have similar performance with the potential game when cooperation is enforced, but with a higher variability across users. The no-regret learning formulation is particularly useful to accommodate selfish users. Non-cooperative learning games have the advantage of a very low overhead for information exchange in the network. We show that cooperation based spectrum sharing etiquette improves the overall network performance at the expense of an increased overhead required for information exchange", "title": "Adaptive channel allocation spectrum etiquette for cognitive radio networks"}, "9bd4660dbd56036886fe406a82a1afd8cf42fb6f": {"paper_id": "9bd4660dbd56036886fe406a82a1afd8cf42fb6f", "abstract": "The process of extraction of interesting patterns or knowledge from the bulk of data refers to the data mining technique. \u201cIt is the process of discovering meaningful, new correlation patterns and trends through non-trivial extraction of implicit, previously unknown information from large amount of data stored in repositories using pattern recognition as well as statistical and mathematical techniques\u201d. Due to the wide deployment of Internet and information technology, storage and processing of data technologies, the ever-growing privacy concern has been a major issue in data mining for information sharing. This gave rise to a new path in research, known as Privacy Preserving Data Mining (PPDM). The literature paper discusses various privacy preserving data mining algorithms and provide a wide analyses for the representative techniques for privacy preserving data mining along with their merits and demerits. The paper describes an overview of some of the well-known PPDM algorithms. Most of the algorithms are usually a modification of a well-known data-mining algorithm along with some privacy preserving techniques. This paper also focuses on the problems and directions for the future research here. The paper finally discusses the comparative analysis of some well-known privacy preservation techniques that are used. This paper is intended to be a summary and an overview of PPDM.", "title": "Privacy preserving data mining \u2014 \u2018A state of the art\u2019"}, "9492fb8d3b3ce09451fc1df46d5e3c200095f5eb": {"paper_id": "9492fb8d3b3ce09451fc1df46d5e3c200095f5eb", "abstract": "In recent years, privacy-preserving data mining has been studied extensively, because of the wide proliferation of sensitive information on the internet. This paper investigates data mining as a technique for masking data; therefore, termed data mining based privacy protection. This approach incorporates partially the requirement of a targeted data mining task into the process of masking data so that essential structure is preserved in the masked data. The following privacy problem is considered in this paper: a data holder wants to release a version of data for building classification models, but wants to protect against linking the released data to an external source for inferring sensitive information. An iterative bottom-up generalization is adapted from data mining to generalize the data. The generalized data remains useful to classification but becomes difficult to link to other sources. The generalization space is specified by a hierarchical structure of generalizations. A key is identifying the best generalization to climb up the hierarchy at each iteration.", "title": "Bottom-Up Generalization : A Data Mining Solution to Privacy Protection"}, "3dfce4601c3f413605399267b3314b90dc4b3362": {"paper_id": "3dfce4601c3f413605399267b3314b90dc4b3362", "abstract": "\u00d0Today's globally networked society places great demand on the dissemination and sharing of information. While in the past released information was mostly in tabular and statistical form, many situations call today for the release of specific data (microdata). In order to protect the anonymity of the entities (called respondents) to which information refers, data holders often remove or encrypt explicit identifiers such as names, addresses, and phone numbers. Deidentifying data, however, provides no guarantee of anonymity. Released information often contains other data, such as race, birth date, sex, and ZIP code, that can be linked to publicly available information to reidentify respondents and inferring information that was not intended for disclosure. In this paper we address the problem of releasing microdata while safeguarding the anonymity of the respondents to which the data refer. The approach is based on the definition of k-anonymity. A table provides k-anonymity if attempts to link explicitly identifying information to its content map the information to at least k entities. We illustrate how k-anonymity can be provided without compromising the integrity (or truthfulness) of the information released by using generalization and suppression techniques. We introduce the concept of minimal generalization that captures the property of the release process not to distort the data more than needed to achieve k-anonymity, and present an algorithm for the computation of such a generalization. We also discuss possible preference policies to choose among different minimal", "title": "Protecting Respondents' Identities in Microdata Release"}, "19a09658e6c05b44136baf54571a884eb1a7b52e": {"paper_id": "19a09658e6c05b44136baf54571a884eb1a7b52e", "abstract": "A fruitful direction for future data mining research will be the development of techniques that incorporate privacy concerns. Specifically, we address the following question. Since the primary task in data mining is the development of models about aggregated data, can we develop accurate models without access to precise information in individual data records? We consider the concrete case of building a decision-tree classifier from training data in which the values of individual records have been perturbed. The resulting data records look very different from the original records and the distribution of data values is also very different from the original distribution. While it is not possible to accurately estimate original values in individual data records, we propose a novel reconstruction procedure to accurately estimate the distribution of original data values. By using these reconstructed distributions, we are able to build classifiers whose accuracy is comparable to the accuracy of classifiers built with the original data.", "title": "Privacy-Preserving Data Mining"}, "5c15b11610d7c3ee8d6d99846c276795c072eec3": {"paper_id": "5c15b11610d7c3ee8d6d99846c276795c072eec3", "abstract": "The proliferation of information on the Internet and access to fast computers with large storage capacities has increased the volume of information collected and disseminated about individuals. The existence os these other data sources makes it much easier to re-identify individuals whose private information is released in data believed to be anonymous. At the same time, increasing demands are made on organizations to release individualized data rather than aggregate statistical information. Even when explicit identi ers, such as name and phone number, are removed or encrypted when releasing individualized data, other characteristic data, which we term quasi-identi ers, can exist which allow the data recipient to re-identify individuals to whom the data refer. In this paper, we provide a computational disclosure technique for releasing information from a private table such that the identity of any individual to whom the released data refer cannot be de nitively recognized. Our approach protects against linking to other data. It is based on the concepts of generalization, by which stored values can be replaced with semantically consistent and truthful but less precise alternatives, and of k-anonymity . A table is said to provide k-anonymity when the contained data do not allow the recipient to associate the released information to a set of individuals smaller than k. We introduce the notions of generalized table and of minimal generalization of a table with respect to a k-anonymity requirement. As an optimization problem, the objective is to minimally distort the data while providing adequate protection. We describe an algorithm that, given a table, e ciently computes a preferred minimal generalization to provide anonymity.", "title": "Generalizing Data to Provide Anonymity when Disclosing Information (Abstract)"}, "6cfac8d88bb85c9167fd1f74f04b7d283426de20": {"paper_id": "6cfac8d88bb85c9167fd1f74f04b7d283426de20", "abstract": "The \u201cMozart effect\u201d reported by Rauscher, Shaw, and Ky (1993, 1995) indicates that spatial-temporal abilities are enhanced after listening to music composed by Mozart. We replicated and extended the effect in Experiment 1: Performance on a spatial-temporal task was better after participants listened to a piece composed by Mozart or by Schubert than after they sat in silence. In Experiment 2, the advantage for the music condition disappeared when the control condition consisted of a narrated story instead of silence. Rather, performance was a function of listeners\u2019preference (music or story), with better performance following the preferred condition. Claims that exposure to music composed by Mozart improves spatial-temporal abilities (Rauscher, Shaw, & Ky, 1993, 1995) have received widespread attention in the news media. Based on these findings, Georgia Governor Zell Miller recently budgeted for a compact disc or cassette for each infant born in state. Reports published in Science(Holden, 1994), the APA Monitor(Martin, 1994), and the popular press indicate that scientists and the general public are giving serious consideration to the possibility that music listening and music lessons improve other abilities. If these types of associations can be confirmed, the implications would be considerable. For example, listening to music could improve the performance of pilots and structural engineers. Such associations would also provide evidence against contemporary theories of modularity (Fodor, 1983) and multiple intelligences (Gardner, 1993), which argue for independence of functioning across domains. Although facilitation in spatial-temporal performance following exposure to music (Rauscher et al., 1993, 1995) is temporary (10 to 15 min), long-term improvements in spatial-temporal reasoning as a consequence of music lessons have also been reported (Gardiner, Fox, Knowles, & Jeffrey, 1996; Rauscher et al., 1997). Unfortunately, the media have not been careful to distinguish these disparate findings. The purpose of the present study was to provide a more complete explanation of the short-term phenomenon. Rauscher and her colleagues have proposed that the so-called Mozart effect can be explained by the trion model (Leng & Shaw, 1991), which posits that exposure to complex musical compositions excites cortical firing patterns similar to those used in spatial-temporal reasoning, so that performance on spatial-temporal tasks is positively affected by exposure to music. On the surface, the Mozart effect is similar to robust psychological phenomena such as transfer or priming. For example, the effect could be considered an instance of positive, nonspecific transfer across domains and modalities (i.e., music listening and visual-spatial performance) that do not have a well-documented association. Transfer is said to occur when knowledge or skill acquired in one situation influences performance in another (Postman, 1971). In the case of the Mozart effect, however, passive listening to music\u2014rather than overt learning\u2014influences spatial-temporal performance. The Mozart effect also bears similarities to associative priming effects and spreading activation (Collins & Loftus, 1975). But priming effects tend to disappear when the prime and the target have few features in common (Klimesch, 1994, pp. 163\u2013165), and cross-modal priming effects are typically weak (Roediger & McDermott, 1993). Moreover, it is far from obvious which features are shared by stimuli as diverse as a Mozart sonata and a spatial-temporal task. In short, the Mozart effect described by Rauscher et al. (1993, 1995) is difficult to situate in a context of known cognitive phenomena. Stough, Kerkin, Bates, and Mangan (1994) failed to replicate the findings of Rauscher et al., although their use of Raven\u2019s Advanced Progressive Matrices rather than spatial tasks from the Stanford-Binet Intelligence Scale (Rauscher et al., 1993, 1995) to assess spatial abilities may account for the discrepancies. Whereas tasks measuring spatial recognition (such as the Raven\u2019s test) require a search for physical similarities among visually presented stimuli, spatial-temporal tasks (e.g., the Paper Folding and Cutting, PF&C, subtest of the StanfordBinet; mental rotation tasks; jigsaw puzzles) require mental transformation of the stimuli (Rauscher & Shaw, 1998). In their review of previous successes and failures at replicating the Mozart effect, Rauscher and Shaw (1998) concluded that the effect is obtainable only with spatial-temporal tasks. Our goal in Experiment 1 was to replicate and extend the basic findings of Rauscher et al. (1993, 1995). A completely computercontrolled procedure was used to test adults\u2019 performance on a PF&C task immediately after they listened to music or sat in silence. Half of the participants listened to Mozart during the music condition; the other half listened to Schubert. The purpose of Experiment 2 was to test the hypothesis that the Mozart effect is actually a consequence of participants\u2019 preference for one testing condition over another, the assumption being that better performance would follow the preferred condition. Control conditions in Rauscher et al. (1993) included a period of silence or listening to a relaxation tape, both of which might have been less interesting or arousing than listening to a Mozart sonata. Consequently, if the participants in that study preferred the Mozart condition, this factor might account for the differential performance on the spatial-temporal task that followed. In a subsequent experiment (Rauscher et al., 1995), comparison conditions involved silence or a combination of minimalist music (Philip Glass), a taped short story, and repetitive dance music. Minimalist and repetitive music might also induce boredom or low levels of arousal, much like silence, and the design precluded direct comparison of the short-story and music conditions. Indeed, in all other instances in which the Mozart effect has been successfully replicated (see Rauscher & Shaw, 1998), control conditions consisted of sitting in silence or listening to relaxation tapes or repetitive music. In Experiment 2, our control condition involved simply listening to a short story. Address correspondence to E. Glenn Schellenberg, Department of Psychology, University of Toronto at Mississauga, Mississauga, Ontario, Canada L5L 1C6; e-mail: g.schellenberg@utoronto.ca. PSYCHOLOGICAL SCIENCE Kristin M. Nantais and E. Glenn Schellenberg", "title": "THE MOZART EFFECT : An Artifact of Preference"}, "50d5035bae37531d2d8108055d52f4a000d66dbe": {"paper_id": "50d5035bae37531d2d8108055d52f4a000d66dbe", "abstract": "Four experiments indicated that positive affect, induced by means of seeing a few minutes of a comedy film or by means of receiving a small bag of candy, improved performance on two tasks that are generally regarded as requiring creative ingenuity: Duncker's (1945) candle task and M. T. Mednick, S. A. Mednick, and E. V. Mednick's (1964) Remote Associates Test. One condition in which negative affect was induced and two in which subjects engaged in physical exercise (intended to represent affectless arousal) failed to produce comparable improvements in creative performance. The influence of positive affect on creativity was discussed in terms of a broader theory of the impact of positive affect on cognitive organization.", "title": "Positive affect facilitates creative problem solving."}, "927c10385d93d538e2791f8ef28c5eaf96e08a73": {"paper_id": "927c10385d93d538e2791f8ef28c5eaf96e08a73", "abstract": "The intent of this paper is the presentation of an associative interpretation of the process of creative thinking. The explanation is not directed to any specific field of application such as art or science but attempts to delineate processes that underlie all creative thought. The discussion will take the following form, (a) First, we will define creative thinking in associative terms and indicate three ways in which creative solutions may be achieved\u2014serendipity, similarity, and mediation, (b) This definition will allow us to deduce those individual difference variables which will facilitate creative performance, (c) Consideration of the definition of the creative process has suggested an operational statement of the definition in the form of a test. The test will be briefly described along with some preliminary research results. (d) The paper will conclude with a discussion of predictions regarding the influence of certain experimentally manipulable variables upon the creative process. Creative individuals and the processes by which they manifest their creativity have excited a good deal of", "title": "The associative basis of the creative process."}, "3ba6f8a90843600bd45df1cc1a5f054cc096a16d": {"paper_id": "3ba6f8a90843600bd45df1cc1a5f054cc096a16d", "abstract": "Given that the synthesis of cumulated knowledge is an essential condition for any field to grow and develop, we believe that the enhanced role of IS reviews requires that this expository form be given careful scrutiny. Over the past decade, several senior scholars have made calls for more review papers in our field. While the number of IS review papers has substantially increased in recent years, no prior research has attempted to develop a general framework to conduct and evaluate the rigor of standalone reviews. In this paper, we fill this gap. More precisely, we present a set of guidelines for guiding and evaluating IS literature reviews and specify to which review types they apply. To do so, we first distinguish between four broad categories of review papers and then propose a set of guidelines that are grouped according to the generic phases and steps of the review process. We hope our work will serve as a valuable source for those conducting, evaluating, and/or interpreting reviews in our field.", "title": "A Framework for Guiding and Evaluating Literature Reviews"}, "00963166e0a7157cdde5cd42757bff776f5a5667": {"paper_id": "00963166e0a7157cdde5cd42757bff776f5a5667", "abstract": "AIM\nThis paper discusses the literature on establishing rigour in research studies. It describes the methodological trinity of reliability, validity and generalization and explores some of the issues relating to establishing rigour in naturalistic inquiry.\n\n\nBACKGROUND\nThose working within the naturalistic paradigm have questioned the issue of using validity, reliability and generalizability to demonstrate robustness of qualitative research. Triangulation has been used to demonstrate confirmability and completeness and has been one means of ensuring acceptability across paradigms. Emerging criteria such as goodness and trustworthiness can be used to evaluate the robustness of naturalistic inquiry.\n\n\nDISCUSSION\nIt is argued that the transference of terms across paradigms is inappropriate; however, if we reject the concepts of validity and reliability, we reject the concept of rigour. Rejection of rigour undermines acceptance of qualitative research as a systematic process that can contribute to the advancement of knowledge. Emerging criteria for demonstrating robustness in qualitative inquiry, such as authenticity, trustworthiness and goodness, need to be considered. Goodness, when not seen as a separate construct but as an integral and embedded component of the research process, should be useful in assuring quality of the entire study. Triangulation is a tried and tested means of offering completeness, particularly in mixed-method research. When multiple types of triangulation are used appropriately as the 'triangulation state of mind', they approach the concept of crystallization, which allows for infinite variety of angles of approach.\n\n\nCONCLUSION\nQualitative researchers need to be explicit about how and why they choose specific legitimizing criteria in ensuring the robustness of their inquiries. A shift from a position of fundamentalism to a more pluralistic approach as a means of legitimizing naturalistic inquiry is advocated.", "title": "Methodological rigour within a qualitative framework."}, "1ef5be55265e09cc1e14758a0749f547c4b2453c": {"paper_id": "1ef5be55265e09cc1e14758a0749f547c4b2453c", "abstract": "Innovation diffusion theory provides a useful perspective on one of the most persistently challenging topics in the IT field, namely, how to improve technology assessment, adoption and implementation. For this reason, diffusion is growing in popularity as a reference theory for empirical studies of information technology adoption and diffusion, although no comprehensive review of this body of work has been published to date. This paper presents the results of a critical review of eighteen empirical studies published during the period 1981-1991. Conclusive results were most likely when the adoption context closely matched the contexts in which classical diffusion theory was developed (for example, individual adoption of personal-use technologies) or when researchers extended diffusion theory to account for new factors specific to the IT adoption context under study. Based on classical diffusion theory and other recent conceptual work, a framework is developed to guide future research in IT diffusion. The framework maps two classes of technology (ones that conform closely to classical diffusion assumptions versus ones that do no0 against locus of adoption (individual versus organizational), resulting in four IT adoption contexts. For each adoption context, variables impacting adoption and diffusion are identified. Additionally, directions for future research are discussed.", "title": "Information Technology Diffusion: A Review of Empirical Research"}, "3969e582e68e418a2b79c604cd35d5d81de9b35d": {"paper_id": "3969e582e68e418a2b79c604cd35d5d81de9b35d", "abstract": "Valid measurement scales for predicting user acceptance of computers are in short supply. Most subjective measures used in practice are unvalidated, and their relationship to system usage is unknown. The present research develops and validates new scales for two specific variables, perceived usefulness and perceived ease of use, which are hypothesized to be fundamental determinants of user acceptance. Definitions for these two variables were used to develop scale items that were pretested for content validity and then tested for reliability and construct validity in two studies involving a total of 152 users and four application programs. The measures were refined and streamlined, resulting in two six-item scales with reliabilities of .98 for usefulness and .94 for ease of use. The scales exhibited high convergent, discriminant, and factorial validity. Perceived usefulness was significantly correlated with both selfreported current usage (r=.63, Study 1) and self-predicted future usage (r =.85, Study 2). Perceived ease of use was also significantly correlated with current usage (r= .45, Study 1) and future usage (r =.59, Study 2). In both studies, usefulness had a significantly greater correlation with usage behavior than did ease of use. Regression analyses suggest that perceived ease of use may actually be a causal antecedent to perceived usefulness, as opposed to a parallel, direct determinant of system usage. Implications are drawn for future research on user acceptance.", "title": "Perceived Usefulness, Perceived Ease of Use, and User Acceptance of Information Technology"}, "a3e4af4e75ddad4205b3764a8e38b70f014950d3": {"paper_id": "a3e4af4e75ddad4205b3764a8e38b70f014950d3", "abstract": "Nowadays, most nurses, pre- and post-qualification, will be required to undertake a literature review at some point, either as part of a course of study, as a key step in the research process, or as part of clinical practice development or policy. For student nurses and novice researchers it is often seen as a difficult undertaking. It demands a complex range of skills, such as learning how to define topics for exploration, acquiring skills of literature searching and retrieval, developing the ability to analyse and synthesize data as well as becoming adept at writing and reporting, often within a limited time scale. The purpose of this article is to present a step-by-step guide to facilitate understanding by presenting the critical elements of the literature review process. While reference is made to different types of literature reviews, the focus is on the traditional or narrative review that is undertaken, usually either as an academic assignment or part of the research process.", "title": "Undertaking a literature review: a step-by-step approach."}, "8bb8b26e59af0547f739704bb1c42c2b0140fe09": {"paper_id": "8bb8b26e59af0547f739704bb1c42c2b0140fe09", "abstract": "When caring for patients, it is essential that nurses are using the current best practice. To determine what this is, nurses must be able to read research critically. But for many qualified and student nurses, the terminology used in research can be difficult to understand, thus making critical reading even more daunting. It is imperative in nursing that care has its foundations in sound research, and it is essential that all nurses have the ability to critically appraise research to identify what is best practice. This article is a step-by-step approach to critiquing quantitative research to help nurses demystify the process and decode the terminology.", "title": "Step-by-step guide to critiquing research. Part 1: quantitative research."}, "de270dcf3a03059af50f6da3a89ff1ba5631eb04": {"paper_id": "de270dcf3a03059af50f6da3a89ff1ba5631eb04", "abstract": "0950-5849/$ see front matter 2009 Elsevier B.V. A doi:10.1016/j.infsof.2009.11.005 * Corresponding author. Tel.: +44 (0) 1782 734090; E-mail address: m.turner@cs.keele.ac.uk (M. Turne Context: The technology acceptance model (TAM) was proposed in 1989 as a means of predicting technology usage. However, it is usually validated by using a measure of behavioural intention to use (BI) rather than actual usage. Objective: This review examines the evidence that the TAM predicts actual usage using both subjective and objective measures of actual usage. Method: We performed a systematic literature review based on a search of six digital libraries, along with vote-counting meta-analysis to analyse the overall results. Results: The search identified 79 relevant empirical studies in 73 articles. The results show that BI is likely to be correlated with actual usage. However, the TAM variables perceived ease of use (PEU) and perceived usefulness (PU) are less likely to be correlated with actual usage. Conclusion: Care should be taken using the TAM outside the context in which it has been validated. 2009 Elsevier B.V. All rights reserved.", "title": "Does the technology acceptance model predict actual use? A systematic literature review"}, "be8aeabd40db44898df38ecf3a125758864896be": {"paper_id": "be8aeabd40db44898df38ecf3a125758864896be", "abstract": "This paper presents the findings of two studies that replicate previous work by Fred Davis on the subject of perceived usefulness, ease of use, and usage of information technology. The two studies focus on evaluating the psychometric properties of the ease of use and usefulness scales, while examining the relationship between ease of use, usefulness, and system usage. Study 1 provides a strong assessment of the convergent validity of the two scales by examining heterogeneous user groups dealing with heterogeneous implementations of messaging technology. In addition, because one might expect users to share similar perspectives about voice and electronic mail, the study also represents a strong test of discriminant validity. In this study a total of 118 respondents from 10 different organizations were surveyed for their attitudes toward two messaging technologies: voice and electronic mail. Study 2 complements the approach taken in Study 1 by focusing on the ability to demonstrate discriminant validity. Three popular software applications (WordPerfect, Lotus 1-2-3, and Harvard Graphics) were examined based on the expectation that they would all be rated highly on both scales. In this study a total of 73 users rated the three packages in terms of ease of use and usefulness. The results of the studies demonstrate reliable and valid scales for measurement of perceived ease of use and usefulness. In addition, the paper tests the relationships between ease of use, usefulness, and usage using structural equation modelling. The results of this model are consistent with previous research for Study 1, suggesting that usefulness is an important determinant of system use. For Study 2 the results are somewhat mixed, but indicate the importance of both ease of use and usefulness. Differences in conditions of usage are explored to explain these findings.", "title": "Perceived Usefulness, Ease of Use, and Usage of Information Technology: A Replication"}, "66392ca9b0fc8bb8c8d27312ddd90ca3b418516a": {"paper_id": "66392ca9b0fc8bb8c8d27312ddd90ca3b418516a", "abstract": "In this paper, we empirically investigate an extension of the Technology Acceptance Model (TAM, Davis, 1989) to explain the individual acceptance and usage of websites. Conceptually, we examine perceived ease-of-use, usefulness, enjoyment, and their impact on attitude towards using, intention to use and actual use. The paper also introduces a new construct, \u201cperceived visual attractiveness\u201d of the website and suggest that it influences usefulness, enjoyment, and ease-of-use. For our empirical research we partnered with a Dutch generic portal site with over 300 000 subscribers at the time the research was conducted. The websurvey resulted in sample size of 825 respondents. The results confirmed all of the 12 hypotheses formulated. Three findings are worth mentioning in particular: (1) intention is most dominantly influenced by attitude (\u03b2 = 0.51), (2) ease-of-use, enjoyment, and usefulness contribute equally to attitude towards using (\u03b2 = 0.23, 0.23, and 0.17 respectively) and (3) visual attractiveness contributes remarkably well to both ease-of-use, enjoyment, and usefulness (\u03b2 = 0.41, 0.35, and 0.21). Although this is not the first research to apply TAM to an internet context, we claim three major contributions: (1) a single website as the unit of analysis, (2) the introduction of visual attractiveness, and (3) the use of \u201creal\u201d website visitors rather than student samples. Promising future research lies in the conceptual connection between actual website features and website use, a connection for which the TAM framework provides a meaningful bridge. Factors Influencing the Usage of Websites: The Case of a Generic Portal in the Netherlands", "title": "Factors Influencing the Usage of Websites: The Case of a Generic Portal in the Netherlands"}, "781274c22a436f45e4fd1ca6600ed4d4f6e27eb1": {"paper_id": "781274c22a436f45e4fd1ca6600ed4d4f6e27eb1", "abstract": "Information technology (IT) acceptance research has yielded many competing models, each with different sets of acceptance determinants. In this paper, we (1) review user acceptance literature and discuss eight prominent models, (2) empirically compare the eight models and their extensions , (3) formulate a unified model that integrates elements across the eight models, and (4) empirically validate the unified model. The eight models reviewed are the theory of reasoned action, the technology acceptance model, the motivational model, the theory of planned behavior, a model combining the technology acceptance model and the theory of planned behavior, the model of PC utilization, the innovation diffusion theory, and the social cognitive theory. Using data from four organizations over a six-month period with three points of measurement, the eight models explained between 17 percent and 53 percent of the variance in user intentions to use information technology. Next, a unified model, called the Unified Theory of Acceptance and Use of Technology (UTAUT), was formulated, with four core determinants of intention and usage, and up to four moderators of key relationships. UTAUT was then tested using the original data and found to outperform the eight individual models (adjusted R 2 of 69 percent). UTAUT was then confirmed with data from two new organizations with similar results (adjusted R 2 of 70 percent). UTAUT thus provides a useful tool for managers needing to Venkatesh et al./User Acceptance of IT 426 MIS Quarterly Vol. 27 No. 3/September 2003 assess the likelihood of success for new technology introductions and helps them understand the drivers of acceptance in order to proactively design interventions (including training, marketing, etc.) targeted at populations of users that may be less inclined to adopt and use new systems. The paper also makes several recommendations for future research including developing a deeper understanding of the dynamic influences studied here, refining measurement of the core constructs used in UTAUT, and understanding the organizational outcomes associated with new technology use.", "title": "User Acceptance of Information Technology: Toward a Unified View"}, "440c170f5fdf29acdafb70b78792cf405641664c": {"paper_id": "440c170f5fdf29acdafb70b78792cf405641664c", "abstract": "The construct of trait emotional intelligence (trait EI or trait emotional self-efficacy) provides a comprehensive operationalization of emotion-related self-perceptions and dispositions. In the first part of the present study (N=274, 92 males), we performed two joint factor analyses to determine the location of trait EI in Eysenckian and Big Five factor space. The results showed that trait EI is a compound personality construct located at the lower levels of the two taxonomies. In the second part of the study, we performed six two-step hierarchical regressions to investigate the incremental validity of trait EI in predicting, over and above the Giant Three and Big Five personality dimensions, six distinct criteria (life satisfaction, rumination, two adaptive and two maladaptive coping styles). Trait EI incrementally predicted four criteria over the Giant Three and five criteria over the Big Five. The discussion addresses common questions about the operationalization of emotional intelligence as a personality trait.", "title": "The location of trait emotional intelligence in personality factor space."}, "1a9ed0d0bdde77886f45d0a0bdfe8802d4c16d9e": {"paper_id": "1a9ed0d0bdde77886f45d0a0bdfe8802d4c16d9e", "abstract": "This article reports the development and validation of a scale to measure global life satisfaction, the Satisfaction With Life Scale (SWLS). Among the various components of subjective well-being, the SWLS is narrowly focused to assess global life satisfaction and does not tap related constructs such as positive affect or loneliness. The SWLS is shown to have favorable psychometric properties, including high internal consistency and high temporal reliability. Scores on the SWLS correlate moderately to highly with other measures of subjective well-being, and correlate predictably with specific personality characteristics. It is noted that the SWLS is Suited for use with different age groups, and other potential uses of the scale are discussed.", "title": "The Satisfaction With Life Scale."}, "1d3d32becea6af5ddcd06c6d8f368e7c5b3a5436": {"paper_id": "1d3d32becea6af5ddcd06c6d8f368e7c5b3a5436", "abstract": "\u2014The significant growth of online shopping makes the competition in this industry become more intense. Maintaining customer loyalty has been recognized as one of the essential factor for business survival and growth. The purpose of this study is to examine empirically the influence of satisfaction, trust and commitment on customer loyalty in online shopping. This paper describes a theoretical model for investigating the influence of satisfaction, trust and commitment on customer loyalty toward online shopping. Based on the theoretical model, hypotheses were formulated. The primary data were collected from the respondents which consists of 300 students. Multiple regression and qualitative analysis were used to test the study hypotheses. The empirical study results revealed that satisfaction, trust and commitment have significant impact on student loyalty toward online shopping.", "title": "Factors Influencing Customer Loyalty Toward Online Shopping"}, "4ec1e0fa79f8377c2b16d9c1e00a6a3d16021126": {"paper_id": "4ec1e0fa79f8377c2b16d9c1e00a6a3d16021126", "abstract": "An e-vendor\u2019s website inseparably embodies an interaction with the vendor and an interaction with the IT website interface. Accordingly, research has shown two sets of unrelated usage antecedents by customers: 1) customer trust in the e-vendor and 2) customer assessments of the IT itself, specifically the perceived usefulness and perceived ease-of-use of the website as depicted in the technology acceptance model (TAM). Research suggests, however, that the degree and impact of trust, perceived usefulness, and perceived ease of use change with experience. Using existing, validated scales, this study describes a free-simulation experiment that compares the degree and relative importance of customer trust in an e-vendor vis-\u00e0-vis TAM constructs of the website, between potential (i.e., new) customers and repeat (i.e., experienced) ones. The study found that repeat customers trusted the e-vendor more, perceived the website to be more useful and easier to use, and were more inclined to purchase from it. The data also show that while repeat customers\u2019 purchase intentions were influenced by both their trust in the e-vendor and their perception that the website was useful, potential customers were not influenced by perceived usefulness, but only by their trust in the e-vendor. Implications of this apparent trust-barrier and guidelines for practice are discussed.", "title": "Inexperience and experience with online stores: the importance of TAM and trust"}, "257e30fba2d8e159f73d362ee4ba3c38216ff715": {"paper_id": "257e30fba2d8e159f73d362ee4ba3c38216ff715", "abstract": "Ease of use and usefulness are believed to be fundamental in determining the acceptance and use of various, corporate ITs. These beliefs, however, may not explain the user's behavior toward newly emerging ITs, such as the World-Wide-Web (WWW). In this study, we introduce playfulness as a new factor that re \u0304ects the user's intrinsic belief in WWW acceptance. Using it as an intrinsic motivation factor, we extend and empirically validate the Technology Acceptance Model (TAM) for the WWW context. # 2001 Elsevier Science B.V. All rights reserved.", "title": "Extending the TAM for a World-Wide-Web context"}, "3787715114e286042aac4fd9b612114c226c6fe9": {"paper_id": "3787715114e286042aac4fd9b612114c226c6fe9", "abstract": "The growing interest in Structured Equation Modeling (SEM) techniques and recognition of their importance in IS research suggests the need to compare and contrast different types of SEM techniques so that research designs can be selected appropriately. After assessing the extent to which these techniques are currently being used in IS research, the article presents a running example which analyzes the same dataset via three very different statistical techniques. It then compares two classes of SEM: covariance-based SEM and partial-least-squaresbased SEM. Finally, the article discusses linear regression models and offers guidelines as to when SEM techniques and when regression techniques should be used. The article concludes with heuristics and rule of thumb thresholds to guide practice, and a discussion of the extent to which practice is in accord with these guidelines.", "title": "Structural Equation Modeling and Regression: Guidelines for Research Practice"}, "2484c717f39bf94e46503744201f4b57a5f1be4b": {"paper_id": "2484c717f39bf94e46503744201f4b57a5f1be4b", "abstract": "This study investigates the development of trust in a Web-based vendor during two stages of a consumer\u2019s Web experience: exploration and commitment. Through an experimental design, the study tests the effects of third party endorsements, reputation, and individual differences on trust in the vendor during these two stages.", "title": "Trust in e-commerce vendors: a two-stage model"}, "b38f5a3f1ee7c8dd9d86c632cae52f5fa2bec2e3": {"paper_id": "b38f5a3f1ee7c8dd9d86c632cae52f5fa2bec2e3", "abstract": "Web site usability is a critical metric for assessing the quality of a firm\u2019s Web presence. A measure of usability must not only provide a global rating for a specific Web site, ideally it should also illuminate specific strengths and weaknesses associated with site design. In this paper, we describe a heuristic evaluation procedure for examining the usability of Web sites. The procedure utilizes a comprehensive set of usability guidelines developed by Microsoft. We present the categories and subcategories comprising these guidelines, and discuss the development of an instrument that operationalizes the measurement of usability. The proposed instrument was tested in a heuristic evaluation study where 1,475 users rated multiple Web sites from four different industry sectors: airlines, online bookstores, automobile manufacturers, and car rental agencies. To enhance the external validity of the study, users were asked to assume the role of a consumer or an investor when assessing usability. Empirical results suggest that the evaluation procedure, the instrument, as well as the usability metric exhibit good properties. Implications of the findings for researchers, for Web site designers, and for heuristic evaluation methods in usability testing are offered. (Usability; Heuristic Evaluation; Microsoft Usability Guidelines; Human-Computer Interaction; Web Interface)", "title": "Assessing a Firm's Web Presence: A Heuristic Evaluation Procedure for the Measurement of Usability"}, "c2f63316528e9d8c5aa8d1c0aa7afe02d03bc1db": {"paper_id": "c2f63316528e9d8c5aa8d1c0aa7afe02d03bc1db", "abstract": "Trust is central to all transactions and yet economists rarely discuss the notion. It is treated rather as background environment, present whenever called upon, a sort of ever-ready lubricant that permits voluntary participation in production and exchange. In the standard model of a market economy it is taken for granted that consumers meet their budget constraints: they are not allowed to spend more than their wealth. Moreover, they always deliver the goods and services they said they would. But the model is silent on the rectitude of such agents. We are not told if they are persons of honour, conditioned by their upbringing always to meet the obligations they have chosen to undertake, or if there is a background agency which enforces contracts, credibly threatening to mete out punishment if obligations are not fulfilled a punishment sufficiently stiff to deter consumers from ever failing to fulfil them. The same assumptions are made for producers. To be sure, the standard model can be extended to allow for bankruptcy in the face of an uncertain future. One must suppose that there is a special additional loss to becoming bankrupt a loss of honour when honour matters, social and economic ostracism, a term in a debtors\u2019 prison, and so forth. Otherwise, a person may take silly risks or, to make a more subtle point, take insufficient care in managing his affairs, but claim that he ran into genuine bad luck, that it was Mother Nature\u2019s fault and not his own lack of ability or zeal.", "title": "Trust as a Commodity"}, "53bd4b5c6e35c6945d021f50c4065968fb8fd131": {"paper_id": "53bd4b5c6e35c6945d021f50c4065968fb8fd131", "abstract": "This study aims to provide a picture of how relationship quality can influence customer loyalty or loyalty in the business-to-business context. Building on prior research, we propose relationship quality as a higher construct comprising trust, commitment, satisfaction and service quality. These dimensions of relationship quality can reasonably explain the influence of relationship quality on customer loyalty. This study follows the composite loyalty approach providing both behavioural aspects (purchase intentions) and attitudinal loyalty in order to fully explain the concept of customer loyalty. A literature search is undertaken in the areas of customer loyalty, relationship quality, perceived service quality, trust, commitment and satisfaction. This study then seeks to address the following research issues: Does relationship quality influence both aspects of customer loyalty? Which relationship quality dimensions influence each of the components of customer loyalty? This study was conducted in a business-to-business setting of the courier and freight delivery service industry in Australia. The survey was targeted to Australian Small to Medium Enterprises (SMEs). Two methods were chosen for data collection: mail survey and online survey. The total number of usable respondents who completed both survey was 306. In this study, a two step approach (Anderson and Gerbing 1988) was selected for measurement model and structural model. The results also show that all measurement models of relationship dimensions achieved a satisfactory level of fit to the data. The hypothesized relationships were estimated using structural equation modeling. The overall goodness of fit statistics shows that the structural model fits the data well. As the results show, to maintain customer loyalty to the supplier, a supplier may enhance all four aspects of relationship quality which are trust, commitment, satisfaction and service quality. Specifically, in order to enhance customer\u2019s trust, a supplier should promote the customer\u2019s trust in the supplier. In efforts to emphasize commitment, a supplier should focus on building affective aspects of commitment rather than calculative aspects. Satisfaction appears to be a crucial factor in maintaining purchase intentions whereas service quality will strongly enhance both purchase intentions and attitudinal loyalty.", "title": "Relationship Quality as a Predictor of B 2 B Customer loyalty"}, "2acb77daf7ba60eb523c276a8c6260f6ed076a12": {"paper_id": "2acb77daf7ba60eb523c276a8c6260f6ed076a12", "abstract": "JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. American Marketing Association is collaborating with JSTOR to digitize, preserve and extend access to The Journal of Marketing.", "title": "Servicescapes : The Impact of Physical Surroundings on Customers and Employees"}, "3254ac068130d4788ced89dbc0659bb2fdef3620": {"paper_id": "3254ac068130d4788ced89dbc0659bb2fdef3620", "abstract": "Customer loyalty presents a paradox. Many see it as primarily an attitude-based phenomenon that can be influenced significantly by Customer Relationship Management initiatives such as the increasingly popular loyalty and affinity programs. However, empirical research shows that loyalty in competitive repeat-purchase markets is shaped more by the passive acceptance of brands than by strongly-held attitudes about them. From this perspective, the demandenhancing potential of loyalty programs is more limited than might be hoped. We review three different perspectives on loyalty, and relate these to a framework for understanding customer loyalty that encompasses Customer Brand Commitment, Customer Brand Acceptance and Customer Brand Buying. This framework is used to analyze the demand-side potential of loyalty programs. We discuss where these programs might work and where they are unlikely to succeed on any large scale. A checklist for marketers is provided.", "title": "Customer Loyalty and Customer Loyalty Programs"}, "ff66f4ea4e3ebb2f59a1b67ffda8fb64afb86016": {"paper_id": "ff66f4ea4e3ebb2f59a1b67ffda8fb64afb86016", "abstract": "Customer Relationship Management (CRM) is getting more and more a key strategy for companies big and small. Customer care strategy and CRM software go hand in hand. In particular SME\u2019s need a CRM software that easily adapts to their customer care needs while still being low cost. In this paper I discuss the benefits of CRM for SME\u2019s and their special requirements wrt. CRM software. Further, I introduce the IST-project CARUSO whose objective is to provide SME\u2019s with a software framework to implement low cost, customized CRM applications. This paper presents the rational behind the CARUSO project, the architecture of the framework, and the software development process used to build the framework.", "title": "Customer Relationship Management for SME \u2019 s"}, "bc12715a1ddf1a540dab06bf3ac4f3a32a26b135": {"paper_id": "bc12715a1ddf1a540dab06bf3ac4f3a32a26b135", "abstract": "Standardized benchmarks are crucial for the majority of computer vision applications. Although leaderboards and ranking tables should not be over-claimed, benchmarks often provide the most objective measure of performance and are therefore important guides for research. We present a benchmark for Multiple Object Tracking launched in the late 2014, with the goal of creating a framework for the standardized evaluation of multiple object tracking methods. This paper collects the two releases of the benchmark made so far, and provides an in-depth analysis of almost 50 state-of-the-art trackers that were tested on over 11000 frames. We show the current trends and weaknesses of multiple people tracking methods, and provide pointers of what researchers should be focusing on to push the field forward.", "title": "Tracking the Trackers: An Analysis of the State of the Art in Multiple Object Tracking"}, "5014f5dd9d3586dae0572325085647ddc187fa3f": {"paper_id": "5014f5dd9d3586dae0572325085647ddc187fa3f", "abstract": "Online multi-object tracking aims at producing complete tracks of multiple objects using the information accumulated up to the present moment. It still remains a difficult problem in complex scenes, because of frequent occlusion by clutter or other objects, similar appearances of different objects, and other factors. In this paper, we propose a robust online multi-object tracking method that can handle these difficulties effectively. We first propose the tracklet confidence using the detectability and continuity of a tracklet, and formulate a multi-object tracking problem based on the tracklet confidence. The multi-object tracking problem is then solved by associating tracklets in different ways according to their confidence values. Based on this strategy, tracklets sequentially grow with online-provided detections, and fragmented tracklets are linked up with others without any iterative and expensive associations. Here, for reliable association between tracklets and detections, we also propose a novel online learning method using an incremental linear discriminant analysis for discriminating the appearances of objects. By exploiting the proposed learning method, tracklet association can be successfully achieved even under severe occlusion. Experiments with challenging public datasets show distinct performance improvement over other batch and online tracking methods.", "title": "Robust Online Multi-object Tracking Based on Tracklet Confidence and Online Discriminative Appearance Learning"}, "1db11bd3e2d0794cbb0fab25508b494e0f0a46ea": {"paper_id": "1db11bd3e2d0794cbb0fab25508b494e0f0a46ea", "abstract": "We describe an online approach to learn non-linear motion patterns and robust appearance models for multi-target tracking in a tracklet association framework. Unlike most previous approaches that use linear motion methods only, we online build a non-linear motion map to better explain direction changes and produce more robust motion affinities between tracklets. Moreover, based on the incremental learned entry/exit map, a multiple instance learning method is devised to produce strong appearance models for tracking; positive sample pairs are collected from different track-lets so that training samples have high diversity. Finally, using online learned moving groups, a tracklet completion process is introduced to deal with tracklets not reaching entry/exit points. We evaluate our approach on three public data sets, and show significant improvements compared with state-of-art methods.", "title": "Multi-target tracking by online learning of non-linear motion patterns and robust appearance models"}, "bae912f94bccb41e5492d8f1efb1603c86acd1fc": {"paper_id": "bae912f94bccb41e5492d8f1efb1603c86acd1fc", "abstract": "This paper presents an online detection-based two-stage multi-object tracking method in dense visual surveillances scenarios with a single camera. In the local stage, a particle filter with observer selection that could deal with partial object occlusion is used to generate a set of reliable tracklets. In the global stage, the detection responses are collected from a temporal sliding window to deal with ambiguity caused by full object occlusion to generate a set of potential tracklets. The reliable tracklets generated in the local stage and the potential tracklets generated within the temporal sliding window are associated by Hungarian algorithm on a modified pairwise tracklets association cost matrix to get the global optimal association. This method is applied to the pedestrian class and evaluated on two challenging datasets. The experimental results prove the effectiveness of our method.", "title": "Multi-object tracking through occlusions by local tracklets filtering and global tracklets association with detection responses"}, "49bdd3fb166e0faf7ad1c917aee32c22ebc0f9db": {"paper_id": "49bdd3fb166e0faf7ad1c917aee32c22ebc0f9db", "abstract": "Detection and tracking of humans in video streams is important for many applications. We present an approach to automatically detect and track multiple, possibly partially occluded humans in a walking or standing pose from a single camera, which may be stationary or moving. A human body is represented as an assembly of body parts. Part detectors are learned by boosting a number of weak classifiers which are based on edgelet features. Responses of part detectors are combined to form a joint likelihood model that includes an analysis of possible occlusions. The combined detection responses and the part detection responses provide the observations used for tracking. Trajectory initialization and termination are both automatic and rely on the confidences computed from the detection responses. An object is tracked by data association and meanshift methods. Our system can track humans with both inter-object and scene occlusions with static or non-static backgrounds. Evaluation results on a number of images and videos and comparisons with some previous methods are given.", "title": "Detection and Tracking of Multiple, Partially Occluded Humans by Bayesian Combination of Edgelet based Part Detectors"}, "2258e01865367018ed6f4262c880df85b94959f8": {"paper_id": "2258e01865367018ed6f4262c880df85b94959f8", "abstract": "Simultaneous tracking of multiple persons in real-world environments is an active research field and several approaches have been proposed, based on a variety of features and algorithms. Recently, there has been a growing interest in organizing systematic evaluations to compare the various techniques. Unfortunately, the lack of common metrics for measuring the performance of multiple object trackers still makes it hard to compare their results. In this work, we introduce two intuitive and general metrics to allow for objective comparison of tracker characteristics, focusing on their precision in estimating object locations, their accuracy in recognizing object configurations and their ability to consistently label objects over time. These metrics have been extensively used in two large-scale international evaluations, the 2006 and 2007 CLEAR evaluations, to measure and compare the performance of multiple object trackers for a wide variety of tracking tasks. Selected performance results are presented and the advantages and drawbacks of the presented metrics are discussed based on the experience gained during the evaluations.", "title": "Evaluating Multiple Object Tracking Performance: The CLEAR MOT Metrics"}, "10d6b12fa07c7c8d6c8c3f42c7f1c061c131d4c5": {"paper_id": "10d6b12fa07c7c8d6c8c3f42c7f1c061c131d4c5", "abstract": "We study the question of feature sets for robust visual object recognition; adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.", "title": "Histograms of oriented gradients for human detection"}, "075bfb99ce2dbaa2005500dff90f893b7caa68c2": {"paper_id": "075bfb99ce2dbaa2005500dff90f893b7caa68c2", "abstract": "Boosting has become very popular in computer vision, showing impressive performance in detection and recognition tasks. Mainly off-line training methods have been used, which implies that all training data has to be a priori given; training and usage of the classifier are separate steps. Training the classifier on-line and incrementally as new data becomes available has several advantages and opens new areas of application for boosting in computer vision. In this paper we propose a novel on-line AdaBoost feature selection method. In conjunction with efficient feature extraction methods the method is real time capable. We demonstrate the multifariousness of the method on such diverse tasks as learning complex background models, visual tracking and object detection. All approaches benefit significantly by the on-line training.", "title": "On-line Boosting and Vision"}, "99e386cf71727ee66d5ac77693e107c78508f899": {"paper_id": "99e386cf71727ee66d5ac77693e107c78508f899", "abstract": "We introduce a computationally efficient algorithm for multi-object tracking by detection that addresses four main challenges: appearance similarity among targets, missing data due to targets being out of the field of view or occluded behind other objects, crossing trajectories, and camera motion. The proposed method uses motion dynamics as a cue to distinguish targets with similar appearance, minimize target mis-identification and recover missing data. Computational efficiency is achieved by using a Generalized Linear Assignment (GLA) coupled with efficient procedures to recover missing data and estimate the complexity of the underlying dynamics. The proposed approach works with track lets of arbitrary length and does not assume a dynamical model a priori, yet it captures the overall motion dynamics of the targets. Experiments using challenging videos show that this framework can handle complex target motions, non-stationary cameras and long occlusions, on scenarios where appearance cues are not available or poor.", "title": "The Way They Move: Tracking Multiple Targets with Similar Appearance"}, "9d2f2b45d89f840f2076f70eac46d46121b9267c": {"paper_id": "9d2f2b45d89f840f2076f70eac46d46121b9267c", "abstract": "The generalized assignment problem can be viewed as the following problem of scheduling parallel machines with costs. Each job is to be processed by exactly one machine; processing job j on machine i requires time pif and incurs a cost of c,f, each machine / is available for 7\", t ime units, and the objective is.t\u00bbminimize the total cost incurred. Our main result is as follows. There is a polynomial-time algorithm that, given a value C, either proves that no feasible schedule of cost C exists, or else finds a schedule of cost at most C where each machine / is used for at most 27\", time units. We also extend this result to a variant of the problem where, instead of a fixed processing time p,r there is a range of possible processing times for each machine-job pair, and the cost linearly increases as (he processing time decreases. We show that these results imply a polynomial-time 2-approximation algorithm to minimize a weighted sum of the cost and the makespan, i.e., the maximum job completion time. We also consider the objective of minimizing the mean job completion time. We show that there is a polynomial-time algorithm that, given values M and 7\", either proves that no schedule of mean job completion time M and makespan /\"exists, or else finds a schedule of mean job completion time at most M and makespan at most 27\".", "title": "An approximation algorithm for the generalized assignment problem"}, "30950db8a2cae3630057efe731b85f7b567848b8": {"paper_id": "30950db8a2cae3630057efe731b85f7b567848b8", "abstract": "The problem of tracking curves in dense visual clutter is challenging. Kalman filtering is inadequate because it is based on Gaussian densities which, being unimo dal, cannot represent simultaneous alternative hypotheses. The Condensation algorithm uses \u201cfactored sampling\u201d, previously applied to the interpretation of static images, in which the probability distribution of possible interpretations is represented by a randomly generated set. Condensation uses learned dynamical models, together with visual observations, to propagate the random set over time. The result is highly robust tracking of agile motion. Notwithstanding the use of stochastic methods, the algorithm runs in near real-time.", "title": "CONDENSATION\u2014Conditional Density Propagation for Visual Tracking"}, "0addfc35fc8f4419f9e1adeccd19c07f26d35cac": {"paper_id": "0addfc35fc8f4419f9e1adeccd19c07f26d35cac", "abstract": "This paper describes a discriminatively trained, multiscale, deformable part model for object detection. Our system achieves a two-fold improvement in average precision over the best performance in the 2006 PASCAL person detection challenge. It also outperforms the best results in the 2007 challenge in ten out of twenty categories. The system relies heavily on deformable parts. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL challenge. Our system also relies heavily on new methods for discriminative training. We combine a margin-sensitive approach for data mining hard negative examples with a formalism we call latent SVM. A latent SVM, like a hidden CRF, leads to a non-convex training problem. However, a latent SVM is semi-convex and the training problem becomes convex once latent information is specified for the positive examples. We believe that our training methods will eventually make possible the effective use of more latent information such as hierarchical (grammar) models and models involving latent three dimensional pose.", "title": "A discriminatively trained, multiscale, deformable part model"}, "7d3698c0e828d05f147682b0f5bfcd3b681ff205": {"paper_id": "7d3698c0e828d05f147682b0f5bfcd3b681ff205", "abstract": "In this paper, we present a novel method based on online target-specific metric learning and coherent dynamics estimation for tracklet (track fragment) association by network flow optimization in long-term multi-person tracking. Our proposed framework aims to exploit appearance and motion cues to prevent identity switches during tracking and to recover missed detections. Furthermore, target-specific metrics (appearance cue) and motion dynamics (motion cue) are proposed to be learned and estimated online, i.e., during the tracking process. Our approach is effective even when such cues fail to identify or follow the target due to occlusions or object-to-object interactions. We also propose to learn the weights of these two tracking cues to handle the difficult situations, such as severe occlusions and object-to-object interactions effectively. Our method has been validated on several public datasets and the experimental results show that it outperforms several state-of-the-art tracking methods.", "title": "Tracklet Association by Online Target-Specific Metric Learning and Coherent Dynamics Estimation"}, "436f798d1a4e54e5947c1e7d7375c31b2bdb4064": {"paper_id": "436f798d1a4e54e5947c1e7d7375c31b2bdb4064", "abstract": "Automatic recovery of 3D human pose from monocular image sequences is a challenging and important research topic with numerous applications. Although current methods are able to recover 3D pose for a single person in controlled environments, they are severely challenged by real-world scenarios, such as crowded street scenes. To address this problem, we propose a three-stage process building on a number of recent advances. The first stage obtains an initial estimate of the 2D articulation and viewpoint of the person from single frames. The second stage allows early data association across frames based on tracking-by-detection. These two stages successfully accumulate the available 2D image evidence into robust estimates of 2D limb positions over short image sequences (= tracklets). The third and final stage uses those tracklet-based estimates as robust image observations to reliably recover 3D pose. We demonstrate state-of-the-art performance on the HumanEva II benchmark, and also show the applicability of our approach to articulated 3D tracking in realistic street conditions.", "title": "Monocular 3D pose estimation and tracking by detection"}, "abf8ec8f0b29ac0a1976c90c0e62e5f6035d1404": {"paper_id": "abf8ec8f0b29ac0a1976c90c0e62e5f6035d1404", "abstract": "We propose a linear programming relaxation scheme for the class of multiple object tracking problems where the inter-object interaction metric is convex and the intra-object term quantifying object state continuity may use any metric. The proposed scheme models object tracking as a multi-path searching problem. It explicitly models track interaction, such as object spatial layout consistency or mutual occlusion, and optimizes multiple object tracks simultaneously. The proposed scheme does not rely on track initialization and complex heuristics. It has much less average complexity than previous efficient exhaustive search methods such as extended dynamic programming and is found to be able to find the global optimum with high probability. We have successfully applied the proposed method to multiple object tracking in video streams.", "title": "A Linear Programming Approach for Multiple Object Tracking"}, "5bae9822d703c585a61575dced83fa2f4dea1c6d": {"paper_id": "5bae9822d703c585a61575dced83fa2f4dea1c6d", "abstract": "In the recent past, the computer vision community has developed centralized benchmarks for the performance evaluation of a variety of tasks, including generic object and pedestrian detection, 3D reconstruction, optical flow, single-object short-term tracking, and stereo estimation. Despite potential pitfalls of such benchmarks, they have proved to be extremely helpful to advance the state of the art in the respective area. Interestingly, there has been rather limited work on the standardization of quantitative benchmarks for multiple target tracking. One of the few exceptions is the well-known PETS dataset [20], targeted primarily at surveillance applications. Despite being widely used, it is often applied inconsistently, for example involving using different subsets of the available data, different ways of training the models, or differing evaluation scripts. This paper describes our work toward a novel multiple object tracking benchmark aimed to address such issues. We discuss the challenges of creating such a framework, collecting existing and new data, gathering state-of-the-art methods to be tested on the datasets, and finally creating a unified evaluation system. With MOTChallenge we aim to pave the way toward a unified evaluation framework for a more meaningful quantification of multi-target tracking.", "title": "MOTChallenge 2015: Towards a Benchmark for Multi-Target Tracking"}, "dc8d0f31c95149af0426ea82f5e082de4b97c2b2": {"paper_id": "dc8d0f31c95149af0426ea82f5e082de4b97c2b2", "abstract": "In recent years, artificial intelligence has achieved great success in many important applications. Both novel machine learning algorithms (e.g., deep neural networks), and their distributed implementations play very critical roles in the success. In this tutorial, we will first review popular machine learning algorithms and the optimization techniques they use. Second, we will introduce widely used ways of parallelizing machine learning algorithms (including both data parallelism and model parallelism, both synchronous and asynchronous parallelization), and discuss their theoretical properties, strengths, and weakness. Third, we will present some recent works that try to improve standard parallelization mechanisms. Last, we will provide some practical examples of parallelizing given machine learning algorithms in online application (e.g. Recommendation and Ranking) by using popular distributed platforms, such as Spark MlLib, DMTK, and Tensorflow. By listening to this tutorial, the audience can form a clear knowledge framework about distributed machine learning, and gain some hands-on experiences on parallelizing a given machine learning algorithm using popular distributed systems.", "title": "Distributed Machine Learning: Foundations, Trends, and Practices"}, "5d3a0eba8e82853c28e315003e131811ebd18143": {"paper_id": "5d3a0eba8e82853c28e315003e131811ebd18143", "abstract": "We provide stronger and more general primal-dual convergence results for FrankWolfe-type algorithms (a.k.a. conditional gradient) for constrained convex optimization, enabled by a simple framework of duality gap certificates. Our analysis also holds if the linear subproblems are only solved approximately (as well as if the gradients are inexact), and is proven to be worst-case optimal in the sparsity of the obtained solutions. On the application side, this allows us to unify a large variety of existing sparse greedy methods, in particular for optimization over convex hulls of an atomic set, even if those sets can only be approximated, including sparse (or structured sparse) vectors or matrices, low-rank matrices, permutation matrices, or max-norm bounded matrices. We present a new general framework for convex optimization over matrix factorizations, where every Frank-Wolfe iteration will consist of a low-rank update, and discuss the broad application areas of this approach.", "title": "Revisiting Frank-Wolfe: Projection-Free Sparse Convex Optimization"}, "9f6a3844158c36ab9f8cd9be4429da52ec404573": {"paper_id": "9f6a3844158c36ab9f8cd9be4429da52ec404573", "abstract": "We consider the problem of selecting grouped variables (factors) for accurate prediction in regression. Such a problem arises naturally in many practical situations with the multifactor analysis-of-variance problem as the most important and well-known example. Instead of selecting factors by stepwise backward elimination, we focus on the accuracy of estimation and consider extensions of the lasso, the LARS algorithm and the non-negative garrotte for factor selection. The lasso, the LARS algorithm and the non-negative garrotte are recently proposed regression methods that can be used to select individual variables. We study and propose efficient algorithms for the extensions of these methods for factor selection and show that these extensions give superior performance to the traditional stepwise backward elimination method in factor selection problems.We study the similarities and the differences between these methods. Simulations and real examples are used to illustrate the methods.", "title": "Model selection and estimation in regression with grouped variables"}, "1267fe36b5ece49a9d8f913eb67716a040bbcced": {"paper_id": "1267fe36b5ece49a9d8f913eb67716a040bbcced", "abstract": "We study the numerical performance of a limited memory quasi Newton method for large scale optimization which we call the L BFGS method We compare its performance with that of the method developed by Buckley and LeNir which combines cyles of BFGS steps and conjugate direction steps Our numerical tests indicate that the L BFGS method is faster than the method of Buckley and LeNir and is better able to use additional storage to accelerate convergence We show that the L BFGS method can be greatly accelerated by means of a simple scaling We then compare the L BFGSmethod with the partitioned quasi Newton method of Griewank and Toint a The results show that for some problems the partitioned quasi Newton method is clearly superior to the L BFGS method However we nd that for other problems the L BFGS method is very competitive due to its low iteration cost We also study the convergence properties of the L BFGS method and prove global convergence on uniformly convex problems", "title": "On the limited memory BFGS method for large scale optimization"}, "515dab07e990cdaf0a60bb009949c8686d109750": {"paper_id": "515dab07e990cdaf0a60bb009949c8686d109750", "abstract": "Distributed optimization algorithms are highly attractive for solving big data problems. In particular, many machine learning problems can be formulated as the global consensus optimization problem, which can then be solved in a distributed manner by the alternating direction method of multipliers (ADMM) algorithm. However, this suffers from the straggler problem as its updates have to be synchronized. In this paper, we propose an asynchronous ADMM algorithm by using two conditions to control the asynchrony: partial barrier and bounded delay. The proposed algorithm has a simple structure and good convergence guarantees (its convergence rate can be reduced to that of its synchronous counterpart). Experiments on different distributed ADMM applications show that asynchrony reduces the time on network waiting, and achieves faster convergence than its synchronous counterpart in terms of the wall clock time.", "title": "Asynchronous Distributed ADMM for Consensus Optimization"}, "153adef6b1ee85833fb4261b3c5f616fc06c5a1e": {"paper_id": "153adef6b1ee85833fb4261b3c5f616fc06c5a1e", "abstract": "In this paper we discuss the development and use of low-rank approximate nonnegative matrix factorization (NMF) algorithms for feature extraction and identification in the fields of text mining and spectral data analysis. The evolution and convergence properties of hybrid methods based on both sparsity and smoothness constraints for the resulting nonnegative matrix factors are discussed. The interpretability of NMF outputs in specific contexts are provided along with opportunities for future work in the modification of NMF algorithms for large-scale and time-varying datasets.", "title": "Algorithms and applications for approximate nonnegative matrix factorization"}, "1617cfa2103e8d5dd33e65401327a3260c28d059": {"paper_id": "1617cfa2103e8d5dd33e65401327a3260c28d059", "abstract": "The Alternating Direction Method of Multipliers (ADMM) has received lots of attention recently due to the tremendous demand from large-scale and data-distributed machine learning applications. In this paper, we present a stochastic setting for optimization problems with non-smooth composite objective functions. To solve this problem, we propose a stochastic ADMM algorithm. Our algorithm applies to a more general class of convex and nonsmooth objective functions, beyond the smooth and separable least squares loss used in lasso. We also demonstrate the rates of convergence for our algorithm under various structural assumptions of the stochastic function: O(1/ \u221a t) for convex functions and O(log t/t) for strongly convex functions. Compared to previous literature, we establish the convergence rate of ADMM for convex problems in terms of both the objective value and the feasibility violation. A novel application named GraphGuided SVM is proposed to demonstrate the usefulness of our algorithm.", "title": "Stochastic Alternating Direction Method of Multipliers"}, "1aa84de3a79f1fea7638ce79b200c10ceb8a4495": {"paper_id": "1aa84de3a79f1fea7638ce79b200c10ceb8a4495", "abstract": "We consider a network of agents that are cooperatively solving a global optimization problem, where the objective function is the sum of privately known local objective functions of the agents and the decision variables are coupled via linear constraints. Recent literature focused on special cases of this formulation and studied their distributed solution through either subgradient based methods with O(1/\u221ak) rate of convergence (where k is the iteration number) or Alternating Direction Method of Multipliers (ADMM) based methods, which require a synchronous implementation and a globally known order on the agents. In this paper, we present a novel asynchronous ADMM based distributed method for the general formulation and show that it converges at the rate O (1=k).", "title": "On the O(1=k) convergence of asynchronous distributed alternating Direction Method of Multipliers"}, "6a75182ccf3738cc57e8dd069fe45c8694ec383c": {"paper_id": "6a75182ccf3738cc57e8dd069fe45c8694ec383c", "abstract": "The question of how to incorporate curvature information in stochastic approximation methods is challenging. The direct application of classical quasiNewton updating techniques for deterministic optimization leads to noisy curvature estimates that have harmful effects on the robustness of the iteration. In this paper, we propose a stochastic quasi-Newton method that is efficient, robust and scalable. It employs the classical BFGS update formula in its limited memory form, and is based on the observation that it is beneficial to collect curvature information pointwise, and at regular intervals, through (sub-sampled) Hessian-vector products. This technique differs from the classical approach that would compute differences of gradients at every iteration, and where controlling the quality of the curvature estimates can be difficult. We present numerical results on problems arising in machine learning that suggest that the proposed method shows much promise. \u2217Department of Computer Science, University of Colorado, Boulder, CO, USA. This author was supported by National Science Foundation grant DMS-1216554 and Department of Energy grant DE-SC0001774. \u2020Department of Engineering Sciences and Applied Mathematics, Northwestern University, Evanston, IL, USA. This author was supported by National Science Foundation grant DMS-0810213. \u2021Department of Industrial Engineering and Management Sciences, Northwestern University, Evanston, IL, USA. This material is based upon work supported by the U.S. Department of Energy Office of Science, Office of Advanced Scientific Computing Research, Applied Mathematics program under Award Number FG02-87ER25047. This author was also supported by the Office of Naval Research award N000141410313. \u00a7Google Research", "title": "A Stochastic Quasi-Newton Method for Large-Scale Optimization"}, "2e9de0de9aa8ab46a9d3e20fe21472104f42cbbe": {"paper_id": "2e9de0de9aa8ab46a9d3e20fe21472104f42cbbe", "abstract": "TheSGD-QN algorithm is a stochastic gradient descent algorithm that m akes careful use of secondorder information and splits the parameter update into inde pendently scheduled components. Thanks to this design, SGD-QN iterates nearly as fast as a first-order stochastic gradient escent but requires less iterations to achieve the same accuracy. This algorith m won the \u201cWild Track\u201d of the first PASCAL Large Scale Learning Challenge (Sonnenburg et al., 2008 ).", "title": "SGD-QN: Careful Quasi-Newton Stochastic Gradient Descent"}, "5c3785bc4dc07d7e77deef7e90973bdeeea760a5": {"paper_id": "5c3785bc4dc07d7e77deef7e90973bdeeea760a5", "abstract": "Mart\u0131\u0301n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Man\u00e9, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi\u00e9gas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng Google Research\u2217 Abstract", "title": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems"}, "14318685b5959b51d0f1e3db34643eb2855dc6d9": {"paper_id": "14318685b5959b51d0f1e3db34643eb2855dc6d9", "abstract": "We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.", "title": "Going deeper with convolutions"}, "80d800dfadbe2e6c7b2367d9229cc82912d55889": {"paper_id": "80d800dfadbe2e6c7b2367d9229cc82912d55889", "abstract": "I present a new way to parallelize the training of convolutional neural networks across multiple GPUs. The method scales significantly better than all alternatives when applied to modern convolutional neural", "title": "One weird trick for parallelizing convolutional neural networks"}, "0122e063ca5f0f9fb9d144d44d41421503252010": {"paper_id": "0122e063ca5f0f9fb9d144d44d41421503252010", "abstract": "Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestlysized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm.", "title": "Large Scale Distributed Deep Networks"}, "41e71c53ca2a7be0ba90919af8f3049d957e665e": {"paper_id": "41e71c53ca2a7be0ba90919af8f3049d957e665e", "abstract": "This paper introduces CIEL, a universal execution engine for distributed data-flow programs. Like previous execution engines, CIEL masks the complexity of distributed programming. Unlike those systems, a CIEL job can make data-dependent control-flow decisions, which enables it to compute iterative and recursive algorithms. We have also developed Skywriting, a Turingcomplete scripting language that runs directly on CIEL. The execution engine provides transparent fault tolerance and distribution to Skywriting scripts and highperformance code written in other programming languages. We have deployed CIEL on a cloud computing platform, and demonstrate that it achieves scalable performance for both iterative and non-iterative algorithms.", "title": "CIEL: A Universal Execution Engine for Distributed Data-Flow Computing"}, "043afbd936c95d0e33c4a391365893bd4102f1a7": {"paper_id": "043afbd936c95d0e33c4a391365893bd4102f1a7", "abstract": "Large deep neural network models have recently demonstrated state-of-the-art accuracy on hard visual recognition tasks. Unfortunately such models are extremely time consuming to train and require large amount of compute cycles. We describe the design and implementation of a distributed system called Adam comprised of commodity server machines to train such models that exhibits world-class performance, scaling and task accuracy on visual recognition tasks. Adam achieves high efficiency and scalability through whole system co-design that optimizes and balances workload computation and communication. We exploit asynchrony throughout the system to improve performance and show that it additionally improves the accuracy of trained models. Adam is significantly more efficient and scalable than was previously thought possible and used 30x fewer machines to train a large 2 billion connection model to 2x higher accuracy in comparable time on the ImageNet 22,000 category image classification task than the system that previously held the record for this benchmark. We also show that task accuracy improves with larger models. Our results provide compelling evidence that a distributed systems-driven approach to deep learning using current training algorithms is worth pursuing.", "title": "Project Adam: Building an Efficient and Scalable Deep Learning Training System"}, "81b7dcaef4a53daab41658a4d1e97972d04b3384": {"paper_id": "81b7dcaef4a53daab41658a4d1e97972d04b3384", "abstract": "We present a library that provides optimized implementations for deep learning primitives. Deep learning workloads are computationally intensive, and optimizing the kernels of deep learning workloads is difficult and time-consuming. As parallel architectures evolve, kernels must be reoptimized for new processors, which makes maintaining codebases difficult over time. Similar issues have long been addressed in the HPC community by libraries such as the Basic Linear Algebra Subroutines (BLAS) [2]. However, there is no analogous library for deep learning. Without such a library, researchers implementing deep learning workloads on parallel processors must create and optimize their own implementations of the main computational kernels, and this work must be repeated as new parallel processors emerge. To address this problem, we have created a library similar in intent to BLAS, with optimized routines for deep learning workloads. Our implementation contains routines for GPUs, and similarly to the BLAS library, could be implemented for other platforms. The library is easy to integrate into existing frameworks, and provides optimized performance and memory usage. For example, integrating cuDNN into Caffe, a popular framework for convolutional networks, improves performance by 36% on a standard model while also reducing memory consumption.", "title": "cuDNN: Efficient Primitives for Deep Learning"}, "bef1beb71c80e79e84f5c428fb5a0a527c1d78b4": {"paper_id": "bef1beb71c80e79e84f5c428fb5a0a527c1d78b4", "abstract": "Stochastic gradient descent (SGD) is a widely used optimization algorithm in machine learning. In order to accelerate the convergence of SGD, a few advanced techniques have been developed in recent years, including variance reduction, stochastic coordinate sampling, and Nesterov\u2019s acceleration method. Furthermore, in order to improve the training speed and/or leverage larger-scale training data, asynchronous parallelization of SGD has also been studied. Then, a natural question is whether these techniques can be seamlessly integrated with each other, and whether the integration has desirable theoretical guarantee on its convergence. In this paper, we provide our formal answer to this question. In particular, we consider the asynchronous parallelization of SGD, accelerated by leveraging variance reduction, coordinate sampling, and Nesterov\u2019s method. We call the new algorithm asynchronous accelerated SGD (AASGD). Theoretically, we proved a convergence rate of AASGD, which indicates that (i) the three acceleration methods are complementary to each other and can make their own contributions to the improvement of convergence rate; (ii) asynchronous parallelization does not hurt the convergence rate, and can achieve considerable speedup under appropriate parameter setting. Empirically, we tested AASGD on a few benchmark datasets. The experimental results verified our theoretical findings and indicated that AASGD could be a highly effective and efficient algorithm for practical use.", "title": "Asynchronous Accelerated Stochastic Gradient Descent"}, "a58c2fa2f5539592225af87274be7c6cf4f156dc": {"paper_id": "a58c2fa2f5539592225af87274be7c6cf4f156dc", "abstract": "Regularized risk minimization often involves non-smooth o ptimization, either because of the loss function (e.g., hinge loss) or the regulari zer (e.g.,l1-regularizer). Gradient methods, though highly scalable and easy to implem ent, are known to converge slowly. In this paper, we develop a novel accelerat ed gradient method for stochastic optimization while still preserving their c omputational simplicity and scalability. The proposed algorithm, called SAGE (Stoc hastic Accelerated GradiEnt), exhibits fast convergence rates on stochastic c omposite optimization with convex or strongly convex objectives. Experimental re sults show that SAGE is faster than recent (sub)gradient methods including FOLO S, SMIDAS and SCD. Moreover, SAGE can also be extended for online learning, res ulting in a simple algorithm but with the best regret bounds currently known fo r these problems.", "title": "Accelerated Gradient Methods for Stochastic Optimization and Online Learning"}, "2adf43d278581192e58c2e0e1557eb4fc3c13eb3": {"paper_id": "2adf43d278581192e58c2e0e1557eb4fc3c13eb3", "abstract": "We consider regularized empirical risk minimization problems. In particular, we minimize the sum of a smooth empirical risk function and a nonsmooth regularization function. When the regularization function is block separable, we can solve the minimization problems in a randomized block coordinate descent (RBCD) manner. Existing RBCD methods usually decrease the objective value by exploiting the partial gradient of a randomly selected block of coordinates in each iteration. Thus they need all data to be accessible so that the partial gradient of the block gradient can be exactly obtained. However, such a \"batch\" setting may be computationally expensive in practice. In this paper, we propose a mini-batch randomized block coordinate descent (MRBCD) method, which estimates the partial gradient of the selected block based on a mini-batch of randomly sampled data in each iteration. We further accelerate the MRBCD method by exploiting the semi-stochastic optimization scheme, which effectively reduces the variance of the partial gradient estimators. Theoretically, we show that for strongly convex functions, the MRBCD method attains lower overall iteration complexity than existing RBCD methods. As an application, we further trim the MRBCD method to solve the regularized sparse learning problems. Our numerical experiments shows that the MRBCD method naturally exploits the sparsity structure and achieves better computational performance than existing methods.", "title": "Accelerated Mini-batch Randomized Block Coordinate Descent Method"}, "09396d113a7f5ce282574ff8aa02bf93003bee03": {"paper_id": "09396d113a7f5ce282574ff8aa02bf93003bee03", "abstract": "We present a novel Newton-type method for distributed optimization, which is particularly well suited for stochastic optimization and learning problems. For quadratic objectives, the method enjoys a linear rate of convergence which provably improves with the data size, requiring an essentially constant number of iterations under reasonable assumptions. We provide theoretical and empirical evidence of the advantages of our method compared to other approaches, such as one-shot parameter averaging and ADMM.", "title": "Communication Efficient Distributed Optimization using an Approximate Newton-type Method"}, "77e9dcd17ee6f5d85ee0ac6e96617d10e883d600": {"paper_id": "77e9dcd17ee6f5d85ee0ac6e96617d10e883d600", "abstract": "This thesis proposes to analyse symbolic musical data under a statistical viewpoint, using state-of-the-art machine learning techniques. Our main argument is to show that it is possible to design generative models that are able to predict and to generate music given arbitrary contexts in a genre similar to a training corpus, using a minimal amount of data. For instance, a carefully designed generative model could guess what would be a good accompaniment for a given melody. Conversely, we propose generative models in this thesis that can be sampled to generate realistic melodies given harmonic context. Most computer music research has been devoted so far to the direct modeling of audio data. However, most of the music models today do not consider the musical structure at all. We argue that reliable symbolic music models such a the ones presented in this thesis could dramatically improve the performance of audio algorithms applied in more general contexts. Hence, our main contributions in this thesis are three-fold: \u2022 We have shown empirically that long term dependencies are present in music data and we provide quantitative measures of such dependencies; \u2022 We have shown empirically that using domain knowledge allows to capture long term dependencies in music signal better than with standard statistical models for temporal data. We describe many probabilistic models aimed to capture various aspects of symbolic polyphonic music. Such models can be used for music prediction. Moreover, these models can be sampled to generate realistic music sequences; \u2022 We designed various representations for music that could be used as observations by the proposed probabilistic models.", "title": "Probabilistic models for music"}, "0f9b608cd19afeb083e0244df4cd0db1a00e029b": {"paper_id": "0f9b608cd19afeb083e0244df4cd0db1a00e029b", "abstract": "We present a technique for constructing random elds from a set of training samples. The learning paradigm builds increasingly complex elds by allowing potential functions, or features, that are supported by increasingly large subgraphs. Each feature has a weight that is trained by minimizing the Kullback-Leibler divergence between the model and the empirical distribution of the training data. A greedy algorithm determines how features are incrementally added to the eld and an iterative scaling algorithm is used to estimate the optimal values of the weights. The random eld models and techniques introduced in this paper di er from those common to much of the computer vision literature in that the underlying random elds are nonMarkovian and have a large number of parameters that must be estimated. Relations to other learning approaches including decision trees and Boltzmann machines are given. As a demonstration of the method, we describe its application to the problem of automatic word classi cation in natural language processing.", "title": "Inducing Features of Random Fields"}, "2e3e09e48a7a62dc30efd8ef7fc4665a53e84d7a": {"paper_id": "2e3e09e48a7a62dc30efd8ef7fc4665a53e84d7a", "abstract": "The computotionol power of massively parallel networks of simple processing elements resides in the communication bandwidth provided by the hardware connections between elements. These connections con allow a significant fraction of the knowledge of the system to be applied to an instance of a problem in o very short time. One kind of computation for which massively porollel networks appear to be well suited is large constraint satisfaction searches, but to use the connections efficiently two conditions must be met: First, a search technique that is suitable for parallel networks must be found. Second, there must be some way of choosing internal representations which allow the preexisting hardware connections to be used efficiently for encoding the constraints in the domain being searched. We describe a generol parallel search method, based on statistical mechanics, and we show how it leads to a general learning rule for modifying the connection strengths so as to incorporate knowledge obout o task domain in on efficient way. We describe some simple examples in which the learning algorithm creates internal representations thot ore demonstrobly the most efficient way of using the preexisting connectivity structure.", "title": "A Learning Algorithm for Boltzmann Machines"}, "783480acff435bfbc15ffcdb4f15eccddaa0c810": {"paper_id": "783480acff435bfbc15ffcdb4f15eccddaa0c810", "abstract": "We address the problem of predicting a word from previous words in a sample of text. In particular, we discuss n-gram models based on classes of words. We also discuss several statistical algorithms for assigning words to classes based on the frequency of their co-occurrence with other words. We find that we are able to extract classes that have the flavor of either syntactically based groupings or semantically based groupings, depending on the nature of the underlying statistics.", "title": "Class-Based n-gram Models of Natural Language"}, "4af182338ee63754d4569c26cb6a5c3bbdd8cf2a": {"paper_id": "4af182338ee63754d4569c26cb6a5c3bbdd8cf2a", "abstract": "The concept of maximum entropy can be traced back along multiple threads to Biblical times Only recently however have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition In this paper we describe a method for statistical modeling based on maximum entropy We present a maximum likelihood approach for automatically con structing maximum entropy models and describe how to implement this approach e ciently using as examples several problems in natural language processing", "title": "A Maximum Entropy Approach to Natural Language Processing"}, "400489bfda2b1973a31b944b0fedfc681f0e31c1": {"paper_id": "400489bfda2b1973a31b944b0fedfc681f0e31c1", "abstract": null, "title": "A Semantic Analysis Of Kisukuma Spatial Prepositions Using Image Schema Theory"}, "65a3cfd5bfbd74856a4a03645753dba12c8150db": {"paper_id": "65a3cfd5bfbd74856a4a03645753dba12c8150db", "abstract": "s, bibliographische Angaben. T\u00fcbingen: Niemeyer. Wotjak, Gerd (1977) Untersuchungen zur Struktur der Bedeutung: Ein Beitrag zu Gegenstand und Methode der modernen Bedeutungsforschung unter besonderer Ber\u00fccksichtigung der semantischen Konstituentenanalyse. 2nd edn. Berlin: Akademie. Wunderlich, Dieter (1991) How do prepositional phrases fit into compositional syntax and semantics? Linguistics 29: 591\u2013621. (1993) On German um: semantic and conceptual aspects. Linguistics 31: 111\u201333. Wundt, Wilhelm (1900) V\u00f6lkerpsychologie: Eine Untersuchung der Entwicklungsgesetze von Sprache, Mythus und Sitte. Leipzig: Kr\u00f6ner. Yu, Ning (1998) The Contemporary Theory of Metaphor: A Perspective from Chinese. Amsterdam: Benjamins. (2009) The Chinese Heart in a Cognitive Perspective: Culture, Body, and Language. Berlin: Mouton de Gruyter. Zauner, Adolf (1903) Die romanischen Namen der K\u00f6rperteile: eine onomasiologische Studie. Romanische Forschungen 14: 339\u2013530. Zelinsky-Wibbelt, Cornelia (ed.) (1993) The Semantics of Prepositions: From Mental Processing to Natural Language Processing. Berlin: Mouton de Gruyter. Ziemke, Tom, Jordan Zlatev, and Roslyn M. Frank (eds.) (2007) Body, Language, and Mind 1: Embodiment. Berlin: Mouton de Gruyter. Zlatev, Jordan (2003) Polysemy or generality? Mu. In Cuyckens et al. (2003: 447\u201394). (2005) What\u2019s in a schema? Bodily mimesis and the grounding of language. In Beate Hampe (ed.), From Perception to Meaning: Image Schemas in Cognitive Linguistics, 313\u201342. Berlin: Mouton de Gruyter. Zwicky, Arnold, and Jerry Sadock (1975) Ambiguity tests and how to fail them. In John Kimball (ed.), Syntax and Semantics 4, 1\u201336. New York: Academic Press.", "title": "Theories of lexical semantics"}, "0dd6795ae207ae4bc455c9ac938c3eebd84897c8": {"paper_id": "0dd6795ae207ae4bc455c9ac938c3eebd84897c8", "abstract": "The $64,000 question in computational linguistics these days is: \u201cWhat should I read to learn about statistical natural language processing?\u201d I have been asked this question over and over, and each time I have given basically the same reply: there is no text that addresses this topic directly, and the best one can do is find a good probability-theory textbook and a good information-theory textbook, and supplement those texts with an assortment of conference papers and journal articles. Understanding the disappointment this answer provoked, I was delighted to hear that someone had finally written a bookdirectly addressing this topic. However, after reading Eugene Charniak\u2019s Statistical Language Learning, I have very mixed feelings about the impact this bookmight have on the ever-growing field of statistical NLP. The book begins with a very brief description of the classic artificial intelligence approach toNLP (chapter 1), including morphology, syntax, semantics, and pragmatics. It presents a few definitions from probability theory and information theory (chapter 2), then proceeds to introduce hidden Markov models (chapters 3\u20134) and probabilistic context-free grammars (chapters 5\u20136). The book concludes with a few chapters discussing advanced topics in statistical language learning, such as grammar induction (chapter 7), syntactic disambiguation (chapter 8), word clustering (chapter 9), andword sense disambiguation (chapter 10). To its credit, the book serves as an interesting popular discussion of statistical modeling in NLP. It is well-written and entertaining, and very accessible to the reader with a limited mathematical background. It presents a good selection of statistical NLP topics to introduce the reader to the field. And the descriptions of the forward-backward algorithm for hidden Markov models and the inside-outside algorithm for probabilistic context-free grammars are intuitive and easy to follow. However, as a resource for someone interested in entering this area of research, this book falls far short of its author\u2019s goals. These goals are clearly stated in the preface:", "title": "Statistical language learning"}, "3661a34f302883c759b9fa2ce03de0c7173d2bb2": {"paper_id": "3661a34f302883c759b9fa2ce03de0c7173d2bb2", "abstract": "In this work, we present a novel peak-piloted deep network (PPDN) that uses a sample with peak expression (easy sample) to supervise the intermediate feature responses for a sample of non-peak expression (hard sample) of the same type and from the same subject. The expression evolving process from nonpeak expression to peak expression can thus be implicitly embedded in the network to achieve the invariance to expression intensities.", "title": "Peak-Piloted Deep Network for Facial Expression Recognition"}, "0359f7357ea8191206b9da45298902de9f054c92": {"paper_id": "0359f7357ea8191206b9da45298902de9f054c92", "abstract": "Automated Facial Expression Recognition (FER) has remained a challenging and interesting problem in computer vision. Despite efforts made in developing various methods for FER, existing approaches lack generalizability when applied to unseen images or those that are captured in wild setting (i.e. the results are not significant). Most of the existing approaches are based on engineered features (e.g. HOG, LBPH, and Gabor) where the classifier's hyper-parameters are tuned to give best recognition accuracies across a single database, or a small collection of similar databases. This paper proposes a deep neural network architecture to address the FER problem across multiple well-known standard face datasets. Specifically, our network consists of two convolutional layers each followed by max pooling and then four Inception layers. The network is a single component architecture that takes registered facial images as the input and classifies them into either of the six basic or the neutral expressions. We conducted comprehensive experiments on seven publicly available facial expression databases, viz. MultiPIE, MMI, CK+, DISFA, FERA, SFEW, and FER2013. The results of our proposed architecture are comparable to or better than the state-of-the-art methods and better than traditional convolutional neural networks in both accuracy and training time.", "title": "Going deeper in facial expression recognition using deep neural networks"}, "33c68d6bc73e74ea042dc5b9fe966625565c475e": {"paper_id": "33c68d6bc73e74ea042dc5b9fe966625565c475e", "abstract": "Recently, fully-connected and convolutional neural networks have been trained to achieve state-of-the-art performance on a wide variety of tasks such as speech recognition, image classification, natural language processing, and bioinformatics. For classification tasks, most of these \u201cdeep learning\u201d models employ the softmax activation function for prediction and minimize cross-entropy loss. In this paper, we demonstrate a small but consistent advantage of replacing the softmax layer with a linear support vector machine. Learning minimizes a margin-based loss instead of the cross-entropy loss. While there have been various combinations of neural nets and SVMs in prior art, our results using L2-SVMs show that by simply replacing softmax with linear SVMs gives significant gains on popular deep learning datasets MNIST, CIFAR-10, and the ICML 2013 Representation Learning Workshop\u2019s face expression recognition challenge.", "title": "Deep Learning using Linear Support Vector Machines"}, "4d9a02d080636e9666c4d1cc438b9893391ec6c7": {"paper_id": "4d9a02d080636e9666c4d1cc438b9893391ec6c7", "abstract": "In 2000, the Cohn-Kanade (CK) database was released for the purpose of promoting research into automatically detecting individual facial expressions. Since then, the CK database has become one of the most widely used test-beds for algorithm development and evaluation. During this period, three limitations have become apparent: 1) While AU codes are well validated, emotion labels are not, as they refer to what was requested rather than what was actually performed, 2) The lack of a common performance metric against which to evaluate new algorithms, and 3) Standard protocols for common databases have not emerged. As a consequence, the CK database has been used for both AU and emotion detection (even though labels for the latter have not been validated), comparison with benchmark algorithms is missing, and use of random subsets of the original database makes meta-analyses difficult. To address these and other concerns, we present the Extended Cohn-Kanade (CK+) database. The number of sequences is increased by 22% and the number of subjects by 27%. The target expression for each sequence is fully FACS coded and emotion labels have been revised and validated. In addition to this, non-posed sequences for several types of smiles and their associated metadata have been added. We present baseline results using Active Appearance Models (AAMs) and a linear support vector machine (SVM) classifier using a leave-one-out subject cross-validation for both AU and emotion detection for the posed data. The emotion and AU labels, along with the extended image data and tracked landmarks will be made available July 2010.", "title": "The Extended Cohn-Kanade Dataset (CK+): A complete dataset for action unit and emotion-specified expression"}, "853bd61bc48a431b9b1c7cab10c603830c488e39": {"paper_id": "853bd61bc48a431b9b1c7cab10c603830c488e39", "abstract": "Pushing by big data and deep convolutional neural network (CNN), the performance of face recognition is becoming comparable to human. Using private large scale training datasets, several groups achieve very high performance on LFW, i.e., 97% to 99%. While there are many open source implementations of CNN, none of large scale face dataset is publicly available. The current situation in the field of face recognition is that data is more important than algorithm. To solve this problem, this paper proposes a semi-automatical way to collect face images from Internet and builds a large scale dataset containing about 10,000 subjects and 500,000 images, called CASIAWebFace. Based on the database, we use a 11-layer CNN to learn discriminative representation and obtain state-of-theart accuracy on LFW and YTF. The publication of CASIAWebFace will attract more research groups entering this field and accelerate the development of face recognition in the wild.", "title": "Learning Face Representation from Scratch"}, "560e0e58d0059259ddf86fcec1fa7975dee6a868": {"paper_id": "560e0e58d0059259ddf86fcec1fa7975dee6a868", "abstract": "Recognizing faces in unconstrained videos is a task of mounting importance. While obviously related to face recognition in still images, it has its own unique characteristics and algorithmic requirements. Over the years several methods have been suggested for this problem, and a few benchmark data sets have been assembled to facilitate its study. However, there is a sizable gap between the actual application needs and the current state of the art. In this paper we make the following contributions. (a) We present a comprehensive database of labeled videos of faces in challenging, uncontrolled conditions (i.e., \u2018in the wild\u2019), the \u2018YouTube Faces\u2019 database, along with benchmark, pair-matching tests1. (b) We employ our benchmark to survey and compare the performance of a large variety of existing video face recognition techniques. Finally, (c) we describe a novel set-to-set similarity measure, the Matched Background Similarity (MBGS). This similarity is shown to considerably improve performance on the benchmark tests.", "title": "Face recognition in unconstrained videos with matched background similarity"}, "4d423acc78273b75134e2afd1777ba6d3a398973": {"paper_id": "4d423acc78273b75134e2afd1777ba6d3a398973", "abstract": "Between October 2000 and December 2000 we collected a database of over 40,000 facial images of 68 people. Using the CMU 3D Room we imaged each person across 13 different poses, under 43 different illumination conditions, and with 4 different expressions. We call this database the CMU Pose, Illumination, and Expression (PIE) database. In this paper we describe the imaging hardware, the collection procedure, the organization of the database, several potential uses of the database, and how to obtain the database.", "title": "The CMU Pose, Illumination, and Expression (PIE) Database"}, "a538b05ebb01a40323997629e171c91aa28b8e2f": {"paper_id": "a538b05ebb01a40323997629e171c91aa28b8e2f", "abstract": "Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these \u201cStepped Sigmoid Units\u201d are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.", "title": "Rectified Linear Units Improve Restricted Boltzmann Machines"}, "600f164c81dbaa0327e7bd659fd9eb7f511f9e9a": {"paper_id": "600f164c81dbaa0327e7bd659fd9eb7f511f9e9a", "abstract": "Many efforts have been made in recent years to tackle the unconstrained face recognition challenge. For the benchmark of this challenge, the Labeled Faces in theWild (LFW) database has been widely used. However, the standard LFW protocol is very limited, with only 3,000 genuine and 3,000 impostor matches for classification. Today a 97% accuracy can be achieved with this benchmark, remaining a very limited room for algorithm development. However, we argue that this accuracy may be too optimistic because the underlying false accept rate may still be high (e.g. 3%). Furthermore, performance evaluation at low FARs is not statistically sound by the standard protocol due to the limited number of impostor matches. Thereby we develop a new benchmark protocol to fully exploit all the 13,233 LFW face images for large-scale unconstrained face recognition evaluation under both verification and open-set identification scenarios, with a focus at low FARs. Based on the new benchmark, we evaluate 21 face recognition approaches by combining 3 kinds of features and 7 learning algorithms. The benchmark results show that the best algorithm achieves 41.66% verification rates at FAR=0.1%, and 18.07% open-set identification rates at rank 1 and FAR=1%. Accordingly we conclude that the large-scale unconstrained face recognition problem is still largely unresolved, thus further attention and effort is needed in developing effective feature representations and learning algorithms. We thereby release a benchmark tool to advance research in this field.", "title": "A benchmark study of large-scale unconstrained face recognition"}, "2116b2eaaece4af9c28c32af2728f3d49b792cf9": {"paper_id": "2116b2eaaece4af9c28c32af2728f3d49b792cf9", "abstract": "When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This overfitting is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random \u201cdropout\u201d gives big improvements on many benchmark tasks and sets new records for speech and object recognition. A feedforward, artificial neural network uses layers of non-linear hidden units between its inputs and its outputs. By adapting the weights on the incoming connections of these hidden units it learns feature detectors that enable it to predict the correct output when given an input vector [15]. If the relationship between the input and the correct output is complicated and the network has enough hidden units to model it accurately, there will typically be many different settings of the weights that can model the training set almost perfectly, especially if there is only a limited amount of labeled training data. Each of these weight vectors will make different predictions on held-out test data and almost all of them will do worse on the test data than on the training data because the feature detectors have been tuned to work well together on the training data but not on the test data. Overfitting can be reduced by using \u201cdropout\u201d to prevent complex co-adaptations on the training data. On each presentation of each training case, each hidden unit is randomly omitted from the network with a probability of 0.5, so a hidden unit cannot rely on other hidden units being present. Another way to view the dropout procedure is as a very efficient way of performing model averaging with neural networks. A good way to reduce the error on the test set is to average the predictions produced by a very large number of different networks. The standard way to do this is to train many separate networks and then to apply each of these networks to the test data, but this is computationally expensive during both training and testing. Random dropout makes it possible to train a huge number of different networks in a reasonable time. There is almost certainly a different network for each presentation of each training case but all of these networks share the same weights for the hidden units that are present. We use the standard, stochastic gradient descent procedure for training the dropout neural networks on mini-batches of training cases, but we modify the penalty term that is normally used to prevent the weights from growing too large. Instead of penalizing the squared length (L2 norm) of the whole weight vector, we set an upper bound on the L2 norm of the incoming weight vector for each individual hidden unit. If a weight-update violates this constraint, we renormalize the weights of the hidden unit by division. Using a constraint rather than a penalty prevents weights from growing very large no matter how large the proposed weight-update is. This makes it possible to start with a", "title": "Improving neural networks by preventing co-adaptation of feature detectors"}, "bbbd015155bbe5098aad6b49a548e9f3570e49ec": {"paper_id": "bbbd015155bbe5098aad6b49a548e9f3570e49ec", "abstract": "This paper introduces a novel Gabor-Fisher (1936) classifier (GFC) for face recognition. The GFC method, which is robust to changes in illumination and facial expression, applies the enhanced Fisher linear discriminant model (EFM) to an augmented Gabor feature vector derived from the Gabor wavelet representation of face images. The novelty of this paper comes from 1) the derivation of an augmented Gabor feature vector, whose dimensionality is further reduced using the EFM by considering both data compression and recognition (generalization) performance; 2) the development of a Gabor-Fisher classifier for multi-class problems; and 3) extensive performance evaluation studies. In particular, we performed comparative studies of different similarity measures applied to various classifiers. We also performed comparative experimental studies of various face recognition schemes, including our novel GFC method, the Gabor wavelet method, the eigenfaces method, the Fisherfaces method, the EFM method, the combination of Gabor and the eigenfaces method, and the combination of Gabor and the Fisherfaces method. The feasibility of the new GFC method has been successfully tested on face recognition using 600 FERET frontal face images corresponding to 200 subjects, which were acquired under variable illumination and facial expressions. The novel GFC method achieves 100% accuracy on face recognition using only 62 features.", "title": "Gabor feature based classification using the enhanced fisher linear discriminant model for face recognition"}, "b8084d5e193633462e56f897f3d81b2832b72dff": {"paper_id": "b8084d5e193633462e56f897f3d81b2832b72dff", "abstract": "The state-of-the-art of face recognition has been significantly advanced by the emergence of deep learning. Very deep neural networks recently achieved great success on general object recognition because of their superb learning capacity. This motivates us to investigate their effectiveness on face recognition. This paper proposes two very deep neural network architectures, referred to as DeepID3, for face recognition. These two architectures are rebuilt from stacked convolution and inception layers proposed in VGG net [10] and GoogLeNet [16] to make them suitable to face recognition. Joint face identification-verification supervisory signals are added to both intermediate and final feature extraction layers during training. An ensemble of the proposed two architectures achieves 99.53% LFW face verification accuracy and 96.0% LFW rank-1 face identification accuracy, respectively. A further discussion of LFW face verification result is given in the end.", "title": "DeepID3: Face Recognition with Very Deep Neural Networks"}, "370b5757a5379b15e30d619e4d3fb9e8e13f3256": {"paper_id": "370b5757a5379b15e30d619e4d3fb9e8e13f3256", "abstract": "Face recognition has benefitted greatly from the many databases that have been produced to study it. Most of these databases have been created under controlled conditions t o facilitate the study of specific parameters on the face recognition problem. These parameters include such variables as position, pose, lighting, expression, background, camera quality, occlusion , age, and gender. While there are many applications for face recognition technology in which one can control the parameters of image acquisition, there are also many applications in which the practitioner has little or no control over such parameters. This database is provided as an aid in studying the latter, unconstrained, face recognition problem. The database represents an initial attempt to provide a set of labeled face photographs spanning the range of conditions typically encountered by people in their everyday lives. The database exhibits \u201cnatural\u201d variability in pose, lighting, focus, resolution, facial expression, age, gender, race, acces sories, make-up, occlusions, background, and photographic quality. Despite this variability, the images in the database are presented in a simple and consistent format for maximum ease of use. In addition to describing the details of the database and its acquisition, we provide specific experimental paradigms for which the database is suitable. This is done in an effort to make research performed with the database as consistent and comparable as possible.", "title": "Labeled Faces in the Wild: A Database forStudying Face Recognition in Unconstrained Environments"}, "177bc509dd0c7b8d388bb47403f28d6228c14b5c": {"paper_id": "177bc509dd0c7b8d388bb47403f28d6228c14b5c", "abstract": "This paper proposes to learn a set of high-level feature representations through deep learning, referred to as Deep hidden IDentity features (DeepID), for face verification. We argue that DeepID can be effectively learned through challenging multi-class face identification tasks, whilst they can be generalized to other tasks (such as verification) and new identities unseen in the training set. Moreover, the generalization capability of DeepID increases as more face classes are to be predicted at training. DeepID features are taken from the last hidden layer neuron activations of deep convolutional networks (ConvNets). When learned as classifiers to recognize about 10, 000 face identities in the training set and configured to keep reducing the neuron numbers along the feature extraction hierarchy, these deep ConvNets gradually form compact identity-related features in the top layers with only a small number of hidden neurons. The proposed features are extracted from various face regions to form complementary and over-complete representations. Any state-of-the-art classifiers can be learned based on these high-level representations for face verification. 97:45% verification accuracy on LFW is achieved with only weakly aligned faces.", "title": "Deep Learning Face Representation from Predicting 10,000 Classes"}, "41951953579a0e3620f0235e5fcb80b930e6eee3": {"paper_id": "41951953579a0e3620f0235e5fcb80b930e6eee3", "abstract": "The key challenge of face recognition is to develop effective feature representations for reducing intra-personal variations while enlarging inter-personal differences. In this paper, we show that it can be well solved with deep learning and using both face identification and verification signals as supervision. The Deep IDentification-verification features (DeepID2) are learned with carefully designed deep convolutional networks. The face identification task increases the inter-personal variations by drawing DeepID2 features extracted from different identities apart, while the face verification task reduces the intra-personal variations by pulling DeepID2 features extracted from the same identity together, both of which are essential to face recognition. The learned DeepID2 features can be well generalized to new identities unseen in the training data. On the challenging LFW dataset [11], 99.15% face verification accuracy is achieved. Compared with the best previous deep learning result [20] on LFW, the error rate has been significantly reduced by 67%.", "title": "Deep Learning Face Representation by Joint Identification-Verification"}, "061356704ec86334dbbc073985375fe13cd39088": {"paper_id": "061356704ec86334dbbc073985375fe13cd39088", "abstract": "In this work we investigate the effect of the convolutional n etwork depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achi eved by pushing the depth to 16\u201319 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first a nd he second places in the localisation and classification tracks respec tively. We also show that our representations generalise well to other datasets, whe re t y achieve the stateof-the-art results. Importantly, we have made our two bestp rforming ConvNet models publicly available to facilitate further research o n the use of deep visual representations in computer vision.", "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition"}, "03563dfaf4d2cfa397d3c12d742e9669f4e95bab": {"paper_id": "03563dfaf4d2cfa397d3c12d742e9669f4e95bab", "abstract": "This work proposes a learning method for deep architectures that takes advantage of sequential data, in particular from the temporal coherence that naturally exists in unlabeled video recordings. That is, two successive frames are likely to contain the same object or objects. This coherence is used as a supervisory signal over the unlabeled data, and is used to improve the performance on a supervised task of interest. We demonstrate the effectiveness of this method on some pose invariant object and face recognition tasks.", "title": "Deep learning from temporal coherence in video"}, "6d5e12ee5d75d5f8c04a196dd94173f96dc8603f": {"paper_id": "6d5e12ee5d75d5f8c04a196dd94173f96dc8603f", "abstract": "We present a method for training a similarity metric from data. The method can be used for recognition or verification applications where the number of categories is very large and not known during training, and where the number of training samples for a single category is very small. The idea is to learn a function that maps input patterns into a target space such that the L/sub 1/ norm in the target space approximates the \"semantic\" distance in the input space. The method is applied to a face verification task. The learning process minimizes a discriminative loss function that drives the similarity metric to be small for pairs of faces from the same person, and large for pairs from different persons. The mapping from raw to the target space is a convolutional network whose architecture is designed for robustness to geometric distortions. The system is tested on the Purdue/AR face database which has a very high degree of variability in the pose, lighting, expression, position, and artificial occlusions such as dark glasses and obscuring scarves.", "title": "Learning a similarity metric discriminatively, with application to face verification"}, "4e061a302816f5890a621eb278c6efa6e37d7e2f": {"paper_id": "4e061a302816f5890a621eb278c6efa6e37d7e2f", "abstract": "This paper presents a new discriminative deep metric learning (DDML) method for face verification in the wild. Different from existing metric learning-based face verification methods which aim to learn a Mahalanobis distance metric to maximize the inter-class variations and minimize the intra-class variations, simultaneously, the proposed DDML trains a deep neural network which learns a set of hierarchical nonlinear transformations to project face pairs into the same feature subspace, under which the distance of each positive face pair is less than a smaller threshold and that of each negative pair is higher than a larger threshold, respectively, so that discriminative information can be exploited in the deep network. Our method achieves very competitive face verification performance on the widely used LFW and YouTube Faces (YTF) datasets.", "title": "Discriminative Deep Metric Learning for Face Verification in the Wild"}, "14ce7635ff18318e7094417d0f92acbec6669f1c": {"paper_id": "14ce7635ff18318e7094417d0f92acbec6669f1c", "abstract": "In modern face recognition, the conventional pipeline consists of four stages: detect => align => represent => classify. We revisit both the alignment step and the representation step by employing explicit 3D face modeling in order to apply a piecewise affine transformation, and derive a face representation from a nine-layer deep neural network. This deep network involves more than 120 million parameters using several locally connected layers without weight sharing, rather than the standard convolutional layers. Thus we trained it on the largest facial dataset to-date, an identity labeled dataset of four million facial images belonging to more than 4, 000 identities. The learned representations coupling the accurate model-based alignment with the large facial database generalize remarkably well to faces in unconstrained environments, even with a simple classifier. Our method reaches an accuracy of 97.35% on the Labeled Faces in the Wild (LFW) dataset, reducing the error of the current state of the art by more than 27%, closely approaching human-level performance.", "title": "DeepFace: Closing the Gap to Human-Level Performance in Face Verification"}, "4624a22816daf82313f48a978afc7d53ed0ec368": {"paper_id": "4624a22816daf82313f48a978afc7d53ed0ec368", "abstract": "We propose a new technique for direct visual matching of images for the purposes of face recognition and image retrieval, using a probabilistic measure of similarity, based primarily on a Bayesian (MAP) analysis of image differences. The performance advantage of this probabilistic matching technique over standard Euclidean nearest-neighbor eigenface matching was demonstrated using results from DARPA\u2019s 1996 \u201cFERET\u201d face recognition competition, in which this Bayesian matching algorithm was found to be the top performer. In addition, we derive a simple method of replacing costly computation of nonlinear (on-line) Bayesian similarity measures by inexpensive linear (off-line) subspace projections and simple Euclidean norms, thus resulting in a significant computational speed-up for implementation with very large databases. Appears in: Pattern Recognition, Vol. 33, No. 11, pps. 1771-1782, November, 2000. This work may not be copied or reproduced in whole or in part for any commercial purpose. Permission to copy in whole or in part without payment of fee is granted for nonprofit educational and research purposes provided that all such whole or partial copies include the following: a notice that such copying is by permission of Mitsubishi Electric Research Laboratories, Inc.; an acknowledgment of the authors and individual contributions to the work; and all applicable portions of the copyright notice. Copying, reproduction, or republishing for any other purpose shall require a license with payment of fee to Mitsubishi Electric Research Laboratories, Inc. All rights reserved. Copyright c Mitsubishi Electric Research Laboratories, Inc., 2002 201 Broadway, Cambridge, Massachusetts 02139 Appears in: Pattern Recognition, Vol. 33, No. 11, pps. 1771-1782, November, 2000.", "title": "Bayesian face recognition"}, "713f73ce5c3013d9fb796c21b981dc6629af0bd5": {"paper_id": "713f73ce5c3013d9fb796c21b981dc6629af0bd5", "abstract": "Deep Neural Networks (DNNs) have recently shown outstanding performance on image classification tasks [14]. In this paper we go one step further and address the problem of object detection using DNNs, that is not only classifying but also precisely localizing objects of various classes. We present a simple and yet powerful formulation of object detection as a regression problem to object bounding box masks. We define a multi-scale inference procedure which is able to produce high-resolution object detections at a low cost by a few network applications. State-of-the-art performance of the approach is shown on Pascal VOC.", "title": "Deep Neural Networks for Object Detection"}, "0674792f5edac72b77fb1297572c15b153576418": {"paper_id": "0674792f5edac72b77fb1297572c15b153576418", "abstract": "Deep convolutional neural networks have recently achieved state-of-the-art performance on a number of image recognition benchmarks, including the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC-2012). The winning model on the localization sub-task was a network that predicts a single bounding box and a confidence score for each object category in the image. Such a model captures the whole-image context around the objects but cannot handle multiple instances of the same object in the image without naively replicating the number of outputs for each instance. In this work, we propose a saliency-inspired neural network model for detection, which predicts a set of class-agnostic bounding boxes along with a single score for each box, corresponding to its likelihood of containing any object of interest. The model naturally handles a variable number of instances for each class and allows for cross-class generalization at the highest levels of the network. We are able to obtain competitive recognition performance on VOC2007 and ILSVRC2012, while using only the top few predicted locations in each image and a small number of neural network evaluations.", "title": "Scalable Object Detection Using Deep Neural Networks"}, "02a88a2f2765b17c9ea76fe13148b4b8a9050b95": {"paper_id": "02a88a2f2765b17c9ea76fe13148b4b8a9050b95", "abstract": "We propose a method for human pose estimation based on Deep Neural Networks (DNNs). The pose estimation is formulated as a DNN-based regression problem towards body joints. We present a cascade of such DNN regres- sors which results in high precision pose estimates. The approach has the advantage of reasoning about pose in a holistic fashion and has a simple but yet powerful formula- tion which capitalizes on recent advances in Deep Learn- ing. We present a detailed empirical analysis with state-of- art or better performance on four academic benchmarks of diverse real-world images.", "title": "DeepPose: Human Pose Estimation via Deep Neural Networks"}, "009fba8df6bbca155d9e070a9bd8d0959bc693c2": {"paper_id": "009fba8df6bbca155d9e070a9bd8d0959bc693c2", "abstract": "Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.", "title": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation"}, "14b5e8ba23860f440ea83ed4770e662b2a111119": {"paper_id": "14b5e8ba23860f440ea83ed4770e662b2a111119", "abstract": "Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark (Krizhevsky et al., 2012). However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al. on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.", "title": "Visualizing and Understanding Convolutional Networks"}, "5ca4abab527f6b0270e50548f0dea30638c9b86e": {"paper_id": "5ca4abab527f6b0270e50548f0dea30638c9b86e", "abstract": "Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed can nevertheless remain finite: for a special class of initial conditions on the weights, very deep networks incur only a finite, depth independent, delay in learning speed relative to shallow networks. We show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, while scaled random Gaussian initializations cannot. We further exhibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos. Deep learning methods have realized impressive performance in a range of applications, from visual object classification [1, 2, 3] to speech recognition [4] and natural language processing [5, 6]. These successes have been achieved despite the noted difficulty of training such deep architectures [7, 8, 9, 10, 11]. Indeed, many explanations for the difficulty of deep learning have been advanced in the literature, including the presence of many local minima, low curvature regions due to saturating nonlinearities, and exponential growth or decay of back-propagated gradients [12, 13, 14, 15]. Furthermore, many neural network simulations have observed 1 ar X iv :1 31 2. 61 20 v3 [ cs .N E ] 1 9 Fe b 20 14 strikingly nonlinear learning dynamics, including long plateaus of little apparent improvement followed by almost stage-like transitions to better performance. However, a quantitative, analytical understanding of the rich dynamics of deep learning remains elusive. For example, what determines the time scales over which deep learning unfolds? How does training speed retard with depth? Under what conditions will greedy unsupervised pretraining speed up learning? And how do the final learned internal representations depend on the statistical regularities inherent in the training data? Here we provide an exact analytical theory of learning in deep linear neural networks that quantitatively answers these questions for this restricted setting. Because of its linearity, the input-output map of a deep linear network can always be rewritten as a shallow network. In this sense, a linear network does not gain expressive power from depth, and hence will underfit and perform poorly on complex real world problems. But while it lacks this important aspect of practical deep learning systems, a deep linear network can nonetheless exhibit highly nonlinear learning dynamics, and these dynamics change with increasing depth. Indeed, the training error, as a function of the network weights, is non-convex, and gradient descent dynamics on this non-convex error surface exhibits a subtle interplay between different weights across multiple layers of the network. Hence deep linear networks provide an important starting point for understanding deep learning dynamics. To answer these questions, we derive and analyze a set of nonlinear coupled differential equations describing learning dynamics on weight space as a function of the statistical structure of the inputs and outputs. We find exact time-dependent solutions to these nonlinear equations, as well as find conserved quantities in the weight dynamics arising from symmetries in the error function. These solutions provide intuition into how a deep network successively builds up information about the statistical structure of the training data and embeds this information into its weights and internal representations. Moreover, we compare our analytical solutions of learning dynamics in deep linear networks to numerical simulations of learning dynamics in deep non-linear networks, and find that our analytical solutions provide a reasonable approximation. Our solutions also reflect nonlinear phenomena seen in simulations, including alternating plateaus and sharp periods of rapid improvement. Indeed, it has been shown previously [16] that this nonlinear learning dynamics in deep linear networks is sufficient to qualitatively capture aspects of the progressive, hierarchical differentiation of conceptual structure seen in infant development. Next, we apply these solutions to investigate the commonly used greedy layer-wise pretraining strategy for training deep networks [17, 18], and recover conditions under which such pretraining speeds learning. We show that these conditions are approximately satisfied for the MNIST dataset, and that unsupervised pretraining therefore confers an optimization advantage for deep linear networks applied to MNIST. Finally, we exhibit a new class of random orthogonal initial conditions on weights that, in linear networks, provide depth independent learning times, and we show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos. In this regime, synaptic gains are tuned so that linear amplification due to propagation of neural activity through weight matrices exactly balances dampening of activity due to saturating nonlinearities. In particular, we show that even in nonlinear networks, operating in this special regime, Jacobians that are involved in backpropagating error signals act like near isometries. 1 General learning dynamics of gradient descent W 21 W 32 x \u2208 R1 h \u2208 R2 y \u2208 R3 Figure 1: The three layer network analyzed in this section. We begin by analyzing learning in a three layer network (input, hidden, and output) with linear activation functions (Fig 1). We letNi be the number of neurons in layer i. The inputoutput map of the network is y = W W x. We wish to train the network to learn a particular input-output map from", "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks"}, "0ee1916a0cb2dc7d3add086b5f1092c3d4beb38a": {"paper_id": "0ee1916a0cb2dc7d3add086b5f1092c3d4beb38a", "abstract": "The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection. This paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension.", "title": "The Pascal Visual Object Classes (VOC) Challenge"}, "53698b91709112e5bb71eeeae94607db2aefc57c": {"paper_id": "53698b91709112e5bb71eeeae94607db2aefc57c", "abstract": "We investigate architectures of discriminatively trained deep Convolutional Networks (ConvNets) for action recognition in video. The challenge is to capture the complementary information on appearance from still frames and motion between frames. We also aim to incorporate into the network design aspects of the best performing hand-crafted features. Our contribution is three-fold. First, we propose a two-stream ConvNet architecture which incorporates spatial and temporal networks. Second, we demonstrate that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data. Finally, we show that multitask learning, applied to two different action classification datasets, can be used to increase the amount of training data and improve the performance on both. Our architecture is trained and evaluated on the standard video actions benchmarks of UCF-101 and HMDB-51, where it matches the state of the art. It also exceeds by a large margin previous attempts to use deep nets for video classification.", "title": "Two-Stream Convolutional Networks for Action Recognition in Videos"}, "659fc2a483a97dafb8fb110d08369652bbb759f9": {"paper_id": "659fc2a483a97dafb8fb110d08369652bbb759f9", "abstract": "The Fisher kernel (FK) is a generic framework which combines the benefits of generative and discriminative approaches. In the context of image classification the FK was shown to extend the popular bag-of-visual-words (BOV) by going beyond count statistics. However, in practice, this enriched representation has not yet shown its superiority over the BOV. In the first part we show that with several well-motivated modifications over the original framework we can boost the accuracy of the FK. On PASCAL VOC 2007 we increase the Average Precision (AP) from 47.9% to 58.3%. Similarly, we demonstrate state-of-the-art accuracy on CalTech 256. A major advantage is that these results are obtained using only SIFT descriptors and costless linear classifiers. Equipped with this representation, we can now explore image classification on a larger scale. In the second part, as an application, we compare two abundant resources of labeled images to learn classifiers: ImageNet and Flickr groups. In an evaluation involving hundreds of thousands of training images we show that classifiers learned on Flickr groups perform surprisingly well (although they were not intended for this purpose) and that they can complement classifiers learned on more carefully annotated datasets.", "title": "Improving the Fisher Kernel for Large-Scale Image Classification"}, "2dc9b005e936c9c303386caacc8d41cabdb1a0a1": {"paper_id": "2dc9b005e936c9c303386caacc8d41cabdb1a0a1", "abstract": "The latest generation of Convolutional Neural Networks (CNN) have achieved impressive results in challenging benchmarks on image recognition and object detection, significantly raising the interest of the community in these methods. Nevertheless, it is still unclear how different CNN methods compare with each other and with previous state-of-the-art shallow representations such as the Bag-of-Visual-Words and the Improved Fisher Vector. This paper conducts a rigorous evaluation of these new techniques, exploring different deep architectures and comparing them on a common ground, identifying and disclosing important implementation details. We identify several useful properties of CNN-based representations, including the fact that the dimensionality of the CNN output layer can be reduced significantly without having an adverse effect on performance. We also identify aspects of deep and shallow methods that can be successfully shared. In particular, we show that the data augmentation techniques commonly applied to CNN-based methods can also be applied to shallow methods, and result in an analogous performance boost. Source code and models to reproduce the experiments in the paper is made publicly available.", "title": "Return of the Devil in the Details: Delving Deep into Convolutional Nets"}, "398c296d0cc7f9d180f84969f8937e6d3a413796": {"paper_id": "398c296d0cc7f9d180f84969f8937e6d3a413796", "abstract": "Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible, wide and deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive MNIST handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks.", "title": "Multi-column deep neural networks for image classification"}, "2911e7f0fb6803851b0eddf8067a6fc06e8eadd6": {"paper_id": "2911e7f0fb6803851b0eddf8067a6fc06e8eadd6", "abstract": "Temporal information has useful features for recognizing facial expressions. However, to manually design useful features requires a lot of effort. In this paper, to reduce this effort, a deep learning technique, which is regarded as a tool to automatically extract useful features from raw data, is adopted. Our deep network is based on two different models. The first deep network extracts temporal appearance features from image sequences, while the other deep network extracts temporal geometry features from temporal facial landmark points. These two models are combined using a new integration method in order to boost the performance of the facial expression recognition. Through several experiments, we show that the two models cooperate with each other. As a result, we achieve superior performance to other state-of-the-art methods in the CK+ and Oulu-CASIA databases. Furthermore, we show that our new integration method gives more accurate results than traditional methods, such as a weighted summation and a feature concatenation method.", "title": "Joint Fine-Tuning in Deep Neural Networks for Facial Expression Recognition"}, "aeeea6eec2f063c006c13be865cec0c350244e5b": {"paper_id": "aeeea6eec2f063c006c13be865cec0c350244e5b", "abstract": "We have acquired a set of audio-visual recordings of induced emotions. A collage of comedy clips and clips of disgusting content were shown to a number of participants, who displayed mostly expressions of disgust, happiness, and surprise in response. While displays of induced emotions may differ from those shown in everyday life in aspects such as the frequency with which they occur, they are regarded as highly naturalistic and spontaneous. We recorded 25 participants for approximately 5 minutes each. This collection of recordings has been added to the MMI Facial Expression Database, an online accessible, easily searchable resource that is freely available to the scientific community.", "title": "Induced Disgust , Happiness and Surprise : an Addition to the MMI Facial Expression Database"}, "39a6cc80b1590bcb2927a9d4c6c8f22d7480fbdd": {"paper_id": "39a6cc80b1590bcb2927a9d4c6c8f22d7480fbdd", "abstract": "In this paper we introduce a 3-dimensional (3D) SIFT descriptor for video or 3D imagery such as MRI data. We also show how this new descriptor is able to better represent the 3D nature of video data in the application of action recognition. This paper will show how 3D SIFT is able to outperform previously used description methods in an elegant and efficient manner. We use a bag of words approach to represent videos, and present a method to discover relationships between spatio-temporal words in order to better describe the video data.", "title": "A 3-dimensional sift descriptor and its application to action recognition"}, "56e95f8efb7dbbc0b1820eaf365edc6f3b3f6719": {"paper_id": "56e95f8efb7dbbc0b1820eaf365edc6f3b3f6719", "abstract": "In this work, we present a novel local descriptor for video sequences. The proposed descriptor is based on histograms of oriented 3D spatio-temporal gradients. Our contribution is four-fold. (i) To compute 3D gradients for arbitrary scales, we develop a memory-efficient algorithm based on integral videos. (ii) We propose a generic 3D orientation quantization which is based on regular polyhedrons. (iii) We perform an in-depth evaluation of all descriptor parameters and optimize them for action recognition. (iv) We apply our descriptor to various action datasets (KTH, Weizmann, Hollywood) and show that we outperform the state-of-the-art.", "title": "A Spatio-Temporal Descriptor Based on 3D-Gradients"}, "b68e2a23508100c82935ea6b27a3ae032e4244d3": {"paper_id": "b68e2a23508100c82935ea6b27a3ae032e4244d3", "abstract": "This paper demonstrates the development of ontology for satellite databases. First, I create a computational ontology for the Union of Concerned Scientists (UCS) Satellite Database (UCSSD for short), called the UCS Satellite Ontology (or UCSSO). Second, in developing UCSSO I show that The Space Situational Awareness Ontology (SSAO)-\u2014an existing space domain reference ontology\u2014-and related ontology work by the author (Rovetto 2015, 2016) can be used either (i) with a database-specific local ontology such as UCSSO, or (ii) in its stead. In case (i), local ontologies such as UCSSO can reuse SSAO terms, perform term mappings, or extend it. In case (ii), the author_s orbital space ontology work, such as the SSAO, is usable by the UCSSD and organizations with other space object catalogs, as a reference ontology suite providing a common semantically-rich domain model. The SSAO, UCSSO, and the broader Orbital Space Environment Domain Ontology project is online at https://purl.org/space-ontology and GitHub. This ontology effort aims, in part, to provide accurate formal representations of the domain for various applications. Ontology engineering has the potential to facilitate the sharing and integration of satellite data from federated databases and sensors for safer spaceflight.", "title": "An ontology for satellite databases"}, "47c9c4ea22d4d4a286e74ed1f8b8f62d9bea54fb": {"paper_id": "47c9c4ea22d4d4a286e74ed1f8b8f62d9bea54fb", "abstract": "This paper gives an overview about the development of the field of Knowledge Engineering over the last 15 years. We discuss the paradigm shift from a transfer view to a modeling view and describe two approaches which considerably shaped research in Knowledge Engineering: Role-limiting Methods and Generic Tasks. To illustrate various concepts and methods which evolved in the last years we describe three modeling frameworks: CommonKADS, MIKE, and PROT\u00c9G\u00c9-II. This description is supplemented by discussing some important methodological developments in more detail: specification languages for knowledge-based systems, problem-solving methods, and ontologies. We conclude with outlining the relationship of Knowledge Engineering to Software Engineering, Information Integration and Knowledge Management.", "title": "Knowledge Engineering: Principles and Methods"}, "1c27cb8364a7655b2e4e8aa799970a08f90dea61": {"paper_id": "1c27cb8364a7655b2e4e8aa799970a08f90dea61", "abstract": "Introduction Linguistic Resources Knowledge-based machine translation (KBMT) systems have achieved excellent results in constrained domains, but have not yet scaled up to newspaper text. The reason is that knowledge resources (lexicons, grammar rules, world models) must be painstakingly handcrafted from scratch. One of the hypotheses being tested in the PANGLOSS machine translation project is whether or not these resources can be semi-automatically acquired on a very large scale. This paper focuses on the construction of a large ontology (or knowledge base, or world model) for supporting KBMT. It contains representations for some 70,000 commonly encountered objects, processes, qualities, and relations. The ontology was constructed by merging various online dictionaries, semantic networks, and bilingual resources, through semi-automatic methods. Some of these methods (e.g., conceptual matching of semantic taxonomies) are broadly applicable to problems of importing/exporting knowledge from one KB to another. Other methods (e.g., bilingual matching) allow a knowledge engineer to build up an index to a KB in a second language, such as Spanish or Japanese. USC/Information Sciences Institute 4676 Admiralty Way Marina del Rey, CA 90292 knight,luk @isi.edu", "title": "Building a Large-Scale Knowledge Base for Machine Translation"}, "c4b7784b7cc0a3b7d640ab46c85d8a60beee8200": {"paper_id": "c4b7784b7cc0a3b7d640ab46c85d8a60beee8200", "abstract": "The issue of handwritten character recognition is still a big challenge to the scientific community. Several approaches to address this challenge have been attempted in the last years, mostly focusing on the English pre-printed or handwritten characters space. Thus, the need to attempt a research related to Arabic handwritten text recognition. Algorithms based on neural networks have proved to give better results than conventional methods when applied to problems where the decision rules of the classification problem are not clearly defined. Two neural networks were built to classify already segmented characters of handwritten Arabic text. The two neural networks correctly recognized 73% of the characters. However, one hurdle was encountered in the above scenario, which can be summarized as follows: there are a lot of handwritten characters that can be segmented and classified into two or more different classes depending on whether they are looked at separately, or in a word, or even in a sentence. In other words, character classification, especially handwritten Arabic characters, depends largely on contextual information, not only on topographic features extracted from these characters.", "title": "Arabic Text Recognition"}, "352a677ea4fbb69989739004f3584c98a5bf5f9b": {"paper_id": "352a677ea4fbb69989739004f3584c98a5bf5f9b", "abstract": "OBJECTIVES\nThere is a clinical impression of dissatisfaction with treatment for hypothyroidism among some patients. Psychometric properties of the new ThyTSQ questionnaire are evaluated. The questionnaire, measuring patients' satisfaction with their treatment for hypothyroidism, has two parts: the seven-item ThyTSQ-Present and four-item ThyTSQ-Past, measuring satisfaction with present and past treatment, respectively, on scales from 6 (very satisfied) to 0 (very dissatisfied).\n\n\nMETHODS\nThe questionnaire was completed once by 103 adults with hypothyroidism, age (mean [SD]) 55.2 [14.4], range 23-84 years (all treated with thyroxine).\n\n\nRESULTS\nCompletion rates were very high. Internal consistency reliability was excellent for both ThyTSQ-Present and ThyTSQ-Past (Cronbach's alpha = 0.91 and 0.90, respectively [N = 102 and 103]). Principal components analyses indicated that the seven items of the ThyTSQ-Present and the four items of the ThyTSQ-Past could be summed into separate Present Satisfaction and Past Satisfaction total scores. Mean Present Satisfaction was 32.5 (7.8), maximum range 0-42, and mean Past Satisfaction was 17.5 (6.1), maximum range 0-24, indicating considerable room for improvement. Patients were least satisfied with their present understanding of their condition, mean 4.2 (1.7) (maximum range 0-6), and with information provided about hypothyroidism around the time of diagnosis, mean 3.9 (1.8) (maximum range 0-6).\n\n\nCONCLUSIONS\nThe ThyTSQ is highly acceptable to patients with hypothyroidism (excellent completion rates), and has established internal consistency reliability. It will assist health professionals in considering psychological outcomes when treating people with hypothyroidism, and is suitable for clinical trials and routine clinical monitoring.", "title": "Psychometric evaluation of a new questionnaire measuring treatment satisfaction in hypothyroidism: the ThyTSQ."}, "c87194b9d3b9552a9b41ae26644d8bf94d181ca2": {"paper_id": "c87194b9d3b9552a9b41ae26644d8bf94d181ca2", "abstract": "The aim of this study was to evaluate the range of techniques used by radiologists performing shoulder, hip, and knee arthrography using fluoroscopic guidance. Questionnaires on shoulder, hip, and knee arthrography were distributed to radiologists at a national radiology meeting. We enquired regarding years of experience, preferred approaches, needle gauge, gadolinium dilution, and volume injected. For each approach, the radiologist was asked their starting and end needle position based on a numbered and lettered grid superimposed on a radiograph. Sixty-eight questionnaires were returned. Sixty-eight radiologists performed shoulder and hip arthrography, and 65 performed knee arthrograms. Mean experience was 13.5 and 12.8\u00a0years, respectively. For magnetic resonance arthrography, a gadolinium dilution of 1/200 was used by 69\u201371%. For shoulder arthrography, an anterior approach was preferred by 65/68 (96%). The most common site of needle end position, for anterior and posterior approaches, was immediately lateral to the humeral cortex. A 22-gauge needle was used by 46/66 (70%). Mean injected volume was 12.7\u00a0ml (5\u201330). For hip arthrography, an anterior approach was preferred by 51/68 (75%). The most common site of needle end position, for anterior and lateral approaches, was along the lateral femoral head/neck junction. A 22-gauge needle was used by 53/68 (78%). Mean injected volume was 11.5\u00a0ml (5\u201320). For knee arthrography, a lateral approach was preferred by 41/64 (64%). The most common site of needle end position, for lateral and medial approaches, was mid-patellofemoral joint level. A 22-gauge needle was used by 36/65 (56%). Mean injected volume was 28.2\u00a0ml (5\u201360). Arthrographic approaches for the shoulder, hip, and knee vary among radiologists over a wide range of experience levels.", "title": "Shoulder, hip, and knee arthrography needle placement using fluoroscopic guidance: practice patterns of musculoskeletal radiologists in North America"}, "df7a9c107ee81e00db7b782304c68ca4aa7aec63": {"paper_id": "df7a9c107ee81e00db7b782304c68ca4aa7aec63", "abstract": "This paper presents the design and analysis of lowest form factor planar inverted-F Antenna at 28GHz for 5G Mobile applications. Feeding and shorting of the antenna is done using metallic strips. The Important properties of this antenna are its small footprint (0.25\u03bbg), good gain of 4.5dBi, 10dB impedance bandwidth of 1.55 GHz and radiation efficiency of 94%. The total size of the PIFA antenna is 0.25\u03bbg. PIFA antenna is one of the antennas whose performance greatly depends on ground plane size and its position on the ground plane. It shows good radiation pattern when placed at the corner or edge.", "title": "Small form factor PIFA antenna design at 28 GHz for 5G applications"}, "1ff60c86fadee8507224928011c9c18739ac9083": {"paper_id": "1ff60c86fadee8507224928011c9c18739ac9083", "abstract": "With great versatile characteristics, micro/nano-bubble related research have attracted much attention due to their extensive applications in the last half century. Researchers not merely focus on their physi-chemical properties, but also aim at their well-controlled generation methods and potential adhibition field. It can be expected that the future prospects of micro/nano-bubble related research will be tremendous and that there will be even more to be explored. In this case study, a bibliometric analysis was conducted to evaluate micro/nano-bubble related research from 1991 to 2014, based on the Science Citation Index EXPANDED database. The Ultrasound in Medicine and Biology with the highest h-index of 56 is the leading journal in this field, publishing 6.9\u00a0% of articles over this period, followed by Langmuir and Journal of the Acoustical Society of America. USA and the Univ Toronto, Canada were the most productive country and institution, respectively, while the USA, was the most internationally collaborative and had the highest h-index (111) of all countries. A new method named \u201cword cluster analysis\u201d was successfully applied to trace the research hotspots. Innovation in detection means and novel pathways for medical applications via micro/nano-bubble is considered to relate to the increasingly new types and varieties of diseases or cancers, as well as the well-controlled generation of micro/nano-bubbles.", "title": "A bibliometric analysis of micro/nano-bubble related research: current trends, present application, and future prospects"}, "3d7659956e747daaf872a53be2ee828ff0cf7597": {"paper_id": "3d7659956e747daaf872a53be2ee828ff0cf7597", "abstract": "Visual surface inspection is a challenging task due to the highly inconsistent appearance of the target surfaces and the abnormal regions. Most of the state-of-the-art methods are highly dependent on the labelled training samples, which are difficult to collect in practical industrial applications. To address this problem, we propose a generative adversarial network based framework for unsupervised surface inspection. The generative adversarial network is trained to generate the fake images analogous to the normal surface images. It implies that a well-trained GAN indeed learns a good representation of the normal surface images in a latent feature space. And consequently, the discriminator of GAN can serve as a naturally one-class classifier. We use the first three conventional layer of the discriminator as the feature extractor, whose response is sensitive to the abnormal regions. Particularly, a multi-scale fusion strategy is adopted to fuse the responses of the three convolution layers and thus improve the segmentation performance of abnormal detection. Various experimental results demonstrate the effectiveness of our proposed method.", "title": "A Generative Adversarial Network Based Framework for Unsupervised Visual Surface Inspection"}, "35756f711a97166df11202ebe46820a36704ae77": {"paper_id": "35756f711a97166df11202ebe46820a36704ae77", "abstract": "In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsuper-vised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolu-tional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks-demonstrating their applicability as general image representations .", "title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks"}, "79acc3ee28818cbb0f3a38a2c35a036258b0183f": {"paper_id": "79acc3ee28818cbb0f3a38a2c35a036258b0183f", "abstract": "We give a probabilistic interpretation of canonical correlation (CCA) analysis as a latent variable model for two Gaussian random vectors. Our interpretation is similar to the probabilistic interpretation of principal component analysis (Tipping and Bishop, 1999, Roweis, 1998). In addition, we cast Fisher linear discriminant analysis (LDA) within the CCA framework.", "title": "A Probabilistic Interpretation of Canonical Correlation Analysis"}, "01bc4a02cea3fdb24e0d5fe80bb214023cc3a4b9": {"paper_id": "01bc4a02cea3fdb24e0d5fe80bb214023cc3a4b9", "abstract": "We consider criteria for variational representations of non-Gaussian latent variables, and derive variational EM algorithms in general form. We establish a general equivalence among convex bounding methods, evidence based methods, and ensemble learning/Variational Bayes methods, which has previously been demonstrated only for particular cases.", "title": "Variational EM Algorithms for Non-Gaussian Latent Variable Models"}, "297c6b45234077330eb6e976567673bdba5916b6": {"paper_id": "297c6b45234077330eb6e976567673bdba5916b6", "abstract": "Hidden Markov models (HMMs) have proven to be one of the most widely used tools for learning probabilistic models of time series data. In an HMM, information about the past is conveyed through a single discrete variable\u2014the hidden state. We discuss a generalization of HMMs in which this state is factored into multiple state variables and is therefore represented in a distributed manner. We describe an exact algorithm for inferring the posterior probabilities of the hidden state variables given the observations, and relate it to the forward\u2013backward algorithm for HMMs and to algorithms for more general graphical models. Due to the combinatorial nature of the hidden state representation, this exact algorithm is intractable. As in other intractable systems, approximate inference can be carried out using Gibbs sampling or variational methods. Within the variational framework, we present a structured approximation in which the the state variables are decoupled, yielding a tractable algorithm for learning the parameters of the model. Empirical comparisons suggest that these approximations are efficient and provide accurate alternatives to the exact methods. Finally, we use the structured approximation to model Bach's chorales and show that factorial HMMs can capture statistical structure in this data set which an unconstrained HMM cannot.", "title": "Factorial Hidden Markov Models"}, "ba753286b9e2f32c5d5a7df08571262e257d2e53": {"paper_id": "ba753286b9e2f32c5d5a7df08571262e257d2e53", "abstract": "Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.", "title": "Conditional Generative Adversarial Nets"}, "6de2b1058c5b717878cce4e7e50d3a372cc4aaa6": {"paper_id": "6de2b1058c5b717878cce4e7e50d3a372cc4aaa6", "abstract": "We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1 2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.", "title": "Generative Adversarial Nets"}, "0a10d64beb0931efdc24a28edaa91d539194b2e2": {"paper_id": "0a10d64beb0931efdc24a28edaa91d539194b2e2", "abstract": "We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.", "title": "Efficient Estimation of Word Representations in Vector Space"}, "1f88427d7aa8225e47f946ac41a0667d7b69ac52": {"paper_id": "1f88427d7aa8225e47f946ac41a0667d7b69ac52", "abstract": "In many recent object recognition systems, feature extraction stages are generally composed of a filter bank, a non-linear transformation, and some sort of feature pooling layer. Most systems use only one stage of feature extraction in which the filters are hard-wired, or two stages where the filters in one or both stages are learned in supervised or unsupervised mode. This paper addresses three questions: 1. How does the non-linearities that follow the filter banks influence the recognition accuracy? 2. does learning the filter banks in an unsupervised or supervised manner improve the performance over random filters or hardwired filters? 3. Is there any advantage to using an architecture with two stages of feature extraction, rather than one? We show that using non-linearities that include rectification and local contrast normalization is the single most important ingredient for good accuracy on object recognition benchmarks. We show that two stages of feature extraction yield better accuracy than one. Most surprisingly, we show that a two-stage system with random filters can yield almost 63% recognition rate on Caltech-101, provided that the proper non-linearities and pooling layers are used. Finally, we show that with supervised refinement, the system achieves state-of-the-art performance on NORB dataset (5.6%) and unsupervised pre-training followed by supervised refinement produces good accuracy on Caltech-101 (\u226b 65%), and the lowest known error rate on the undistorted, unprocessed MNIST dataset (0.53%).", "title": "What is the best multi-stage architecture for object recognition?"}, "27f3c2b0bb917091f92e4161863ec3559452280f": {"paper_id": "27f3c2b0bb917091f92e4161863ec3559452280f", "abstract": "We present a new estimation principle for parameterized statistical models. The idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially generated noise, using the model log-density function in the regression nonlinearity. We show that this leads to a consistent (convergent) estimator of the parameters, and analyze the asymptotic variance. In particular, the method is shown to directly work for unnormalized models, i.e. models where the density function does not integrate to one. The normalization constant can be estimated just like any other parameter. For a tractable ICA model, we compare the method with other estimation methods that can be used to learn unnormalized models, including score matching, contrastive divergence, and maximum-likelihood where the normalization constant is estimated with importance sampling. Simulations show that noise-contrastive estimation offers the best trade-off between computational and statistical efficiency. The method is then applied to the modeling of natural images: We show that the method can successfully estimate a large-scale two-layer model and a Markov random field.", "title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models"}, "2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc": {"paper_id": "2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc", "abstract": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.", "title": "ImageNet Classification with Deep Convolutional Neural Networks"}, "4aa4069693bee00d1b0759ca3df35e59284e9845": {"paper_id": "4aa4069693bee00d1b0759ca3df35e59284e9845", "abstract": "Modern visual recognition systems are often limited in their ability to scale to large numbers of object categories. This limitation is in part due to the increasing difficulty of acquiring sufficient training data in the form of labeled images as the number of object categories grows. One remedy is to leverage data from other sources \u2013 such as text data \u2013 both to train visual models and to constrain their predictions. In this paper we present a new deep visual-semantic embedding model trained to identify visual objects using both labeled image data as well as semantic information gleaned from unannotated text. We demonstrate that this model matches state-of-the-art performance on the 1000-class ImageNet object recognition challenge while making more semantically reasonable errors, and also show that the semantic information can be exploited to make predictions about tens of thousands of image labels not observed during training. Semantic knowledge improves such zero-shot predictions by up to 65%, achieving hit rates of up to 10% across thousands of novel labels never seen by the visual model.", "title": "DeViSE: A Deep Visual-Semantic Embedding Model"}, "357776cd7ee889af954f0dfdbaee71477c09ac18": {"paper_id": "357776cd7ee889af954f0dfdbaee71477c09ac18", "abstract": "In this paper we propose a new method for regularizing autoencoders by imposing an arbitrary prior on the latent representation of the autoencoder. Our method, named \u201cadversarial autoencoder\u201d, uses the recently proposed generative adversarial networks (GAN) in order to match the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior. Matching the aggregated posterior to the prior ensures that there are no \u201choles\u201d in the prior, and generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how adversarial autoencoders can be used to disentangle style and content of images and achieve competitive generative performance on MNIST, Street View House Numbers and Toronto Face datasets.", "title": "Adversarial Autoencoders"}, "245414e768c3b8c8288ac0651604a36b1a44a446": {"paper_id": "245414e768c3b8c8288ac0651604a36b1a44a446", "abstract": "We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent an approximate posterior distribution and uses this for optimisation of a variational lower bound. We develop stochastic backpropagation \u2013 rules for gradient backpropagation through stochastic variables \u2013 and derive an algorithm that allows for joint optimisation of the parameters of both the generative and recognition models. We demonstrate on several real-world data sets that by using stochastic backpropagation and variational inference, we obtain models that are able to generate realistic samples of data, allow for accurate imputations of missing data, and provide a useful tool for high-dimensional data visualisation.", "title": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models"}, "abcf42a99accaac8fede0f6c077f6f6735fd0702": {"paper_id": "abcf42a99accaac8fede0f6c077f6f6735fd0702", "abstract": "The search for efficient image denoising methods still is a valid challenge, at the crossing of functional analysis and statistics. In spite of the sophistication of the recently proposed methods, most algorithms have not yet attained a desirable level of applicability. All show an outstanding performance when the image model corresponds to the algorithm assumptions, but fail in general and create artifacts or remove image fine structures. The main focus of this paper is, first, to define a general mathematical and experimental methodology to compare and classify classical image denoising algorithms, second, to propose an algorithm (Non Local Means) addressing the preservation of structure in a digital image. The mathematical analysis is based on the analysis of the \u201cmethod noise\u201d, defined as the difference between a digital image and its denoised version. The NL-means algorithm is proven to be asymptotically optimal under a generic statistical image model. The denoising performance of all considered methods are compared in four ways; mathematical: asymptotic order of magnitude of the method noise under regularity assumptions; perceptual-mathematical: the algorithms artifacts and their explanation as a violation of the image model; quantitative experimental: by tables of L distances of the denoised version to the original image. The most powerful evaluation method seems, however, to be the visualization of the method noise on natural images. The more this method noise looks like a real white noise, the better the method.", "title": "A Review of Image Denoising Algorithms, with a New One"}, "373f76633cc1f6c7a421e31c989842021a52fca4": {"paper_id": "373f76633cc1f6c7a421e31c989842021a52fca4", "abstract": "We show how to use complementary priors to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.", "title": "A Fast Learning Algorithm for Deep Belief Nets"}, "006e0ce2f2367b66b1e5da178e53216c3d01d97b": {"paper_id": "006e0ce2f2367b66b1e5da178e53216c3d01d97b", "abstract": "Deep learning has enjoyed a great deal of success because of its ability to learn useful features for tasks such as classification. But there has been less exploration in learning the factors of variation apart from the classification signal. By augmenting autoencoders with simple regularization terms during training, we demonstrate that standard deep architectures can discover and explicitly represent factors of variation beyond those relevant for categorization. We introduce a cross-covariance penalty (XCov) as a method to disentangle factors like handwriting style for digits and subject identity in faces. We demonstrate this on the MNIST handwritten digit database, the Toronto Faces Database (TFD) and the Multi-PIE dataset by generating manipulated instances of the data. Furthermore, we demonstrate these deep networks can extrapolate \u2018hidden\u2019 variation in the supervised signal.", "title": "Discovering Hidden Factors of Variation in Deep Networks"}, "284b18d7196f608448ca3d9496bf220b1dfffcf5": {"paper_id": "284b18d7196f608448ca3d9496bf220b1dfffcf5", "abstract": "We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode, and dataindependent expectations are approximated using persistent Markov chains. The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer \u201cpre-training\u201d phase that allows variational inference to be initialized with a single bottomup pass. We present results on the MNIST and NORB datasets showing that deep Boltzmann machines learn good generative models and perform well on handwritten digit and visual object recognition tasks.", "title": "Deep Boltzmann Machines"}, "0c32edc991f7d2bde8a5c208e79f09c0c3a0ed65": {"paper_id": "0c32edc991f7d2bde8a5c208e79f09c0c3a0ed65", "abstract": "The variational autoencoder (VAE; Kingma & Welling (2014)) is a recently proposed generative model pairing a top-down generative network with a bottom-up recognition network which approximates posterior inference. It typically makes strong assumptions about posterior inference, for instance that the posterior distribution is approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. As we show empirically, the VAE objective can lead to overly simplified representations which fail to use the network\u2019s entire modeling capacity. We present the importance weighted autoencoder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors which do not fit the VAE modeling assumptions. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks.", "title": "Importance Weighted Autoencoders"}, "463bec3d0298e96e3702e071e241e3898f76eff2": {"paper_id": "463bec3d0298e96e3702e071e241e3898f76eff2", "abstract": "With modern computer architecture evolving, two problems conspire against the state-of-the-art approaches in parallel query execution: (i) to take advantage of many-cores, all query work must be distributed evenly among (soon) hundreds of threads in order to achieve good speedup, yet (ii) dividing the work evenly is difficult even with accurate data statistics due to the complexity of modern out-of-order cores. As a result, the existing approaches for plan-driven parallelism run into load balancing and context-switching bottlenecks, and therefore no longer scale. A third problem faced by many-core architectures is the decentralization of memory controllers, which leads to Non-Uniform Memory Access (NUMA). In response, we present the morsel-driven query execution framework, where scheduling becomes a fine-grained run-time task that is NUMA-aware. Morsel-driven query processing takes small fragments of input data (morsels) and schedules these to worker threads that run entire operator pipelines until the next pipeline breaker. The degree of parallelism is not baked into the plan but can elastically change during query execution, so the dispatcher can react to execution speed of different morsels but also adjust resources dynamically in response to newly arriving queries in the workload. Further, the dispatcher is aware of data locality of the NUMA-local morsels and operator state, such that the great majority of executions takes place on NUMA-local memory. Our evaluation on the TPC-H and SSB benchmarks shows extremely high absolute performance and an average speedup of over 30 with 32 cores.", "title": "Morsel-driven parallelism: a NUMA-aware query evaluation framework for the many-core age"}, "89d00fe0b6aab5d8d2ddef4d6b56e7b99c8478ef": {"paper_id": "89d00fe0b6aab5d8d2ddef4d6b56e7b99c8478ef", "abstract": "We use the full query set of the TPC-H Benchmark as a case study for the efficient implementation of decision support queries on main memory column-store databases. Instead of splitting a query into separate independent operators, we consider the query as a whole and translate the execution plan into a single function performing the query. This allows highly efficient CPU utilization, minimal materialization, and execution in a single pass over the data for most queries. The single pass is performed in parallel and scales near-linearly with the number of cores. The resulting query plans for most of the 22 queries are remarkably simple and are suited for automatic generation and fast compilation. Using a data-parallel, NUMA-aware many-core implementation with block summaries, inverted index data structures, and efficient aggregation algorithms, we achieve one to two orders of magnitude better performance than the current record holders of the TPC-H Benchmark.", "title": "Efficient many-core query execution in main memory column-stores"}, "1024da80d950b8d3142ace378324644a67aa2d72": {"paper_id": "1024da80d950b8d3142ace378324644a67aa2d72", "abstract": "There has been a significant amount of excitement and recent work on column-oriented database systems (\"column-stores\"). These database systems have been shown to perform more than an order of magnitude better than traditional row-oriented database systems (\"row-stores\") on analytical workloads such as those found in data warehouses, decision support, and business intelligence applications. The elevator pitch behind this performance difference is straightforward: column-stores are more I/O efficient for read-only queries since they only have to read from disk (or from memory) those attributes accessed by a query.\n This simplistic view leads to the assumption that one can obtain the performance benefits of a column-store using a row-store: either by vertically partitioning the schema, or by indexing every column so that columns can be accessed independently. In this paper, we demonstrate that this assumption is false. We compare the performance of a commercial row-store under a variety of different configurations with a column-store and show that the row-store performance is significantly slower on a recently proposed data warehouse benchmark. We then analyze the performance difference and show that there are some important differences between the two systems at the query executor level (in addition to the obvious differences at the storage layer level). Using the column-store, we then tease apart these differences, demonstrating the impact on performance of a variety of column-oriented query execution techniques, including vectorized query processing, compression, and a new join algorithm we introduce in this paper. We conclude that while it is not impossible for a row-store to achieve some of the performance advantages of a column-store, changes must be made to both the storage layer and the query executor to fully obtain the benefits of a column-oriented approach.", "title": "Column-stores vs. row-stores: how different are they really?"}, "4483c133f637170fedcb39a971da7e26a3c3f842": {"paper_id": "4483c133f637170fedcb39a971da7e26a3c3f842", "abstract": "We present the application of customized code generation to database query evaluation. The idea is to use a collection of highly efficient code templates and dynamically instantiate them to create query- and hardware-specific source code. The source code is compiled and dynamically linked to the database server for processing. Code generation diminishes the bloat of higher-level programming abstractions necessary for implementing generic, interpreted, SQL query engines. At the same time, the generated code is customized for the hardware it will run on. We term this approach holistic query evaluation. We present the design and development of a prototype system called HIQUE, the Holistic Integrated Query Engine, which incorporates our proposals. We undertake a detailed experimental study of the system's performance. The results show that HIQUE satisfies its design objectives, while its efficiency surpasses that of both well-established and currently-emerging query processing techniques.", "title": "Generating code for holistic query evaluation"}, "86e0d2d7a234d58df96f169fee9e03c6324329b5": {"paper_id": "86e0d2d7a234d58df96f169fee9e03c6324329b5", "abstract": "Compiling database queries into executable (sub-) programs provides substantial benefits comparing to traditional interpreted execution. Many of these benefits, such as reduced interpretation overhead, better instruction code locality, and providing opportunities to use SIMD instructions, have previously been provided by redesigning query processors to use a vectorized execution model. In this paper, we try to shed light on the question of how state-of-the-art compilation strategies relate to vectorized execution for analytical database workloads on modern CPUs. For this purpose, we carefully investigate the behavior of vectorized and compiled strategies inside the Ingres VectorWise database system in three use cases: Project, Select and Hash Join. One of the findings is that compilation should always be combined with block-wise query execution. Another contribution is identifying three cases where \"loop-compilation\" strategies are inferior to vectorized execution. As such, a careful merging of these two strategies is proposed for optimal performance: either by incorporating vectorized execution principles into compiled query plans or using query compilation to create building blocks for vectorized processing.", "title": "Vectorization vs. compilation in query execution"}, "9aed0787a5abf4afd7bc4a9b1ae0bf1be1886aa4": {"paper_id": "9aed0787a5abf4afd7bc4a9b1ae0bf1be1886aa4", "abstract": "Autonomic communications seek to improve the ability of network and services to cope with unpredicted change, including changes in topology, load, task, the physical and logical characteristics of the networks that can be accessed, and so forth. Broad-ranging autonomic solutions require designers to account for a range of end-to-end issues affecting programming models, network and contextual modeling and reasoning, decentralised algorithms, trust acquisition and maintenance---issues whose solutions may draw on approaches and results from a surprisingly broad range of disciplines. We survey the current state of autonomic communications research and identify significant emerging trends and techniques.", "title": "A survey of autonomic communications"}, "f14e8ce6207d1cc005fc257014350400c923a47c": {"paper_id": "f14e8ce6207d1cc005fc257014350400c923a47c", "abstract": "As computing becomes more pervasive, the nature of applications must change accordingly. In particular, applications must become more flexible in order to respond to highly dynamic computing environments, and more autonomous, to reflect the growing ratio of applications to users and the corresponding decline in the attention a user can devote to each. That is, applications must become more context-aware. To facilitate the programming of such applications, infrastructure is required to gather, manage, and disseminate context information to applications. This paper is concerned with the development of appropriate context modeling concepts for pervasive computing, which can form the basis for such a context management infrastructure. This model overcomes problems associated with previous context models, including their lack of formality and generality, and also tackles issues such as wide variations in information quality, the existence of complex relationships amongst context information and temporal aspects of context.", "title": "Modeling Context Information in Pervasive Computing Systems"}, "38c02c40f49e0cecf266a92df2ce783c601f8ebf": {"paper_id": "38c02c40f49e0cecf266a92df2ce783c601f8ebf", "abstract": "The popularity of wireless networks has increased in recent years and is becoming a common addition to LANs. In this paper we investigate a novel use for a wireless network based on the IEEE 802.11 standard: inferring the location of a wireless client from signal quality measures. Similar work has been limited to prototype systems that rely on nearest-neighbor techniques to infer location. In this paper, we describe Nibble, a Wi-Fi location service that uses Bayesian networks to infer the location of a device. We explain the general theory behind the system and how to use the system, along with describing our experiences at a university campus building and at a research lab. We also discuss how probabilistic modeling can be applied to a diverse range of applications that use sensor data.", "title": "A Probabilistic Room Location Service for Wireless Networked Environments"}, "17d1103f141bbcfd30a83855ea687f981d3334d6": {"paper_id": "17d1103f141bbcfd30a83855ea687f981d3334d6", "abstract": "In Natural Language Processing (NLP), research results from software engineering and software technology have often been neglected. This paper describes some factors that add complexity to the task of engineering reusable NLP systems (beyond conventional software systems). Current work in the area of design patterns and composition languages is described and claimed relevant for natural language processing. The benefits of NLP componentware and barriers to reuse are outlined, and the dichotomies \u201csystem versus experiment\u201d and \u201ctoolkit versus framework\u201d are discussed. It is argued that in order to live up to its name language engineering must not neglect component quality and architectural evaluation when reporting new NLP research.", "title": "Current Issues In Software Engineering For Natural Language Processing"}, "8c4bcbaee18aaae417e2f2da7a7b95bd8edaf063": {"paper_id": "8c4bcbaee18aaae417e2f2da7a7b95bd8edaf063", "abstract": "Lossy image compression is generally formulated as a joint rate-distortion optimization problem to learn encoder, quantizer, and decoder. Due to the non-differentiable quantizer and discrete entropy estimation, it is very challenging to develop a convolutional network (CNN)-based image compression system. In this paper, motivated by that the local information content is spatially variant in an image, we suggest that: (i) the bit rate of the different parts of the image is adapted to local content, and (ii) the content-aware bit rate is allocated under the guidance of a content-weighted importance map. The sum of the importance map can thus serve as a continuous alternative of discrete entropy estimation to control compression rate. The binarizer is adopted to quantize the output of encoder and a proxy function is introduced for approximating binary operation in backward propagation to make it differentiable. The encoder, decoder, binarizer and importance map can be jointly optimized in an end-to-end manner. And a convolutional entropy encoder is further presented for lossless compression of importance map and binary codes. In low bit rate image compression, experiments show that our system significantly outperforms JPEG and JPEG 2000 by structural similarity (SSIM) index, and can produce the much better visual result with sharp edges, rich textures, and fewer artifacts.", "title": "Learning Convolutional Networks for Content-Weighted Image Compression"}, "fd23c9168418324e81881365f297fb6a1caa3a07": {"paper_id": "fd23c9168418324e81881365f297fb6a1caa3a07", "abstract": "The state of the art in data compression is arithmetic coding, not the better-known Huffman method. Arithmetic coding gives greater compression, is faster for adaptive models, and clearly separates the model from the channel encoding.", "title": "Arithmetic Coding for Data Compression"}, "f51026239f5786b31ee28dbb6dee4024a2f6dcd7": {"paper_id": "f51026239f5786b31ee28dbb6dee4024a2f6dcd7", "abstract": "Compressibiity of lndividmd sequences by the class of generalid fide-state iuformatlon-lomless encoders ls investigated. These encodersrpnoperateinavariable-ratemodeasweUasaflxedrateone,nnd they allow for any fllte-state scheme of variable-length-to-variable-Ien@ coding. For every individual lnfllte seqence x a quantity p (x) is defined, calledthecompressibil ityofx,whirhisshowntobetheasymptotieatly attainable lower bound on the compression ratio tbat cao be achieved for x by any finite-state encoder. This is demonstrated by means of a amshctivecodtngtbeoremanditsconversethat,apartfnnntheirafymptotic siguificauca?, alsu prwidc useful performance criteria for finite and practfcal data-compression tasks. The proposed concept of compressibility is also shown to play a role analogous to that of entropy in classicai informatfon theory where one deals with probabilistic ensembles of sequences ratbez Manuscript received June 10, 1977; revised February 20, 1978. J. Ziv is with Bell Laboratories, Murray Hill, NJ 07974, on leave from the Department of Electrical Engineering, Technion-Israel Institute of Technology, Haifa, Israel. A. Lempel is with Sperry Research Center, Sudbury, MA 01776, on leave from the Department of Electrical Engineering, Technion-Israel Institute of Technology, Haifa, Israel. than with individual sequences. Wbiie the deflnition of p (x) allows a different machlne for each different sequence to be compressed, the coustruetive eodiug theorem leads to a universal algorithm that is asymp to&ally optfmal for ail sequences.", "title": "Compression of individual sequences via variable-rate coding"}, "8a76de2c9fc7021e360c770b96640860571037a4": {"paper_id": "8a76de2c9fc7021e360c770b96640860571037a4", "abstract": "We introduce a method to train Binarized Neural Networks (BNNs) neural networks with binary weights and activations at run-time. At training-time the binary weights and activations are used for computing the parameters gradients. During the forward pass, BNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations, which is expected to substantially improve power-efficiency. To validate the effectiveness of BNNs we conduct two sets of experiments on the Torch7 and Theano frameworks. On both, BNNs achieved nearly state-of-the-art results over the MNIST, CIFAR-10 and SVHN datasets. Last but not least, we wrote a binary matrix multiplication GPU kernel with which it is possible to run our MNIST BNN 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The code for training and running our BNNs is available on-line.", "title": "Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1"}, "1ff9a37d766e3a4f39757f5e1b235a42dacf18ff": {"paper_id": "1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "abstract": "Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems. Also, conventional networks fix the architecture before training starts; as a result, training cannot improve the architecture. To address these limitations, we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections. Our method prunes redundant connections using a three-step method. First, we train the network to learn which connections are important. Next, we prune the unimportant connections. Finally, we retrain the network to fine tune the weights of the remaining connections. On the ImageNet dataset, our method reduced the number of parameters of AlexNet by a factor of 9\u00d7, from 61 million to 6.7 million, without incurring accuracy loss. Similar experiments with VGG-16 found that the total number of parameters can be reduced by 13\u00d7, from 138 million to 10.3 million, again with no loss of accuracy.", "title": "Learning both Weights and Connections for Efficient Neural Networks"}, "6c8b30f63f265c32e26d999aa1fef5286b8308ad": {"paper_id": "6c8b30f63f265c32e26d999aa1fef5286b8308ad", "abstract": "Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different \u201cthinned\u201d networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.", "title": "Dropout: a simple way to prevent neural networks from overfitting"}, "38f35dd624cd1cf827416e31ac5e0e0454028eca": {"paper_id": "38f35dd624cd1cf827416e31ac5e0e0454028eca", "abstract": "We introduce DropConnect, a generalization of Dropout (Hinton et al., 2012), for regularizing large fully-connected layers within neural networks. When training with Dropout, a randomly selected subset of activations are set to zero within each layer. DropConnect instead sets a randomly selected subset of weights within the network to zero. Each unit thus receives input from a random subset of units in the previous layer. We derive a bound on the generalization performance of both Dropout and DropConnect. We then evaluate DropConnect on a range of datasets, comparing to Dropout, and show state-of-the-art results on several image recognition benchmarks by aggregating multiple DropConnect-trained models.", "title": "Regularization of Neural Networks using DropConnect"}, "8b053389eb8c18c61b84d7e59a95cb7e13f205b7": {"paper_id": "8b053389eb8c18c61b84d7e59a95cb7e13f205b7", "abstract": "We propose DoReFa-Net, a method to train convolutional neural networks that have low bitwidth weights and activations using low bitwidth parameter gradients. In particular, during backward pass, parameter gradients are stochastically quantized to low bitwidth numbers before being propagated to convolutional layers. As convolutions during forward/backward passes can now operate on low bitwidth weights and activations/gradients respectively, DoReFa-Net can use bit convolution kernels to accelerate both training and inference. Moreover, as bit convolutions can be efficiently implemented on CPU, FPGA, ASIC and GPU, DoReFatNet opens the way to accelerate training of low bitwidth neural network on these hardware. Our experiments on SVHN and ImageNet datasets prove that DoReFa-Net can achieve comparable prediction accuracy as 32-bit counterparts. For example, a DoReFa-Net derived from AlexNet that has 1-bit weights, 2-bit activations, can be trained from scratch using 4-bit gradients to get 47% top-1 accuracy on ImageNet validation set.1 The DoReFa-Net AlexNet model is released publicly.", "title": "DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients"}, "62c76ca0b2790c34e85ba1cce09d47be317c7235": {"paper_id": "62c76ca0b2790c34e85ba1cce09d47be317c7235", "abstract": "Stochastic neurons and hard non-linearities can be useful for a number of reasons in deep learning models, but in many cases they pose a challenging problem: how to estimate the gradient of a loss function with respect to the input of such stochastic or non-smooth neurons? I.e., can we \u201cback-propagate\u201d through these stochastic neurons? We examine this question, existing approaches, and compare four families of solutions, applicable in different settings. One of them is the minimum variance unbiased gradient estimator for stochatic binary neurons (a special case of the REINFORCE algorithm). A second approach, introduced here, decomposes the operation of a binary stochastic neuron into a stochastic binary part and a smooth differentiable part, which approximates the expected effect of the pure stochatic binary neuron to first order. A third approach involves the injection of additive or multiplicative noise in a computational graph that is otherwise differentiable. A fourth approach heuristically copies the gradient with respect to the stochastic output directly as an estimator of the gradient with respect to the sigmoid argument (we call this the straight-through estimator). To explore a context where these estimators are useful, we consider a small-scale version of conditional computation, where sparse stochastic units form a distributed representation of gaters that can turn off in combinatorially many ways large chunks of the computation performed in the rest of the neural network. In this case, it is important that the gating units produce an actual 0 most of the time. The resulting sparsity can be potentially be exploited to greatly reduce the computational cost of large deep networks for which conditional computation would be useful.", "title": "Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation"}, "23d14ab0f18fa881a2ac8ae027be6b9f2c91d74d": {"paper_id": "23d14ab0f18fa881a2ac8ae027be6b9f2c91d74d", "abstract": "Many companies are deploying services, either for consumers or industry, which are largely based on machine-learning algorithms for sophisticated processing of large amounts of data. The state-of-the-art and most popular such machine-learning algorithms are Convolutional and Deep Neural Networks (CNNs and DNNs), which are known to be both computationally and memory intensive. A number of neural network accelerators have been recently proposed which can offer high computational capacity/area ratio, but which remain hampered by memory accesses.\n However, unlike the memory wall faced by processors on general-purpose workloads, the CNNs and DNNs memory footprint, while large, is not beyond the capability of the on-chip storage of a multi-chip system. This property, combined with the CNN/DNN algorithmic characteristics, can lead to high internal bandwidth and low external communications, which can in turn enable high-degree parallelism at a reasonable area cost. In this article, we introduce a custom multi-chip machine-learning architecture along those lines. We show that, on a subset of the largest known neural network layers, it is possible to achieve a speedup of 450.65x over a GPU, and reduce the energy by 150.31x on average for a 64-chip system. We implement the node down to the place and route at 28nm, containing a combination of custom storage and computational units, with industry-grade interconnects.", "title": "DaDianNao: A Machine-Learning Supercomputer"}, "fbeaa499e10e98515f7e1c4ad89165e8c0677427": {"paper_id": "fbeaa499e10e98515f7e1c4ad89165e8c0677427", "abstract": "Recent advances in deep learning have made the use of large, deep neural networks with tens of millions of parameters suitable for a number of applications that require real-time processing. The sheer size of these networks can represent a challenging computational burden, even for modern CPUs. For this reason, GPUs are routinely used instead to train and run such networks. This paper is a tutorial for students and researchers on some of the techniques that can be used to reduce this computational cost considerably on modern x86 CPUs. We emphasize data layout, batching of the computation, the use of SSE2 instructions, and particularly leverage SSSE3 and SSE4 fixed-point instructions which provide a 3\u00d7 improvement over an optimized floating-point baseline. We use speech recognition as an example task, and show that a real-time hybrid hidden Markov model / neural network (HMM/NN) large vocabulary system can be built with a 10\u00d7 speedup over an unoptimized baseline and a 4\u00d7 speedup over an aggressively optimized floating-point baseline at no cost in accuracy. The techniques described extend readily to neural network training and provide an effective alternative to the use of specialized hardware.", "title": "Improving the speed of neural networks on CPUs"}, "6f4d58486b1c6d710586b1d182ddad7d09a8da11": {"paper_id": "6f4d58486b1c6d710586b1d182ddad7d09a8da11", "abstract": "Machine-Learning tasks are becoming pervasive in a broad range of domains, and in a broad range of systems (from embedded systems to data centers). At the same time, a small set of machine-learning algorithms (especially Convolutional and Deep Neural Networks, i.e., CNNs and DNNs) are proving to be state-of-the-art across many applications. As architectures evolve towards heterogeneous multi-cores composed of a mix of cores and accelerators, a machine-learning accelerator can achieve the rare combination of efficiency (due to the small number of target algorithms) and broad application scope.\n Until now, most machine-learning accelerator designs have focused on efficiently implementing the computational part of the algorithms. However, recent state-of-the-art CNNs and DNNs are characterized by their large size. In this study, we design an accelerator for large-scale CNNs and DNNs, with a special emphasis on the impact of memory on accelerator design, performance and energy.\n We show that it is possible to design an accelerator with a high throughput, capable of performing 452 GOP/s (key NN operations such as synaptic weight multiplications and neurons outputs additions) in a small footprint of 3.02 mm2 and 485 mW; compared to a 128-bit 2GHz SIMD processor, the accelerator is 117.87x faster, and it can reduce the total energy by 21.08x. The accelerator characteristics are obtained after layout at 65 nm. Such a high throughput in a small footprint can open up the usage of state-of-the-art machine-learning algorithms in a broad set of systems and for a broad set of applications.", "title": "DianNao: a small-footprint high-throughput accelerator for ubiquitous machine-learning"}, "123ae35aa7d6838c817072032ce5615bb891652d": {"paper_id": "123ae35aa7d6838c817072032ce5615bb891652d", "abstract": "We introduce BinaryNet, a method which trains DNNs with binary weights and activations when computing parameters\u2019 gradient. We show that it is possible to train a Multi Layer Perceptron (MLP) on MNIST and ConvNets on CIFAR-10 and SVHN with BinaryNet and achieve nearly state-of-the-art results. At run-time, BinaryNet drastically reduces memory usage and replaces most multiplications by 1-bit exclusive-not-or (XNOR) operations, which might have a big impact on both general-purpose and dedicated Deep Learning hardware. We wrote a binary matrix multiplication GPU kernel with which it is possible to run our MNIST MLP 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The code for BinaryNet is available.", "title": "BinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1"}, "4500d9f74ee8cfeeba24643f3248fdb439f31976": {"paper_id": "4500d9f74ee8cfeeba24643f3248fdb439f31976", "abstract": "Recent results show that deep neural networks achieve excellent performance even when, during training, weights are quantized and projected to a binary representation. Here, we show that this is just the tip of the iceberg: these same networks, during testing, also exhibit a remarkable robustness to distortions beyond quantization, including additive and multiplicative noise, and a class of non-linear projections where binarization is just a special case. To quantify this robustness, we show that one such network achieves 11% test error on CIFAR-10 even with 0.68 effective bits per weight. Furthermore, we find that a common training heuristic\u2014 namely, projecting quantized weights during backpropagation\u2014can be altered (or even removed) and networks still achieve a base level of robustness during testing. Specifically, training with weight projections other than quantization also works, as does simply clipping the weights, both of which have never been reported before. We confirm our results for CIFAR-10 and ImageNet datasets. Finally, drawing from these ideas, we propose a stochastic projection rule that leads to a new state of the art network with 7.64% test error on CIFAR-10 using no data augmentation.", "title": "Deep neural networks are robust to weight binarization and other non-linear distortions"}, "3d2126066c6244e05ec4d2631262252f4369d9c1": {"paper_id": "3d2126066c6244e05ec4d2631262252f4369d9c1", "abstract": "Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce \u201cdeep compression\u201d, a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35\u00d7 to 49\u00d7 without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9\u00d7 to 13\u00d7; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35\u00d7, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49\u00d7 from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3\u00d7 to 4\u00d7 layerwise speedup and 3\u00d7 to 7\u00d7 better energy efficiency.", "title": "Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding"}, "53290c029ecf9bb16d35a02198fef03a30ca0dff": {"paper_id": "53290c029ecf9bb16d35a02198fef03a30ca0dff", "abstract": "We present a novel approach to low-level vision problems that combines sparse coding and deep networks pre-trained with denoising auto-encoder (DA). We propose an alternative training scheme that successfully adapts DA, originally designed for unsupervised feature learning, to the tasks of image denoising and blind inpainting. Our method achieves state-of-the-art performance in the image denoising task. More importantly, in blind image inpainting task, the proposed method provides solutions to some complex problems that have not been tackled before. Specifically, we can automatically remove complex patterns like superimposed text from an image, rather than simple patterns like pixels missing at random. Moreover, the proposed method does not need the information regarding the region that requires inpainting to be given a priori. Experimental results demonstrate the effectiveness of the proposed method in the tasks of image denoising and blind inpainting. We also show that our new training scheme for DA is more effective and can improve the performance of unsupervised feature learning.", "title": "Image Denoising and Inpainting with Deep Neural Networks"}, "6dba6e15051ecc42997be8eb6dbc8dc5ad337085": {"paper_id": "6dba6e15051ecc42997be8eb6dbc8dc5ad337085", "abstract": "We present an approach to low-level vision that combines two main ideas: the use of convolutional networks as an image processing architecture and an unsupervised learning procedure that synthesizes training samples from specific noise models. We demonstrate this approach on the challenging problem of natural image denoising. Using a test set with a hundred natural images, we find that convolutional networks provide comparable and in some cases superior performance to state of the art wavelet and Markov random field (MRF) methods. Moreover, we find that a convolutional network offers similar performance in the blind denoising setting as compared to other techniques in the non-blind setting. We also show how convolutional networks are mathematically related to MRF approaches by presenting a mean field theory for an MRF specially designed for image denoising. Although these approaches are related, convolutional networks avoid computational difficulties in MRF approaches that arise from probabilistic learning and inference. This makes it possible to learn image processing architectures that have a high degree of representational power (we train models with over 15,000 parameters), but whose computational expense is significantly less than that associated with inference in MRF approaches with even hundreds of parameters.", "title": "Natural Image Denoising with Convolutional Networks"}, "1fdc785c0152d86d661213038150195058a24703": {"paper_id": "1fdc785c0152d86d661213038150195058a24703", "abstract": "Motivated in part by the hierarchical organization of the cortex, a number of algorithms have recently been proposed that try to learn hierarchical, or \u201cdeep,\u201d structure from unlabeled data. While several authors have formally or informally compared their algorithms to computations performed in visual area V1 (and the cochlea), little attempt has been made thus far to evaluate these algorithms in terms of their fidelity for mimicking computations at deeper levels in the cortical hierarchy. This paper presents an unsupervised learning model that faithfully mimics certain properties of visual area V2. Specifically, we develop a sparse variant of the deep belief networks of Hinton et al. (2006). We learn two layers of nodes in the network, and demonstrate that the first layer, similar to prior work on sparse coding and ICA, results in localized, oriented, edge filters, similar to the Gabor functions known to model V1 cell receptive fields. Further, the second layer in our model encodes correlations of the first layer responses in the data. Specifically, it picks up both colinear (\u201ccontour\u201d) features as well as corners and junctions. More interestingly, in a quantitative comparison, the encoding of these more complex \u201ccorner\u201d features matches well with the results from the Ito & Komatsu\u2019s study of biological V2 responses. This suggests that our sparse variant of deep belief networks holds promise for modeling more higher-order features.", "title": "Sparse deep belief net model for visual area V2"}, "158fd3c7537d9d0af4c828cc0e3948e157287f83": {"paper_id": "158fd3c7537d9d0af4c828cc0e3948e157287f83", "abstract": "Algorithms for data-driven learning of domain-specific overcomplete dictionaries are developed to obtain maximum likelihood and maximum a posteriori dictionary estimates based on the use of Bayesian models with concave/Schur-concave (CSC) negative log priors. Such priors are appropriate for obtaining sparse representations of environmental signals within an appropriately chosen (environmentally matched) dictionary. The elements of the dictionary can be interpreted as concepts, features, or words capable of succinct expression of events encountered in the environment (the source of the measured signals). This is a generalization of vector quantization in that one is interested in a description involving a few dictionary entries (the proverbial 25 words or less), but not necessarily as succinct as one entry. To learn an environmentally adapted dictionary capable of concise expression of signals generated by the environment, we develop algorithms that iterate between a representative set of sparse representations found by variants of FOCUSS and an update of the dictionary using these sparse representations. Experiments were performed using synthetic data and natural images. For complete dictionaries, we demonstrate that our algorithms have improved performance over other independent component analysis (ICA) methods, measured in terms of signal-to-noise ratios of separated sources. In the overcomplete case, we show that the true underlying dictionary and sparse sources can be accurately recovered. In tests with natural images, learned overcomplete dictionaries are shown to have higher coding efficiency than complete dictionaries; that is, images encoded with an overcomplete dictionary have both higher compression (fewer bits per pixel) and higher accuracy (lower mean square error).", "title": "Dictionary Learning Algorithms for Sparse Representation"}, "4c9cec89a2c9c8173ee53ab4cda2c021421eb7a5": {"paper_id": "4c9cec89a2c9c8173ee53ab4cda2c021421eb7a5", "abstract": "Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a structural similarity index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000. A MATLAB implementation of the proposed algorithm is available online at http://www.cns.nyu.edu//spl sim/lcv/ssim/.", "title": "Image quality assessment: from error visibility to structural similarity"}, "3e77117e5ff402b504c624765fd75fa3a73d89d2": {"paper_id": "3e77117e5ff402b504c624765fd75fa3a73d89d2", "abstract": "The discrete wavelet transform (DWT) decomposes an image into bands that vary in spatial frequency and orientation. It is widely used for image compression, measures of the visibility of DWT quantization errors are required to achieve optimal compression. Uniform quantization of a single band of coefficients results in an artifact that we call DWT uniform quantization noise; it is the sum of a lattice of random amplitude basis functions of the corresponding DWT synthesis filter. We measured visual detection thresholds for samples of DWT uniform quantization noise in Y, Cb, and Cr color channels. The spatial frequency of a wavelet is r2/sup -/spl lambda//, where r is the display visual resolution in pixels/degree, and /spl lambda/ is the wavelet level. Thresholds increase rapidly with wavelet spatial frequency. Thresholds also increase from Y to Cr to Cb, and with orientation from lowpass to horizontal/vertical to diagonal. We construct a mathematical model for DWT noise detection thresholds that is a function of level, orientation, and display visual resolution. This allows calculation of a \"perceptually lossless\" quantization matrix for which all errors are in theory below the visual threshold. The model may also be used as the basis for adaptive quantization schemes.", "title": "Visibility of wavelet quantization noise"}, "99480bf75024a4955f6753c23650f776f191d151": {"paper_id": "99480bf75024a4955f6753c23650f776f191d151", "abstract": "The ability to automatically detect \u2018visually interesting\u2019 regions in an image has many practical applications especially in the design of active machine vision systems. This paper describes a data-driven approach that uses eye tracking in tandem with principal component analysis to extract lowlevel image features that attract human gaze. Data analysis on an ensemble of image patches extracted at the observer\u2019s point of gaze revealed features that resemble derivatives of the 2D Gaussian operator. Dissimilarities between human and random fixations are investigated by comparing the features extracted at the point of gaze to the general image structure obtained by random sampling in monte-carlo simulations. Finally, a simple application where these features are used to predict fixations is illustrated.", "title": "Image features that draw fixations"}, "b4e1853acf75a91f64bd65de91ae05f4f7ef35a4": {"paper_id": "b4e1853acf75a91f64bd65de91ae05f4f7ef35a4", "abstract": "Image quality assessment plays an important role in various image processing applications. A great deal of effort has been made in recent years to develop objective image quality metrics that correlate with perceived quality measurement. Unfortunately, only limited success has been achieved. In this paper, we provide some insights on why image quality assessment is so difficult by pointing out the weaknesses of the error sensitivity based framework, which has been used by most image quality assessment approaches in the literature. Furthermore, we propose a new philosophy in designing image quality metrics: The main function of the human eyes is to extract structural information from the viewing field, and the human visual system is highly adapted for this purpose. Therefore, a measurement of structural distortion should be a good approximation of perceived image distortion. Based on the new philosophy, we implemented a simple but effective image quality indexing algorithm, which is very promising as shown by our current results.", "title": "Why is image quality assessment so difficult?"}, "cb6461c90d6733d189fe7cf31c40a4475471b31f": {"paper_id": "cb6461c90d6733d189fe7cf31c40a4475471b31f", "abstract": "In this paper we compare the performance of a number of representative instrumental models for image dissimilarity with respect to their ability to predict both image dissimilarity and image quality, as perceived by human subjects. Two sets of experimental data, one for images degraded by noise and blur, and one for JPEG-coded images, are used in the comparison. ( 1998 Elsevier Science B.V. All rights reserved.", "title": "Image dissimilarity"}, "e2f9f89f1c30a9f1f0d9493601f5803ce33b02ef": {"paper_id": "e2f9f89f1c30a9f1f0d9493601f5803ce33b02ef", "abstract": null, "title": "Perceptual Criteria for Image Quality Evaluation"}, "1fe577cdc3881149e9eebea3b1138a4901722f47": {"paper_id": "1fe577cdc3881149e9eebea3b1138a4901722f47", "abstract": "This paper analyzes the Sampled Value (SV) Process Bus concept that was recently introduced by the IEC 61850-9-2 standard. This standard proposes that the Current and Voltage Transformer (CT, PT) outputs that are presently hard wired to various devices (relays, meters, IED, and SCADA) be digitized at the source and then communicated to those devices using an Ethernet-Based Local Area Network (LAN). The approach is especially interesting for modern optical CT/PT devices that possess high quality information about the primary voltage/current waveforms, but are often forced to degrade output signal accuracy in order to meet traditional analog interface requirements (5 A/120 V). While very promising, the SV-based process bus brings along a distinct set of issues regarding the overall reliability of the new Ethernet communications-based protection and control system. This paper looks at the Merging Unit Concept, analyzes the protection system reliability in the process bus environment, and proposes an alternate approach that can be used to successfully deploy this technology. Multiple scenarios used with the associated equipment configurations are compared. Additional issues that need to be addressed by various standards bodies and interoperability challenges posed by the SV process bus LAN on real-time monitoring and control applications (substation HMI, SCADA, engineering access) are also identified.", "title": "IEC 61850-9-2 Process Bus and Its Impact on Power System Protection and Control Reliability"}, "6a39e4c99d2027df63f50ed16fbe3865951e1ed0": {"paper_id": "6a39e4c99d2027df63f50ed16fbe3865951e1ed0", "abstract": "The orientation of patches on the surface of an object can be determined from multiple images taken with different illumination, but from the same viewing position. The method, referred to as photometric stereo, can be implemented using table lookup based on numerical inversion of reflectance maps. Here we concentrate on objects with specularly reflecting surfaces, since these are of importance in industrial applications. Previous methods, intended for diffusely reflecting surfaces, employed point source illumination, which is quite unsuitable in this case. Instead, we use a distributed light source obtained by uneven illumination of a diffusely reflecting planar surface. Experimental results are shown to verify analytic expressions obtained for a method employing three light source distributions.", "title": "Determining Surface Orientations of Specular Surfaces by Using the Photometric Stereo Method"}, "3de9457c9c22bdc1c9ea23d9c1364dd18bbb9bdb": {"paper_id": "3de9457c9c22bdc1c9ea23d9c1364dd18bbb9bdb", "abstract": "SIMD vectorization has received significant attention in the past decade as an important method to accelerate scientific applications, media and embedded applications on SIMD architectures such as Intel\u00ae SSE, AVX, and IBM* AltiVec. However, most of the focus has been directed at loops, effectively executing their iterations on multiple SIMD lanes concurrently relying upon program hints and compiler analysis. This paper presents a set of new C/C++ high-level vector extensions for SIMD programming, and the Intel\u00ae C++ product compiler that is extended to translate these vector extensions and produce optimized SIMD instruction sequences of vectorized functions and loops. For a function, our main idea is to vectorize the entire function for callers instead of just vectorizing loops (if any) inside the function. It poses the challenge of dealing with complicated control-flow in the function body, and matching caller and callee for SIMD vector calls while vectorizing caller functions (or loops) and callee functions. Our compilation methods for automatically compiling vector extensions are described. We present performance results of several non-trivial visual computing, computational, and simulation workloads, utilizing SIMD units through the vector extensions on Intel\u00ae Multicore 128-bit SIMD processors, and we show that significant SIMD speedups (3.07x to 4.69x) are achieved over the serial execution.", "title": "Compiling C/C++ SIMD Extensions for Function and Loop Vectorizaion on Multicore-SIMD Processors"}, "794c409ac74f2f26fa55e3f8a6d0a38f1b1f9c8d": {"paper_id": "794c409ac74f2f26fa55e3f8a6d0a38f1b1f9c8d", "abstract": "Recent extensions to the Intel\u00ae Architecture feature the SIMD technique to enhance the performance of computational intensive applications that perform the same operation on different elements in a data set. To date, much of the code that exploits these extensions has been hand-coded. The task of the programmer is substantially simplified, however, if a compiler does this exploitation automatically. The high-performance Intel\u00ae C++/Fortran compiler supports automatic translation of serial loops into code that uses the SIMD extensions to the Intel\u00ae Architecture. This paper provides a detailed overview of the automatic vectorization methods used by this compiler together with an experimental validation of their effectiveness.", "title": "Automatic Intra-Register Vectorization for the Intel\u00ae Architecture"}, "3fc840191f8358def2aa745cca1c0425cf2c5938": {"paper_id": "3fc840191f8358def2aa745cca1c0425cf2c5938", "abstract": "When generating codes for today's multimedia extensions, one of the major challenges is to deal with memory alignment issues. While hand programming still yields best performing SIMD codes, it is both time consuming and error prone. Compiler technology has greatly improved, including techniques that simdize loops with misaligned accesses by automatically rearranging mis-aligned memory streams in registers. Current techniques are applicable to runtime alignments, but they aggressively reduce the alignment overhead only when all alignments are known at compile time. This paper presents two major enhancements to the state of the art, improving both performance and coverage. First, we propose a novel technique to simdize loops with runtime alignment nearly as efficiently as those with compile-time misalignment. Runtime alignment is pervasive in real applications because it is either part of the algorithms, or it is an artifact of the compiler's inability to extract accurate alignment information from complex applications. Second, we incorporate length conversion operations, e.g., conversions between data of different sizes, into the alignment handling framework. Length conversions are pervasive in multimedia applications where mixed integer types are often used. Supporting length conversion can greatly improve the coverage of simdizable loops. Experimental results indicate that our runtime alignment technique achieves a 19% to 32% speedup increase over prior art for a benchmark stressing the impact of misaligned data. We also demonstrate speedup factors of up to 8.11 for real benchmarks over sequential execution.", "title": "Efficient SIMD code generation for runtime alignment and length conversion"}, "55ebc9c86efc3dae3e0c5345dda662c210f62ff2": {"paper_id": "55ebc9c86efc3dae3e0c5345dda662c210f62ff2", "abstract": "When vectorizing for SIMD architectures that are commonly employed by today's multimedia extensions, one of the new challenges that arise is the handling of memory alignment. Prior research has focused primarily on vectorizing loops where all memory references are properly aligned. An important aspect of this problem, namely, how to vectorize misaligned memory references, still remains unaddressed.This paper presents a compilation scheme that systematically vectorizes loops in the presence of misaligned memory references. The core of our technique is to automatically reorganize data in registers to satisfy the alignment requirement imposed by the hardware. To reduce the data reorganization overhead, we propose several techniques to minimize the number of data reorganization operations generated. During the code generation, our algorithm also exploits temporal reuse when aligning references that access contiguous memory across loop iterations. Our code generation scheme guarantees to never load the same data associated with a single static access twice. Experimental results indicate near peak speedup factors, e.g., 3.71 for 4 data per vector and 6.06 for 8 data per vector, respectively, for a set of loops where 75% or more of the static memory references are misaligned.", "title": "Vectorization for SIMD architectures with alignment constraints"}, "5da58bb88b96aee8571d048df662c42b9af21266": {"paper_id": "5da58bb88b96aee8571d048df662c42b9af21266", "abstract": "This paper presents modelling of ball and plate systems based on first principles by considering balance of forces and torques. A non-linear model is derived considering the dynamics of motors, gears, ball and plate. The non-linear model is linearized near the operating region to obtain a standard state space model. This linear model is used for discrete optimal control of the ball and plate system \u2014 the trajectory of the ball is controlled by control voltages to the motor.", "title": "Modelling of ball and plate system based on first principle model and optimal control"}, "9e76c9d048ef3eef1ae667c892ec642ffd25ec40": {"paper_id": "9e76c9d048ef3eef1ae667c892ec642ffd25ec40", "abstract": "Gabor wavelets have been successfully applied for a variety of machine vision applications such as Texture segmentation, Edge detection, Boundary detection etc. As the Fourier transform is not suitable for detecting local defects, and the Wavelet transforms posses only limited number of orientations, Gabor wavelet transform is chosen and applied to detect the defects in fabrics. Gabor filters scheme that imitates the early human vision process is applied to the sample under inspection. Defects can be automatically segmented from the regular texture by applying the proposed method. Proper thresholding ensures segmentation of the defect from the texture background. The results obtained using this method confirms its efficiency. This can also be applied to detect defects on surfaces and materials that have regular periodic texture.", "title": "Fault segmentation in fabric images using Gabor wavelet transform"}, "f09a1acd38a781a3ab6cb31c8e4f9f29be5f58cb": {"paper_id": "f09a1acd38a781a3ab6cb31c8e4f9f29be5f58cb", "abstract": "Faced with continuously increasing scale of data, original back-propagation neural network based machine learning algorithm presents two non-trivial challenges: huge amount of data makes it difficult to maintain both efficiency and accuracy; redundant data aggravates the system workload. This project is mainly focused on the solution to the issues above, combining deep learning algorithm with cloud computing platform to deal with large-scale data. A MapReduce-based handwriting character recognizer will be designed in this project to verify the efficiency improvement this mechanism will achieve on training and practical large-scale data. Careful discussion and experiment will be developed to illustrate how deep learning algorithm works to train handwritten digits data, how MapReduce is implemented on deep learning neural network, and why this combination accelerates computation. Besides performance, the scalability and robustness will be mentioned in this report as well. Our system comes with two demonstration software that visually illustrates our handwritten digit recognition/encoding application. 1", "title": "Large-scale Artificial Neural Network: MapReduce-based Deep Learning"}, "508f71d19adf9caf48ed995011eb7ecbcc94cbe5": {"paper_id": "508f71d19adf9caf48ed995011eb7ecbcc94cbe5", "abstract": "Competition in the wireless telecommunications industry is fierce. To maintain profitability, wireless carriers must control churn, which is the loss of subscribers who switch from one carrier to another.We explore techniques from statistical machine learning to predict churn and, based on these predictions, to determine what incentives should be offered to subscribers to improve retention and maximize profitability to the carrier. The techniques include logit regression, decision trees, neural networks, and boosting. Our experiments are based on a database of nearly 47,000 U.S. domestic subscribers and includes information about their usage, billing, credit, application, and complaint history. Our experiments show that under a wide variety of assumptions concerning the cost of intervention and the retention rate resulting from intervention, using predictive techniques to identify potential churners and offering incentives can yield significant savings to a carrier. We also show the importance of a data representation crafted by domain experts. Finally, we report on a real-world test of the techniques that validate our simulation experiments.", "title": "Predicting subscriber dissatisfaction and improving retention in the wireless telecommunications industry"}, "a8ebda8d11185b504e5df02b419fefd8850847fc": {"paper_id": "a8ebda8d11185b504e5df02b419fefd8850847fc", "abstract": "Boosting is a general method for improving the performance of learning algorithms. A recently proposed boosting algorithm, Ada Boost, has been applied with great success to several benchmark machine learning problems using mainly decision trees as base classifiers. In this article we investigate whether Ada Boost also works as well with neural networks, and we discuss the advantages and drawbacks of different versions of the Ada Boost algorithm. In particular, we compare training methods based on sampling the training set and weighting the cost function. The results suggest that random resampling of the training data is not the main explanation of the success of the improvements brought by Ada Boost. This is in contrast to bagging, which directly aims at reducing variance and for which random resampling is essential to obtain the reduction in generalization error. Our system achieves about 1.4 error on a data set of on-line handwritten digits from more than 200 writers. A boosted multilayer network achieved 1.5 error on the UCI letters and 8.1 error on the UCI satellite data set, which is significantly better than boosted decision trees.", "title": "Boosting Neural Networks"}, "3d8650c28ae2b0f8d8707265eafe53804f83f416": {"paper_id": "3d8650c28ae2b0f8d8707265eafe53804f83f416", "abstract": "In an earlier paper [9], we introduced a new \u201cboosting\u201d algorithm called AdaBoost which, theoretically, can be used to significantly reduce the error of any learning algorithm that consistently generates classifiers whose performance is a little better than random guessing. We also introduced the related notion of a \u201cpseudo-loss\u201d which is a method for forcing a learning algorithm of multi-label concepts to concentrate on the labels that are hardest to discriminate. In this paper, we describe experiments we carried out to assess how well AdaBoost with and without pseudo-loss, performs on real learning problems. We performed two sets of experiments. The first set compared boosting to Breiman\u2019s [1] \u201cbagging\u201d method when used to aggregate various classifiers (including decision trees and single attribute-value tests). We compared the performance of the two methods on a collection of machine-learning benchmarks. In the second set of experiments, we studied in more detail the performance of boosting using a nearest-neighbor classifier on an OCR problem.", "title": "Experiments with a New Boosting Algorithm"}, "a7a1a298c44b5aff9bdca6ea9908c425796404b7": {"paper_id": "a7a1a298c44b5aff9bdca6ea9908c425796404b7", "abstract": "Practical experience has shown that in order to obtain the best possible performance, prior knowledge about invariances of a classification problem at hand ought to be incorporated into the training procedure. We describe and review all known methods for doing so in support vector machines, provide experimental results, and discuss their respective merits. One of the significant new results reported in this work is our recent achievement of the lowest reported test error on the well-known MNIST digit recognition benchmark task, with SVM training times that are also significantly faster than previous SVM methods.", "title": "Training Invariant Support Vector Machines"}, "7b1cc19dec9289c66e7ab45e80e8c42273509ab6": {"paper_id": "7b1cc19dec9289c66e7ab45e80e8c42273509ab6", "abstract": "Neural networks are a powerful technology for classification of visual inputs arising from documents. However, there is a confusing plethora of different neural network methods that are used in the literature and in industry. This paper describes a set of concrete best practices that document analysis researchers can use to get good results with neural networks. The most important practice is getting a training set as large as possible: we expand the training set by adding a new form of distorted data. The next most important practice is that convolutional neural networks are better suited for visual document tasks than fully connected networks. We propose that a simple \u201cdo-it-yourself\u201d implementation of convolution with a flexible architecture is suitable for many visual document problems. This simple convolutional neural network does not require complex methods, such as momentum, weight decay, structuredependent learning rates, averaging layers, tangent prop, or even finely-tuning the architecture. The end result is a very simple yet general architecture which can yield state-of-the-art performance for document analysis. We illustrate our claims on the MNIST set of English digit", "title": "Best Practices for Convolutional Neural Networks Applied to Visual Document Analysis"}, "2077d0f30507d51a0d3bbec4957d55e817d66a59": {"paper_id": "2077d0f30507d51a0d3bbec4957d55e817d66a59", "abstract": "We develop a framework for learning generic, expressive image priors that capture the statistics of natural scenes and can be used for a variety of machine vision tasks. The approach extends traditional Markov random field (MRF) models by learning potential functions over extended pixel neighborhoods. Field potentials are modeled using a Products-of-Experts framework that exploits nonlinear functions of many linear filter responses. In contrast to previous MRF approaches all parameters, including the linear filters themselves, are learned from training data. We demonstrate the capabilities of this Field of Experts model with two example applications, image denoising and image inpainting, which are implemented using a simple, approximate inference scheme. While the model is trained on a generic image database and is not tuned toward a specific application, we obtain results that compete with and even outperform specialized techniques.", "title": "Fields of Experts: a framework for learning image priors"}, "589b8659007e1124f765a5d1bd940b2bf4d79054": {"paper_id": "589b8659007e1124f765a5d1bd940b2bf4d79054", "abstract": "Your use of the JSTOR archive indicates your acceptance of JSTOR's Terms and Conditions of Use, available at http://www.jstor.org/page/info/about/policies/terms.jsp. JSTOR's Terms and Conditions of Use provides, in part, that unless you have obtained prior permission, you may not download an entire issue of a journal or multiple copies of articles, and you may use content in the JSTOR archive only for your personal, non-commercial use.", "title": "Projection Pursuit Regression"}, "acb1c4074176962299717b940d4f4b9bead5c6e4": {"paper_id": "acb1c4074176962299717b940d4f4b9bead5c6e4", "abstract": "A smart Unmanned Ground Vehicle (UGV) is designed and developed for some application specific missions to operate predominantly in hazardous environments. In our work, we have developed a small and lightweight vehicle to operate in general crosscountry terrains in or without daylight. The UGV can send visual feedbacks to the operator at a remote location. Onboard infrared sensors can detect the obstacles around the UGV and sends signals to the operator.", "title": "Design of a Smart Unmanned Ground Vehicle for Hazardous Environments"}, "6d0ec4e74cdca18e493a05d8ef938990c71e09eb": {"paper_id": "6d0ec4e74cdca18e493a05d8ef938990c71e09eb", "abstract": "Neuromyelitis optica (NMO) is an inflammatory CNS syndrome distinct from multiple sclerosis (MS) that is associated with serum aquaporin-4 immunoglobulin G antibodies (AQP4-IgG). Prior NMO diagnostic criteria required optic nerve and spinal cord involvement but more restricted or more extensive CNS involvement may occur. The International Panel for NMO Diagnosis (IPND) was convened to develop revised diagnostic criteria using systematic literature reviews and electronic surveys to facilitate consensus. The new nomenclature defines the unifying term NMO spectrum disorders (NMOSD), which is stratified further by serologic testing (NMOSD with or without AQP4-IgG). The core clinical characteristics required for patients with NMOSD with AQP4-IgG include clinical syndromes or MRI findings related to optic nerve, spinal cord, area postrema, other brainstem, diencephalic, or cerebral presentations. More stringent clinical criteria, with additional neuroimaging findings, are required for diagnosis of NMOSD without AQP4IgG or when serologic testing is unavailable. The IPND also proposed validation strategies and achieved consensus on pediatric NMOSD diagnosis and the concepts of monophasic NMOSD and opticospinal MS. Neurology\u00ae 2015;85:1\u201313 GLOSSARY ADEM 5 acute disseminated encephalomyelitis; AQP4 5 aquaporin-4; IgG 5 immunoglobulin G; IPND 5 International Panel for NMO Diagnosis; LETM 5 longitudinally extensive transverse myelitis lesions; MOG 5 myelin oligodendrocyte glycoprotein; MS 5 multiple sclerosis; NMO 5 neuromyelitis optica; NMOSD 5 neuromyelitis optica spectrum disorders; SLE 5 systemic lupus erythematosus; SS 5 Sj\u00f6gren syndrome. Neuromyelitis optica (NMO) is an inflammatory CNS disorder distinct from multiple sclerosis (MS). It became known as Devic disease following a seminal 1894 report. Traditionally, NMO was considered a monophasic disorder consisting of simultaneous bilateral optic neuritis and transverse myelitis but relapsing cases were described in the 20th century. MRI revealed normal brain scans and$3 vertebral segment longitudinally extensive transverse myelitis lesions (LETM) in NMO. The nosology of NMO, especially whether it represented a topographically restricted form of MS, remained controversial. A major advance was the discovery that most patients with NMO have detectable serum antibodies that target the water channel aquaporin-4 (AQP4\u2013immunoglobulin G [IgG]), are highly specific for clinically diagnosed NMO, and have pathogenic potential. In 2006, AQP4-IgG serology was incorporated into revised NMO diagnostic criteria that relaxed clinical From the Departments of Neurology (D.M.W.) and Library Services (K.E.W.), Mayo Clinic, Scottsdale, AZ; the Children\u2019s Hospital of Philadelphia (B.B.), PA; the Departments of Neurology and Ophthalmology (J.L.B.), University of Colorado Denver, Aurora; the Service de Neurologie (P.C.), Centre Hospitalier Universitaire de Fort de France, Fort-de-France, Martinique; Department of Neurology (W.C.), Sir Charles Gairdner Hospital, Perth, Australia; the Department of Neurology (T.C.), Massachusetts General Hospital, Boston; the Department of Neurology (J.d.S.), Strasbourg University, France; the Department of Multiple Sclerosis Therapeutics (K.F.), Tohoku University Graduate School of Medicine, Sendai, Japan; the Departments of Neurology and Neurotherapeutics (B.G.), University of Texas Southwestern Medical Center, Dallas; The Walton Centre NHS Trust (A.J.), Liverpool, UK; the Molecular Neuroimmunology Group, Department of Neurology (S.J.), University Hospital Heidelberg, Germany; the Center for Multiple Sclerosis Investigation (M.L.-P.), Federal University of Minas Gerais Medical School, Belo Horizonte, Brazil; the Department of Neurology (M.L.), Johns Hopkins University, Baltimore, MD; Portland VA Medical Center and Oregon Health and Sciences University (J.H.S.), Portland; the Department of Neurology (S.T.), National Pediatric Hospital Dr. Juan P. Garrahan, Buenos Aires, Argentina; the Department of Medicine (A.L.T.), University of British Columbia, Vancouver, Canada; Nuffield Department of Clinical Neurosciences (P.W.), University of Oxford, UK; and the Department of Neurology (B.G.W.), Mayo Clinic, Rochester, MN. Go to Neurology.org for full disclosures. Funding information and disclosures deemed relevant by the authors, if any, are provided at the end of the article. The Article Processing Charge was paid by the Guthy-Jackson Charitable Foundation. This is an open access article distributed under the terms of the Creative Commons Attribution-NonCommercial-NoDerivatives License 4.0 (CC BY-NC-ND), which permits downloading and sharing the work provided it is properly cited. The work cannot be changed in any way or used commercially. \u00a9 2015 American Academy of Neurology 1 a 2015 American Academy of Neurology. Unauthorized reproduction of this article is prohibited. Published Ahead of Print on June 19, 2015 as 10.1212/WNL.0000000000001729", "title": "requirements by permitting unilateral optic neuritis or asymptomatic brain MRI lesions"}, "5c3711609f82240a95415ce63186cd3caf93314f": {"paper_id": "5c3711609f82240a95415ce63186cd3caf93314f", "abstract": "This paper presents a electromyography (EMG) control for a hand exoskeleton. The device was developed with focus on support of the rehabilitation process after hand injuries or strokes. As the device is designed for the later use on patients, which have limited hand mobility, fast undesired movements have to be averted. Safety precautions in the hardware and software design of the system must be taken to ensure this. The construction allows controlling the motion of finger joints. However, due to friction in gears and mechanical construction it is not possible to move finger joints within the construction without help of actuators. Therefore force sensors are integrated into the construction to measure force exchanged between human and exoskeleton. These allow the human to control the movements of the hand exoskeleton which is useful to teach new trajectories, for muscle training, or for diagnostic purposes. The control method using electromyography (EMG) sensor presented in this paper uses the EMG sensor values to generate a trajectory, which is executed by a position control loop based on sliding mode control.", "title": "Electromyography sensor based control for a hand exoskeleton"}, "2b0dde28e69af453b26273a2f5084118a6b77ea5": {"paper_id": "2b0dde28e69af453b26273a2f5084118a6b77ea5", "abstract": "The principal goal of this work is the development, testing and experimentation of a device for the hand rehabilitation. The system we designed is intended for people who have partially lost the ability to control correctly the hand musculature, for example after a stroke or a spinal cord injure. Based on EMG signals the system can \"understand\" the subject volition to move the hand and thanks to its actuators can help the fingers movement in order to perform the task. In this paper we describe the device and discuss the first results conducted on a healthy volunteer.", "title": "An EMG-controlled exoskeleton for hand rehabilitation"}, "d2a839a07e5cb9ff5ed66e03e3a5ba042a868827": {"paper_id": "d2a839a07e5cb9ff5ed66e03e3a5ba042a868827", "abstract": "This paper presents a force-based control mode for a hand exoskeleton. This device has been developed with focus on support of the rehabilitation process after hand injuries or strokes. As the device is designed for the later use on patients, which have limited hand mobility, fast undesired movements have to be averted. Safety precautions in the hardware and software design of the system must be taken to ensure this. The construction allows controlling motions of the finger joints. However, due to friction in gears and mechanical construction, it is not possible to move finger joints within the construction without help of actuators. Therefore force sensors are integrated into the construction to sense force exchanged between human and exoskeleton. These allow the human to control the movements of the hand exoskeleton, which is useful to teach new trajectories or can be used for diagnostic purposes. The force control scheme presented in this paper uses the force sensor values to generate a trajectory which is executed by a position control loop based on sliding mode control", "title": "Force Control Strategy for a Hand Exoskeleton Based on Sliding Mode Position Control"}, "3d1584dc3e7a1186cbe0a5d0b44f1c47c90dc985": {"paper_id": "3d1584dc3e7a1186cbe0a5d0b44f1c47c90dc985", "abstract": "This paper discusses the working of buck-boost converter as power factor correction controller applicable for battery charging in electric vehicles application. This topology will work in buck mode and boost mode of operation reliant on the rectified voltage at input side and battery voltage at load side. This paper presents an informal and effective line frequency current shaping control arrangement used for achieving power factor nearer to unity. It also satisfies the harmonic compliance of source current in accordance to the IEEE519 recommendations. This objective is achieved by active power factor control circuit using continuous conduction mode (CCM) of buck-boost converter implementing adaptable duty cycle control scheme. The control is very simple and gives good performance. The performance of the control scheme is simulated for both open-loop and closed-loop control in Matlab/Simulink environment.", "title": "Buck-Boost converter as power factor correction controller for plug-in electric vehicles and battery charging application"}, "4b459775f86584c35d3bc2eb8b2cf7d8569052e2": {"paper_id": "4b459775f86584c35d3bc2eb8b2cf7d8569052e2", "abstract": "In a multi-domain, multi-turn spoken language understanding session, information from the history often greatly reduces the ambiguity of the current turn. In this paper, we apply the recurrent neural network (RNN) to exploit contextual information for query domain classification. The Jordan-type RNN directly sends the vector of output distribution to the next query turn as additional input features to the convolutional neural network (CNN). We evaluate our approach against SVM with and without contextual features. On our contextually labeled dataset, we observe a 1.4% absolute (8.3% relative) improvement in classification error rate over the non-contextual SVM, and 0.9% absolute (5.5% relative) improvement over the contextual SVM.", "title": "Contextual domain classification in spoken language understanding systems using recurrent neural network"}, "23694a80bf1b9b38215be3e23068dd75296bc90f": {"paper_id": "23694a80bf1b9b38215be3e23068dd75296bc90f", "abstract": "A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.", "title": "A Neural Probabilistic Language Model"}, "969a9ec5f24dabcfb9c70c7ee04625075a6c0a98": {"paper_id": "969a9ec5f24dabcfb9c70c7ee04625075a6c0a98", "abstract": "The description of a novel type of rn-gram language model is given. The model offers, via a nonlinear recursive procedure, a computation and space efficient solution to the problem of estimating probabilities from sparse data. This solution compares favorably to other proposed methods. While the method has been developed for and successfully implemented in the IBM Real Time Speech Recognizers, its generality makes it applicable in other areas where the problem of estimating probabilities from sparse data arises. Sparseness of data is an inherent property of any real text, and it is a problem that one always encounters while collecting frequency statistics on words and word sequences (m-grams) from a text of finite size. This means that even for a very large data collection, the maximum likelihood estimation method does not allow us to adequately estimate probabilities of rare but nevertheless possible word sequences-many sequences occur only once (\u201csingletons\u201d); many more do not occur at all. Inadequacy of the maximum likelihood estimator and the necessity to estimate the probabilities of m-grams which did not occur in the text constitute the essence of the problem. The main idea of the proposed solution to the problem is to reduce unreliable probability estimates given by the observed frequencies and redistribute the \u201cfreed\u201d probability \u201cmass\u201d among m-grams which never occurred in the text. The reduction is achieved by replacing maximum likelihood estimates for m-grams having low counts with renormalized Turing\u2019s estimates [l], and the redistribution is done via the recursive utilization of lower level conditional distributions. We found Turing\u2019s method attractive because of its simplicity and its characterization as the optimal empirical Bayes\u2019 estimator of a multinomial probability. Robbins in [2] introduces the empirical Bayes\u2019 methodology and Nadas in [3] gives various derivations of the Turing\u2019s formula. Let N be a sample text size and let n, be the number of words (m-grams) which occurred in the text exactly r times, so that N = C rn,. (1) Turing\u2019s estimate PT for a probability of a word (m-gram) which occurred in the sample r times is r", "title": "Estimation of probabilities from sparse data for the language model component of a speech recognizer"}, "197a7fc2f8d57d93727b348851b59b34ce990afd": {"paper_id": "197a7fc2f8d57d93727b348851b59b34ce990afd", "abstract": "SRILM is a collection of C++ libraries, executable programs, and helper scripts designed to allow both production of and experimentation with statistical language models for speech recognition and other applications. SRILM is freely available for noncommercial purposes. The toolkit supports creation and evaluation of a variety of language model types based on N-gram statistics, as well as several related tasks, such as statistical tagging and manipulation of N-best lists and word lattices. This paper summarizes the functionality of the toolkit and discusses its design and implementation, highlighting ease of rapid prototyping, reusability, and combinability of tools.", "title": "SRILM - an extensible language modeling toolkit"}, "65f48abff4c8d74f3fe2081af2ad3deebe7fa18f": {"paper_id": "65f48abff4c8d74f3fe2081af2ad3deebe7fa18f", "abstract": "In the past several years, a number of different language modeling improvements over simple trigram models have been found, including caching, higher-order n-grams, skipping, interpolated Kneser\u2013Ney smoothing, and clustering. We present explorations of variations on, or of the limits of, each of these techniques, including showing that sentence mixture models may have more potential. While all of these techniques have been studied separately, they have rarely been studied in combination. We compare a combination of all techniques together to a Katz smoothed trigram model with no count cutoffs. We achieve perplexity reductions between 38 and 50% (1 bit of entropy), depending on training data size, as well as a word error rate reduction of 8 .9%. Our perplexity reductions are perhaps the highest reported compared to a fair baseline. c \u00a9 2001 Academic Press", "title": "A Bit of Progress in Language Modeling"}, "d87ceda3042f781c341ac17109d1e94a717f5f60": {"paper_id": "d87ceda3042f781c341ac17109d1e94a717f5f60", "abstract": "WordNet is perhaps the most important and widely used lexical resource for natural language processing systems up to now. WordNet: An Electronic Lexical Database, edited by Christiane Fellbaum, discusses the design of WordNet from both theoretical and historical perspectives, provides an up-to-date description of the lexical database, and presents a set of applications of WordNet. The book contains a foreword by George Miller, an introduction by Christiane Fellbaum, seven chapters from the Cognitive Sciences Laboratory of Princeton University, where WordNet was produced, and nine chapters contributed by scientists from elsewhere. Miller's foreword offers a fascinating account of the history of WordNet. He discusses the presuppositions of such a lexical database, how the top-level noun categories were determined, and the sources of the words in WordNet. He also writes about the evolution of WordNet from its original incarnation as a dictionary browser to a broad-coverage lexicon, and the involvement of different people during its various stages of development over a decade. It makes very interesting reading for casual and serious users of WordNet and anyone who is grateful for the existence of WordNet. The book is organized in three parts. Part I is about WordNet itself and consists of four chapters: \"Nouns in WordNet\" by George Miller, \"Modifiers in WordNet\" by Katherine Miller, \"A semantic network of English verbs\" by Christiane Fellbaum, and \"Design and implementation of the WordNet lexical database and search software\" by Randee Tengi. These chapters are essentially updated versions of four papers from Miller (1990). Compared with the earlier papers, the chapters in this book focus more on the underlying assumptions and rationales behind the design decisions. The description of the information contained in WordNet, however, is not as detailed as in Miller (1990). The main new additions in these chapters include an explanation of sense grouping in George Miller's chapter, a section about adverbs in Katherine Miller's chapter, observations about autohyponymy (one sense of a word being a hyponym of another sense of the same word) and autoantonymy (one sense of a word being an antonym of another sense of the same word) in Fellbaum's chapter, and Tengi's description of the Grinder, a program that converts the files the lexicographers work with to searchable lexical databases. The three papers in Part II are characterized as \"extensions, enhancements and", "title": "WordNet : an electronic lexical database"}, "47a87c2cbdd928bb081974d308b3d9cf678d257e": {"paper_id": "47a87c2cbdd928bb081974d308b3d9cf678d257e", "abstract": "A new recurrent neural network based language model (RNN LM) with applications to speech recognition is presented. Results indicate that it is possible to obtain around 50% reduction of perplexity by using mixture of several RNN LMs, compared to a state of the art backoff language model. Speech recognition experiments show around 18% reduction of word error rate on the Wall Street Journal task when comparing models trained on the same amount of data, and around 5% on the much harder NIST RT05 task, even when the backoff model is trained on much more data than the RNN LM. We provide ample empirical evidence to suggest that connectionist language models are superior to standard n-gram techniques, except their high computational (training) complexity.", "title": "Recurrent neural network based language model"}, "0be2f4b0643ccb49d3941491fdbef11e6446dece": {"paper_id": "0be2f4b0643ccb49d3941491fdbef11e6446dece", "abstract": "Previous work on statistical language modeling has shown that it is possible to train a feedforward neural network to approximate probabilities over sequences of words, resulting in significant error reduction when compared to standard baseline models based on n-grams. However, training the neural network model with the maximum-likelihood criterion requires computations proportional to the number of words in the vocabulary. In this paper, we introduce adaptive importance sampling as a way to accelerate training of the model. The idea is to use an adaptive n-gram model to track the conditional distributions produced by the neural network. We show that a very significant speedup can be obtained on standard problems.", "title": "Adaptive Importance Sampling to Accelerate Training of a Neural Probabilistic Language Model"}, "7f9e4f3e404ea350f2f9a97315ef4b1771367d8a": {"paper_id": "7f9e4f3e404ea350f2f9a97315ef4b1771367d8a", "abstract": "Speech recognition of inflectional and morphologically rich languages like Czech is currently quite a challenging task, because simple n-gram techniques are unable to capture important regularities in the data. Several possible solutions were proposed, namely class based models, factored models, decision trees and neural networks. This paper describes improvements obtained in recognition of spoken Czech lectures using language models based on neural networks. Relative reductions in word error rate are more than 15% over baseline obtained with adapted 4-gram backoff language model using modified Kneser-Ney smoothing.", "title": "Neural network based language models for highly inflective languages"}, "bfea4d58717c83c67ac3f9eab855d15c59754757": {"paper_id": "bfea4d58717c83c67ac3f9eab855d15c59754757", "abstract": "In recent years, variants of a neural network architecture for statistical language modeling have been proposed and successfully applied, e.g. in the language modeling component of speech recognizers. The main advantage of these architectures is that they learn an embedding for words (or other symbols) in a continuous space that helps to smooth the language model and provide good generalization even when the number of training examples is insufficient. However, these models are extremely slow in comparison to the more commonly used n-gram models, both for training and recognition. As an alternative to an importance sampling method proposed to speed-up training, we introduce a hierarchical decomposition of the conditional probabilities that yields a speed-up of about 200 both during training and recognition. The hierarchical decomposition is a binary hierarchical clustering constrained by the prior knowledge extracted from the WordNet semantic hierarchy.", "title": "Hierarchical Probabilistic Neural Network Language Model"}, "4b7a0ba426690b08489a86038db161846ffcfaa9": {"paper_id": "4b7a0ba426690b08489a86038db161846ffcfaa9", "abstract": "This paper provides guidance to some of the concepts surrounding recurrent neural networks. Contrary to feedforward networks, recurrent networks can be sensitive, and be adapted to past inputs. Backpropagation learning is described for feedforward networks, adapted to suit our (probabilistic) modeling needs, and extended to cover recurrent networks. The aim of this brief paper is to set the scene for applying and understanding recurrent neural networks.", "title": "A guide to recurrent neural networks and backpropagation"}, "f14049b1f44b782134e248102c1f03539436fa4c": {"paper_id": "f14049b1f44b782134e248102c1f03539436fa4c", "abstract": "During the last years there has been growing interest in using neural networks for language modeling. In contrast to the well known back-offn-gram language models, the neural network approach attempts to overcome the data sparseness problem by performing the estimation in a continuous space. This type of language model was mostly used for tasks for which only a very limited amount of in-domain training data is available. In this paper we present new algorithms to train a neural network language model on very large text corpora. This makes possible the use of the approach in domains where several hundreds of millions words of texts are available. The neural network language model is evaluated in a state-ofthe-art real-time continuous speech recognizer for French Broadcast News. Word error reductions of 0.5% absolute are reported using only a very limited amount of additional processing time.", "title": "Training Neural Network Language Models on Very Large Corpora"}, "39f63dbdce9207b87878290c0e3983e84cfcecd9": {"paper_id": "39f63dbdce9207b87878290c0e3983e84cfcecd9", "abstract": "We propose a novel context-dependent (CD) model for large-vocabulary speech recognition (LVSR) that leverages recent advances in using deep belief networks for phone recognition. We describe a pre-trained deep neural network hidden Markov model (DNN-HMM) hybrid architecture that trains the DNN to produce a distribution over senones (tied triphone states) as its output. The deep belief network pre-training algorithm is a robust and often helpful way to initialize deep neural networks generatively that can aid in optimization and reduce generalization error. We illustrate the key components of our model, describe the procedure for applying CD-DNN-HMMs to LVSR, and analyze the effects of various modeling choices on performance. Experiments on a challenging business search dataset demonstrate that CD-DNN-HMMs can significantly outperform the conventional context-dependent Gaussian mixture model (GMM)-HMMs, with an absolute sentence accuracy improvement of 5.8% and 9.2% (or relative error reduction of 16.0% and 23.2%) over the CD-GMM-HMMs trained using the minimum phone error rate (MPE) and maximum-likelihood (ML) criteria, respectively.", "title": "Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition"}, "1835227a28b84b8e7c93a7232437b54c31c52a02": {"paper_id": "1835227a28b84b8e7c93a7232437b54c31c52a02", "abstract": "It is possible to combine multiple latent-variable models of the same data by multiplying their probability distributions together and then renormalizing. This way of combining individual expert models makes it hard to generate samples from the combined model but easy to infer the values of the latent variables of each expert, because the combination rule ensures that the latent variables of different experts are conditionally independent when given the data. A product of experts (PoE) is therefore an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary. Training a PoE by maximizing the likelihood of the data is difficult because it is hard even to approximate the derivatives of the renormalization term in the combination rule. Fortunately, a PoE can be trained using a different objective function called contrastive divergence whose derivatives with regard to the parameters can be approximated accurately and efficiently. Examples are presented of contrastive divergence learning using several types of expert on several types of data.", "title": "Training Products of Experts by Minimizing Contrastive Divergence"}, "a928ee872e4b91d8a32bd6ee1abde15f2b7e508d": {"paper_id": "a928ee872e4b91d8a32bd6ee1abde15f2b7e508d", "abstract": "This paper investigates the use of deep belief networks (DBN) for semantic tagging, a sequence classification task, in spoken language understanding (SLU). We evaluate the performance of the DBN based sequence tagger on the well-studied ATIS task and compare our technique to conditional random fields (CRF), a state-of-the-art classifier for sequence classification. In conjunction with lexical and named entity features, we also use dependency parser based syntactic features and part of speech (POS) tags [1]. Under both noisy conditions (output of automatic speech recognition system) and clean conditions (manual transcriptions), our deep belief network based sequence tagger outperforms the best CRF based system described in [1] by an absolute 2% and 1% F-measure, respectively.Upon carrying out an analysis of cases where CRF and DBN models made different predictions, we observed that when discrete features are projected onto a continuous space during neural network training, the model learns to cluster these features leading to its improved generalization capability, relative to a CRF model, especially in cases where some features are either missing or noisy.", "title": "Deep belief network based semantic taggers for spoken language understanding"}, "111fd833a4ae576cfdbb27d87d2f8fc0640af355": {"paper_id": "111fd833a4ae576cfdbb27d87d2f8fc0640af355", "abstract": null, "title": "Learning internal representations by error propagation"}, "f8d77bb8da085ec419866e0f87e4efc2577b6141": {"paper_id": "f8d77bb8da085ec419866e0f87e4efc2577b6141", "abstract": "My sincere thanks to Donald Norman and David Rumelhart for their support of many years. I also wish to acknowledge the help of The views and conclusions contained in this document are those of the author and should not be interpreted as necessarily representing the official policies, either expressed or implied, of the sponsoring agencies. Approved for public release; distribution unlimited. Reproduction in whole or in part is permitted for any purpose of the United States Government Requests for reprints should be sent to the", "title": "Serial Order: A Parallel Distributed Processing Approach"}, "29a5e1d5f4f3a33f473f81bcc89efb174ee81053": {"paper_id": "29a5e1d5f4f3a33f473f81bcc89efb174ee81053", "abstract": "We analyze a Relational Neighbor (RN) classifier, a simple relational predictive model that predicts only based on class labels of related neighbors, using no learning and no inherent attributes. We show that it performs surprisingly well by comparing it to more complex models such as Probabilistic Relational Models and Relational Probability Trees on three data sets from published work. We argue that a simple model such as this should be used as a baseline to assess the performance of relational learners.", "title": "A Simple Relational Classifier"}, "1b8290ff2fe1b04df14f2504b38beb749e2e75ca": {"paper_id": "1b8290ff2fe1b04df14f2504b38beb749e2e75ca", "abstract": "Zero-shot recognition (ZSR) deals with the problem of predicting class labels for target domain instances based on source domain side information (e.g. attributes) of unseen classes. We formulate ZSR as a binary prediction problem. Our resulting classifier is class-independent. It takes an arbitrary pair of source and target domain instances as input and predicts whether or not they come from the same class, i.e. whether there is a match. We model the posterior probability of a match since it is a sufficient statistic and propose a latent probabilistic model in this context. We develop a joint discriminative learning framework based on dictionary learning to jointly learn the parameters of our model for both domains, which ultimately leads to our class-independent classifier. Many of the existing embedding methods can be viewed as special cases of our probabilistic model. On ZSR our method shows 4.90% improvement over the state-of-the-art in accuracy averaged across four benchmark datasets. We also adapt ZSR method for zero-shot retrieval and show 22.45% improvement accordingly in mean average precision (mAP).", "title": "Zero-Shot Learning via Joint Latent Similarity Embedding"}, "0f6911bc1e6abee8bbf9dd3f8d54d40466429da7": {"paper_id": "0f6911bc1e6abee8bbf9dd3f8d54d40466429da7", "abstract": "We consider the problem of zero-shot learning, where the goal is to learn a classifier f : X \u2192 Y that must predict novel values of Y that were omitted from the training set. To achieve this, we define the notion of a semantic output code classifier (SOC) which utilizes a knowledge base of semantic properties of Y to extrapolate to novel classes. We provide a formalism for this type of classifier and study its theoretical properties in a PAC framework, showing conditions under which the classifier can accurately predict novel classes. As a case study, we build a SOC classifier for a neural decoding task and show that it can often predict words that people are thinking about from functional magnetic resonance images (fMRI) of their neural activity, even without training examples for those words.", "title": "Zero-shot Learning with Semantic Output Codes"}, "dbe8c61628896081998d1cd7d10343a45b7061bd": {"paper_id": "dbe8c61628896081998d1cd7d10343a45b7061bd", "abstract": "Several strategies are described that overcome limitations of basic network models as steps towards the design of large connectionist speech recognition systems. The two major areas of concern are the problem of time and the problem of scaling. Speech signals continuously vary over time and encode and transmit enormous amounts of human knowledge. To decode these signals, neural networks must be able to use appropriate representations of time and it must be possible to extend these nets to almost arbitrary sizes and complexity within finite resources. The problem of time is addressed by the development of a Time-Delay Neural Network; the problem of scaling by Modularity and Incremental Design of large nets based on smaller subcomponent nets. It is shown that small networks trained to perform limited tasks develop time invariant, hidden abstractions that can subsequently be exploited to train larger, more complex nets efficiently. Using these techniques, phoneme recognition networks of increasing complexity can be constructed that all achieve superior recognition performance.", "title": "Modular Construction of Time-Delay Neural Networks for Speech Recognition"}, "573ae3286d050281ffe4f6c973b64df171c9d5a5": {"paper_id": "573ae3286d050281ffe4f6c973b64df171c9d5a5", "abstract": "We consider the problem of detecting a large number of different classes of objects in cluttered scenes. Traditional approaches require applying a battery of different classifiers to the image, at multiple locations and scales. This can be slow and can require a lot of training data since each classifier requires the computation of many different image features. In particular, for independently trained detectors, the (runtime) computational complexity and the (training-time) sample complexity scale linearly with the number of classes to be detected. We present a multitask learning procedure, based on boosted decision stumps, that reduces the computational and sample complexity by finding common features that can be shared across the classes (and/or views). The detectors for each class are trained jointly, rather than independently. For a given performance level, the total number of features required and, therefore, the runtime cost of the classifier, is observed to scale approximately logarithmically with the number of classes. The features selected by joint training are generic edge-like features, whereas the features chosen by training each class separately tend to be more object-specific. The generic features generalize better and considerably reduce the computational cost of multiclass object detection", "title": "Sharing Visual Features for Multiclass and Multiview Object Detection"}, "10eb7bfa7687f498268bdf74b2f60020a151bdc6": {"paper_id": "10eb7bfa7687f498268bdf74b2f60020a151bdc6", "abstract": "We present a new technique called \u201ct-SNE\u201d that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large datasets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of datasets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on almost all of the datasets.", "title": "Visualizing Data using t-SNE"}, "2e384f057211426ac5922f1b33d2aa8df5d51f57": {"paper_id": "2e384f057211426ac5922f1b33d2aa8df5d51f57", "abstract": "We propose to shift the goal of recognition from naming to describing. Doing so allows us not only to name familiar objects, but also: to report unusual aspects of a familiar object (\u201cspotty dog\u201d, not just \u201cdog\u201d); to say something about unfamiliar objects (\u201chairy and four-legged\u201d, not just \u201cunknown\u201d); and to learn how to recognize new objects with few or no visual examples. Rather than focusing on identity assignment, we make inferring attributes the core problem of recognition. These attributes can be semantic (\u201cspotty\u201d) or discriminative (\u201cdogs have it but sheep do not\u201d). Learning attributes presents a major new challenge: generalization across object categories, not just across instances within a category. In this paper, we also introduce a novel feature selection method for learning attributes that generalize well across categories. We support our claims by thorough evaluation that provides insights into the limitations of the standard recognition paradigm of naming and demonstrates the new abilities provided by our attribute-based framework.", "title": "Describing objects by their attributes"}, "33da83b54410af11d0cd18fd07c74e1a99f67e84": {"paper_id": "33da83b54410af11d0cd18fd07c74e1a99f67e84", "abstract": "We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be repurposed to novel generic tasks. Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks. We investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges. We compare the efficacy of relying on various network levels to define a fixed feature, and report novel results that significantly outperform the state-of-the-art on several important vision challenges. We are releasing DeCAF, an open-source implementation of these deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms.", "title": "DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition"}, "fa6cbc948677d29ecce76f1a49cea01a75686619": {"paper_id": "fa6cbc948677d29ecce76f1a49cea01a75686619", "abstract": "In this paper, we propose a computational model of the recognition of real world scenes that bypasses the segmentation and the processing of individual objects or regions. The procedure is based on a very low dimensional representation of the scene, that we term the Spatial Envelope. We propose a set of perceptual dimensions (naturalness, openness, roughness, expansion, ruggedness) that represent the dominant spatial structure of a scene. Then, we show that these dimensions may be reliably estimated using spectral and coarsely localized information. The model generates a multidimensional space in which scenes sharing membership in semantic categories (e.g., streets, highways, coasts) are projected closed together. The performance of the spatial envelope model shows that specific information about object shape or identity is not a requirement for scene categorization and that modeling a holistic representation of the scene informs about its probable semantic category.", "title": "Modeling the Shape of the Scene: A Holistic Representation of the Spatial Envelope"}, "7da1c2d2ea8f51011b1a3006eabc03b342f792d4": {"paper_id": "7da1c2d2ea8f51011b1a3006eabc03b342f792d4", "abstract": "In many real world applications of machine learning, the distribution of the training data (on which the machine learning model is trained) is different from the distribution of the test data (where the learnt model is actually deployed). This is known as the problem of Domain Adaptation. We propose a novel deep learning model for domain adaptation which attempts to learn a predictively useful representation of the data by taking into account information from the distribution shift between the training and test data. Our key proposal is to successively learn multiple intermediate representations along an \u201cinterpolating path\u201d between the train and test domains. Our experiments on a standard object recognition dataset show a significant performance improvement over the state-of-the-art. 1. Problem Motivation and Context Oftentimes in machine learning applications, we have to learn a model to accomplish a specific task using training data drawn from one distribution (the source domain), and deploy the learnt model on test data drawn from a different distribution (the target domain). For instance, consider the task of creating a mobile phone application for \u201cimage search for products\u201d; where the goal is to look up product specifications and comparative shopping options from the internet, given a picture of the product taken with a user\u2019s mobile phone. In this case, the underlying object recognizer will typically be trained on a labeled corpus of images (perhaps scraped from the internet), and tested on the images taken using the user\u2019s phone camera. The challenge here is that the distribution of training and test images is not the same. A naively Appeared in the proceedings of the ICML 2013, Workshop on Representation Learning, Atlanta, Georgia, USA, 2013. trained object recognizer, that is just trained on the training images and applied directly to the test images, cannot be expected to have good performance. Such issues of a mismatched train and test sets occur not only in the field of Computer Vision (Duan et al., 2009; Jain & Learned-Miller, 2011; Wang & Wang, 2011), but also in Natural Language Processing (Blitzer et al., 2006; 2007; Glorot et al., 2011), and Automatic Speech Recognition (Leggetter & Woodland, 1995). The problem of differing train and test data distributions is referred to as Domain Adaptation (Daume & Marcu, 2006; Daume, 2007). Two variations of this problem are commonly discussed in the literature. In the first variation, known as Unsupervised Domain Adaptation, no target domain labels are provided during training. One only has access to source domain labels. In the second version of the problem, called Semi-Supervised Domain Adaptation, besides access to source domain labels, we additionally assume access to a few target domain labels during training. Previous approaches to domain adaptation can broadly be classified into a few main groups. One line of research starts out assuming the input representations are fixed (the features given are not learnable) and seeks to address domain shift by modeling the source/target distributional difference via transformations of the given representation. These transformations lead to a different distance metric which can be used in the domain adaptation classification/regression task. This is the approach taken, for instance, in (Saenko et al., 2010) and the recent linear manifold papers of (Gopalan et al., 2011; Gong et al., 2012). Another set of approaches in this fixed representation view of the problem treats domain adaptation as a conventional semi-supervised learning (Bergamo & Torresani, 2010; Dai et al., 2007; Yang et al., 2007; Duan et al., 2012). These works essentially construct a classifier using the labeled source data, and Often, the number of such labelled target samples are not sufficient to train a robust model using target data alone. DLID: Deep Learning for Domain Adaptation by Interpolating between Domains impose structural constraints on the classifier using unlabeled target data. A second line of research focusses on directly learning the representation of the inputs that is somewhat invariant across domains. Various models have been proposed (Daume, 2007; Daume et al., 2010; Blitzer et al., 2006; 2007; Pan et al., 2009), including deep learning models (Glorot et al., 2011). There are issues with both kinds of the previous proposals. In the fixed representation camp, the type of projection or structural constraint imposed often severely limits the capacity/strength of representations (linear projections for example, are common). In the representation learning camp, existing deep models do not attempt to explicitly encode the distributional shift between the source and target domains. In this paper we propose a novel deep learning model for the problem of domain adaptation which combines ideas from both of the previous approaches. We call our model (DLID): Deep Learning for domain adaptation by Interpolating between Domains. By operating in the deep learning paradigm, we also learn hierarchical non-linear representation of the source and target inputs. However, we explicitly define and use an \u201cinterpolating path\u201d between the source and target domains while learning the representation. This interpolating path captures information about structures intermediate to the source and target domains. The resulting representation we obtain is highly rich (containing source to target path information) and allows us to handle the domain adaptation task extremely well. There are multiple benefits to our approach compared to those proposed in the literature. First, we are able to train intricate non-linear representations of the input, while explicitly modeling the transformation between the source and target domains. Second, instead of learning a representation which is independent of the final task, our model can learn representations with information from the final classification/regression task. This is achieved by fine-tuning the pre-trained intermediate feature extractors using feedback from the final task. Finally, our approach can gracefully handle additional training data being made available in the future. We would simply fine-tune our model with the new data, as opposed to having to retrain the entire model again from scratch. We evaluate our model on the domain adaptation problem of object recognition on a standard dataset (Saenko et al., 2010). Empirical results show that our model out-performs the state of the art by a significant margin. In some cases there is an improvement of over 40% from the best previously reported results. An analysis of the learnt representations sheds some light onto the properties that result in such excellent performance (Ben-David et al., 2007). 2. An Overview of DLID At a high level, the DLID model is a deep neural network model designed specifically for the problem of domain adaptation. Deep networks have had tremendous success recently, achieving state-of-the-art performance on a number of machine learning tasks (Bengio, 2009). In large part, their success can be attributed to their ability to learn extremely powerful hierarchical non-linear representations of the inputs. In particular, breakthroughs in unsupervised pre-training (Bengio et al., 2006; Hinton et al., 2006; Hinton & Salakhutdinov, 2006; Ranzato et al., 2006), have been critical in enabling deep networks to be trained robustly. As with other deep neural network models, DLID also learns its representation using unsupervised pretraining. The key difference is that in DLID model, we explicitly capture information from an \u201cinterpolating path\u201d between the source domain and the target domain. As mentioned in the introduction, our interpolating path is motivated by the ideas discussed in Gopalan et al. (2011); Gong et al. (2012). In these works, the original high dimensional features are linearly projected (typically via PCA/PLS) to a lower dimensional space. Because these are linear projections, the source and target lower dimensional subspaces lie on the Grassman manifold. Geometric properties of the manifold, like shortest paths (geodesics), present an interesting and principled way to transition/interpolate smoothly between the source and target subspaces. It is this path information on the manifold that is used by Gopalan et al. (2011); Gong et al. (2012) to construct more robust and accurate classifiers for the domain adaptation task. In DLID, we define a somewhat different notion of an interpolating path between source and target domains, but appeal to a similar intuition. Figure 1 shows an illustration of our model. Let the set of data samples for the source domain S be denoted by DS , and that of the target domain T be denoted by DT . Starting with all the source data samples DS , we generate intermediate sampled datasets, where for each successive dataset we gradually increase the proportion of samples randomly drawn from DT , and decrease the proportion of samples drawn from DS . In particular, let p \u2208 [1, . . . , P ] be an index over the P datasets we generate. Then we have Dp = DS for p = 1, Dp = DT for p = P . For p \u2208 [2, . . . , P \u2212 1], datasets Dp and Dp+1 are created in a way so that the proportion of samples from DT in Dp is less than in Dp+1. Each of these data sets can be thought of as a single point on a particular kind of interpolating path between S and T . DLID: Deep Learning for Domain Adaptation by Interpolating between Domains", "title": "DLID: Deep Learning for Domain Adaptation by Interpolating between Domains"}, "0b7c1bcd0289058b5dfc0d3ff114972712bc7f1a": {"paper_id": "0b7c1bcd0289058b5dfc0d3ff114972712bc7f1a", "abstract": "The traditional SPM approach based on bag-of-features (BoF) requires nonlinear classifiers to achieve good image classification performance. This paper presents a simple but effective coding scheme called Locality-constrained Linear Coding (LLC) in place of the VQ coding in traditional SPM. LLC utilizes the locality constraints to project each descriptor into its local-coordinate system, and the projected coordinates are integrated by max pooling to generate the final representation. With linear classifier, the proposed approach performs remarkably better than the traditional nonlinear SPM, achieving state-of-the-art performance on several benchmarks. Compared with the sparse coding strategy [22], the objective function used by LLC has an analytical solution. In addition, the paper proposes a fast approximated LLC method by first performing a K-nearest-neighbor search and then solving a constrained least square fitting problem, bearing computational complexity of O(M + K2). Hence even with very large codebooks, our system can still process multiple frames per second. This efficiency significantly adds to the practical values of LLC for real applications.", "title": "Locality-constrained Linear Coding for image classification"}, "29d591806cdc6ef0d580e4a21f32e5ad9d09d148": {"paper_id": "29d591806cdc6ef0d580e4a21f32e5ad9d09d148", "abstract": "Image annotation datasets are becoming larger and larger, with tens of millions of images and tens of thousands of possible annotations. We propose a strongly performing method that scales to such datasets by simultaneously learning to optimize precision at k of the ranked list of annotations for a given image and learning a low-dimensional joint embedding space for both images and annotations. Our method both outperforms several baseline methods and, in comparison to them, is faster and consumes less memory. We also demonstrate how our method learns an interpretable model, where annotations with alternate spellings or even languages are close in the embedding space. Hence, even when our model does not predict the exact annotation given by a human labeler, it often predicts similar annotations, a fact that we try to quantify by measuring the newly introduced \u201csibling\u201d precision metric, where our method also obtains excellent results.", "title": "Large scale image annotation: learning\u00a0to\u00a0rank with\u00a0joint word-image embeddings"}, "40e927fdee7517fb7513d03735754af80fb9c7b0": {"paper_id": "40e927fdee7517fb7513d03735754af80fb9c7b0", "abstract": "Multi-class classification becomes challenging at test time when the number of classes is very large and testing against every possible class can become computationally infeasible. This problem can be alleviated by imposing (or learning) a structure over the set of classes. We propose an algorithm for learning a treestructure of classifiers which, by optimizing the overall tree loss, provides superior accuracy to existing tree labeling methods. We also propose a method that learns to embed labels in a low dimensional space that is faster than non-embedding approaches and has superior accuracy to existing embedding approaches. Finally we combine the two ideas resulting in the label embedding tree that outperforms alternative methods including One-vs-Rest while being orders of magnitude faster.", "title": "Label Embedding Trees for Large Multi-Class Tasks"}, "709198f1a7d42fb87d46a8f5dc48e23e6564df1c": {"paper_id": "709198f1a7d42fb87d46a8f5dc48e23e6564df1c", "abstract": "Many computer vision approaches take for granted positive answers to questions such as \u201cAre semantic categories visually separable?\u201d and \u201cIs visual similarity correlated to semantic similarity?\u201d. In this paper, we study experimentally whether these assumptions hold and show parallels to questions investigated in cognitive science about the human visual system. The insights gained from our analysis enable building a novel distance function between images assessing whether they are from the same basic-level category. This function goes beyond direct visual distance as it also exploits semantic similarity measured through ImageNet. We demonstrate experimentally that it outperforms purely visual distances.", "title": "Visual and semantic similarity in ImageNet"}, "06554235c2c9361a14c0569206b58a355a63f01b": {"paper_id": "06554235c2c9361a14c0569206b58a355a63f01b", "abstract": "This work introduces a model that can recognize objects in images even if no training data is available for the objects. The only necessary knowledge about the unseen categories comes from unsupervised large text corpora. In our zero-shot framework distributional information in language can be seen as spanning a semantic basis for understanding what objects look like. Most previous zero-shot learning models can only differentiate between unseen classes. In contrast, our model can both obtain state of the art performance on classes that have thousands of training images and obtain reasonable performance on unseen classes. This is achieved by first using outlier detection in the semantic space and then two separate recognition models. Furthermore, our model does not require any manually defined semantic features for either words or images.", "title": "Zero-Shot Learning Through Cross-Modal Transfer"}, "774f67303ea4a3a94874f08cf9a9dacc69b40782": {"paper_id": "774f67303ea4a3a94874f08cf9a9dacc69b40782", "abstract": "Many object detection systems are constrained by the time required to convolve a target image with a bank of filters that code for different aspects of an object's appearance, such as the presence of component parts. We exploit locality-sensitive hashing to replace the dot-product kernel operator in the convolution with a fixed number of hash-table probes that effectively sample all of the filter responses in time independent of the size of the filter bank. To show the effectiveness of the technique, we apply it to evaluate 100,000 deformable-part models requiring over a million (part) filters on multiple scales of a target image in less than 20 seconds using a single multi-core processor with 20GB of RAM. This represents a speed-up of approximately 20,000 times - four orders of magnitude - when compared with performing the convolutions explicitly on the same hardware. While mean average precision over the full set of 100,000 object classes is around 0.16 due in large part to the challenges in gathering training data and collecting ground truth for so many classes, we achieve a mAP of at least 0.20 on a third of the classes and 0.30 or better on about 20% of the classes.", "title": "Fast, Accurate Detection of 100,000 Object Classes on a Single Machine"}, "573edc20ae85e708f3570113723ab443e98d3c56": {"paper_id": "573edc20ae85e708f3570113723ab443e98d3c56", "abstract": "A novel method is proposed for performing multilabel, interactive image segmentation. Given a small number of pixels with user-defined (or predefined) labels, one can analytically and quickly determine the probability that a random walker starting at each unlabeled pixel will first reach one of the prelabeled pixels. By assigning each pixel to the label for which the greatest probability is calculated, a high-quality image segmentation may be obtained. Theoretical properties of this algorithm are developed along with the corresponding connections to discrete potential theory and electrical circuits. This algorithm is formulated in discrete space (i.e., on a graph) using combinatorial analogues of standard operators and principles from continuous potential theory, allowing it to be applied in arbitrary dimension on arbitrary graphs", "title": "Random Walks for Image Segmentation"}, "ca08de0b39976f5ef1ff0fbc29efdf2763667b22": {"paper_id": "ca08de0b39976f5ef1ff0fbc29efdf2763667b22", "abstract": "In recent years, interest in nonlinear dimensionality reduction has grown. This has led to the proposal of various new nonlinear techniques that are claimed to be capable of dealing with complex low-dimensional data. These techniques have been shown to outperform traditional linear techniques on artificial tasks such as the Swiss roll task. Hitherto, the dimensionality reduction techniques have not been compared in a systematic way. In this paper, we discuss and compare ten nonlinear techniques for dimensionality reduction. We investigate the performance of the nonlinear techniques for dimensionality reduction on artificial and natural tasks, and compare their performances to those of the two principal linear techniques: (1) Principal Components Analysis, and (2) Linear Discriminant Analysis. The experiments reveal that nonlinear techniques for dimensionality reduction perform well on selected artificial tasks, but do not outperform the linear techniques on many real-world tasks. The paper concludes that the results suggest that despite their theoretical capability to find complex low-dimensional embeddings, nonlinear techniques for dimensionality reduction are not yet capable of outperforming traditional linear techniques on real-world tasks. We foresee that the performance of nonlinear techniques for dimensionality reduction will be improved by the development of new techniques that represent the global structure of manifolds by a number of separate linear models.", "title": "Dimensionality Reduction: A Comparative Review"}, "3eaca435527a765572ed04acd06ba512faac89eb": {"paper_id": "3eaca435527a765572ed04acd06ba512faac89eb", "abstract": "Drawing on the correspondence between the graph Laplacian the Laplace Beltrami operator on a manifold and the connections to the heat equation we propose a geometrically motivated algorithm for constructing a representation for data sampled from a low di mensional manifold embedded in a higher dimensional space The algorithm provides a computationally e cient approach to non linear dimensionality reduction that has locality preserving prop erties and a natural connection to clustering Several applications are considered In many areas of arti cial intelligence information retrieval and data mining one is often confronted with intrinsically low dimensional data lying in a very high di mensional space For example gray scale n n images of a xed object taken with a moving camera yield data points in R However the intrinsic dimensionality of the space of all images of the same object is the number of degrees of freedom of the camera in fact the space has the natural structure of a manifold embedded in R n While there is a large body of work on dimensionality reduction in general most existing approaches do not explicitly take into account the structure of the manifold on which the data may possibly reside Recently there has been some interest Tenenbaum et al Roweis and Saul in the problem of devel oping low dimensional representations of data in this particular context In this paper we present a new algorithm and an accompanying framework of analysis for geometrically motivated dimensionality reduction The core algorithm is very simple has a few local computations and one sparse eigenvalue problem The solution re ects the intrinsic geometric structure of the manifold The justi cation comes from the role of the Laplacian operator in pro viding an optimal embedding The Laplacian of the graph obtained from the data points may be viewed as an approximation to the Laplace Beltrami operator de ned on the manifold The embedding maps for the data come from approximations to a natural map that is de ned on the entire manifold The framework of analysis presented here makes this connection explicit While this connection is known to geometers and specialists in spectral graph theory for example see to the best of our knowledge we do not know of any application to data representation yet The connection of the Laplacian to the heat kernel enables us to choose the weights of the graph in a principled manner The locality preserving character of the Laplacian Eigenmap algorithmmakes it rel atively insensitive to outliers and noise A byproduct of this is that the algorithm implicitly emphasizes the natural clusters in the data Connections to spectral clus tering algorithms developed in learning and computer vision see Shi and Malik become very clear Following the discussion of Roweis and Saul and Tenenbaum et al we note that the biological perceptual apparatus is con fronted with high dimensional stimuli from which it must recover low dimensional structure One might argue that if the approach to recovering such low dimensional structure is inherently local then a natural clustering will emerge and thus might serve as the basis for the development of categories in biological perception The Algorithm Given k points x xk in R we construct a weighted graph with k nodes one for each point and the set of edges connecting neighboring points to each other Step Constructing the Graph We put an edge between nodes i and j if xi and xj are close There are two variations a neighborhoods parameter R Nodes i and j are connected by an edge if jjxi xj jj Advantages geometrically motivated the relationship is naturally symmetric Disadvantages often leads to graphs with several connected compo nents di cult to choose b n nearest neighbors parameter n N Nodes i and j are connected by an edge if i is among n nearest neighbors of j or j is among n nearest neighbors of i Advantages simpler to choose tends to lead to connected graphs Disadvantages less geometrically intuitive Step Choosing the weights Here as well we have two variations for weighting the edges a Heat kernel parameter t R If nodes i and j are connected put Wij e jjxi j jj t The justi cation for this choice of weights will be provided later b Simple minded No parameters Wij if and only if vertices i and j are connected by an edge A simpli cation which avoids the necessity of choosing t Step Eigenmaps Assume the graph G constructed above is connected otherwise proceed with Step for each connected component 0 20 40 0 10 20 30 40 \u22125 0 5 x 10 \u22123 \u22128 \u22126 \u22124 \u22122 0 2 4 6 8 x 10 \u22123", "title": "Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering"}, "018e730f8947173e1140210d4d1760d05c9d3854": {"paper_id": "018e730f8947173e1140210d4d1760d05c9d3854", "abstract": "In principle, zero-shot learning makes it possible to train a recognition model simply by specifying the category\u2019s attributes. For example, with classifiers for generic attributes like striped and four-legged, one can construct a classifier for the zebra category by enumerating which properties it possesses\u2014even without providing zebra training images. In practice, however, the standard zero-shot paradigm suffers because attribute predictions in novel images are hard to get right. We propose a novel random forest approach to train zero-shot models that explicitly accounts for the unreliability of attribute predictions. By leveraging statistics about each attribute\u2019s error tendencies, our method obtains more robust discriminative models for the unseen classes. We further devise extensions to handle the few-shot scenario and unreliable attribute descriptions. On three datasets, we demonstrate the benefit for visual category learning with zero or few training examples, a critical domain for rare categories or categories defined on the fly.", "title": "Zero Shot Recognition with Unreliable Attributes"}, "dfd388d6edc406a69f6d8ff0ff7f7ef8539fb286": {"paper_id": "dfd388d6edc406a69f6d8ff0ff7f7ef8539fb286", "abstract": "In this paper, a new method of fuzzy decision trees called soft decision trees (SDT) is presented. This method combines tree growing and pruning, to determine the structure of the soft decision tree, with re4tting and back4tting, to improve its generalization capabilities. The method is explained and motivated and its behavior is 4rst analyzed empirically on 3 large databases in terms of classi4cation error rate, model complexity and CPU time. A comparative study on 11 standard UCI Repository databases then shows that the soft decision trees produced by this method are signi4cantly more accurate than standard decision trees. Moreover, a global model variance study shows a much lower variance for soft decision trees than for standard trees as a direct cause of the improved accuracy. c \u00a9 2003 Elsevier B.V. All rights reserved.", "title": "A complete fuzzy decision tree technique"}, "0baee7f68c08f1a6b5190755adebc57145d18ccf": {"paper_id": "0baee7f68c08f1a6b5190755adebc57145d18ccf", "abstract": "Our Approach, 0.66 GIST 29.7 Spa>al Pyramid HOG 29.8 Spa>al Pyramid SIFT 34.4 ROI-\u00ad\u2010GIST 26.5 Scene DPM 30.4 MM-\u00ad\u2010Scene 28.0 Object Bank 37.6 Ours 38.1 Ours+GIST 44.0 Ours+SP 46.4 Ours+GIST+SP 47.5 Ours+DPM 42.4 Ours+GIST+DPM 46.9 Ours+SP+DPM 46.4 GIST+SP+DPM 43.1 Ours+GIST+SP+DPM 49.4 Two key requirements \u2022 representa,ve: Need to occur frequently enough \u2022 discrimina,ve: Need to be different enough from the rest of the \u201cvisual world\u201d Goal: a mid-\u00ad\u2010level visual representa>on Experimental Analysis Bonus: works even be`er if weakly supervised!", "title": "Unsupervised Discovery of Mid-Level Discriminative Patches"}, "05ad478ca69b935c1bba755ac1a2a90be6679129": {"paper_id": "05ad478ca69b935c1bba755ac1a2a90be6679129", "abstract": "When we look at an image, some properties or attributes of the image stand out more than others. When describing an image, people are likely to describe these dominant attributes first. Attribute dominance is a result of a complex interplay between the various properties present or absent in the image. Which attributes in an image are more dominant than others reveals rich information about the content of the image. In this paper we tap into this information by modeling attribute dominance. We show that this helps improve the performance of vision systems on a variety of human-centric applications such as zero-shot learning, image search and generating textual descriptions of images.", "title": "Attribute Dominance: What Pops Out?"}, "0566bf06a0368b518b8b474166f7b1dfef3f9283": {"paper_id": "0566bf06a0368b518b8b474166f7b1dfef3f9283", "abstract": "We study the problem of object classification when training and test classes are disjoint, i.e. no training examples of the target classes are available. This setup has hardly been studied in computer vision research, but it is the rule rather than the exception, because the world contains tens of thousands of different object classes and for only a very few of them image, collections have been formed and annotated with suitable class labels. In this paper, we tackle the problem by introducing attribute-based classification. It performs object detection based on a human-specified high-level description of the target objects instead of training images. The description consists of arbitrary semantic attributes, like shape, color or even geographic information. Because such properties transcend the specific learning task at hand, they can be pre-learned, e.g. from image datasets unrelated to the current task. Afterwards, new classes can be detected based on their attribute representation, without the need for a new training phase. In order to evaluate our method and to facilitate research in this area, we have assembled a new large-scale dataset, \u201cAnimals with Attributes\u201d, of over 30,000 animal images that match the 50 classes in Osherson's classic table of how strongly humans associate 85 semantic attributes with animal classes. Our experiments show that by using an attribute layer it is indeed possible to build a learning object detection system that does not require any training images of the target classes.", "title": "Learning to detect unseen object classes by between-class attribute transfer"}, "df1d1e45714cae031091797cf24b6b2a0751fb46": {"paper_id": "df1d1e45714cae031091797cf24b6b2a0751fb46", "abstract": "In this paper, a novel controller for brushless DC (BLDC) motor has been presented. The proposed controller is based on Adaptive Neuro-Fuzzy Inference System (ANFIS) and the rigorous analysis through simulation is performed using simulink tool box in MATLAB environment. The performance of the motor with proposed ANFIS controller is analyzed and compared with classical Proportional Integral (PI) controller, Fuzzy Tuned PID controller and Fuzzy Variable Structure controller. The dynamic characteristics of the brushless DC motor is observed and analyzed using the developed MATLAB/simulink model. Control system response parameters such as overshoot, undershoot, rise time, recovery time and steady state error are measured and compared for the above controllers. In order to validate the performance of the proposed controller under realistic working environment, simulation result has been obtained and analyzed for varying load and varying set speed conditions. & 2014 Elsevier B.V. All rights reserved.", "title": "Adaptive Neuro-Fuzzy Inference System based speed controller for brushless DC motor"}, "bfca7d6743caa4306cfd265c6c3e01f97720cb96": {"paper_id": "bfca7d6743caa4306cfd265c6c3e01f97720cb96", "abstract": "In this paper, Faster R-CNN was employed to recognize the CAPTCHA (Completely Automated Public Turing test to tell Computers and Humans Apart). Unlike traditional method, the proposed method is based on deep learning object detection framework. By inputting the database into the network and training the Faster R-CNN, the feature map can be obtained through the convolutional layers. The proposed method can recognize the character and it is location. Experiments show that Faster R-CNN can be used in CAPTCHA recog\u2010 nition with promising speed and accuracy. The experimental results also show that the mAP (mean average precision) value will improve with the depth of the network increasing.", "title": "CAPTCHA Recognition Based on Faster R-CNN"}, "abc618b5c4f69a34c655bbb93c6003cc671b0f72": {"paper_id": "abc618b5c4f69a34c655bbb93c6003cc671b0f72", "abstract": "Detecting pedestrian has been arguably addressed as a special topic beyond general object detection. Although recent deep learning object detectors such as Fast/Faster R-CNN [1,2] have shown excellent performance for general object detection, they have limited success for detecting pedestrian, and previous leading pedestrian detectors were in general hybrid methods combining hand-crafted and deep convolutional features. In this paper, we investigate issues involving Faster R-CNN [2] for pedestrian detection. We discover that the Region Proposal Network (RPN) in Faster R-CNN indeed performs well as a stand-alone pedestrian detector, but surprisingly, the downstream classifier degrades the results. We argue that two reasons account for the unsatisfactory accuracy: (i) insufficient resolution of feature maps for handling small instances, and (ii) lack of any bootstrapping strategy for mining hard negative examples. Driven by these observations, we propose a very simple but effective baseline for pedestrian detection, using an RPN followed by boosted forests on shared, high-resolution convolutional feature maps. We comprehensively evaluate this method on several benchmarks (Caltech, INRIA, ETH, and KITTI), presenting competitive accuracy and good speed. Code will be made publicly available.", "title": "Is Faster R-CNN Doing Well for Pedestrian Detection?"}, "06150e6e69a379c27e1d0100fcd7660f073cbacf": {"paper_id": "06150e6e69a379c27e1d0100fcd7660f073cbacf", "abstract": "Even with the advent of more sophisticated, data-hungry methods, boosted decision trees remain extraordinarily successful for fast rigid object detection, achieving top accuracy on numerous datasets. While effective, most boosted detectors use decision trees with orthogonal (single feature) splits, and the topology of the resulting decision boundary may not be well matched to the natural topology of the data. Given highly correlated data, decision trees with oblique (multiple feature) splits can be effective. Use of oblique splits, however, comes at considerable computational expense. Inspired by recent work on discriminative decorrelation of HOG features, we instead propose an efficient feature transform that removes correlations in local neighborhoods. The result is an overcomplete but locally decorrelated representation ideally suited for use with orthogonal decision trees. In fact, orthogonal trees with our locally decorrelated features outperform oblique trees trained over the original features at a fraction of the computational cost. The overall improvement in accuracy is dramatic: on the Caltech Pedestrian Dataset, we reduce false positives nearly tenfold over the previous state-of-the-art.", "title": "Local Decorrelation For Improved Pedestrian Detection"}, "25d7da85858a4d89b7de84fd94f0c0a51a9fc67a": {"paper_id": "25d7da85858a4d89b7de84fd94f0c0a51a9fc67a", "abstract": "This paper addresses the problem of generating possible object locations for use in object recognition. We introduce selective search which combines the strength of both an exhaustive search and segmentation. Like segmentation, we use the image structure to guide our sampling process. Like exhaustive search, we aim to capture all possible object locations. Instead of a single technique to generate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible. Our selective search results in a small set of data-driven, class-independent, high quality locations, yielding 99\u00a0% recall and a Mean Average Best Overlap of 0.879 at 10,097 locations. The reduced number of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition. In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition. The selective search software is made publicly available (Software: http://disi.unitn.it/~uijlings/SelectiveSearch.html ).", "title": "Selective Search for Object Recognition"}, "1abf6491d1b0f6e8af137869a01843931996a562": {"paper_id": "1abf6491d1b0f6e8af137869a01843931996a562", "abstract": "We present a technique for adding global context to deep convolutional networks for semantic segmentation. The approach is simple, using the average feature for a layer to augment the features at each location. In addition, we study several idiosyncrasies of training, significantly increasing the performance of baseline networks (e.g. from FCN [19]). When we add our proposed global feature, and a technique for learning normalization parameters, accuracy increases consistently even over our improved versions of the baselines. Our proposed approach, ParseNet, achieves state-of-the-art performance on SiftFlow and PASCAL-Context with small additional computational cost over baselines, and near current state-of-theart performance on PASCAL VOC 2012 semantic segmentation with a simple approach. Code is available at https: //github.com/weiliu89/caffe/tree/fcn.", "title": "ParseNet: Looking Wider to See Better"}, "5092375789732afbfbfe2f5ede0792af6c562813": {"paper_id": "5092375789732afbfbfe2f5ede0792af6c562813", "abstract": "Boosted decision trees are among the most popular learning techniques in use today. While exhibiting fast speeds at test time, relatively slow training renders them impractical for applications with real-time learning requirements. We propose a principled approach to overcome this drawback. We prove a bound on the error of a decision stump given its preliminary error on a subset of the training data; the bound may be used to prune unpromising features early in the training process. We propose a fast training algorithm that exploits this bound, yielding speedups of an order of magnitude at no cost in the final performance of the classifier. Our method is not a new variant of Boosting; rather, it is used in conjunction with existing Boosting algorithms and other sampling methods to achieve even greater speedups.", "title": "Quickly Boosting Decision Trees - Pruning Underachieving Features Early"}, "f72f6a45ee240cc99296a287ff725aaa7e7ebb35": {"paper_id": "f72f6a45ee240cc99296a287ff725aaa7e7ebb35", "abstract": "Pedestrian detection is a key problem in computer vision, with several applications that have the potential to positively impact quality of life. In recent years, the number of approaches to detecting pedestrians in monocular images has grown steadily. However, multiple data sets and widely varying evaluation protocols are used, making direct comparisons difficult. To address these shortcomings, we perform an extensive evaluation of the state of the art in a unified framework. We make three primary contributions: 1) We put together a large, well-annotated, and realistic monocular pedestrian detection data set and study the statistics of the size, position, and occlusion patterns of pedestrians in urban scenes, 2) we propose a refined per-frame evaluation methodology that allows us to carry out probing and informative comparisons, including measuring performance in relation to scale and occlusion, and 3) we evaluate the performance of sixteen pretrained state-of-the-art detectors across six data sets. Our study allows us to assess the state of the art and provides a framework for gauging future efforts. Our experiments show that despite significant progress, performance still has much room for improvement. In particular, detection is disappointing at low resolutions and for partially occluded pedestrians.", "title": "Pedestrian Detection: An Evaluation of the State of the Art"}, "455da02e5048dffb51fb6ab5eb8aeca5926c9d9a": {"paper_id": "455da02e5048dffb51fb6ab5eb8aeca5926c9d9a", "abstract": "Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g., 224<inline-formula><tex-math>$\\times$ </tex-math><alternatives><inline-graphic xlink:type=\"simple\" xlink:href=\"he-ieq1-2389824.gif\"/></alternatives></inline-formula>224) input image. This requirement is \u201cartificial\u201d and may reduce the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with another pooling strategy, \u201cspatial pyramid pooling\u201d, to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. Pyramid pooling is also robust to object deformations. With these advantages, SPP-net should in general improve all CNN-based image classification methods. On the ImageNet 2012 dataset, we demonstrate that SPP-net boosts the accuracy of a variety of CNN architectures despite their different designs. On the Pascal VOC 2007 and Caltech101 datasets, SPP-net achieves state-of-the-art classification results using a single full-image representation and no fine-tuning. The power of SPP-net is also significant in object detection. Using SPP-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors. This method avoids repeatedly computing the convolutional features. In processing test images, our method is 24-102 <inline-formula><tex-math>$\\times$</tex-math><alternatives><inline-graphic xlink:type=\"simple\" xlink:href=\"he-ieq2-2389824.gif\"/> </alternatives></inline-formula> faster than the R-CNN method, while achieving better or comparable accuracy on Pascal VOC 2007. In ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014, our methods rank #2 in object detection and #3 in image classification among all 38 teams. This manuscript also introduces the improvement made for this competition.", "title": "Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition"}, "4549210d81d6524c4d4c36a6f10f1411a95ac30b": {"paper_id": "4549210d81d6524c4d4c36a6f10f1411a95ac30b", "abstract": "This paper introduces a method for scene categorization by modeling ambiguity in the popular codebook approach. The codebook approach describes an image as a bag of discrete visual codewords, where the frequency distributions of these words are used for image categorization. There are two drawbacks to the traditional codebook model: codeword uncertainty and codeword plausibility. Both of these drawbacks stem from the hard assignment of visual features to a single codeword. We show that allowing a degree of ambiguity in assigning codewords improves categorization performance for three state-of-the-art datasets.", "title": "Kernel Codebooks for Scene Categorization"}, "09f2af091f6bf5dfe25700c5a8c82f220fac5631": {"paper_id": "09f2af091f6bf5dfe25700c5a8c82f220fac5631", "abstract": "This paper presents a method for recognizing scene categories based on approximate global geometric correspondence. This technique works by partitioning the image into increasingly fine sub-regions and computing histograms of local features found inside each sub-region. The resulting \"spatial pyramid\" is a simple and computationally efficient extension of an orderless bag-of-features image representation, and it shows significantly improved performance on challenging scene categorization tasks. Specifically, our proposed method exceeds the state of the art on the Caltech-101 database and achieves high accuracy on a large database of fifteen natural scene categories. The spatial pyramid framework also offers insights into the success of several recently proposed image descriptions, including Torralba\u2019s \"gist\" and Lowe\u2019s SIFT descriptors.", "title": "Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories"}, "24d66ec9dd202a6ea02b8723ae9d2fd7ffd32a4a": {"paper_id": "24d66ec9dd202a6ea02b8723ae9d2fd7ffd32a4a", "abstract": "Training a generic objectness measure to produce a small set of candidate object windows, has been shown to speed up the classical sliding window object detection paradigm. We observe that generic objects with well-defined closed boundary can be discriminated by looking at the norm of gradients, with a suitable resizing of their corresponding image windows in to a small fixed size. Based on this observation and computational reasons, we propose to resize the window to 8 \u00d7 8 and use the norm of the gradients as a simple 64D feature to describe it, for explicitly training a generic objectness measure. We further show how the binarized version of this feature, namely binarized normed gradients (BING), can be used for efficient objectness estimation, which requires only a few atomic operations (e.g. ADD, BITWISE SHIFT, etc.). Experiments on the challenging PASCAL VOC 2007 dataset show that our method efficiently (300fps on a single laptop CPU) generates a small set of category-independent, high quality object windows, yielding 96.2% object detection rate (DR) with 1, 000 proposals. Increasing the numbers of proposals and color spaces for computing BING features, our performance can be further improved to 99.5% DR.", "title": "BING: Binarized Normed Gradients for Objectness Estimation at 300fps"}, "0e72a9c87c320093b77f941e95abcb93e7dc1f08": {"paper_id": "0e72a9c87c320093b77f941e95abcb93e7dc1f08", "abstract": "Deep convolutional neural networks (CNN) have shown their promise as a universal representation for recognition. However, global CNN activations at present lack geometric invariance, which limits their robustness for tasks such as classification and matching of highly variable scenes. To improve the invariance of CNN activations without degrading their discriminative power, this paper presents a simple but effective scheme called multi-scale orderless pooling (or MOP-CNN for short). This approach works by extracting CNN activations for local patches at multiple scales, followed by orderless VLAD pooling of these activations at each scale level and concatenating the result. This feature representation decisively outperforms global CNN activations and achieves state-of-the-art performance for scene classification on such challenging benchmarks as SUN397, MIT Indoor Scenes, and ILSVRC2012, as well as for instance-level retrieval on the Holidays dataset.", "title": "Multi-scale Orderless Pooling of Deep Convolutional Activation Features"}, "6ad32b70ee21b6fc16ff4caf7b4ada2aaf13cabc": {"paper_id": "6ad32b70ee21b6fc16ff4caf7b4ada2aaf13cabc", "abstract": "Most successful object recognition systems rely on binary classification, deciding only if an object is present or not, but not providing information on the actual object location. To estimate the object's location, one can take a sliding window approach, but this strongly increases the computational cost because the classifier or similarity function has to be evaluated over a large set of candidate subwindows. In this paper, we propose a simple yet powerful branch and bound scheme that allows efficient maximization of a large class of quality functions over all possible subimages. It converges to a globally optimal solution typically in linear or even sublinear time, in contrast to the quadratic scaling of exhaustive or sliding window search. We show how our method is applicable to different object detection and image retrieval scenarios. The achieved speedup allows the use of classifiers for localization that formerly were considered too slow for this task, such as SVMs with a spatial pyramid kernel or nearest-neighbor classifiers based on the lambda2 distance. We demonstrate state-of-the-art localization performance of the resulting systems on the UIUC Cars data set, the PASCAL VOC 2006 data set, and in the PASCAL VOC 2007 competition.", "title": "Efficient Subwindow Search: A Branch and Bound Framework for Object Localization"}, "13dd25c5e7df2b23ec9a168a233598702c2afc97": {"paper_id": "13dd25c5e7df2b23ec9a168a233598702c2afc97", "abstract": "This paper addresses the problem of segmenting an image into regions. We define a predicate for measuring the evidence for a boundary between two regions using a graph-based representation of the image. We then develop an efficient segmentation algorithm based on this predicate, and show that although this algorithm makes greedy decisions it produces segmentations that satisfy global properties. We apply the algorithm to image segmentation using two different kinds of local neighborhoods in constructing the graph, and illustrate the results with both real and synthetic images. The algorithm runs in time nearly linear in the number of graph edges and is also fast in practice. An important characteristic of the method is its ability to preserve detail in low-variability image regions while ignoring detail in high-variability regions.", "title": "Efficient Graph-Based Image Segmentation"}, "07f488bf2285b290058eb49cf8c25abfd3a13c7d": {"paper_id": "07f488bf2285b290058eb49cf8c25abfd3a13c7d", "abstract": "We describe an approach to object and scene retrieval which searches for and localizes all the occurrences of a user outlined object in a video. The object is represented by a set of viewpoint invariant region descriptors so that recognition can proceed successfully despite changes in viewpoint, illumination and partial occlusion. The temporal continuity of the video within a shot is used to track the regions in order to reject unstable regions and reduce the effects of noise in the descriptors. The analogy with text retrieval is in the implementation where matches on descriptors are pre-computed (using vector quantization), and inverted file systems and document rankings are used. The result is that retrieval is immediate, returning a ranked list of key frames/shots in the manner of Google. The method is illustrated for matching on two full length feature films.", "title": "Video Google: A Text Retrieval Approach to Object Matching in Videos"}, "b91180d8853d00e8f2df7ee3532e07d3d0cce2af": {"paper_id": "b91180d8853d00e8f2df7ee3532e07d3d0cce2af", "abstract": "We present a novel method for generic visual categorization: the problem of identifying the object content of natural images while generalizing across variations inherent to the object class. This bag of keypoints method is based on vector quantization of affine invariant descriptors of image patches. We propose and compare two alternative implementations using different classifiers: Na\u00efve Bayes and SVM. The main advantages of the method are that it is simple, computationally efficient and intrinsically invariant. We present results for simultaneously classifying seven semantic visual categories. These results clearly demonstrate that the method is robust to background clutter and produces good categorization accuracy even without exploiting geometric information.", "title": "Visual Categorization with Bags of Keypoints"}, "0585b80713848a5b54b82265a79f031a4fbd3332": {"paper_id": "0585b80713848a5b54b82265a79f031a4fbd3332", "abstract": "In this paper we are interested in how semantic segmentation can help object detection. Towards this goal, we propose a novel deformable part-based model which exploits region-based segmentation algorithms that compute candidate object regions by bottom-up clustering followed by ranking of those regions. Our approach allows every detection hypothesis to select a segment (including void), and scores each box in the image using both the traditional HOG filters as well as a set of novel segmentation features. Thus our model ``blends'' between the detector and segmentation models. Since our features can be computed very efficiently given the segments, we maintain the same complexity as the original DPM. We demonstrate the effectiveness of our approach in PASCAL VOC 2010, and show that when employing only a root filter our approach outperforms Dalal & Triggs detector on all classes, achieving 13% higher average AP. When employing the parts, we outperform the original DPM in $19$ out of $20$ classes, achieving an improvement of 8% AP. Furthermore, we outperform the previous state-of-the-art on VOC 2010 test by 4%.", "title": "Bottom-Up Segmentation for Top-Down Detection"}, "76f02eca773414828271da315b379efa9e1f57fa": {"paper_id": "76f02eca773414828271da315b379efa9e1f57fa", "abstract": "We present a novel framework to generate and rank plausible hypotheses for the spatial extent of objects in images using bottom-up computational processes and mid-level selection cues. The object hypotheses are represented as figure-ground segmentations, and are extracted automatically, without prior knowledge of the properties of individual object classes, by solving a sequence of Constrained Parametric Min-Cut problems (CPMC) on a regular image grid. In a subsequent step, we learn to rank the corresponding segments by training a continuous model to predict how likely they are to exhibit real-world regularities (expressed as putative overlap with ground truth) based on their mid-level region properties, then diversify the estimated overlap score using maximum marginal relevance measures. We show that this algorithm significantly outperforms the state of the art for low-level segmentation in the VOC 2009 and 2010 data sets. In our companion papers [1], [2], we show that the algorithm can be used, successfully, in a segmentation-based visual object category recognition pipeline. This architecture ranked first in the VOC2009 and VOC2010 image segmentation and labeling challenges.", "title": "CPMC: Automatic Object Segmentation Using Constrained Parametric Min-Cuts"}, "1395f0561db13cad21a519e18be111cbe1e6d818": {"paper_id": "1395f0561db13cad21a519e18be111cbe1e6d818", "abstract": "We address the problem of segmenting and recognizing objects in real world images, focusing on challenging articulated categories such as humans and other animals. For this purpose, we propose a novel design for region-based object detectors that integrates efficiently top-down information from scanning-windows part models and global appearance cues. Our detectors produce class-specific scores for bottom-up regions, and then aggregate the votes of multiple overlapping candidates through pixel classification. We evaluate our approach on the PASCAL segmentation challenge, and report competitive performance with respect to current leading techniques. On VOC2010, our method obtains the best results in 6/20 categories and the highest performance on articulated objects.", "title": "Semantic segmentation using regions and parts"}, "1e511a36cd6c793189c544a6f935958a2d98a737": {"paper_id": "1e511a36cd6c793189c544a6f935958a2d98a737", "abstract": "Datasets are an integral part of contemporary object recognition research. They have been the chief reason for the considerable progress in the field, not just as source of large amounts of training data, but also as means of measuring and comparing performance of competing algorithms. At the same time, datasets have often been blamed for narrowing the focus of object recognition research, reducing it to a single benchmark performance number. Indeed, some datasets, that started out as data capture efforts aimed at representing the visual world, have become closed worlds unto themselves (e.g. the Corel world, the Caltech-101 world, the PASCAL VOC world). With the focus on beating the latest benchmark numbers on the latest dataset, have we perhaps lost sight of the original purpose? The goal of this paper is to take stock of the current state of recognition datasets. We present a comparison study using a set of popular datasets, evaluated based on a number of criteria including: relative data bias, cross-dataset generalization, effects of closed-world assumption, and sample value. The experimental results, some rather surprising, suggest directions that can improve dataset collection as well as algorithm evaluation protocols. But more broadly, the hope is to stimulate discussion in the community regarding this very important, but largely neglected issue.", "title": "Unbiased look at dataset bias"}, "03cb609fcfce6c60cbe3eb0dd8254069bf6d7573": {"paper_id": "03cb609fcfce6c60cbe3eb0dd8254069bf6d7573", "abstract": "Deep multi-layer neural networks have many levels of non-linearities, which allows them to potentially represent very compactly highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization appears to often get stuck in poor solutions. Hinton et al. recently introduced a greedy layer-wise unsupervised learning algorithm for Deep Belief Networks (DBN), a generative model with many layers of hidden causal variables. In the context of the above optimization problem, we study this algorithm empirically and explore variants to better understand its success and extend it to cases where the inputs are continuous or where the structure of the input distribution is not revealing enough about the variable to be predicted in a supervised task.", "title": "Greedy Layer-Wise Training of Deep Networks"}, "a2c1d14f22c79dd656cbd3b99953aa301c6bbd74": {"paper_id": "a2c1d14f22c79dd656cbd3b99953aa301c6bbd74", "abstract": "We present a hierarchical model that learns image decompositions via alternating layers of convolutional sparse coding and max pooling. When trained on natural images, the layers of our model capture image information in a variety of forms: low-level edges, mid-level edge junctions, high-level object parts and complete objects. To build our model we rely on a novel inference scheme that ensures each layer reconstructs the input, rather than just the output of the layer directly beneath, as is common with existing hierarchical approaches. This makes it possible to learn multiple layers of representation and we show models with 4 layers, trained on images from the Caltech-101 and 256 datasets. When combined with a standard classifier, features extracted from these models outperform SIFT, as well as representations from other feature learning methods.", "title": "Adaptive deconvolutional networks for mid and high level feature learning"}, "05cc38e249a6f642363b5a5cbd71cda67cea5893": {"paper_id": "05cc38e249a6f642363b5a5cbd71cda67cea5893", "abstract": "[1] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient based learning applied to document recognition. Proceeding of the IEEE, 1998. [2] H. Lee, R. Grosse, R. Ranganath, and A.Y. Ng. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In ICML, 2009. [3] M.A. Ranzato, K. Jarrett, K. Kavukcuoglu and Y. LeCun. What is the best multi-stage architecture for object recognition? In ICCV, 2009. [4] A. Hyvarinen and P. Hoyer. Topographic independent component analysis as a model of V1 organization and receptive fields. Neural Computation, 2001. [5] A. Hyvarinen, J. Hurri, and P. Hoyer. Natural Image Statistics. Springer, 2009. [6] K. Kavukcuoglu, M. Ranzato, R. Fergus, Y. LeCun. Learning invariant features through topographic filter maps . In CVPR, 2009. [7] K. Gregor, Y. LeCun. Emergence of Complex-Like Cells in a Temporal Product Network with Local Receptive Fields. ARXIV, 2010.", "title": "Tiled convolutional neural networks"}, "0df6ad955ff43a6aebec23a709e7b2bacec6b695": {"paper_id": "0df6ad955ff43a6aebec23a709e7b2bacec6b695", "abstract": "Software-Defined Networking (SDN) is now envisioned for Wide Area Networks (WAN) and constrained overlay networks. Such networks require a resilient, scalable and easily extensible SDN control plane. In this paper, we propose DISCO, an extensible DIstributed SDN COntrol plane able to cope with the distributed and heterogeneous nature of modern overlay networks. A DISCO controller manages its own network domain and communicates with other controllers to provide end-to-end network services. This east-west communication is based on a lightweight and highly manageable control channel. We implemented DISCO on top of the Floodlight OpenFlow controller and the AMQP protocol and we evaluated it through an inter-domain topology disruption use case.", "title": "DISCO: Distributed multi-domain SDN controllers"}, "35deb0910773b810a642ff3b546de4eecfdc3ac3": {"paper_id": "35deb0910773b810a642ff3b546de4eecfdc3ac3", "abstract": "We present the design, implementation, and evaluation of B4, a private WAN connecting Google's data centers across the planet. B4 has a number of unique characteristics: i) massive bandwidth requirements deployed to a modest number of sites, ii) elastic traffic demand that seeks to maximize average bandwidth, and iii) full control over the edge servers and network, which enables rate limiting and demand measurement at the edge.\n These characteristics led to a Software Defined Networking architecture using OpenFlow to control relatively simple switches built from merchant silicon. B4's centralized traffic engineering service drives links to near 100% utilization, while splitting application flows among multiple paths to balance capacity against application priority/demands. We describe experience with three years of B4 production deployment, lessons learned, and areas for future work.", "title": "B4: experience with a globally-deployed software defined wan"}, "3dde3fec553b8d24a85d7059a3cc629ab33f7578": {"paper_id": "3dde3fec553b8d24a85d7059a3cc629ab33f7578", "abstract": "This whitepaper proposes OpenFlow: a way for researchers to run experimental protocols in the networks they use every day. OpenFlow is based on an Ethernet switch, with an internal flow-table, and a standardized interface to add and remove flow entries. Our goal is to encourage networking vendors to add OpenFlow to their switch products for deployment in college campus backbones and wiring closets. We believe that OpenFlow is a pragmatic compromise: on one hand, it allows researchers to run experiments on heterogeneous switches in a uniform way at line-rate and with high port-density; while on the other hand, vendors do not need to expose the internal workings of their switches. In addition to allowing researchers to evaluate their ideas in real-world traffic settings, OpenFlow could serve as a useful campus component in proposed large-scale testbeds like GENI. Two buildings at Stanford University will soon run OpenFlow networks, using commercial Ethernet switches and routers. We will work to encourage deployment at other schools; and We encourage you to consider deploying OpenFlow in your university network too", "title": "OpenFlow: enabling innovation in campus networks"}, "6a6c794083cbdf79de0fcd2065699477290b5546": {"paper_id": "6a6c794083cbdf79de0fcd2065699477290b5546", "abstract": "The fundamental feature of an OpenFlow network is that the controller is responsible for the initial establishment of every flow by contacting related switches. Thus the performance of the controller could be a bottleneck. This paper shows how this fundamental problem is addressed by parallelism. The state of the art OpenFlow controller, called NOX, achieves a simple programming model for control function development by having a single-threaded event-loop. Yet NOX has not considered exploiting parallelism. We propose Maestro which keeps the simple programming model for programmers, and exploits parallelism in every corner together with additional throughput optimization techniques. We experimentally show that the throughput of Maestro can achieve near linear scalability on an eight core server machine. Keywords-OpenFlow, network management, multithreading, performance optimization", "title": "Maestro: A System for Scalable OpenFlow Control"}, "3192a953370bc8bf4b906261e8e2596355d2b610": {"paper_id": "3192a953370bc8bf4b906261e8e2596355d2b610", "abstract": "Today's data networks are surprisingly fragile and difficult to manage. We argue that the root of these problems lies in the complexity of the control and management planes--the software and protocols coordinating network elements--and particularly the way the decision logic and the distributed-systems issues are inexorably intertwined. We advocate a complete refactoring of the functionality and propose three key principles--network-level objectives, network-wide views, and direct control--that we believe should underlie a new architecture. Following these principles, we identify an extreme design point that we call \"4D,\" after the architecture's four planes: decision, dissemination, discovery, and data. The 4D architecture completely separates an AS's decision logic from pro-tocols that govern the interaction among network elements. The AS-level objectives are specified in the decision plane, and en-forced through direct configuration of the state that drives how the data plane forwards packets. In the 4D architecture, the routers and switches simply forward packets at the behest of the decision plane, and collect measurement data to aid the decision plane in controlling the network. Although 4D would involve substantial changes to today's control and management planes, the format of data packets does not need to change; this eases the deployment path for the 4D architecture, while still enabling substantial innovation in network control and management. We hope that exploring an extreme design point will help focus the attention of the research and industrial communities on this crucially important and intellectually challenging area.", "title": "A clean slate 4D approach to network control and management"}, "25f433f9c30e69356103b594da8a6ca3ff648e3c": {"paper_id": "25f433f9c30e69356103b594da8a6ca3ff648e3c", "abstract": "1. SLICED PROGRAMMABLE NETWORKS OpenFlow [4] has been demonstrated as a way for researchers to run networking experiments in their production network. Last year, we demonstrated how an OpenFlow controller running on NOX [3] could move VMs seamlessly around an OpenFlow network [1]. While OpenFlow has potential [2] to open control of the network, only one researcher can innovate on the network at a time. What is required is a way to divide, or slice, network resources so that researchers and network administrators can use them in parallel. Network slicing implies that actions in one slice do not negatively affect other slices, even if they share the same underlying physical hardware. A common network slicing technique is VLANs. With VLANs, the administrator partitions the network by switch port and all traffic is mapped to a VLAN by input port or explicit tag. This coarse-grained type of network slicing complicates more interesting experiments such as IP mobility or wireless handover. Here, we demonstrate FlowVisor, a special purpose OpenFlow controller that allows multiple researchers to run experiments safely and independently on the same production OpenFlow network. To motivate FlowVisor\u2019s flexibility, we demonstrate four network slices running in parallel: one slice for the production network and three slices running experimental code (Figure 1). Our demonstration runs on real network hardware deployed on our production network at Stanford and a wide-area test-bed with a mix of wired and wireless technologies.", "title": "Carving research slices out of your production networks with OpenFlow"}, "141e96cb1d6c8ef8f904f5f1d32a0ff22718e8f1": {"paper_id": "141e96cb1d6c8ef8f904f5f1d32a0ff22718e8f1", "abstract": "BACKGROUND\nThe effectiveness of complex interventions, as well as their success in reaching relevant populations, is critically influenced by their implementation in a given context. Current conceptual frameworks often fail to address context and implementation in an integrated way and, where addressed, they tend to focus on organisational context and are mostly concerned with specific health fields. Our objective was to develop a framework to facilitate the structured and comprehensive conceptualisation and assessment of context and implementation of complex interventions.\n\n\nMETHODS\nThe Context and Implementation of Complex Interventions (CICI) framework was developed in an iterative manner and underwent extensive application. An initial framework based on a scoping review was tested in rapid assessments, revealing inconsistencies with respect to the underlying concepts. Thus, pragmatic utility concept analysis was undertaken to advance the concepts of context and implementation. Based on these findings, the framework was revised and applied in several systematic reviews, one health technology assessment (HTA) and one applicability assessment of very different complex interventions. Lessons learnt from these applications and from peer review were incorporated, resulting in the CICI framework.\n\n\nRESULTS\nThe CICI framework comprises three dimensions-context, implementation and setting-which interact with one another and with the intervention dimension. Context comprises seven domains (i.e., geographical, epidemiological, socio-cultural, socio-economic, ethical, legal, political); implementation consists of five domains (i.e., implementation theory, process, strategies, agents and outcomes); setting refers to the specific physical location, in which the intervention is put into practise. The intervention and the way it is implemented in a given setting and context can occur on a micro, meso and macro level. Tools to operationalise the framework comprise a checklist, data extraction tools for qualitative and quantitative reviews and a consultation guide for applicability assessments.\n\n\nCONCLUSIONS\nThe CICI framework addresses and graphically presents context, implementation and setting in an integrated way. It aims at simplifying and structuring complexity in order to advance our understanding of whether and how interventions work. The framework can be applied in systematic reviews and HTA as well as primary research and facilitate communication among teams of researchers and with various stakeholders.", "title": "Making sense of complexity in context and implementation: the Context and Implementation of Complex Interventions (CICI) framework"}, "c0413a16e090a8b62cb0f046438e62011ac5ced7": {"paper_id": "c0413a16e090a8b62cb0f046438e62011ac5ced7", "abstract": "The diffusion of innovations according to Rogers. With successive groups of consumers adopting the new technology (shown in blue), its market share (yellow) will eventually reach the saturation level. In mathematics the S curve is known as the logistic function. Diffusion of Innovations is a theory that seeks to explain how, why, and at what rate new ideas and technology spread through cultures. The concept was first studied by the French sociologist Gabriel Tarde (1890) and by German and Austrian anthropologists such as Friedrich Ratzel and Leo Frobenius.[1] Its basic epidemiological or internal-influence form was formulated by H. Earl Pemberton,[2] who provided examples of institutional diffusion such as postage stamps and compulsory school laws.", "title": "Diffusion of innovations 1 Diffusion of innovations"}, "db9ca6885513cf4c32884c597259b0a06fa29a60": {"paper_id": "db9ca6885513cf4c32884c597259b0a06fa29a60", "abstract": "\u25a0 Abstract This chapter provides a conceptual framework for integrating the array of variables defined in diffusion research to explicate their influence on an actor\u2019s decision to adopt an innovation. The framework groups the variables into three major components. The first component includes characteristics of the innovation itself, within which two sets of variables are defined concerning public versus private consequences and benefits versus costs of adoption. A second component involves the characteristics of innovators (actors) that influence the probability of adoption of an innovation. Within this component six sets of variables concern societal entity of innovators (either people, organizations, states, etc.), familiarity with the innovation, status characteristics, socioeconomic characteristics, position in social networks, and personal qualities. The third component involves characteristics of the environmental context that modulate diffusion via structural characteristics of the modern world. These latter characteristics incorporate four sets of variables: geographical settings, societal culture, political conditions, and global uniformity. The concluding analysis highlights the need in diffusion research to incorporate more fully ( a) the interactive character of diffusion variables, ( b) the gating function of diffusion variables, and ( c) effects of an actor\u2019s characteristics on the temporal rate of diffusion.", "title": "INTEGRATING MODELS OF DIFFUSION OF INNOVATIONS : A Conceptual Framework"}, "4c68e7eff1da14003cc7efbfbd9a0a0a3d5d4968": {"paper_id": "4c68e7eff1da14003cc7efbfbd9a0a0a3d5d4968", "abstract": "BACKGROUND\nImplementation science has progressed towards increased use of theoretical approaches to provide better understanding and explanation of how and why implementation succeeds or fails. The aim of this article is to propose a taxonomy that distinguishes between different categories of theories, models and frameworks in implementation science, to facilitate appropriate selection and application of relevant approaches in implementation research and practice and to foster cross-disciplinary dialogue among implementation researchers.\n\n\nDISCUSSION\nTheoretical approaches used in implementation science have three overarching aims: describing and/or guiding the process of translating research into practice (process models); understanding and/or explaining what influences implementation outcomes (determinant frameworks, classic theories, implementation theories); and evaluating implementation (evaluation frameworks). This article proposes five categories of theoretical approaches to achieve three overarching aims. These categories are not always recognized as separate types of approaches in the literature. While there is overlap between some of the theories, models and frameworks, awareness of the differences is important to facilitate the selection of relevant approaches. Most determinant frameworks provide limited \"how-to\" support for carrying out implementation endeavours since the determinants usually are too generic to provide sufficient detail for guiding an implementation process. And while the relevance of addressing barriers and enablers to translating research into practice is mentioned in many process models, these models do not identify or systematically structure specific determinants associated with implementation success. Furthermore, process models recognize a temporal sequence of implementation endeavours, whereas determinant frameworks do not explicitly take a process perspective of implementation.", "title": "Making sense of implementation theories, models and frameworks"}, "d80e7da055f9c25e29f732d0a829daf172eb1fa0": {"paper_id": "d80e7da055f9c25e29f732d0a829daf172eb1fa0", "abstract": "This article summarizes an extensive literature review addressing the question, How can we spread and sustain innovations in health service delivery and organization? It considers both content (defining and measuring the diffusion of innovation in organizations) and process (reviewing the literature in a systematic and reproducible way). This article discusses (1) a parsimonious and evidence-based model for considering the diffusion of innovations in health service organizations, (2) clear knowledge gaps where further research should be focused, and (3) a robust and transferable methodology for systematically reviewing health service policy and management. Both the model and the method should be tested more widely in a range of contexts.", "title": "Diffusion of innovations in service organizations: systematic review and recommendations."}, "7a39ae394aee34209b82057d48f1ad6c57d7c8e5": {"paper_id": "7a39ae394aee34209b82057d48f1ad6c57d7c8e5", "abstract": "While attacks on information systems have for most practical purposes binary outcomes (information was manipulated/eavesdropped, or not), attacks manipulating the sensor or control signals of Industrial Control Systems (ICS) can be tuned by the attacker to cause a continuous spectrum in damages. Attackers that want to remain undetected can attempt to hide their manipulation of the system by following closely the expected behavior of the system, while injecting just enough false information at each time step to achieve their goals. In this work, we study if attack-detection can limit the impact of such stealthy attacks. We start with a comprehensive review of related work on attack detection schemes in the security and control systems community. We then show that many of those works use detection schemes that are not limiting the impact of stealthy attacks. We propose a new metric to measure the impact of stealthy attacks and how they relate to our selection on an upper bound on false alarms. We finally show that the impact of such attacks can be mitigated in several cases by the proper combination and configuration of detection schemes. We demonstrate the effectiveness of our algorithms through simulations and experiments using real ICS testbeds and real ICS systems.", "title": "Limiting the Impact of Stealthy Attacks on Industrial Control Systems"}, "4ec6e2b96ee42346520a10382c68ee32c7171df1": {"paper_id": "4ec6e2b96ee42346520a10382c68ee32c7171df1", "abstract": "This paper analyzes the effect of replay attacks on a control system. We assume an attacker wishes to disrupt the operation of a control system in steady state. In order to inject an exogenous control input without being detected the attacker will hijack the sensors, observe and record their readings for a certain amount of time and repeat them afterwards while carrying out his attack. This is a very common and natural attack (we have seen numerous times intruders recording and replaying security videos while performing their attack undisturbed) for an attacker who does not know the dynamics of the system but is aware of the fact that the system itself is expected to be in steady state for the duration of the attack. We assume the control system to be a discrete time linear time invariant gaussian system applying an infinite horizon Linear Quadratic Gaussian (LQG) controller. We also assume that the system is equipped with a \u03c72 failure detector. The main contributions of the paper, beyond the novelty of the problem formulation, consist in 1) providing conditions on the feasibility of the replay attack on the aforementioned system and 2) proposing a countermeasure that guarantees a desired probability of detection (with a fixed false alarm rate) by trading off either detection delay or LQG performance, either by decreasing control accuracy or increasing control effort.", "title": "Secure control against replay attacks"}, "059e776cacf87b3ed3f6eb9aa87968247fa68be5": {"paper_id": "059e776cacf87b3ed3f6eb9aa87968247fa68be5", "abstract": "Cyber-Physical Systems (CPS) are integrations of computation and physical processes. Embedded computers and networks monitor and control the physical processes, usually with feedback loops where physical processes affect computations and vice versa. The economic and societal potential of such systems is vastly greater than what has been realized, and major investments are being made worldwide to develop the technology. There are considerable challenges, particularly because the physical components of such systems introduce safety and reliability requirements qualitatively different from those in general- purpose computing. Moreover, physical components are qualitatively different from object-oriented software components. Standard abstractions based on method calls and threads do not work. This paper examines the challenges in designing such systems, and in particular raises the question of whether today's computing and networking technologies provide an adequate foundation for CPS. It concludes that it will not be sufficient to improve design processes, raise the level of abstraction, or verify (formally or otherwise) designs that are built on today's abstractions. To realize the full potential of CPS, we will have to rebuild computing and networking abstractions. These abstractions will have to embrace physical dynamics and computation in a unified way.", "title": "Cyber Physical Systems: Design Challenges"}, "996dc6cddb6ab5249d66835eeaf94ffbfc504c08": {"paper_id": "996dc6cddb6ab5249d66835eeaf94ffbfc504c08", "abstract": "In this paper we attempt to answer two questions: (1) Why should we be interested in the security of control systems? And (2) What are the new and fundamentally different requirements and problems for the security of control systems? We also propose a new mathematical framework to analyze attacks against control systems. Within this framework we formulate specific research problems to (1) detect attacks, and (2) survive attacks.", "title": "Research Challenges for the Security of Control Systems"}, "20cdcbeef626e43eb4d316dc14c0ed97c80a482a": {"paper_id": "20cdcbeef626e43eb4d316dc14c0ed97c80a482a", "abstract": "This paper considers control and estimation problems where the sensor signals and the actuator signals are transmitted to various subsystems over a network. In contrast to traditional control and estimation problems, here the observation and control packets may be lost or delayed. The unreliability of the underlying communication network is modeled stochastically by assigning probabilities to the successful transmission of packets. This requires a novel theory which generalizes classical control/estimation paradigms. The paper offers the foundations of such a novel theory. The central contribution is to characterize the impact of the network reliability on the performance of the feedback loop. Specifically, it is shown that for network protocols where successful transmissions of packets is acknowledged at the receiver (e.g., TCP-like protocols), there exists a critical threshold of network reliability (i.e., critical probabilities for the successful delivery of packets), below which the optimal controller fails to stabilize the system. Further, for these protocols, the separation principle holds and the optimal LQG controller is a linear function of the estimated state. In stark contrast, it is shown that when there is no acknowledgement of successful delivery of control packets (e.g., UDP-like protocols), the LQG optimal controller is in general nonlinear. Consequently, the separation principle does not hold in this circumstance", "title": "Foundations of Control and Estimation Over Lossy Networks"}, "a98636c851bf92c8f23b6f95653b00877f3718a8": {"paper_id": "a98636c851bf92c8f23b6f95653b00877f3718a8", "abstract": "Cyber-secure networked control is modeled, analyzed, and experimentally illustrated in this paper. An attack space defined by the adversary's system knowledge, disclosure, and disruption resources is introduced. Adversaries constrained by these resources are modeled for a networked control system architecture. It is shown that attack scenarios corresponding to replay, zero dynamics, and bias injection attacks can be analyzed using this framework. An experimental setup based on a quadruple-tank process controlled over a wireless network is used to illustrate the attack scenarios, their consequences, and potential counter-measures.", "title": "Attack models and scenarios for networked control systems"}, "300fff001ec0164e0f119d1b0e8dbe735f0bebaa": {"paper_id": "300fff001ec0164e0f119d1b0e8dbe735f0bebaa", "abstract": "Future power networks will be characterized by safe and reliable functionality against physical and cyber attacks. This paper proposes a unified framework and advanced monitoring procedures to detect and identify network components malfunction or measurements corruption caused by an omniscient adversary. We model a power system under cyber-physical attack as a linear time-invariant descriptor system with unknown inputs. Our attack model generalizes the prototypical stealth, (dynamic) false-data injection and replay attacks. We characterize the fundamental limitations of both static and dynamic procedures for attack detection and identification. Additionally, we design provably-correct (dynamic) detection and identification procedures based on tools from geometric control theory. Finally, we illustrate the effectiveness of our method through a comparison with existing (static) detection algorithms, and through a numerical study.", "title": "Cyber-physical attacks in power networks: Models, fundamental limitations and monitor design"}, "31093f5a40a3992a4dbef8df93cf26f0e4db7a5b": {"paper_id": "31093f5a40a3992a4dbef8df93cf26f0e4db7a5b", "abstract": "A power grid is a complex system connecting electric power generators to consumers through power transmission and distribution networks across a large geographical area. System monitoring is necessary to ensure the reliable operation of power grids, and state estimation is used in system monitoring to best estimate the power grid state through analysis of meter measurements and power system models. Various techniques have been developed to detect and identify bad measurements, including the interacting bad measurements introduced by arbitrary, non-random causes. At first glance, it seems that these techniques can also defeat malicious measurements injected by attackers.\n In this paper, we present a new class of attacks, called false data injection attacks, against state estimation in electric power grids. We show that an attacker can exploit the configuration of a power system to launch such attacks to successfully introduce arbitrary errors into certain state variables while bypassing existing techniques for bad measurement detection. Moreover, we look at two realistic attack scenarios, in which the attacker is either constrained to some specific meters (due to the physical protection of the meters), or limited in the resources required to compromise meters. We show that the attacker can systematically and efficiently construct attack vectors in both scenarios, which can not only change the results of state estimation, but also modify the results in arbitrary ways. We demonstrate the success of these attacks through simulation using IEEE test systems. Our results indicate that security protection of the electric power grid must be revisited when there are potentially malicious attacks.", "title": "False data injection attacks against state estimation in electric power grids"}, "095b05f6f0803bb1871b677cf3c3d4b41dbe6d18": {"paper_id": "095b05f6f0803bb1871b677cf3c3d4b41dbe6d18", "abstract": "Many different demands can be made of intrusion detection systems. An important requirement is that an intrusion detection system be effective; that is, it should detect a substantial percentage of intrusions into the supervised system, while still keeping the false alarm rate at an acceptable level. This article demonstrates that, for a reasonable set of assumptions, the false alarm rate is the limiting factor for the performance of an intrusion detection system. This is due to the base-rate fallacy phenomenon, that in order to achieve substantial values of the Bayesian detection rate P(Intrusion***Alarm), we have to achieve a (perhaps in some cases unattainably) low false alarm rate. A selection of reports of intrusion detection performance are reviewed, and the conclusion is reached that there are indications that at least some types of intrusion detection have far to go before they can attain such low false alarm rates.", "title": "The base-rate fallacy and the difficulty of intrusion detection"}, "bdf67ee2a13931ca2d5eac458714ed98148d1b34": {"paper_id": "bdf67ee2a13931ca2d5eac458714ed98148d1b34", "abstract": "A model of a real-time intrusion-detection expert system capable of detecting break-ins, penetrations, and other forms of computer abuse is described. The model is based on the hypothesis that security violations can be detected by monitoring a system's audit records for abnormal patterns of system usage. The model includes profiles for representing the behavior of subjects with respect to objects in terms of metrics and statistical models, and rules for acquiring knowledge about this behavior from audit records and for detecting anomalous behavior. The model is independent of any particular system, application environment, system vulnerability, or type of intrusion, thereby providing a framework for a general-purpose intrusion-detection expert system.", "title": "An Intrusion-Detection Model"}, "6db50bb953fae36a812db9f0d2767e1dfa74e42b": {"paper_id": "6db50bb953fae36a812db9f0d2767e1dfa74e42b", "abstract": "In this paper, we describe a novel deep convolutional neural networks (CNN) based approach called contextual deep CNN that can jointly exploit spatial and spectral features for hyperspectral image classification. The contextual deep CNN first concurrently applies multiple 3-dimensional local convolutional filters with different sizes jointly exploiting spatial and spectral features of a hyperspectral image. The initial spatial and spectral feature maps obtained from applying the variable size convolutional filters are then combined together to form a joint spatio-spectral feature map. The joint feature map representing rich spectral and spatial properties of the hyperspectral image is then fed through fully convolutional layers that eventually predict the corresponding label of each pixel vector. The proposed approach is tested on two benchmark datasets: the Indian Pines dataset and the Pavia University scene dataset. Performance comparison shows enhanced classification performance of the proposed approach over the current state of the art on both datasets.", "title": "Contextual deep CNN based hyperspectral classification"}, "2369db9921078c4bb76072ef7d6426e9f1dbfdb5": {"paper_id": "2369db9921078c4bb76072ef7d6426e9f1dbfdb5", "abstract": "Recently, convolutional neural networks have demonstrated excellent performance on various visual tasks, including the classification of common two-dimensional images. In this paper, deep convolutional neural networks are employed to classify hyperspectral images directly in spectral domain. More specifically, the architecture of the proposed classifier contains five layers with weights which are the input layer, the convolutional layer, the max pooling layer, the full connection layer, and the output layer. These five layers are implemented on each spectral signature to discriminate against others. Experimental results based on several hyperspectral image data sets demonstrate that the proposed method can achieve better classification performance than some traditional methods, such as support vector machines and the conventional deep learning-based methods.", "title": "Deep Convolutional Neural Networks for Hyperspectral Image Classification"}, "38cd636e280b78cedeb3efbbe9c44d12325b219d": {"paper_id": "38cd636e280b78cedeb3efbbe9c44d12325b219d", "abstract": "A new spectral-spatial classification scheme for hyperspectral images is proposed. The method combines the results of a pixel wise support vector machine classification and the segmentation map obtained by partitional clustering using majority voting. The ISODATA algorithm and Gaussian mixture resolving techniques are used for image clustering. Experimental results are presented for two hyperspectral airborne images. The developed classification scheme improves the classification accuracies and provides classification maps with more homogeneous regions, when compared to pixel wise classification. The proposed method performs particularly well for classification of images with large spatial structures and when different classes have dissimilar spectral responses and a comparable number of pixels.", "title": "Spectral\u2013Spatial Classification of Hyperspectral Imagery Based on Partitional Clustering Techniques"}, "ef8ae1effca9cd45677086034d8c7b06a69c03e5": {"paper_id": "ef8ae1effca9cd45677086034d8c7b06a69c03e5", "abstract": "Classification is one of the most popular topics in hyperspectral remote sensing. In the last two decades, a huge number of methods were proposed to deal with the hyperspectral data classification problem. However, most of them do not hierarchically extract deep features. In this paper, the concept of deep learning is introduced into hyperspectral data classification for the first time. First, we verify the eligibility of stacked autoencoders by following classical spectral information-based classification. Second, a new way of classifying with spatial-dominated information is proposed. We then propose a novel deep learning framework to merge the two features, from which we can get the highest classification accuracy. The framework is a hybrid of principle component analysis (PCA), deep learning architecture, and logistic regression. Specifically, as a deep learning architecture, stacked autoencoders are aimed to get useful high-level features. Experimental results with widely-used hyperspectral data indicate that classifiers built in this deep learning-based framework provide competitive performance. In addition, the proposed joint spectral-spatial deep neural network opens a new window for future research, showcasing the deep learning-based methods' huge potential for accurate hyperspectral data classification.", "title": "Deep Learning-Based Classification of Hyperspectral Data"}, "722fcc35def20cfcca3ada76c8dd7a585d6de386": {"paper_id": "722fcc35def20cfcca3ada76c8dd7a585d6de386", "abstract": "Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (approx 2 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments.\n Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.", "title": "Caffe: Convolutional Architecture for Fast Feature Embedding"}, "1116467d67f475d1bd2539862f10f792032fe13b": {"paper_id": "1116467d67f475d1bd2539862f10f792032fe13b", "abstract": "1. The striate cortex was studied in lightly anaesthetized macaque and spider monkeys by recording extracellularly from single units and stimulating the retinas with spots or patterns of light. Most cells can be categorized as simple, complex, or hypercomplex, with response properties very similar to those previously described in the cat. On the average, however, receptive fields are smaller, and there is a greater sensitivity to changes in stimulus orientation. A small proportion of the cells are colour coded.2. Evidence is presented for at least two independent systems of columns extending vertically from surface to white matter. Columns of the first type contain cells with common receptive-field orientations. They are similar to the orientation columns described in the cat, but are probably smaller in cross-sectional area. In the second system cells are aggregated into columns according to eye preference. The ocular dominance columns are larger than the orientation columns, and the two sets of boundaries seem to be independent.3. There is a tendency for cells to be grouped according to symmetry of responses to movement; in some regions the cells respond equally well to the two opposite directions of movement of a line, but other regions contain a mixture of cells favouring one direction and cells favouring the other.4. A horizontal organization corresponding to the cortical layering can also be discerned. The upper layers (II and the upper two-thirds of III) contain complex and hypercomplex cells, but simple cells are virtually absent. The cells are mostly binocularly driven. Simple cells are found deep in layer III, and in IV A and IV B. In layer IV B they form a large proportion of the population, whereas complex cells are rare. In layers IV A and IV B one finds units lacking orientation specificity; it is not clear whether these are cell bodies or axons of geniculate cells. In layer IV most cells are driven by one eye only; this layer consists of a mosaic with cells of some regions responding to one eye only, those of other regions responding to the other eye. Layers V and VI contain mostly complex and hypercomplex cells, binocularly driven.5. The cortex is seen as a system organized vertically and horizontally in entirely different ways. In the vertical system (in which cells lying along a vertical line in the cortex have common features) stimulus dimensions such as retinal position, line orientation, ocular dominance, and perhaps directionality of movement, are mapped in sets of superimposed but independent mosaics. The horizontal system segregates cells in layers by hierarchical orders, the lowest orders (simple cells monocularly driven) located in and near layer IV, the higher orders in the upper and lower layers.", "title": "Receptive fields and functional architecture of monkey striate cortex."}, "e2b7f37cd97a7907b1b8a41138721ed06a0b76cd": {"paper_id": "e2b7f37cd97a7907b1b8a41138721ed06a0b76cd", "abstract": "We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoderswhich are trained locally to denoise corrupted versions of t heir inputs. The resulting algorithm is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classification problems to yield sign ificantly lower classification error, thus bridging the performance gap with deep belief networks (DBN), and in several cases surpassing it. Higher level representations learnt in this purely u ns pervised fashion also help boost the performance of subsequent SVM classifiers. Qualitative exp eriments show that, contrary to ordinary autoencoders, denoising autoencoders are able to lear n Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images . Thi work clearly establishes the value of using a denoising criterion as a tractable unsupervised o bjective to guide the learning of useful higher level representations.", "title": "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion"}, "213d7af7107fa4921eb0adea82c9f711fd105232": {"paper_id": "213d7af7107fa4921eb0adea82c9f711fd105232", "abstract": "High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such \"autoencoder\" networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.", "title": "Reducing the dimensionality of data with neural networks."}, "b7682634c8633822145193242e7a3e3739042768": {"paper_id": "b7682634c8633822145193242e7a3e3739042768", "abstract": "In this paper, we present MusicMixer, a computer-aided DJ system that helps DJs, specifically with song mixing. MusicMixer continuously mixes and plays songs using an automatic music mixing method that employs audio similarity calculations. By calculating similarities between song sections that can be naturally mixed, MusicMixer enables seamless song transitions. Though song mixing is the most fundamental and important factor in DJ performance, it is difficult for untrained people to seamlessly connect songs. MusicMixer realizes automatic song mixing using an audio signal processing approach; therefore, users can perform DJ mixing simply by selecting a song from a list of songs suggested by the system, enabling effective DJ song mixing and lowering entry barriers for the inexperienced. We also propose personalization for song suggestions using a preference memorization function of MusicMixer.", "title": "MusicMixer: computer-aided DJ system based on an automatic song mixing"}, "18f17d72bc033dce8cc0ac5e6cf7f647cddbdc43": {"paper_id": "18f17d72bc033dce8cc0ac5e6cf7f647cddbdc43", "abstract": "Patients with significant facial atrophy and age-related loss of facial fat generally achieve suboptimal improvement from both surface treatments of facial skin and surgical lifts. Restoring lost facial volume by fat grafting is a powerful technique that is now acknowledged by most plastic surgeons and other physicians engaged in treating the aging face as one of the most important advances in aesthetic surgery. Properly performed, the addition of fat to areas of the face that have atrophied because of age or disease can produce a significant and sustained improvement in appearance that is unobtainable by other means.", "title": "Fat grafting in facial rejuvenation."}, "f7c21648c0dc2f968993445b27f1eb3ac198de8c": {"paper_id": "f7c21648c0dc2f968993445b27f1eb3ac198de8c", "abstract": "The cytokinesis-block micronucleus cytome (CBMN Cyt) assay is one of the best-validated methods for measuring chromosome damage in human lymphocytes. This paper describes the methodology, biology, and mechanisms underlying the application of this technique for biodosimetry following exposure to ionizing radiation. Apart from the measurement of micronuclei, it is also possible to measure other important biomarkers within the CBMN Cyt assay that are relevant to radiation biodosimetry. These include nucleoplasmic bridges, which are an important additional measure of radiation-induced damage that originate from dicentric chromosomes as well as the proportion of dividing cells and cells undergoing cell death. A brief account is also given of current developments in the automation of this technique and important knowledge gaps that need attention to further enhance the applicability of this important method for radiation biodosimetry.", "title": "The lymphocyte cytokinesis-block micronucleus cytome assay and its application in radiation biodosimetry."}, "52a2f0c109de361fa4b927c52c3ab2aff2664f83": {"paper_id": "52a2f0c109de361fa4b927c52c3ab2aff2664f83", "abstract": "This paper surveys recent literature in the domain of machine learning techniques and artificial intelligence used to predict stock market movements. Artificial Neural Networks (ANNs) are identified to be the dominant machine learning technique in stock market prediction area. Keywords\u2014 Artificial Neural Networks (ANNs); Stock Market; Prediction", "title": "Applications of ANNs in Stock Market Prediction : A Survey"}, "25b0d0316ece493899d74cfb98ce7b77dca8352e": {"paper_id": "25b0d0316ece493899d74cfb98ce7b77dca8352e", "abstract": "This paper discusses a buying and selling timing prediction system for stocks on the Tokyo Stock Exchange and analysis of intemal representation. It is based on modular neural networks[l][2]. We developed a number of learning algorithms and prediction methods for the TOPIX(Toky0 Stock Exchange Prices Indexes) prediction system. The prediction system achieved accurate predictions and the simulation on stocks tradmg showed an excellent profit. The prediction system was developed by Fujitsu and Nikko Securities.", "title": "Stock market prediction system with modular neural networks"}, "7bdaefdd9954b75fdf305135b27105214c8eac66": {"paper_id": "7bdaefdd9954b75fdf305135b27105214c8eac66", "abstract": "-A neural network learning procedure has been applied to the classification ~/sonar returns [kom two undersea targets, a metal cylinder and a similarly shaped rock. Networks with an intermediate layer ~/ hidden processing units achieved a classification accuracy as high as 100% on a training set of l04 returns. These net~orks correctly classified up to 90.4% of 104 test returns not contained in the training set. This perfi~rmance was better than that of a nearest neighbor classifier, which was 82.7%. and was close to that of an optimal Bayes classifie~ Specific signal features extracted by hidden units in a trained network were identified and related to coding schemes in the pattern of connection strengths between the input and the hidden units. Network perlbrmance and class[/~cation strategy was comparable to that of trained human listeners. Keywords--Learning algorithms, Hidden units. Multilayered neural network, Sonar, Signal processing.", "title": "Analysis of hidden units in a layered network trained to classify sonar targets"}, "11463e2a6ed218e87e22cba2c2f24fb5992d0293": {"paper_id": "11463e2a6ed218e87e22cba2c2f24fb5992d0293", "abstract": "Acknowledgements Research in areas where there are many possible paths to follow requires a keen eye for crucial issues. The study of learning systems is such an area. Through the years of working with Andy Barto and Rich Sutton, I have observed many instances of \" fluff cutting \" and the exposure of basic issues. I thank both Andy and Rich for the insights that have rubbed off on me. I also thank Andy for opening up an infinite world of perspectives on learning, ranging from engineering principles to neural processing theories. I thank Rich for showing me the most important step in doing \" science \" \u2014simplify your questions by isolating the issues. Several people contributed to the readability of this dissertation. Andy spent much time carefully reading several drafts. Through his efforts the clarity is much improved. I thank Paul Utgoff, Michael Arbib, and Bill Kilmer for reading drafts of this dissertation and providing valuable criticisms. Paul provided a non-connectionist perspective that widened my view considerably. He never hesitated to work out differences in terms and methodologies that have been developed through research with connectionist vs. symbolic representations. I thank for commenting on an early draft and for many interesting discussions. and the AFOSR for starting and maintaining the research project that supported the work reported in this dis-sertation. I thank Susan Parker for the skill with which she administered the project. And I thank the COINS Department at UMass and the RCF Staff for the maintenance of the research computing environment. Much of the computer graphics software used to generate figures of this dissertation is based on graphics tools provided by Rich Sutton and Andy Cromarty. Most importantly, I thank Stacey and Joseph for always being there to lift my spirits while I pursued distant milestones and to share my excitement upon reaching them. Their faith and confidence helped me maintain a proper perspective. The difficulties of learning in multilayered networks of computational units has limited the use of connectionist systems in complex domains. This dissertation elucidates the issues of learning in a network's hidden units, and reviews methods for addressing these issues that have been developed through the years. Issues of learning in hidden units are shown to be analogous to learning issues for multilayer systems employing symbolic representations. Comparisons of a number of algorithms for learning in hidden units are made by applying them in \u2026", "title": "Learning and Problem Solving with Multilayer Connectionist Systems"}, "bccf38995692eae3f555e91093e31f9b0199d040": {"paper_id": "bccf38995692eae3f555e91093e31f9b0199d040", "abstract": "A theory of memory retrieval is developed and is shown to apply over a range of experimental paradigms. Access to memory traces is viewed in terms of a resonance metaphor. The probe item evokes the search set on the basis of probe-memory item relatedness, just as a ringing tuning fork evokes sympathetic vibrations in other tuning forks. Evidence is accumulated in parallel from each probe-memory item comparison, and each comparison is modeled by a continuous random walk process. In item recognition, the decision process is self-terminating on matching comparisons and exhaustive on nonmatching comparisons. The mathematical model produces predictions about accuracy, mean reaction time, error latency, and reaction time distributions that are in good accord with experimental data. The theory is applied to four item recognition paradigms (Sternberg, prememorized list, study-test, and continuous) and to speed-accuracy paradigms; results are found to provide a basis for comparison of these paradigms. It is noted that neural network models can be interfaced to the retrieval theory with little difficulty and that semantic memory models may benefit from such a retrieval scheme.", "title": "A Theory of Memory Retrieval"}, "23912ec62e6cceb2c209af396f2836c88267707b": {"paper_id": "23912ec62e6cceb2c209af396f2836c88267707b", "abstract": "This paper proposes a novel and more accurate iris segmentation framework to automatically segment iris region from the face images acquired with relaxed imaging under visible or near-infrared illumination, which provides strong feasibility for applications in surveillance, forensics and the search for missing children, etc. The proposed framework is built on a novel total-variation based formulation which uses l1 norm regularization to robustly suppress noisy texture pixels for the accurate iris localization. A series of novel and robust post processing operations are introduced to more accurately localize the limbic boundaries. Our experimental results on three publicly available databases, i.e., FRGC, UBIRIS.v2 and CASIA.v4-distance, achieve significant performance improvement in terms of iris segmentation accuracy over the state-of-the-art approaches in the literature. Besides, we have shown that using iris masks generated from the proposed approach helps to improve iris recognition performance as well. Unlike prior work, all the implementations in this paper are made publicly available to further advance research and applications in biometrics at-d-distance.", "title": "An Accurate Iris Segmentation Framework Under Relaxed Imaging Constraints Using Total Variation Model"}, "6932e39989ddd94a19bc193c96af995311f55d9e": {"paper_id": "6932e39989ddd94a19bc193c96af995311f55d9e", "abstract": "Automated human identification at-a-distance, using completely automated iris segmentation, is highly challenging and has wide range of civilian and forensics applications. Iris images acquired at-a-distance using visible and infrared imaging are often noisy and suffer from divergent spectral changes largely resulting from scattering, albedo and spectral absorbance selectivity. Therefore further research efforts are required to develop feature extraction techniques which are more tolerant to illumination changes and noise. This paper develops a new approach for the automated recognition from such distantly acquired iris images using sparse representation of local Radon transform (LRT) based orientation features. We model the iris representation problem as sparse coding solution based on computationally efficient LRT dictionary which is solved by widely studied convex optimization approach/strategy. The iris recognition and verification performance for the distantly acquired iris images are also evaluated using baseline 1-D log-Gabor filter and monogenic log-Gabor filter based approach. The experimental results are reported on the publically available UBIRIS V2, FRGC and CASIAV4-distance databases. The achieved experimental results on at-a-distance databases are highly promising and confirm the usefulness of the approach.", "title": "Human identification from at-a-distance face images using sparse representation of local iris features"}, "075bc988728788aa033b04dee1753ded711180ee": {"paper_id": "075bc988728788aa033b04dee1753ded711180ee", "abstract": "We consider the problem of automatically recognizing human faces from frontal views with varying expression and illumination, as well as occlusion and disguise. We cast the recognition problem as one of classifying among multiple linear regression models and argue that new theory from sparse signal representation offers the key to addressing this problem. Based on a sparse representation computed by C1-minimization, we propose a general classification algorithm for (image-based) object recognition. This new framework provides new insights into two crucial issues in face recognition: feature extraction and robustness to occlusion. For feature extraction, we show that if sparsity in the recognition problem is properly harnessed, the choice of features is no longer critical. What is critical, however, is whether the number of features is sufficiently large and whether the sparse representation is correctly computed. Unconventional features such as downsampled images and random projections perform just as well as conventional features such as eigenfaces and Laplacianfaces, as long as the dimension of the feature space surpasses certain threshold, predicted by the theory of sparse representation. This framework can handle errors due to occlusion and corruption uniformly by exploiting the fact that these errors are often sparse with respect to the standard (pixel) basis. The theory of sparse representation helps predict how much occlusion the recognition algorithm can handle and how to choose the training images to maximize robustness to occlusion. We conduct extensive experiments on publicly available databases to verify the efficacy of the proposed algorithm and corroborate the above claims.", "title": "Robust Face Recognition via Sparse Representation"}, "9d3133a2d3c536fe628fd1fe1d428371b414ba0e": {"paper_id": "9d3133a2d3c536fe628fd1fe1d428371b414ba0e", "abstract": "In this paper, we propose a new patch distribution feature (PDF) (i.e., referred to as Gabor-PDF) for human gait recognition. We represent each gait energy image (GEI) as a set of local augmented Gabor features, which concatenate the Gabor features extracted from different scales and different orientations together with the X-Y coordinates. We learn a global Gaussian mixture model (GMM) (i.e., referred to as the universal background model) with the local augmented Gabor features from all the gallery GEIs; then, each gallery or probe GEI is further expressed as the normalized parameters of an image-specific GMM adapted from the global GMM. Observing that one video is naturally represented as a group of GEIs, we also propose a new classification method called locality-constrained group sparse representation (LGSR) to classify each probe video by minimizing the weighted l1, 2 mixed-norm-regularized reconstruction error with respect to the gallery videos. In contrast to the standard group sparse representation method that is a special case of LGSR, the group sparsity and local smooth sparsity constraints are both enforced in LGSR. Our comprehensive experiments on the benchmark USF HumanID database demonstrate the effectiveness of the newly proposed feature Gabor-PDF and the new classification method LGSR for human gait recognition. Moreover, LGSR using the new feature Gabor-PDF achieves the best average Rank-1 and Rank-5 recognition rates on this database among all gait recognition algorithms proposed to date.", "title": "Human Gait Recognition Using Patch Distribution Feature and Locality-Constrained Group Sparse Representation"}, "c299b74dcb9ade9bda3ad6eb236fcc9cf2aa4991": {"paper_id": "c299b74dcb9ade9bda3ad6eb236fcc9cf2aa4991", "abstract": "A new image decomposition scheme, called the adaptive directional total variation (ADTV) model, is proposed to achieve effective segmentation and enhancement for latent fingerprint images in this work. The proposed model is inspired by the classical total variation models, but it differentiates itself by integrating two unique features of fingerprints; namely, scale and orientation. The proposed ADTV model decomposes a latent fingerprint image into two layers: cartoon and texture. The cartoon layer contains unwanted components (e.g., structured noise) while the texture layer mainly consists of the latent fingerprint. This cartoon-texture decomposition facilitates the process of segmentation, as the region of interest can be easily detected from the texture layer using traditional segmentation methods. The effectiveness of the proposed scheme is validated through experimental results on the entire NIST SD27 latent fingerprint database. The proposed scheme achieves accurate segmentation and enhancement results, leading to improved feature detection and latent matching performance.", "title": "Adaptive Directional Total-Variation Model for Latent Fingerprint Segmentation"}, "1ae04a7c66b946fd2d231690a2b44ca1f03984c8": {"paper_id": "1ae04a7c66b946fd2d231690a2b44ca1f03984c8", "abstract": "Latent fingerprints are routinely found at crime scenes due to the inadvertent contact of the criminals' finger tips with various objects. As such, they have been used as crucial evidence for identifying and convicting criminals by law enforcement agencies. However, compared to plain and rolled prints, latent fingerprints usually have poor quality of ridge impressions with small fingerprint area, and contain large overlap between the foreground area (friction ridge pattern) and structured or random noise in the background. Accordingly, latent fingerprint segmentation is a difficult problem. In this paper, we propose a latent fingerprint segmentation algorithm whose goal is to separate the fingerprint region (region of interest) from background. Our algorithm utilizes both ridge orientation and frequency features. The orientation tensor is used to obtain the symmetric patterns of fingerprint ridge orientation, and local Fourier analysis method is used to estimate the local ridge frequency of the latent fingerprint. Candidate fingerprint (foreground) regions are obtained for each feature type; an intersection of regions from orientation and frequency features localizes the true latent fingerprint regions. To verify the viability of the proposed segmentation algorithm, we evaluated the segmentation results in two aspects: a comparison with the ground truth foreground and matching performance based on segmented region.", "title": "Automatic segmentation of latent fingerprints"}, "17b700b22d9683bfddabe1564fcd637fb3957057": {"paper_id": "17b700b22d9683bfddabe1564fcd637fb3957057", "abstract": "Latent fingerprint identification is of critical importance to law enforcement agencies in identifying suspects: Latent fingerprints are inadvertent impressions left by fingers on surfaces of objects. While tremendous progress has been made in plain and rolled fingerprint matching, latent fingerprint matching continues to be a difficult problem. Poor quality of ridge impressions, small finger area, and large nonlinear distortion are the main difficulties in latent fingerprint matching compared to plain or rolled fingerprint matching. We propose a system for matching latent fingerprints found at crime scenes to rolled fingerprints enrolled in law enforcement databases. In addition to minutiae, we also use extended features, including singularity, ridge quality map, ridge flow map, ridge wavelength map, and skeleton. We tested our system by matching 258 latents in the NIST SD27 database against a background database of 29,257 rolled fingerprints obtained by combining the NIST SD4, SD14, and SD27 databases. The minutiae-based baseline rank-1 identification rate of 34.9 percent was improved to 74 percent when extended features were used. In order to evaluate the relative importance of each extended feature, these features were incrementally used in the order of their cost in marking by latent experts. The experimental results indicate that singularity, ridge quality map, and ridge flow map are the most effective features in improving the matching accuracy.", "title": "Latent Fingerprint Matching"}, "bcc7d03e5139d44f5a64ab65a9d4f5acc28632f2": {"paper_id": "bcc7d03e5139d44f5a64ab65a9d4f5acc28632f2", "abstract": "Automatic feature extraction in latent fingerprints is a challenging problem due to poor quality of most latents, such as unclear ridge structures, overlapped lines and letters, and overlapped fingerprints. We proposed a latent fingerprint enhancement algorithm which requires manually marked region of interest (ROI) and singular points. The core of the proposed enhancement algorithm is a novel orientation field estimation algorithm, which fits orientation field model to coarse orientation field estimated from skeleton outputted by a commercial fingerprint SDK. Experimental results on NIST SD27 latent fingerprint database indicate that by incorporating the proposed enhancement algorithm, the matching accuracy of the commercial matcher was significantly improved.", "title": "On Latent Fingerprint Enhancement"}, "54205667c1f65a320f667d73c354ed8e86f1b9d9": {"paper_id": "54205667c1f65a320f667d73c354ed8e86f1b9d9", "abstract": "A constrained optimization type of numerical algorithm for removing noise from images is presented. The total variation of the image is minimized subject to constraints involving the statistics of the noise. The constraints are imposed using Lagrange multipliers. The solution is obtained using the gradient-projection method. This amounts to solving a time dependent partial differential equation on a manifold determined by the constraints. As t \u2192 \u221e the solution converges to a steady state which is the denoised image. The numerical algorithm is simple and relatively fast. The results appear to be state-of-the-art for very noisy images. The method is noninvasive, yielding sharp edges in the image. The technique could be interpreted as a first step of moving each level set of the image normal to itself with velocity equal to the curvature of the level set divided by the magnitude of the gradient of the image, and a second step which projects the image back onto the constraint set.", "title": "Nonlinear total variation based noise removal algorithms"}, "5f34c96ddcf992e1b8660b5cb01e3c311b05023c": {"paper_id": "5f34c96ddcf992e1b8660b5cb01e3c311b05023c", "abstract": "Online iris recognition using distantly acquired images in a less imaging constrained environment requires the development of a efficient iris segmentation approach and recognition strategy that can exploit multiple features available for the potential identification. This paper presents an effective solution toward addressing such a problem. The developed iris segmentation approach exploits a random walker algorithm to efficiently estimate coarsely segmented iris images. These coarsely segmented iris images are postprocessed using a sequence of operations that can effectively improve the segmentation accuracy. The robustness of the proposed iris segmentation approach is ascertained by providing comparison with other state-of-the-art algorithms using publicly available UBIRIS.v2, FRGC, and CASIA.v4-distance databases. Our experimental results achieve improvement of 9.5%, 4.3%, and 25.7% in the average segmentation accuracy, respectively, for the UBIRIS.v2, FRGC, and CASIA.v4-distance databases, as compared with most competing approaches. We also exploit the simultaneously extracted periocular features to achieve significant performance improvement. The joint segmentation and combination strategy suggest promising results and achieve average improvement of 132.3%, 7.45%, and 17.5% in the recognition performance, respectively, from the UBIRIS.v2, FRGC, and CASIA.v4-distance databases, as compared with the related competing approaches.", "title": "Towards Online Iris and Periocular Recognition Under Relaxed Imaging Constraints"}, "1314e6ea34a8d749ca6190a0d2dd00b3a1879cc6": {"paper_id": "1314e6ea34a8d749ca6190a0d2dd00b3a1879cc6", "abstract": "A general nonparametric technique is proposed for the analysis of a complex multimodal feature space and to delineate arbitrarily shaped clusters in it. The basic computational module of the technique is an old pattern recognition procedure, the mean shift. We prove for discrete data the convergence of a recursive mean shift procedure to the nearest stationary point of the underlying density function and thus its utility in detecting the modes of the density. The equivalence of the mean shift procedure to the Nadaraya\u2013Watson estimator from kernel regression and the robust M-estimators of location is also established. Algorithms for two low-level vision tasks, discontinuity preserving smoothing and image segmentation are described as applications. In these algorithms the only user set parameter is the resolution of the analysis, and either gray level or color images are accepted as input. Extensive experimental results illustrate their excellent performance.", "title": "Mean Shift: A Robust Approach Toward Feature Space Analysis"}, "a3fcf3d32a5a4fcc83027e3d367ecc0df3ec4f64": {"paper_id": "a3fcf3d32a5a4fcc83027e3d367ecc0df3ec4f64", "abstract": "Iris recognition imaging constraints are receiving increasing attention. There are several proposals to develop systems that operate in the visible wavelength and in less constrained environments. These imaging conditions engender acquired noisy artifacts that lead to severely degraded images, making iris segmentation a major issue. Having observed that existing iris segmentation methods tend to fail in these challenging conditions, we present a segmentation method that can handle degraded images acquired in less constrained conditions. We offer the following contributions: 1) to consider the sclera the most easily distinguishable part of the eye in degraded images, 2) to propose a new type of feature that measures the proportion of sclera in each direction and is fundamental in segmenting the iris, and 3) to run the entire procedure in deterministically linear time in respect to the size of the image, making the procedure suitable for real-time applications.", "title": "Iris Recognition: On the Segmentation of Degraded Images Acquired in the Visible Wavelength"}, "90d6e7f2202f754d8588f9536e3f5b4a24701f24": {"paper_id": "90d6e7f2202f754d8588f9536e3f5b4a24701f24", "abstract": "We study the recognition of surfaces made from different materials such as concrete, rug, marble, or leather on the basis of their textural appearance. Such natural textures arise from spatial variation of two surface attributes: (1) reflectance and (2) surface normal. In this paper, we provide a unified model to address both these aspects of natural texture. The main idea is to construct a vocabulary of prototype tiny surface patches with associated local geometric and photometric properties. We call these 3D textons. Examples might be ridges, grooves, spots or stripes or combinations thereof. Associated with each texton is an appearance vector, which characterizes the local irradiance distribution, represented as a set of linear Gaussian derivative filter outputs, under different lighting and viewing conditions. Given a large collection of images of different materials, a clustering approach is used to acquire a small (on the order of 100) 3D texton vocabulary. Given a few (1 to 4) images of any material, it can be characterized using these textons. We demonstrate the application of this representation for recognition of the material viewed under novel lighting and viewing conditions. We also illustrate how the 3D texton model can be used to predict the appearance of materials under novel conditions.", "title": "Representing and Recognizing the Visual Appearance of Materials using Three-dimensional Textons"}, "c2b80648b2e274f28346b202eea9320ecd66c78d": {"paper_id": "c2b80648b2e274f28346b202eea9320ecd66c78d", "abstract": "Research on Natural Language Processing (NLP) in Indonesian is still limited and the results of available research that can be used for further research are also limited. In a series of natural language processing, the initial step is parsing the sentence in a particular language based on the grammar in order to help understanding the meaning of a sentence. This research aims to produce a simulation of Indonesian parser by adapting the process which was conducted by using Collins Algorithm. The three main stages are: 1) preprocessing to generate corpus and events files, 2) lexical analysis to convert the corpus into tokens, and 3) syntax analysis to build parse tree that requires file events to calculate the probability of the grammar by count the occurrence frequency on file events to determine the best sentence trees. An evaluation was performed to the parser using 30 simple sentences and the outcomes were able to generate a corpus file, file events, parse-tree and probability calculations. Nevertheless some sentences could not be parsed completely true because of the limitations of the Tree bank file in Indonesian. Some future works are to develop complete and valid Tree bank and Lexicon files.", "title": "A Study of Parsing Process on Natural Language Processing in Bahasa Indonesia"}, "76d5e3fa888bee872b7adb7fa810089aa8ab1d58": {"paper_id": "76d5e3fa888bee872b7adb7fa810089aa8ab1d58", "abstract": "We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trMned and tested on the previously established [5,9,10,15,17] \"standard\" sections of the Wall Street Journal treebank. This represents a 13% decrease in error rate over the best single-parser results on this corpus [9]. The major technical innovation is tire use of a \"ma~ximum-entropy-inspired\" model for conditioning and smoothing that let us successfully to test and combine many different conditioning events. We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head.", "title": "A Maximum-Entropy-Inspired Parser"}, "057cf8c16de54b160ef007e0d888865aeb592b7a": {"paper_id": "057cf8c16de54b160ef007e0d888865aeb592b7a", "abstract": "After presenting a novel O(n) parsing algorithm for dependency grammar, we develop three contrasting ways to stochasticize it. We propose (a) a lexical affinity model where words struggle to modify each other, (b) a sense tagging model where words fluctuate randomly in their selectional preferences, and (c) a generative model where the speaker fleshes out each word\u2019s syntactic and conceptual structure without regard to the implications for the hearer. We also give preliminary empirical results from evaluating the three models\u2019 parsing performance on annotated Wall Street Journal training text (derived from the Penn Treebank). In these results, the generative model performs significantly better than the others, and does about equally well at assigning partof-speech tags.", "title": "Three New Probabilistic Models for Dependency Parsing: An Exploration"}, "24beb987b722d4a25d3157a43000e685aa8f8874": {"paper_id": "24beb987b722d4a25d3157a43000e685aa8f8874", "abstract": "This paper presents a statistical model which trains from a corpus annotated with Part Of Speech tags and assigns them to previously unseen text with state of the art accuracy The model can be classi ed as a Maximum Entropy model and simultaneously uses many contextual features to predict the POS tag Furthermore this paper demonstrates the use of specialized fea tures to model di cult tagging decisions discusses the corpus consistency problems discovered during the implementation of these features and proposes a training strategy that mitigates these problems", "title": "A Maximum Entropy Model for Part-Of-Speech Tagging"}, "6b6fa87688f1e0ddb676a9ce5d18a7185f98d0c5": {"paper_id": "6b6fa87688f1e0ddb676a9ce5d18a7185f98d0c5", "abstract": "Traditional indoor laser scanning trolley/backpacks with multi-laser scanner, panorama cameras, and an inertial measurement unit (IMU) installed are a popular solution to the 3D indoor mapping problem. However, the cost of those mapping suits is quite expensive, and can hardly be replicated by consumer electronic components. The consumer RGB-Depth (RGB-D) camera (e.g., Kinect V2) is a low-cost option for gathering 3D point clouds. However, because of the narrow field of view (FOV), its collection efficiency and data coverages are lower than that of laser scanners. Additionally, the limited FOV leads to an increase of the scanning workload, data processing burden, and risk of visual odometry (VO)/simultaneous localization and mapping (SLAM) failure. To find an efficient and low-cost way to collect 3D point clouds data with auxiliary information (i.e., color) for indoor mapping, in this paper we present a prototype indoor mapping solution that is built upon the calibration of multiple RGB-D sensors to construct an array with large FOV. Three time-of-flight (ToF)-based Kinect V2 RGB-D cameras are mounted on a rig with different view directions in order to form a large field of view. The three RGB-D data streams are synchronized and gathered by the OpenKinect driver. The intrinsic calibration that involves the geometry and depth calibration of single RGB-D cameras are solved by homography-based method and ray correction followed by range biases correction based on pixel-wise spline line functions, respectively. The extrinsic calibration is achieved through a coarse-to-fine scheme that solves the initial exterior orientation parameters (EoPs) from sparse control markers and further refines the initial value by an iterative closest point (ICP) variant minimizing the distance between the RGB-D point clouds and the referenced laser point clouds. The effectiveness and accuracy of the proposed prototype and calibration method are evaluated by comparing the point clouds derived from the prototype with ground truth data collected by a terrestrial laser scanner (TLS). The overall analysis of the results shows that the proposed method achieves the seamless integration of multiple point clouds from three Kinect V2 cameras collected at 30 frames per second, resulting in low-cost, efficient, and high-coverage 3D color point cloud collection for indoor mapping applications.", "title": "Calibrate Multiple Consumer RGB-D Cameras for Low-Cost and Efficient 3D Indoor Mapping"}, "6c4fc0ca14a74dd75ba78d7d783b67c4c7c2b15b": {"paper_id": "6c4fc0ca14a74dd75ba78d7d783b67c4c7c2b15b", "abstract": "In this work we present an automatic algorithm to detect basic shapes in unorganized point clouds. The algorithm decomposes the point cloud into a concise, hybrid structure of inherent shapes and a set of remaining points. Each detected shape serves as a proxy for a set of corresponding points. Our method is based on random sampling and detects planes, spheres, cylinders, cones and tori. For models with surfaces composed of these basic shapes only, e.g. CAD models, we automatically obtain a representation solely consisting of shape proxies. We demonstrate that the algorithm is robust even in the presence of many outliers and a high degree of noise. The proposed method scales well with respect to the size of the input point cloud and the number and size of the shapes within the data. Even point sets with several millions of samples are robustly decomposed within less than a minute. Moreover the algorithm is conceptually simple and easy to implement. Application areas include measurement of physical parameters, scan registration, surface compression, hybrid rendering, shape classification, meshing, simplification, approximation and reverse engineering.", "title": "Efficient RANSAC for Point-Cloud Shape Detection"}, "02a808de5aa34685955fd1473433161edd20fd80": {"paper_id": "02a808de5aa34685955fd1473433161edd20fd80", "abstract": "Computer Graphics, 26, 2, July 1992 Surface Reconstruction from Unorganized Points Hugues Hoppe* Tony DeRose* Tom Duchampt John McDonald$ Werner Stuetzle~ University of Washington Seattle, WA 98195 We describe and demonstrate an algorithm that takes as input an unorganized set of points {xl, . . . . x.} c IR3 on or near an unknown manifold M, and produces as output a simplicial surface that approximates M. Neither the topology, the presence of boundaries, nor the geometry of M are assumed to be known in advance \u2014 all are inferred automatically from the data. This problem natu rally arises in a variety of practical situations such as range scanning an object from multiple view points, recovery of biological shapes from two-dimensional slices, and interactive surface sketching. CR", "title": "Surface reconstruction from unorganized points"}, "1751e6cc3b2b8ad03ae6c1cde82b2fd2fc94fca4": {"paper_id": "1751e6cc3b2b8ad03ae6c1cde82b2fd2fc94fca4", "abstract": "RGB-D cameras provide both color images and per-pixel depth stimates. The richness of this data and the recent development of low-c ost sensors have combined to present an attractive opportunity for mobile robot ics research. In this paper, we describe a system for visual odometry and mapping using an RGB-D camera, and its application to autonomous flight. By leveraging resu lts from recent stateof-the-art algorithms and hardware, our system enables 3D fl ight in cluttered environments using only onboard sensor data. All computation an d se sing required for local position control are performed onboard the vehicle, r educing the dependence on unreliable wireless links. However, even with accurate 3 D sensing and position estimation, some parts of the environment have more percept ual structure than others, leading to state estimates that vary in accuracy across the environment. If the vehicle plans a path without regard to how well it can localiz e itself along that path, it runs the risk of becoming lost or worse. We show how the Belief Roadmap (BRM) algorithm (Prentice and Roy, 2009), a belief space extensio n of the Probabilistic Roadmap algorithm, can be used to plan vehicle trajectories that incorporate the sensing model of the RGB-D camera. We evaluate the effective ness of our system for controlling a quadrotor micro air vehicle, demonstrate its use for constructing detailed 3D maps of an indoor environment, and discuss its li mitations. Abraham Bachrach and Samuel Prentice contributed equally to this work. Abraham Bachrach, Samuel Prentice, Ruijie He, Albert Huang a nd Nicholas Roy Computer Science and Artificial Intelligence Laboratory, Ma ss chusetts Institute of Technology, Cambridge, MA 02139. e-mail: abachrac, ruijie, albert, prentice, nickroy@mit.ed u Peter Henry, Michael Krainin and Dieter Fox University of Washington, Department of Computer Science & Engi neering, Seattle, WA. e-mail: peter, mkrainin, fox@cs.washington.edu. Daniel Maturana The Robotics Institute, Carnegie Mellon University, Pittsbur gh, PA. e-mail: dimatura@cmu.edu", "title": "Estimation, planning, and mapping for autonomous flight using an RGB-D camera in GPS-denied environments"}, "cee023137c9cf0fb8b89f073815512520bbb3a45": {"paper_id": "cee023137c9cf0fb8b89f073815512520bbb3a45", "abstract": "We present a real-time monocular vision based range measurement method for Simultaneous Localization and Mapping (SLAM) for an Autonomous Micro Aerial Vehicle (MAV) with significantly constrained payload. Our navigation strategy assumes a GPS denied manmade environment, whose indoor architecture is represented via corner based feature points obtained through a monocular camera. We experiment on a case study mission of vision based path-finding through a conventional maze of corridors in a large building.", "title": "Mono-vision corner SLAM for indoor navigation"}, "2097923990ad6d188bd9afc69a1f8abc0580076b": {"paper_id": "2097923990ad6d188bd9afc69a1f8abc0580076b", "abstract": "This paper proposes a novel algorithm for multiview stereopsis that outputs a dense set of small rectangular patches covering the surfaces visible in the images. Stereopsis is implemented as a match, expand, and filter procedure, starting from a sparse set of matched keypoints, and repeatedly expanding these before using visibility constraints to filter away false matches. The keys to the performance of the proposed algorithm are effective techniques for enforcing local photometric consistency and global visibility constraints. Simple but effective methods are also proposed to turn the resulting patch model into a mesh which can be further refined by an algorithm that enforces both photometric consistency and regularization constraints. The proposed approach automatically detects and discards outliers and obstacles and does not require any initialization in the form of a visual hull, a bounding box, or valid depth ranges. We have tested our algorithm on various data sets including objects with fine surface details, deep concavities, and thin structures, outdoor scenes observed from a restricted set of viewpoints, and \"crowded\" scenes where moving obstacles appear in front of a static structure of interest. A quantitative evaluation on the Middlebury benchmark [1] shows that the proposed method outperforms all others submitted so far for four out of the six data sets.", "title": "Accurate, Dense, and Robust Multiview Stereopsis"}, "849eaeeef1e11280bb7812239d34712b30023165": {"paper_id": "849eaeeef1e11280bb7812239d34712b30023165", "abstract": "RGB-D cameras are novel sensing systems that capture RGB images along with per-pixel depth information. RGB-D cameras rely on either structured light patterns combined with stereo sensing [6,10] or time-of-flight laser sensing [1] to generate depth estimates that can be associated with RGB pixels. Very soon, small, high-quality RGB-D cameras developed for computer gaming and home entertainment applications will become available at cost below $100. In this paper we investigate how such cameras can be used in the context of robotics, specifically for building dense 3D maps of indoor environments. Such maps have applications in robot navigation, manipulation, semantic mapping, and telepresence. The robotics and computer vision communities have developed a variety of techniques for 3D mapping based on laser range scans [8, 11], stereo cameras [7], monocular cameras [3], and unsorted collections of photos [4]. While RGB-D cameras provide the opportunity to build 3D maps of unprecedented richness, they have drawbacks that make their application to 3D mapping difficult: They provide depth only up to a limited distance (typically less than 5m), depth values are much noisier than those provided by laser scanners, and their field of view (\u223c 60\u25e6) is far more constrained than that of specialized cameras or laser scanners typically used for 3D mapping (\u223c 180\u25e6). In our work, we use a camera developed by PrimeSense [10]. The key insights of this investigation are: first, that existing frame matching techniques are not sufficient to provide robust visual odometry with these cameras; second, that a tight integration of depth and color information can yield robust frame matching and loop closure detection; third, that building on best practice techniques in SLAM and computer graphics makes it possible to build and visualize accurate and extremely rich 3D maps with such cameras; and, fourth, that it will be feasible to build complete robot navigation and interaction systems solely based on cheap depth cameras.", "title": "RGB-D Mapping: Using Depth Cameras for Dense 3D Modeling of Indoor Environments"}, "b391878646123f5490ef2e2103de09a0947e4dc9": {"paper_id": "b391878646123f5490ef2e2103de09a0947e4dc9", "abstract": "A recognition scheme that scales efficiently to a large number of objects is presented. The efficiency and quality is exhibited in a live demonstration that recognizes CD-covers from a database of 40000 images of popular music CD\u2019s. The scheme builds upon popular techniques of indexing descriptors extracted from local regions, and is robust to background clutter and occlusion. The local region descriptors are hierarchically quantized in a vocabulary tree. The vocabulary tree allows a larger and more discriminatory vocabulary to be used efficiently, which we show experimentally leads to a dramatic improvement in retrieval quality. The most significant property of the scheme is that the tree directly defines the quantization. The quantization and the indexing are therefore fully integrated, essentially being one and the same. The recognition quality is evaluated through retrieval on a database with ground truth, showing the power of the vocabulary tree approach, going as high as 1 million images.", "title": "Scalable Recognition with a Vocabulary Tree"}, "539a06ab025005ff2ad5d8435515faa058c73b07": {"paper_id": "539a06ab025005ff2ad5d8435515faa058c73b07", "abstract": "We present a new SLAM system capable of producing high quality globally consistent surface reconstructions over hundreds of metres in real-time with only a low-cost commodity RGB-D sensor. By using a fused volumetric surface reconstruction we achieve a much higher quality map over what would be achieved using raw RGB-D point clouds. In this paper we highlight three key techniques associated with applying a volumetric fusion-based mapping system to the SLAM problem in real-time. First, the use of a GPU-based 3D cyclical buffer trick to efficiently extend dense every frame volumetric fusion of depth maps to function over an unbounded spatial region. Second, overcoming camera pose estimation limitations in a wide variety of environments by combining both dense geometric and photometric camera pose constraints. Third, efficiently updating the dense map according to place recognition and subsequent loop closure constraints by the use of an \u201cas-rigid-as-possible\u201d space deformation. We present results on a wide variety of aspects of the system and show through evaluation on de facto standard RGB-D benchmarks that our system performs strongly in terms of trajectory estimation, map quality and computational performance in comparison to other state-of-the-art systems.", "title": "Real-time large-scale dense RGB-D SLAM with volumetric fusion"}, "5da3c40f2668ab4e44c71e267c559fca273e12ca": {"paper_id": "5da3c40f2668ab4e44c71e267c559fca273e12ca", "abstract": "This paper proposes an approach to real-time dense localisation and mapping that aims at unifying two different representations commonly used to define dense models. On one hand, much research has looked at 3D dense model representations using voxel grids in 3D. On the other hand, image-based key-frame representations for dense environment mapping have been developed. Both techniques have their relative advantages and disadvantages which will be analysed in this paper. In particular each representation's space-size requirements, their effective resolution, the computation efficiency, their accuracy and robustness will be compared. This paper then proposes a new model which unifies various concepts and exhibits the main advantages of each approach within a common framework. One of the main results of the proposed approach is its ability to perform large scale reconstruction accurately at the scale of mapping a building.", "title": "On unifying key-frame and voxel-based dense visual SLAM at large scales"}, "2319118c8c6a3a17458b5c1dd1fc4e0b9293f093": {"paper_id": "2319118c8c6a3a17458b5c1dd1fc4e0b9293f093", "abstract": "This paper describes extensions to the Kintinuous [1] algorithm for spatially extended KinectFusion, incorporating the following additions: (i) the integration of multiple 6DOF camera odometry estimation methods for robust tracking; (ii) a novel GPU-based implementation of an existing dense RGB-D visual odometry algorithm; (iii) advanced fused realtime surface coloring. These extensions are validated with extensive experimental results, both quantitative and qualitative, demonstrating the ability to build dense fully colored models of spatially extended environments for robotics and virtual reality applications while remaining robust against scenes with challenging sets of geometric and visual features.", "title": "Robust real-time visual odometry for dense RGB-D mapping"}, "f59c271d0c3655c36d0a823279d2c9da023ced13": {"paper_id": "f59c271d0c3655c36d0a823279d2c9da023ced13", "abstract": "Phase shifting is a widely used method for accurate and dense 3D reconstruction. However, at least three images of the same scene are required for each reconstruction, so measurement errors are inevitable in dynamic scenes, even with high-speed hardware. In this paper, we propose a Fourier transform assisted phase shifting method to overcome the motion vulnerability in phase shifting. A new model with motion-related phase shifts is formulated, and the coarse phase measurements obtained by Fourier transform profilemetry are used to estimate the unknown phase shifts. The phase errors caused by motion are greatly reduced in this way. Experimental results show that the proposed method can obtain accurate and dense 3D reconstruction of dynamic scenes, with regard to different kinds of motion.", "title": "Accurate 3D reconstruction of dynamic scenes with Fourier transform assisted phase shifting"}, "a218def8d229ebb88064ab2d01069d56561cdf05": {"paper_id": "a218def8d229ebb88064ab2d01069d56561cdf05", "abstract": "In this paper we describe a high-resolution, real-time 3D shape acquisition system based on structured light techniques. This system uses a color pattern whose RGB channels are coded with either sinusoidal or trapezoidal fringe patterns. When projected by a modified DLP projector (color filters removed), this color pattern results in three grayscale patterns projected sequentially at a frequency of 240 Hz. A high-speed B/W CCD camera synchronized with the projector captures the three images, from which the 3D shape of the object is reconstructed. A color CCD camera is also used to capture images for texture mapping. The maximum 3D shape acquisition speed is 120 Hz (532 \u00d7 500 pixels), which is high enough for capturing the 3D shapes of moving objects. Two coding methods, sinusoidal phase-shifting method and trapezoidal phase-shifting method, were tested and results with good accuracy were obtained. The trapezoidal phase-shifting algorithm also makes real-time 3D reconstruction possible.", "title": "High-Resolution, Real-time 3D Shape Acquisition"}, "5f8112731bd0a2bfa91158753242bae2f540d72d": {"paper_id": "5f8112731bd0a2bfa91158753242bae2f540d72d", "abstract": "This paper presents a color structured light technique for recovering object shape from one or more images. The technique works by projecting a pattern of stripes of alternating colors and matching the projected color transitions with observed edges in the image. The correspondence problem is solved using a novel, multi-pass dynamic programming algorithm that eliminates global smoothness assumptions and strict ordering constraints present in previous formulations. The resulting approach is suitable for generating both highspeed scans of moving objects when projecting a single stripe pattern and high-resolution scans of static scenes using a short sequence of time-shifted stripe patterns. In the latter case, spacetime analysis is used at each sensor pixel to obtain inter-frame depth localization. Results are demonstrated for a variety of complex scenes.", "title": "Rapid Shape Acquisition Using Color Structured Light and Multi-pass Dynamic Programming"}, "52f89a99c7e5af264a878dd05b62e94622c6bdac": {"paper_id": "52f89a99c7e5af264a878dd05b62e94622c6bdac", "abstract": "Coded structured light is considered one of the most reliable techniques for recovering the surface of objects. This technique is based on projecting a light pattern and viewing the illuminated scene from one or more points of view. Since the pattern is coded, correspondences between image points and points of the projected pattern can be easily found. The decoded points can be triangulated and 3D information is obtained. We present an overview of the existing techniques, as well as a new and de nitive classi cation of patterns for structured light sensors. We have implemented a set of representative techniques in this eld and present some comparative results. The advantages and constraints of the di5erent patterns are also discussed. ? 2003 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.", "title": "Pattern codification strategies in structured light systems"}, "0a4102694c46dbe389177663adc27b6e4aa98d85": {"paper_id": "0a4102694c46dbe389177663adc27b6e4aa98d85", "abstract": "In this paper, we study the quantification, practice, and implications of structural data (e.g., social data, mobility traces) De-Anonymization (DA). First, we address several open problems in structural data DA by quantifying perfect and (1-\u03b5)-perfect structural data DA}, where \u03b5 is the error tolerated by a DA scheme. To the best of our knowledge, this is the first work on quantifying structural data DA under a general data model, which closes the gap between structural data DA practice and theory. Second, we conduct the first large-scale study on the de-anonymizability of 26 real world structural datasets, including Social Networks (SNs), Collaborations Networks, Communication Networks, Autonomous Systems, and Peer-to-Peer networks. We also quantitatively show the conditions for perfect and (1-\u03b5)-perfect DA of the 26 datasets. Third, following our quantification, we design a practical and novel single-phase cold start Optimization based DA} (ODA) algorithm. Experimental analysis of ODA shows that about 77.7% - 83.3% of the users in Gowalla (.2M users and 1M edges) and 86.9% - 95.5% of the users in Google+ (4.7M users and 90.8M edges) are de-anonymizable in different scenarios, which implies optimization based DA is implementable and powerful in practice. Finally, we discuss the implications of our DA quantification and ODA and provide some general suggestions for future secure data publishing.", "title": "Structural Data De-anonymization: Quantification, Practice, and Implications"}, "046c6c8e15d9b9ecd73b5d2ce125db20bbcdec4b": {"paper_id": "046c6c8e15d9b9ecd73b5d2ce125db20bbcdec4b", "abstract": "Location-based services, which employ data from smartphones, vehicles, etc., are growing in popularity. To reduce the threat that shared location data poses to a user's privacy, some services anonymize or obfuscate this data. In this paper, we show these methods can be effectively defeated: a set of location traces can be deanonymized given an easily obtained social network graph. The key idea of our approach is that a user may be identified by those she meets: a \"contact graph\" identifying meetings between anonymized users in a set of traces can be structurally correlated with a social network graph, thereby identifying anonymized users. We demonstrate the effectiveness of our approach using three real world datasets: University of St Andrews mobility trace and social network (27 nodes each), SmallBlue contact trace and Facebook social network (125 nodes), and Infocom 2006 bluetooth contact traces and conference attendees' DBLP social network (78 nodes). Our experiments show that 80% of users are identified precisely, while only 8% are identified incorrectly, with the remainder mapped to a small set of users.", "title": "Deanonymizing mobility traces: using social network as a side-channel"}, "552626d9ab480d8b40052d35e16d516ffb1772ff": {"paper_id": "552626d9ab480d8b40052d35e16d516ffb1772ff", "abstract": "This is a literature survey of computational location privacy, meaning computation-based privacy mechanisms that treat location data as geometric information. This definition includes privacy-preserving algorithms like anonymity and obfuscation as well as privacy-breaking algorithms that exploit the geometric nature of the data. The survey omits non-computational techniques like manually inspecting geotagged photos, and it omits techniques like encryption or access control that treat location data as general symbols. The paper reviews studies of peoples\u2019 attitudes about location privacy, computational threats on leaked location data, and computational countermeasures for mitigating these threats.", "title": "A survey of computational location privacy"}, "568c44678d2bba4ae9d735b555e847437a7e6f15": {"paper_id": "568c44678d2bba4ae9d735b555e847437a7e6f15", "abstract": "We present Tor, a circuit-based low-latency anonymous communication service. This second-generation Onion Routing system addresses limitations in the original design. Tor adds perfect forward secrecy, congestion control, directory servers, integrity checking, configurable exit policies, and a practical design for rendezvous points. Tor works on the real-world Internet, requires no special privileges or kernel modifications, requires little synchronization or coordination between nodes, and provides a reasonable tradeoff between anonymity, usability, and efficiency. We briefly describe our experiences with an international network of more than a dozen hosts. We close with a list of open problems in anonymous communication.", "title": "Tor: The Second-Generation Onion Router"}, "6646683e340bdccfabddcaaa0a357be0990924d4": {"paper_id": "6646683e340bdccfabddcaaa0a357be0990924d4", "abstract": "With the widespread adoption of location tracking technologies like GPS, the domain of intelligent transportation services has seen growing interest in the last few years. Services in this domain make use of real-time location-based data from a variety of sources, combine this data with static location-based data such as maps and points of interest databases, and provide useful information to end-users. Some of the major challenges in this domain include i) scalability, in terms of processing large volumes of real-time and static data; ii) extensibility, in terms of being able to add new kinds of analyses on the data rapidly, and iii) user interaction, in terms of being able to support different kinds of one-time and continuous queries from the end-user. In this paper, we demonstrate the use of IBM InfoSphere Streams, a scalable stream processing platform, for tackling these challenges. We describe a prototype system that generates dynamic, multi-faceted views of transportation information for the city of Stockholm, using real vehicle GPS and road-network data. The system also continuously derives current traffic statistics, and provides useful value-added information such as shortest-time routes from real-time observed and inferred traffic conditions. Our performance experiments illustrate the scalability of the system. For instance, our system can process over 120000 incoming GPS points per second, combine it with a map containing over 600,000 links, continuously generate different kinds of traffic statistics and answer user queries.", "title": "IBM infosphere streams for scalable, real-time, intelligent transportation services"}, "713deb8c66e747375030ea67f137f5131718382b": {"paper_id": "713deb8c66e747375030ea67f137f5131718382b", "abstract": "We study centrality in urban street patterns of different world cities represented as networks in geographical space. The results indicate that a spatial analysis based on a set of four centrality indices allows an extended visualization and characterization of the city structure. A hierarchical clustering analysis based on the distributions of centrality has a certain capacity to distinguish different classes of cities. In particular, self-organized cities exhibit scale-free properties similar to those found in nonspatial networks, while planned cities do not.", "title": "Centrality measures in spatial networks of urban streets."}, "15cd71ac0333ade954201db6979abb39bde3d181": {"paper_id": "15cd71ac0333ade954201db6979abb39bde3d181", "abstract": "The mainstream approach to protecting the location-privacy of mobile users in location-based services (LBSs) is to alter the users' actual locations in order to reduce the location information exposed to the service provider. The location obfuscation algorithm behind an effective location-privacy preserving mechanism (LPPM) must consider three fundamental elements: the privacy requirements of the users, the adversary's knowledge and capabilities, and the maximal tolerated service quality degradation stemming from the obfuscation of true locations. We propose the first methodology, to the best of our knowledge, that enables a designer to find the optimal LPPM for a LBS given each user's service quality constraints against an adversary implementing the optimal inference algorithm. Such LPPM is the one that maximizes the expected distortion (error) that the optimal adversary incurs in reconstructing the actual location of a user, while fulfilling the user's service-quality requirement. We formalize the mutual optimization of user-adversary objectives (location privacy vs. correctness of localization) by using the framework of Stackelberg Bayesian games. In such setting, we develop two linear programs that output the best LPPM strategy and its corresponding optimal inference attack. Our optimal user-centric LPPM can be easily integrated in the users' mobile devices they use to access LBSs. We validate the efficacy of our game theoretic method against real location traces. Our evaluation confirms that the optimal LPPM strategy is superior to a straightforward obfuscation method, and that the optimal localization attack performs better compared to a Bayesian inference attack.", "title": "Protecting location privacy: optimal strategy against localization attacks"}, "2b61e632de33201a205fd4ee350a2b51e6c31af1": {"paper_id": "2b61e632de33201a205fd4ee350a2b51e6c31af1", "abstract": "The increasing trend of embedding positioning capabilities (for example, GPS) in mobile devices facilitates the widespread use of location-based services. For such applications to succeed, privacy and confidentiality are essential. Existing privacy-enhancing techniques rely on encryption to safeguard communication channels, and on pseudonyms to protect user identities. Nevertheless, the query contents may disclose the physical location of the user. In this paper, we present a framework for preventing location-based identity inference of users who issue spatial queries to location-based services. We propose transformations based on the well-established K-anonymity concept to compute exact answers for range and nearest neighbor search, without revealing the query source. Our methods optimize the entire process of anonymizing the requests and processing the transformed spatial queries. Extensive experimental studies suggest that the proposed techniques are applicable to real-life scenarios with numerous mobile users.", "title": "Preventing Location-Based Identity Inference in Anonymous Spatial Queries"}, "22ef4514bcdf001fd7b9edd8c517922b79ac34d6": {"paper_id": "22ef4514bcdf001fd7b9edd8c517922b79ac34d6", "abstract": "The amount of contextual data collected, stored, mined, and shared is increasing exponentially. Street cameras, credit card transactions, chat and Twitter logs, e-mail, web site visits, phone logs and recordings, social networking sites, all are examples of data that persists in a manner not under individual control, leading some to declare the death of privacy. We argue here that the ability to generate convincing fake contextual data can be a basic tool in the fight to preserve privacy. One use for the technology is for an individual to make his actual data indistinguishable amongst a pile of false data.\n In this paper we consider two examples of contextual data, search engine query data and location data. We describe the current state of faking these types of data and our own efforts in this direction.", "title": "Faking contextual data for fun, profit, and privacy"}, "50dad1b5f35c0ba613fd79fae91d7270c64cea0f": {"paper_id": "50dad1b5f35c0ba613fd79fae91d7270c64cea0f", "abstract": "When it comes to software analysis, several approaches exist from heuristic techniques to formal methods, which are helpful at solving different kinds ofproblems. Unfortunately very few initiative seek to aggregate this techniques in the same platform. BINSEC intend to fulfill this lack of binary analysis platform by allowing to perform modular analysis. This work focusses on BINSEC/SE, the new dynamic symbolic execution engine (DSE) implemented in BINSEC. We will highlight the novelties of the engine, especially in terms of interactions between concrete and symbolic execution or optimization of formula generation. Finally, two reverse engineering applications are shown in order to emphasize the tool effectiveness.", "title": "BINSEC/SE: A Dynamic Symbolic Execution Toolkit for Binary-Level Analysis"}, "480b73f8a55a868374176375f45918d6f0570715": {"paper_id": "480b73f8a55a868374176375f45918d6f0570715", "abstract": "Dynamic taint analysis and forward symbolic execution are quickly becoming staple techniques in security analyses. Example applications of dynamic taint analysis and forward symbolic execution include malware analysis, input filter generation, test case generation, and vulnerability discovery. Despite the widespread usage of these two techniques, there has been little effort to formally define the algorithms and summarize the critical issues that arise when these techniques are used in typical security contexts. The contributions of this paper are two-fold. First, we precisely describe the algorithms for dynamic taint analysis and forward symbolic execution as extensions to the run-time semantics of a general language. Second, we highlight important implementation choices, common pitfalls, and considerations when using these techniques in a security context.", "title": "All You Ever Wanted to Know about Dynamic Taint Analysis and Forward Symbolic Execution (but Might Have Been Afraid to Ask)"}, "0d1d0900cf862f11d3d7812c01d28be27c71a6c7": {"paper_id": "0d1d0900cf862f11d3d7812c01d28be27c71a6c7", "abstract": "Dynamic test generation is a form of dynamic program analysis that attempts to compute test inputs to drive a program along a specific program path. Directed Automated Random Testing, or DART for short, blends dynamic test generation with model checking techniques with the goal of systematically executing all feasible program paths of a program while detecting various types of errors using run-time checking tools (like Purify, for instance). Unfortunately, systematically executing all feasible program paths does not scale to large, realistic programs.This paper addresses this major limitation and proposes to perform dynamic test generation compositionally, by adapting known techniques for interprocedural static analysis. Specifically, we introduce a new algorithm, dubbed SMART for Systematic Modular Automated Random Testing, that extends DART by testing functions in isolation, encoding test results as function summaries expressed using input preconditions and output postconditions, and then re-using those summaries when testing higher-level functions. We show that, for a fixed reasoning capability, our compositional approach to dynamic test generation (SMART) is both sound and complete compared to monolithic dynamic test generation (DART). In other words, SMART can perform dynamic test generation compositionally without any reduction in program path coverage. We also show that, given a bound on the maximum number of feasible paths in individual program functions, the number of program executions explored by SMART is linear in that bound, while the number of program executions explored by DART can be exponential in that bound. We present examples of C programs and preliminary experimental results that illustrate and validate empirically these properties.", "title": "Compositional dynamic test generation"}, "05ca17ffa777f64991a8da04f2fd03880ac51236": {"paper_id": "05ca17ffa777f64991a8da04f2fd03880ac51236", "abstract": "In this paper we explore the problem of creating vulnerability signatures. A vulnerability signature matches all exploits of a given vulnerability, even polymorphic or metamorphic variants. Our work departs from previous approaches by focusing on the semantics of the program and vulnerability exercised by a sample exploit instead of the semantics or syntax of the exploit itself. We show the semantics of a vulnerability define a language which contains all and only those inputs that exploit the vulnerability. A vulnerability signature is a representation (e.g., a regular expression) of the vulnerability language. Unlike exploit-based signatures whose error rate can only be empirically measured for known test cases, the quality of a vulnerability signature can be formally quantified for all possible inputs. We provide a formal definition of a vulnerability signature and investigate the computational complexity of creating and matching vulnerability signatures. We also systematically explore the design space of vulnerability signatures. We identify three central issues in vulnerability-signature creation: how a vulnerability signature represents the set of inputs that may exercise a vulnerability, the vulnerability coverage (i.e., number of vulnerable program paths) that is subject to our analysis during signature creation, and how a vulnerability signature is then created for a given representation and coverage. We propose new data-flow analysis and novel adoption of existing techniques such as constraint solving for automatically generating vulnerability signatures. We have built a prototype system to test our techniques. Our experiments show that we can automatically generate a vulnerability signature using a single exploit which is of much higher quality than previous exploit-based signatures. In addition, our techniques have several other security applications, and thus may be of independent interest", "title": "Towards automatic generation of vulnerability-based signatures"}, "092b09f0ec09b2b10763f5697ca77099a37ab022": {"paper_id": "092b09f0ec09b2b10763f5697ca77099a37ab022", "abstract": "We present a simple architectural mechanism called dynamic information flow tracking that can significantly improve the security of computing systems with negligible performance overhead. Dynamic information flow tracking protects programs against malicious software attacks by identifying spurious information flows from untrusted I/O and restricting the usage of the spurious information.Every security attack to take control of a program needs to transfer the program's control to malevolent code. In our approach, the operating system identifies a set of input channels as spurious, and the processor tracks all information flows from those inputs. A broad range of attacks are effectively defeated by checking the use of the spurious values as instructions and pointers.Our protection is transparent to users or application programmers; the executables can be used without any modification. Also, our scheme only incurs, on average, a memory overhead of 1.4% and a performance overhead of 1.1%.", "title": "Secure program execution via dynamic information flow tracking"}, "5578045657a90d2db6ac86bb4afbe38c035fc6a5": {"paper_id": "5578045657a90d2db6ac86bb4afbe38c035fc6a5", "abstract": "Dynamic taint analysis is gaining momentum. Techniques based on dynamic tainting have been successfully used in the context of application security, and now their use is also being explored in different areas, such as program understanding, software testing, and debugging. Unfortunately, most existing approaches for dynamic tainting are defined in an ad-hoc manner, which makes it difficult to extend them, experiment with them, and adapt them to new contexts. Moreover, most existing approaches are focused on data-flow based tainting only and do not consider tainting due to control flow, which limits their applicability outside the security domain. To address these limitations and foster experimentation with dynamic tainting techniques, we defined and developed a general framework for dynamic tainting that (1) is highly flexible and customizable, (2) allows for performing both data-flow and control-flow based tainting conservatively, and (3) does not rely on any customized run-time system. We also present DYTAN, an implementation of our framework that works on x86 executables, and a set of preliminary studies that show how DYTAN can be used to implement different tainting-based approaches with limited effort. In the studies, we also show that DYTAN can be used on real software, by using FIREFOX as one of our subjects, and illustrate how the specific characteristics of the tainting approach used can affect efficiency and accuracy of the taint analysis, which further justifies the use of our framework to experiment with different variants of an approach.", "title": "Dytan: a generic dynamic taint analysis framework"}, "7fa71e17142563013365daa8526a1323f123961a": {"paper_id": "7fa71e17142563013365daa8526a1323f123961a", "abstract": "BAP is a publicly available infrastructure for performing program verification and analysis tasks on binary (i.e., executable) code. In this paper, we describe BAP as well as lessons learned from previous incarnations of binary analysis platforms. BAP explicitly represents all side effects of instructions in an intermediate language (IL), making syntaxdirected analysis possible. We have used BAP to routinely generate and solve verification conditions that are hundreds of megabytes in size and encompass 100,000\u2019s of assembly instructions.", "title": "BAP: A Binary Analysis Platform"}, "2c21f9488edfb2586327528bb59461a41363fc42": {"paper_id": "2c21f9488edfb2586327528bb59461a41363fc42", "abstract": "This paper presents S2E, a platform for analyzing the properties and behavior of software systems. We demonstrate S2E's use in developing practical tools for comprehensive performance profiling, reverse engineering of proprietary software, and bug finding for both kernel-mode and user-mode binaries. Building these tools on top of S2E took less than 770 LOC and 40 person-hours each.\n S2E's novelty consists of its ability to scale to large real systems, such as a full Windows stack. S2E is based on two new ideas: selective symbolic execution, a way to automatically minimize the amount of code that has to be executed symbolically given a target analysis, and relaxed execution consistency models, a way to make principled performance/accuracy trade-offs in complex analyses. These techniques give S2E three key abilities: to simultaneously analyze entire families of execution paths, instead of just one execution at a time; to perform the analyses in-vivo within a real software stack--user programs, libraries, kernel, drivers, etc.--instead of using abstract models of these layers; and to operate directly on binaries, thus being able to analyze even proprietary software.\n Conceptually, S2E is an automated path explorer with modular path analyzers: the explorer drives the target system down all execution paths of interest, while analyzers check properties of each such path (e.g., to look for bugs) or simply collect information (e.g., count page faults). Desired paths can be specified in multiple ways, and S2E users can either combine existing analyzers to build a custom analysis tool, or write new analyzers using the S2E API.", "title": "S2E: a platform for in-vivo multi-path analysis of software systems"}, "9090142233801801411a28b30c653aae5408182a": {"paper_id": "9090142233801801411a28b30c653aae5408182a", "abstract": "Bugs in kernel-level device drivers cause 85% of the system crashes in the Windows XP operating system [44]. One of the sources of these errors is the complexity of the Windows driver API itself: programmers must master a complex set of rules about how to use the driver API in order to create drivers that are good clients of the kernel. We have built a static analysis engine that finds API usage errors in C programs. The Static Driver Verifier tool (SDV) uses this engine to find kernel API usage errors in a driver. SDV includes models of the OS and the environment of the device driver, and over sixty API usage rules. SDV is intended to be used by driver developers \"out of the box.\" Thus, it has stringent requirements: (1) complete automation with no input from the user; (2) a low rate of false errors. We discuss the techniques used in SDV to meet these requirements, and empirical results from running SDV on over one hundred Windows device drivers.", "title": "Thorough static analysis of device drivers"}, "0ab393affe9d674ef790be14fdfade368f3e5989": {"paper_id": "0ab393affe9d674ef790be14fdfade368f3e5989", "abstract": "We present a new tool, named DART, for automatically testing software that combines three main techniques: (1) automated extraction of the interface of a program with its external environment using static source-code parsing; (2) automatic generation of a test driver for this interface that performs random testing to simulate the most general environment the program can operate in; and (3) dynamic analysis of how the program behaves under random testing and automatic generation of new test inputs to direct systematically the execution along alternative program paths. Together, these three techniques constitute Directed Automated Random Testing, or DART for short. The main strength of DART is thus that testing can be performed completely automatically on any program that compiles -- there is no need to write any test driver or harness code. During testing, DART detects standard errors such as program crashes, assertion violations, and non-termination. Preliminary experiments to unit test several examples of C programs are very encouraging.", "title": "DART: directed automated random testing"}, "2960c89331eb7afa86584792e2e11dbf6a125820": {"paper_id": "2960c89331eb7afa86584792e2e11dbf6a125820", "abstract": "We present the internals of QEMU, a fast machine emulator using an original portable dynamic translator. It emulates several CPUs (x86, PowerPC, ARM and Sparc) on several hosts (x86, PowerPC, ARM, Sparc, Alpha and MIPS). QEMU supports full system emulation in which a complete and unmodified operating system is run in a virtual machine and Linux user mode emulation where a Linux process compiled for one target CPU can be run on another CPU.", "title": "QEMU, a Fast and Portable Dynamic Translator"}, "dcb54653a7f348c16f110a9f5de7533fa3476495": {"paper_id": "dcb54653a7f348c16f110a9f5de7533fa3476495", "abstract": "PinOS is an extension of the Pin dynamic instrumentation framework for whole-system instrumentation, i.e., to instrument both kernel and user-level code. It achieves this by interposing between the subject system and hardware using virtualization techniques. Specifically, PinOS is built on top of the Xen virtual machine monitor with Intel VT technology to allow instrumentation of unmodified OSes. PinOS is based on software dynamic translation and hence can perform pervasive fine-grain instrumentation. By inheriting the powerful instrumentation API from Pin, plus introducing some new API for system-level instrumentation, PinOS can be used to write system-wide instrumentation tools for tasks like program analysis and architectural studies. As of today, PinOS can boot Linux on IA-32 in uniprocessor mode, and can instrument complex applications such as database and web servers.", "title": "PinOS: a programmable framework for whole-system dynamic instrumentation"}, "11443efe465ad544f478524da6c66c085b16e28b": {"paper_id": "11443efe465ad544f478524da6c66c085b16e28b", "abstract": "The challenges---and great promise---of modern symbolic execution techniques, and the tools to help implement them.", "title": "Symbolic execution for software testing: three decades later"}, "0653e2ed9f683868cb4539eb8718551242834f6b": {"paper_id": "0653e2ed9f683868cb4539eb8718551242834f6b", "abstract": "Robust and powerful software instrumentation tools are essential for program analysis tasks such as profiling, performance evaluation, and bug detection. To meet this need, we have developed a new instrumentation system called Pin. Our goals are to provide easy-to-use, portable, transparent, and efficient instrumentation. Instrumentation tools (called Pintools) are written in C/C++ using Pin's rich API. Pin follows the model of ATOM, allowing the tool writer to analyze an application at the instruction level without the need for detailed knowledge of the underlying instruction set. The API is designed to be architecture independent whenever possible, making Pintools source compatible across different architectures. However, a Pintool can access architecture-specific details when necessary. Instrumentation with Pin is mostly transparent as the application and Pintool observe the application's original, uninstrumented behavior. Pin uses dynamic compilation to instrument executables while they are running. For efficiency, Pin uses several techniques, including inlining, register re-allocation, liveness analysis, and instruction scheduling to optimize instrumentation. This fully automated approach delivers significantly better instrumentation performance than similar tools. For example, Pin is 3.3x faster than Valgrind and 2x faster than DynamoRIO for basic-block counting. To illustrate Pin's versatility, we describe two Pintools in daily use to analyze production software. Pin is publicly available for Linux platforms on four architectures: IA32 (32-bit x86), EM64T (64-bit x86), Itanium\u00ae, and ARM. In the ten months since Pin 2 was released in July 2004, there have been over 3000 downloads from its website.", "title": "Pin: building customized program analysis tools with dynamic instrumentation"}, "3916407cd711828f206ff378d01b1ae526a6ee84": {"paper_id": "3916407cd711828f206ff378d01b1ae526a6ee84", "abstract": "We describe a new algorithm for fast global register allocation called linear scan. This algorithm is not based on graph coloring, but allocates registers to variables in a single linear-time scan of the variables' live ranges. The linear scan algorithm is considerably faster than algorithms based on graph coloring, is simple to implement, and results in code that is almost as efficient as that obtained using more complex and time-consuming register allocators based on graph coloring. The algorithm is of interest in applications where compile time is a concern, such as dynamic compilation systems, \u201cjust-in-time\u201d compilers, and interactive development environments.", "title": "Linear scan register allocation"}, "376b28d33bd444f178fd75cd185611c923572df9": {"paper_id": "376b28d33bd444f178fd75cd185611c923572df9", "abstract": "EEL (Executable Editing Library) is a library for building tools to analyze and modify an executable (compiled) program. The systems and languages communities have built many tools for error detection, fault isolation, architecture translation, performance measurement, simulation, and optimization using this approach of modifying executables. Currently, however, tools of this sort are difficult and time-consuming to write and are usually closely tied to a particular machine and operating system. EEL supports a machine- and system-independent editing model that enables tool builders to modify an executable without being aware of the details of the underlying architecture or operating system or being concerned with the consequences of deleting instructions or adding foreign code.", "title": "EEL: Machine-Independent Executable Editing"}, "d6473e64e0b4c6a499b96619fe21b50b61688ccc": {"paper_id": "d6473e64e0b4c6a499b96619fe21b50b61688ccc", "abstract": "In this contribution, various sales forecast models for the German automobile market are developed and tested. Our most important criteria for the assessment of these models are the quality of the prediction as well as an easy explicability. Yearly, quarterly and monthly data for newly registered automobiles from 1992 to 2007 serve as the basis for the tests of these models. The time series model used consists of additive components: trend, seasonal, calendar and error component. The three latter components are estimated univariately while the trend component is estimated multivariately by Multiple Linear Regression as well as by a Support Vector Machine. Possible influences which are considered include macro-economic and market-specific factors. These influences are analysed by a feature selection. We found the non-linear model to be superior. Furthermore, the quarterly data provided the most accurate results.", "title": "A Sales Forecast Model for the German Automobile Market Based on Time Series Analysis and Data Mining Methods"}, "004888621a4e4cee56b6633338a89aa036cf5ae5": {"paper_id": "004888621a4e4cee56b6633338a89aa036cf5ae5", "abstract": "In the feature subset selection problem, a learning algorithm is faced with the problem of selecting a relevant subset of features upon which to focus its attention, while ignoring the rest. To achieve the best possible performance with a particular learning algorithm on a particular training set, a feature subset selection method should consider how the algorithm and the training set interact. We explore the relation between optimal feature subset selection and relevance. Our wrapper method searches for an optimal feature subset tailored to a particular algorithm and a domain. We study the strengths and weaknesses of the wrapper approach and show a series of improved designs. We compare the wrapper approach to induction without feature subset selection and to Relief, a filter approach to feature subset selection. Significant improvement in accuracy is achieved for some datasets for the two families of induction algorithms used: decision trees and Naive-Bayes. @ 1997 Elsevier Science B.V.", "title": "Wrappers for Feature Subset Selection"}, "8213dbed4db44e113af3ed17d6dad57471a0c048": {"paper_id": "8213dbed4db44e113af3ed17d6dad57471a0c048", "abstract": null, "title": "The Nature of Statistical Learning Theory"}, "8da1dda34ecc96263102181448c94ec7d645d085": {"paper_id": "8da1dda34ecc96263102181448c94ec7d645d085", "abstract": "Abstr,,ct. In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set ofaffine functionals can uniformly approximate any continuous function of n real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single bidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.", "title": "Approximation by superpositions of a sigmoidal function"}, "b25c668707b98db90f2ea1d8939f9c35c0c7ace3": {"paper_id": "b25c668707b98db90f2ea1d8939f9c35c0c7ace3", "abstract": "It is known that the usual criteria resulting from such, should we dare to say, ad hoc principles as the least squares, the minimum prediction error, and the maximum likelihood principles, cannot be used to estimate the order nor other structure parameters. This very fact seems to suggest that these time-honored principles are successful only insofar as they happen to coincide with one or another facet of some more powerful and comprehensive principle. The shortest description length of individual recursively definable objects was studied by Kolmogorov[1] and others, and it gives rise to the algorithmic notion of entropy. In statistical estimation, however, the data always includes a random element, and the original notion of entropy of random variables turns out to be the appropriate one to use. In a peripheral way we also invoke a simple combinatorial entropy notion which, however, need not cause any con-", "title": "Modeling by shortest data description"}, "05357314fe2da7c2248b03d89b7ab9e358cbf01e": {"paper_id": "05357314fe2da7c2248b03d89b7ab9e358cbf01e", "abstract": "All rights reserved. No part of this book may be reproduced in any form by any electronic or mechanical means (including photocopying, recording, or information storage and retrieval) without permission in writing from the publisher.", "title": "Learning with kernels"}, "14199130a67364b64f461b60017c79ec67987024": {"paper_id": "14199130a67364b64f461b60017c79ec67987024", "abstract": "Comparative analysis of molecular sequence data is essential for reconstructing the evolutionary histories of species and inferring the nature and extent of selective forces shaping the evolution of genes and species. Here, we announce the release of Molecular Evolutionary Genetics Analysis version 5 (MEGA5), which is a user-friendly software for mining online databases, building sequence alignments and phylogenetic trees, and using methods of evolutionary bioinformatics in basic biology, biomedicine, and evolution. The newest addition in MEGA5 is a collection of maximum likelihood (ML) analyses for inferring evolutionary trees, selecting best-fit substitution models (nucleotide or amino acid), inferring ancestral states and sequences (along with probabilities), and estimating evolutionary rates site-by-site. In computer simulation analyses, ML tree inference algorithms in MEGA5 compared favorably with other software packages in terms of computational efficiency and the accuracy of the estimates of phylogenetic trees, substitution parameters, and rate variation among sites. The MEGA user interface has now been enhanced to be activity driven to make it easier for the use of both beginners and experienced scientists. This version of MEGA is intended for the Windows platform, and it has been configured for effective use on Mac OS X and Linux desktops. It is available free of charge from http://www.megasoftware.net.", "title": "MEGA5: molecular evolutionary genetics analysis using maximum likelihood, evolutionary distance, and maximum parsimony methods."}, "b8171ba7ab7cbd3dfe336b958151e0a2cfcd8c4c": {"paper_id": "b8171ba7ab7cbd3dfe336b958151e0a2cfcd8c4c", "abstract": "UNLABELLED\nRAxML-VI-HPC (randomized axelerated maximum likelihood for high performance computing) is a sequential and parallel program for inference of large phylogenies with maximum likelihood (ML). Low-level technical optimizations, a modification of the search algorithm, and the use of the GTR+CAT approximation as replacement for GTR+Gamma yield a program that is between 2.7 and 52 times faster than the previous version of RAxML. A large-scale performance comparison with GARLI, PHYML, IQPNNI and MrBayes on real data containing 1000 up to 6722 taxa shows that RAxML requires at least 5.6 times less main memory and yields better trees in similar times than the best competing program (GARLI) on datasets up to 2500 taxa. On datasets > or =4000 taxa it also runs 2-3 times faster than GARLI. RAxML has been parallelized with MPI to conduct parallel multiple bootstraps and inferences on distinct starting trees. The program has been used to compute ML trees on two of the largest alignments to date containing 25,057 (1463 bp) and 2182 (51,089 bp) taxa, respectively.\n\n\nAVAILABILITY\nicwww.epfl.ch/~stamatak", "title": "RAxML-VI-HPC: maximum likelihood-based phylogenetic analyses with thousands of taxa and mixed models"}, "6f604b37408da4fddcb15f56af376c672be87da8": {"paper_id": "6f604b37408da4fddcb15f56af376c672be87da8", "abstract": "Current efforts to reconstruct the tree of life and histories of multigene families demand the inference of phylogenies consisting of thousands of gene sequences. However, for such large data sets even a moderate exploration of the tree space needed to identify the optimal tree is virtually impossible. For these cases the neighbor-joining (NJ) method is frequently used because of its demonstrated accuracy for smaller data sets and its computational speed. As data sets grow, however, the fraction of the tree space examined by the NJ algorithm becomes minuscule. Here, we report the results of our computer simulation for examining the accuracy of NJ trees for inferring very large phylogenies. First we present a likelihood method for the simultaneous estimation of all pairwise distances by using biologically realistic models of nucleotide substitution. Use of this method corrects up to 60% of NJ tree errors. Our simulation results show that the accuracy of NJ trees decline only by approximately 5% when the number of sequences used increases from 32 to 4,096 (128 times) even in the presence of extensive variation in the evolutionary rate among lineages or significant biases in the nucleotide composition and transition/transversion ratio. Our results encourage the use of complex models of nucleotide substitution for estimating evolutionary distances and hint at bright prospects for the application of the NJ and related methods in inferring large phylogenies.", "title": "Prospects for inferring very large phylogenies by using the neighbor-joining method."}, "917465b084541381d08761b7536cf65d5c94d496": {"paper_id": "917465b084541381d08761b7536cf65d5c94d496", "abstract": "Connectivity has an important role in neural networks, computer network, and clustering. In the design of a network, it is important to analyze connections by the levels. The structural properties of intuitionistic fuzzy graphs provide a tool that allows for the solution of operations research problems. In this paper, we introduce various types of intuitionistic fuzzy bridges, intuitionistic fuzzy cut vertices, intuitionistic fuzzy cycles, and intuitionistic fuzzy trees in intuitionistic fuzzy graphs and investigate some of their interesting properties. Most of these various types are defined in terms of levels. We also describe comparison of these types.", "title": "Intuitionistic Fuzzy Cycles and Intuitionistic Fuzzy Trees"}, "d3c86e4f26b379a091cb915c73480d8db5d8fd9e": {"paper_id": "d3c86e4f26b379a091cb915c73480d8db5d8fd9e", "abstract": "We introduce the notion of strong intuitionistic fuzzy graphs and investigate some of their properties. We discuss some propositions of self complementary and self weak complementary strong intuitionistic fuzzy graphs. We introduce the concept of intuitionistic fuzzy line graphs.", "title": "Strong intuitionistic fuzzy graphs"}, "75a775ff51f2b23596d4e1caa61c88ff427dc5b3": {"paper_id": "75a775ff51f2b23596d4e1caa61c88ff427dc5b3", "abstract": "Article history: Received 27 October 2011 Received in revised form 26 February 2012 Accepted 23 June 2012 Available online 6 July 2012", "title": "Intuitionistic fuzzy hypergraphs with applications"}, "a89b8484e3b4e710de1811cc6a54e60a50802fc8": {"paper_id": "a89b8484e3b4e710de1811cc6a54e60a50802fc8", "abstract": "Article history: Received 31 December 2007 Received in revised form 12 December 2008 Accepted 3 January 2009", "title": "Types of arcs in a fuzzy graph"}, "4b1a3ccd3219d0d4f4918584307d49bb9fa89199": {"paper_id": "4b1a3ccd3219d0d4f4918584307d49bb9fa89199", "abstract": "We introduce a new type of Augmented Reality games: By using a simple webcam and Computer Vision techniques, we turn a standard real game board pawns into an AR game. We use these objects as a tangible interface, and augment them with visual effects. The game logic can be performed automatically by the computer. This results in a better immersion compared to the original board game alone and provides a different experience than a video game. We demonstrate our approach on Monopoly\u2212 [1], but it is very generic and could easily be adapted to any other board game.", "title": "Augmented reality for board games"}, "b3356b85c4173d253ddfaf2bcf3c96b5efc4b134": {"paper_id": "b3356b85c4173d253ddfaf2bcf3c96b5efc4b134", "abstract": "This demo shows BattleBoard 3D which is an Augmented Reality (AR) based game prototype featuring the use of LEGO for the physical and digital pieces. Design concepts, the physical setting and, user interface for the game is illustrated and described. Based on qualitative studies of children playing the game we illustrate design issues for AR board games.", "title": "Designing an augmented reality board game with children: the battleboard 3D experience"}, "5b1b03749b6ec4d6140e519555f0d77441cd35af": {"paper_id": "5b1b03749b6ec4d6140e519555f0d77441cd35af", "abstract": "This paper surveys the field of augmented reality (AR), in which 3D virtual objects are integrated into a 3D real environment in real time. It describes the medical, manufacturing, visualization, path planning, entertainment, and military applications that have been explored. This paper describes the characteristics of augmented reality systems, including a detailed discussion of the tradeoffs between optical and video blending approaches. Registration and sensing errors are two of the biggest problems in building effective augmented reality systems, so this paper summarizes current efforts to overcome these problems. Future directions and areas requiring further research are discussed. This survey provides a starting point for anyone interested in researching or using augmented reality.", "title": "A Survey of Augmented Reality"}, "1d758774416b44ad828ebf4ab35e23da14a273e8": {"paper_id": "1d758774416b44ad828ebf4ab35e23da14a273e8", "abstract": "or more than a decade researchers have tried to create intuitive computer interfaces by blending reality and virtual reality. The goal is for people to interact with the digital domain as easily as with the real world. Various approaches help us achieve this\u2014in the area of tangible interfaces, we use real objects as interface widgets; in augmented reality, researchers overlay 3D virtual imagery onto the real world; and in VR interfaces , we entirely replace the real world with a computer generated environment. As Milgram pointed out, 1 these types of computer interfaces can be placed along a continuum according to how much of the user's environment is computer generated (Figure 1). Tangible interfaces lie far to the left on this reality\u2013virtuality line, while immersive virtual environments are at the right extremity. Most current user interfaces exist as discrete points along this continuum. However, human activity can't always be broken into discrete components and for many tasks users may prefer to move seamlessly along the reality\u2013virtuality continuum. This proves true when interacting with 3D graphical content, either creating virtual models or viewing them. For example, if people want to experience a virtual scene from different scales, then immer-sive virtual reality may be ideal. If they want to have a face-to-face discussion while viewing the virtual scene, an augmented reality interface may be best. 2 The MagicBook project is an early attempt to explore how we can use a physical object to smoothly transport users between reality and virtuality. Young children often fantasize about flying into the pages of a fairy tale and becoming part of the story. The MagicBook project makes this fantasy a reality using a normal book as the main interface object. People can turn the pages of the book, look at the pictures, and read the text without any additional technology (Figure 2a). However, if a person looks at the pages through an augmented reality display, they see 3D virtual models appearing out of the pages (Figure 2b). The models appear attached to the real page so users can see the augmented reality scene from any perspective by moving themselves or the book. The virtual content can be any size and is animated, so the augmented reality view is an enhanced version of a traditional 3D pop-up book. Users can change the virtual models by turning the book pages. When they see a scene they particularly like, \u2026", "title": "The MagicBook - Moving Seamlessly between Reality and Virtuality"}, "ee27785c0e7f1ddacee7edd8bd47abbda0347403": {"paper_id": "ee27785c0e7f1ddacee7edd8bd47abbda0347403", "abstract": "Augmented reality (AR) makes it possible to create games in which virtual objects are overlaid on the real world, and real objects are tracked and used to control virtual ones. We describe the development of an AR racing game created by modifying an existing racing game, using an AR infrastructure that we developed for use with the XNA game development platform. In our game, the driver wears a tracked video see-through head-worn display, and controls the car with a passive tangible controller. Other players can participate by manipulating waypoints that the car must pass and obstacles with which the car can collide. We discuss our AR infrastructure, which supports the creation of AR applications and games in a managed code environment, the user interface we developed for the AR racing game, the game's software and hardware architecture, and feedback and observations from early demonstrations.", "title": "Developing an augmented reality racing game"}, "05bed977601c84ae581a9a8b2054ce484b342e10": {"paper_id": "05bed977601c84ae581a9a8b2054ce484b342e10", "abstract": "Augmented Reality (AR) can naturally complement mobile computing on wearable devices by providing an intuitive interface to a three-dimensional information space embedded within physical reality. Unfortunately, current wearable AR systems are relatively complex, expensive and heavyweight, rendering them unfit for large-scale deployment to untrained users outside a constrained laboratory environment. Consequently, collaborative multi-user experiments have been prevented from exceeding just a hand full of participants. In this paper, we present a software architecture for interactive, infrastructure-independent multiuser AR applications on off-the-shelf handheld devices. We implemented a four-user interactive game installation as an evaluation scenario that would encourage participants to playfully engage in a cooperative task. Over the course of four weeks, more than five thousand visitors from a wide range of professional and socio-demographic backgrounds interacted with our system at a total of four different locations and events. The findings from an informal summative assessment of user performance were generally positive, and will hopefully advance our long-term effort to deploy massively multi-user AR applications to the general public.", "title": "Towards Massively Multi-user Augmented Reality on Handheld Devices"}, "10c57079dbdf23b960583b50334e11466fc1e202": {"paper_id": "10c57079dbdf23b960583b50334e11466fc1e202", "abstract": "CSCW; in this setting computers can provide the same type of collaborative information that people have in face-to-face interactions, such as communication by object manipulation, voice and gesture [1]. Work on the DIVE project [2], GreenSpace [3] and other fully immersive multi-participant virtual environments has shown that collaborative work is indeed intuitive in such surroundings. However most current multi-user VR systems are fully immersive, separating the user from the real world and their traditional tools. As Grudin [4] points out, CSCW tools are generally rejected when they force users to change the way they work. This is because of the introduction of seams or discontinuities between the way people usually work and the way they are forced to work because of the computer interface. Ishii describes in detail the advantages of seamless CSCW interfaces [5]. Obviously immersive VR interfaces introduce a huge discontinuity between the real and virtual worlds. An alternative approach is through Augmented Reality (AR), the overlaying of virtual objects onto the real world. In the past researchers have explored the use of AR approaches to support face-to-face collaboration. Projects such as Studierstube [6], Transvision [7], and AR2 Hockey [8] allow users can see each other as well as 3D virtual objects in the space between them. Users can interact with the real world at the same time as the virtual images, bringing the benefits of VR interfaces into the real world and facilitating very natural collaboration. In a previous paper we found that this meant that users collaborate better on a task in a face-to-face AR setting than for the same task in a Abstract We describe an augmented reality conferencing system which uses the overlay of virtual images on the real world. Remote collaborators are represented on Virtual Monitors which can be freely positioned about a user in space. Users can collaboratively view and interact with virtual objects using a shared virtual whiteboard. This is possible through precise virtual image registration using fast and accurate computer vision techniques and HMD calibration. We propose a method for tracking fiducial markers and a calibration method for optical see-through HMD based on the marker tracking.", "title": "Marker tracking and HMD calibration for a video-based augmented reality conferencing system"}, "23194a2d227bed2cb6db12875ae508834e646586": {"paper_id": "23194a2d227bed2cb6db12875ae508834e646586", "abstract": "MonkeyBridge is a collaborative Augmented Reality (AR) game employing autonomous animated agents embodied by lifelike, animated virtual characters and \"smart\" physical objects. The game serves as a pilot application to examine how \"smart\" software and hardware components capable of observing and reacting to events in the physical and virtual world can be useful in AR applications. We describe the implementation details of our test setups as well as how autonomous agents offer a rich gaming experience in AR games.", "title": "MonkeyBridge: autonomous agents in augmented reality games"}, "363b56f85e12389017ba8894056a1b309e46a5f7": {"paper_id": "363b56f85e12389017ba8894056a1b309e46a5f7", "abstract": "We propose an approach to include contextual features for labeling images, in which each pixel is assigned to one of a finite set of labels. The features are incorporated into a probabilistic framework, which combines the outputs of several components. Components differ in the information they encode. Some focus on the image-label mapping, while others focus solely on patterns within the label field. Components also differ in their scale, as some focus on fine-resolution patterns while others on coarser, more global structure. A supervised version of the contrastive divergence algorithm is applied to learn these features from labeled image data. We demonstrate performance on two real-world image databases and compare it to a classifier and a Markov random field.", "title": "Multiscale conditional random fields for image labeling"}, "958ebc7e24012b419316489515f2c2f908773bd5": {"paper_id": "958ebc7e24012b419316489515f2c2f908773bd5", "abstract": "In this paper we present two techniques for natural feature tracking in real-time on mobile phones. We achieve interactive frame rates of up to 20 Hz for natural feature tracking from textured planar targets on current-generation phones. We use an approach based on heavily modified state-of-the-art feature descriptors, namely SIFT and Ferns. While SIFT is known to be a strong, but computationally expensive feature descriptor, Ferns classification is fast, but requires large amounts of memory. This renders both original designs unsuitable for mobile phones. We give detailed descriptions on how we modified both approaches to make them suitable for mobile phones. We present evaluations on robustness and performance on various devices and finally discuss their appropriateness for augmented reality applications.", "title": "Pose tracking from natural features on mobile phones"}, "7c20a16d1666e990e6499995556578c1649d191b": {"paper_id": "7c20a16d1666e990e6499995556578c1649d191b", "abstract": "This paper presents a robust and flexible framework for augmented reality which does not require instrumenting either the environment or the workpiece. A model-based visual tracking system is combined with with rate gyroscopes to produce a system which can track the rapid camera rotations generated by a head-mounted camera, even if images are substantially degraded by motion blur. This tracking yields estimates of head position at video field rate (50Hz) which are used to align computer-generated graphics on an optical see-through display. Nonlinear optimisation is used for the calibration of display parameters which include a model of optical distortion. Rendered visuals are pre-distorted to correct the optical distortion of the display.", "title": "Robust Visual Tracking for Non-Instrumented Augmented Reality"}, "779f05bf98049762df4298043ac6f38c82a07607": {"paper_id": "779f05bf98049762df4298043ac6f38c82a07607", "abstract": "The idea described in this paper is to use the built-in cameras of consumer mobile phones as sensors for 2-dimensional visual codes. Such codes can be attached to physical objects in order to retrieve object-related information and functionality. They are also suitable for display on electronic screens. The proposed visual code system allows the simultaneous detection of multiple codes, introduces a position-independent coordinate system, and provides the phone\u2019s orientation as a parameter. The ability to detect objects in the user\u2019s vicinity offers a natural way of interaction and strengthens the role of mobile phones in a large number of application scenarios. We describe the hardware requirements, the design of a suitable visual code, a lightweight recognition algorithm, and present some example applications.", "title": "USING CAMERA-EQUIPPED MOBILE PHONES FOR INTERACTING WITH REAL-WORLD OBJECTS"}, "5137516a2604bb0828e59dc517b252d489409986": {"paper_id": "5137516a2604bb0828e59dc517b252d489409986", "abstract": "While feature point recognition is a key component of modern approaches to object detection, existing approaches require computationally expensive patch preprocessing to handle perspective distortion. In this paper, we show that formulating the problem in a Naive Bayesian classification framework makes such preprocessing unnecessary and produces an algorithm that is simple, efficient, and robust. Furthermore, it scales well to handle large number of classes. To recognize the patches surrounding keypoints, our classifier uses hundreds of simple binary features and models class posterior probabilities. We make the problem computationally tractable by assuming independence between arbitrary sets of features. Even though this is not strictly true, we demonstrate that our classifier nevertheless performs remarkably well on image datasets containing very significant perspective changes.", "title": "Fast Keypoint Recognition in Ten Lines of Code"}, "69524d5b10b0bb7e4aa1c9057eefe197b230922e": {"paper_id": "69524d5b10b0bb7e4aa1c9057eefe197b230922e", "abstract": "Mobile phones are an ideal platform for augmented reality. In this paper we describe how they can also be used to support face to face collaborative AR gaming. We have created a custom port of the ARToolKit library to the Symbian mobile phone operating system and then developed a sample collaborative AR game based on this. We describe the game in detail and user feedback from people who have played the game. We also provide general design guidelines that could be useful for others who are developing mobile phone collaborative AR applications.", "title": "Face to Face Collaborative AR on Mobile Phones"}, "10cfa5bfab3da9c8026d3a358695ea2a5eba0f33": {"paper_id": "10cfa5bfab3da9c8026d3a358695ea2a5eba0f33", "abstract": "This paper presents a method of estimating camera pose in an unknown scene. While this has previously been attempted by adapting SLAM algorithms developed for robotic exploration, we propose a system specifically designed to track a hand-held camera in a small AR workspace. We propose to split tracking and mapping into two separate tasks, processed in parallel threads on a dual-core computer: one thread deals with the task of robustly tracking erratic hand-held motion, while the other produces a 3D map of point features from previously observed video frames. This allows the use of computationally expensive batch optimisation techniques not usually associated with real-time operation: The result is a system that produces detailed maps with thousands of landmarks which can be tracked at frame-rate, with an accuracy and robustness rivalling that of state-of-the-art model-based systems.", "title": "Parallel Tracking and Mapping for Small AR Workspaces"}, "12a376e621d690f3e94bce14cd03c2798a626a38": {"paper_id": "12a376e621d690f3e94bce14cd03c2798a626a38", "abstract": "This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The first is the introduction of a new image representation called the \u201cIntegral Image\u201d which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features from a larger set and yields extremely efficient classifiers[6]. The third contribution is a method for combining increasingly more complex classifiers in a \u201ccascade\u201d which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection.", "title": "Rapid Object Detection using a Boosted Cascade of Simple Features"}, "76f560991d56ad689ec32f9e9d13291e0193f4cf": {"paper_id": "76f560991d56ad689ec32f9e9d13291e0193f4cf", "abstract": "This paper presents a general trainable framework for object detection in static images of cluttered scenes. The detection technique we develop is based on a wavelet representation of an object class derived from a statistical analysis of the class instances. By learning an object class in terms of a subset of an overcomplete dictionary of wavelet basis functions, we derive a compact representation of an object class which is used as an input to a suppori vector machine classifier. This representation overcomes both the problem of in-class variability and provides a low false detection rate in unconstrained environments. We demonstrate the capabilities of the technique i n two domains whose inherent information content differs significantly. The first system is face detection and the second is the domain of people which, in contrast to faces, vary greatly in color, texture, and patterns. Unlike previous approaches, this system learns from examples and does not rely on any a priori (handcrafted) models or motion-based segmentation. The paper also presents a motion-based extension to enhance the performance of the detection algorithm over video sequences. The results presented here suggest that this architecture may well be quite general.", "title": "A General Framework for Object Detection"}, "78be48eb6d91c46e1284feab7f8ac60d664268c9": {"paper_id": "78be48eb6d91c46e1284feab7f8ac60d664268c9", "abstract": "In this paper, we describe a statistical method for 3D object detection. We represent the statistics of both object appearance and \" non-object \" appearance using a product of histo-grams. Each histogram represents the joint statistics of a subset of wavelet coefficients and their position on the object. Our approach is to use many such histograms representing a wide variety of visual attributes. Using this method, we have developed the first algorithm that can reliably detect human faces with out-of-plane rotation and the first algorithm that can reliably detect passenger cars over a wide range of viewpoints.", "title": "A Statistical Method for 3D Object Detection Applied to Faces and Cars"}, "0015fa48e4ab633985df789920ef1e0c75d4b7a8": {"paper_id": "0015fa48e4ab633985df789920ef1e0c75d4b7a8", "abstract": "Detection (To appear in the Proceedings of CVPR'97, June 17-19, 1997, Puerto Rico.) Edgar Osunay? Robert Freund? Federico Girosiy yCenter for Biological and Computational Learning and ?Operations Research Center Massachusetts Institute of Technology Cambridge, MA, 02139, U.S.A. Abstract We investigate the application of Support Vector Machines (SVMs) in computer vision. SVM is a learning technique developed by V. Vapnik and his team (AT&T Bell Labs.) that can be seen as a new method for training polynomial, neural network, or Radial Basis Functions classi ers. The decision surfaces are found by solving a linearly constrained quadratic programming problem. This optimization problem is challenging because the quadratic form is completely dense and the memory requirements grow with the square of the number of data points. We present a decomposition algorithm that guarantees global optimality, and can be used to train SVM's over very large data sets. The main idea behind the decomposition is the iterative solution of sub-problems and the evaluation of optimality conditions which are used both to generate improved iterative values, and also establish the stopping criteria for the algorithm. We present experimental results of our implementation of SVM, and demonstrate the feasibility of our approach on a face detection problem that involves a data set of 50,000 data points.", "title": "Training Support Vector Machines: an Application to Face Detection"}, "04a20cd0199d0a24fea8e6bf0e0cc61b26c1f3ac": {"paper_id": "04a20cd0199d0a24fea8e6bf0e0cc61b26c1f3ac", "abstract": "One of the surprising recurring phenomena observed in experiments with boosting is that the test error of the generated classifier usually does not increase as its size becomes very large, and often is observed to decrease even after the training error reaches zero. In this paper, we show that this phenomenon is related to the distribution of margins of the training examples with respect to the generated voting classification rule, where the margin of an example is simply the difference between the number of correct votes and the maximum number of votes received by any incorrect label. We show that techniques used in the analysis of Vapnik\u2019s support vector classifiers and of neural networks with small weights can be applied to voting methods to relate the margin distribution to the test error. We also show theoretically and experimentally that boosting is especially effective at increasing the margins of the training examples. Finally, we compare our explanation to those based on the bias-variance decomposition.", "title": "Boosting the margin: A new explanation for the effectiveness of voting methods"}, "138f8fc3e05509eb9d43d0446fcff21a73cf06ae": {"paper_id": "138f8fc3e05509eb9d43d0446fcff21a73cf06ae", "abstract": "This course will provide an introduction to statistical pattern recognition. The lectures will focus on different techniques including methods for feature extraction, dimensionality reduction, data clustering and pattern classification. State-of-art approaches such as ensemble learning and sparse modelling will be introduced. Selected real-world applications will illustrate how the techniques are applied in practice.", "title": "Statistical Pattern Recognition"}, "b202abc592df34737955cfc65543e48ca32d8da3": {"paper_id": "b202abc592df34737955cfc65543e48ca32d8da3", "abstract": "For Industry 4.0 technology as well as Cyber-Physical Production Systems analysis of data gained more and more importance. But the disparity of data, often make the efficient use of data mining methods difficult due to data with poor quality. To evaluate data quality and further adopt appropriate measures, the proposal develops a data quality model fitted to the specific properties of signal data of industrial processes. Relevant data quality characteristics are identified and a classification of these characteristics is conducted to ascertain important factors. Furthermore, a measurement for the characteristic Completeness, aggregated of its sub-dimensions, is defined. The data quality model is applied to two different use cases showing its effectiveness and validity of the defined measures. The efficient use of real industrial signal data e.g. appropriateness of the data for the specific data mining purpose, is supported by a comprehensive measurement for data quality and the detailed discussion of the influencing factors.", "title": "Metrics for the evaluation of data quality of signal data in industrial processes"}, "7dbfe6c89dd7d39ecf5b9aedf192ade98716b778": {"paper_id": "7dbfe6c89dd7d39ecf5b9aedf192ade98716b778", "abstract": "How good is a company's data quality? Answering this question requires usable data quality metrics. Currently, most data quality measures are developed on an ad hoc basis to solve specific problems [6, 8], and fundamental principles necessary for developing usable metrics in practice are lacking. In this article, we describe principles that can help organizations develop usable data quality metrics.", "title": "Data quality assessment"}, "d1b5a611f4a0519d3b46e012afa362f2bafb5ab2": {"paper_id": "d1b5a611f4a0519d3b46e012afa362f2bafb5ab2", "abstract": "of an organization. A leading computer industry information service firm indicated that it \u201cexpects most business process reengineering initiatives to fail through lack of attention to data quality.\u201d An industry executive report noted that more than 60% of surveyed firms (500 medium-size corporations with annual sales of more than $20 million) had problems with data quality. The Wall Street Journal also reported that, \u201cThanks to computers, huge databases brimming with information are at our fingertips, just waiting to be tapped. They can be mined to find sales Anchoring Data Quality Dimensions Ontological Foundations", "title": "Anchoring Data Quality Dimensions in Ontological Foundations"}, "1f8b930d3a19f8b2ed37808d9e5c2344fad1942e": {"paper_id": "1f8b930d3a19f8b2ed37808d9e5c2344fad1942e", "abstract": "Information quality (IQ) is an inexact science in terms of assessment and benchmarks. Although various aspects of quality and information have been investigated [1, 4, 6, 7, 9, 12], there is still a critical need for a methodology that assesses how well organizations develop information products and deliver information services to consumers. Benchmarks developed from such a methodology can help compare information quality across organizations, and provide a baseline for assessing IQ improvements.", "title": "Information quality benchmarks: product and service performance"}, "4ba375450cf7bbe4f0941abcb9fc0dac10b8217b": {"paper_id": "4ba375450cf7bbe4f0941abcb9fc0dac10b8217b", "abstract": "We examine the problem of authorship attribution in collaborative documents. We seek to develop new deep learning models tailored to this task. We have curated a novel dataset by parsing Wikipedia\u2019s edit history, which we use to demonstrate the feasiblity of deep models to multi-author attribution at the sentence-level. Though we attempt to formulate models which learn stylometric features based on both grammatical structure and vocabulary, our error analysis suggests that our models mostly learn to recognize vocabulary-based cues, making them non-competitive with baselines tailored to vocabulary-based features. We explore why this may be, and suggest directions for future models to mitigate this shortcoming.", "title": "Deep Sentence-Level Authorship Attribution"}, "4e88de2930a4435f737c3996287a90ff87b95c59": {"paper_id": "4e88de2930a4435f737c3996287a90ff87b95c59", "abstract": "Because of their superior ability to preserve sequence information over time, Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. TreeLSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank).", "title": "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks"}, "1510cf4b8abea80b9f352325ca4c132887de21a0": {"paper_id": "1510cf4b8abea80b9f352325ca4c132887de21a0", "abstract": "Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, \u201cpowerful,\u201d \u201cstrong\u201d and \u201cParis\u201d are equally distant. In this paper, we proposeParagraph Vector , an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-ofwords models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.", "title": "Distributed Representations of Sentences and Documents"}, "11ec56898a9e7f401a2affe776b5297bd4e25025": {"paper_id": "11ec56898a9e7f401a2affe776b5297bd4e25025", "abstract": "This paper presents the task on the evaluation of Compositional Distributional Semantics Models on full sentences organized for the first time within SemEval2014. Participation was open to systems based on any approach. Systems were presented with pairs of sentences and were evaluated on their ability to predict human judgments on (i) semantic relatedness and (ii) entailment. The task attracted 21 teams, most of which participated in both subtasks. We received 17 submissions in the relatedness subtask (for a total of 66 runs) and 18 in the entailment subtask (65 runs).", "title": "SemEval-2014 Task 1: Evaluation of Compositional Distributional Semantic Models on Full Sentences through Semantic Relatedness and Textual Entailment"}, "142f38642629b9d268999ad876af482177d36697": {"paper_id": "142f38642629b9d268999ad876af482177d36697", "abstract": "Unsupervised word representations are very useful in NLP tasks both as inputs to learning algorithms and as extra word features in NLP systems. However, most of these models are built with only local context and one representation per word. This is problematic because words are often polysemous and global context can also provide useful information for learning word meanings. We present a new neural network architecture which 1) learns word embeddings that better capture the semantics of words by incorporating both local and global document context, and 2) accounts for homonymy and polysemy by learning multiple embeddings per word. We introduce a new dataset with human judgments on pairs of words in sentential context, and evaluate our model on it, showing that our model outperforms competitive baselines and other neural language models. 1", "title": "Improving Word Representations via Global Context and Multiple Word Prototypes"}, "54c32d432fb624152da7736543f2685840860a57": {"paper_id": "54c32d432fb624152da7736543f2685840860a57", "abstract": "We introduce a type of Deep Boltzmann Machine (DBM) that is suitable for extracting distributed semantic representations from a large unstructured collection of documents. We overcome the apparent difficulty of training a DBM with judicious parameter tying. This enables an efficient pretraining algorithm and a state initialization scheme for fast inference. The model can be trained just as efficiently as a standard Restricted Boltzmann Machine. Our experiments show that the model assigns better log probability to unseen data than the Replicated Softmax model. Features extracted from our model outperform LDA, Replicated Softmax, and DocNADE models on document retrieval and document classification tasks.", "title": "Modeling Documents with Deep Boltzmann Machines"}, "4c15b129a8da55127e4e2fe47f54799d0a313367": {"paper_id": "4c15b129a8da55127e4e2fe47f54799d0a313367", "abstract": "If we take an existing supervised NLP system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih & Hinton, 2009) embeddings of words on both NER and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines. We find further improvements by combining different word representations. You can download our word features, for off-the-shelf use in existing NLP systems, as well as our code, here: http://metaoptimize. com/projects/wordreprs/", "title": "Word Representations: A Simple and General Method for Semi-Supervised Learning"}, "49fa97db6b7f3ab2b3a623c3552aa680b80c8dd2": {"paper_id": "49fa97db6b7f3ab2b3a623c3552aa680b80c8dd2", "abstract": "The problem of automatically determining the gender of a document's author would appear to be a more subtle problem than those of categorization by topic or authorship attribution. Nevertheless, it is shown that automated text categorization techniques can exploit combinations of simple lexical and syntactic features to infer the gender of the author of an unseen formal written document with approximately 80% accuracy. The same techniques can be used to determine if a document is fiction or non-fiction with approximately 98% accuracy.", "title": "Automatically Categorizing Written Texts by Author Gender"}, "4781b899447abc3439eb785281aa754126f1d818": {"paper_id": "4781b899447abc3439eb785281aa754126f1d818", "abstract": "This paper is a comparative study of feature selection methods in statistical learning of text categorization The focus is on aggres sive dimensionality reduction Five meth ods were evaluated including term selection based on document frequency DF informa tion gain IG mutual information MI a test CHI and term strength TS We found IG and CHI most e ective in our ex periments Using IG thresholding with a k nearest neighbor classi er on the Reuters cor pus removal of up to removal of unique terms actually yielded an improved classi cation accuracy measured by average preci sion DF thresholding performed similarly Indeed we found strong correlations between the DF IG and CHI values of a term This suggests that DF thresholding the simplest method with the lowest cost in computation can be reliably used instead of IG or CHI when the computation of these measures are too expensive TS compares favorably with the other methods with up to vocabulary reduction but is not competitive at higher vo cabulary reduction levels In contrast MI had relatively poor performance due to its bias towards favoring rare terms and its sen sitivity to probability estimation errors", "title": "A Comparative Study on Feature Selection in Text Categorization"}, "a4fa9754b555f9c2c2d1e10aecfb3153aea46bf6": {"paper_id": "a4fa9754b555f9c2c2d1e10aecfb3153aea46bf6", "abstract": "Analyzing videos is one of the fundamental problems of computer vision and multimedia content analysis for decades. The task is very challenging as video is an information-intensive media with large variations and complexities. Thanks to the recent development of deep learning techniques, researchers in both computer vision and multimedia communities are now able to boost the performance of video analysis significantly and initiate new research directions to analyze video content. This tutorial will present recent advances under the umbrella of video understanding, which start from a unified deep learning toolkit--Microsoft Cognitive Toolkit (CNTK) that supports popular model types such as convolutional nets and recurrent networks, to fundamental challenges of video representation learning and video classification, recognition, and finally to an emerging area of video and language.", "title": "Deep Learning for Intelligent Video Analysis"}, "2c258eec8e4da9e65018f116b237f7e2e0b2ad17": {"paper_id": "2c258eec8e4da9e65018f116b237f7e2e0b2ad17", "abstract": "Deep convolutional neural networks (CNNs) have proven highly effective for visual recognition, where learning a universal representation from activations of convolutional layer plays a fundamental problem. In this paper, we present Fisher Vector encoding with Variational Auto-Encoder (FV-VAE), a novel deep architecture that quantizes the local activations of convolutional layer in a deep generative model, by training them in an end-to-end manner. To incorporate FV encoding strategy into deep generative models, we introduce Variational Auto-Encoder model, which steers a variational inference and learning in a neural network which can be straightforwardly optimized using standard stochastic gradient method. Different from the FV characterized by conventional generative models (e.g., Gaussian Mixture Model) which parsimoniously fit a discrete mixture model to data distribution, the proposed FV-VAE is more flexible to represent the natural property of data for better generalization. Extensive experiments are conducted on three public datasets, i.e., UCF101, ActivityNet, and CUB-200-2011 in the context of video action recognition and fine-grained image classification, respectively. Superior results are reported when compared to state-of-the-art representations. Most remarkably, our proposed FV-VAE achieves to-date the best published accuracy of 94.2% on UCF101.", "title": "Deep Quantization: Encoding Convolutional Activations with Deep Generative Model"}, "06cad81a163e345828c0804f42252177049dd1bc": {"paper_id": "06cad81a163e345828c0804f42252177049dd1bc", "abstract": "Real-world videos often have complex dynamics, methods for generating open-domain video descriptions should be sensitive to temporal structure and allow both input (sequence of frames) and output (sequence of words) of variable length. To approach this problem we propose a novel end-to-end sequence-to-sequence model to generate captions for videos. For this we exploit recurrent neural networks, specifically LSTMs, which have demonstrated state-of-the-art performance in image caption generation. Our LSTM model is trained on video-sentence pairs and learns to associate a sequence of video frames to a sequence of words in order to generate a description of the event in the video clip. Our model naturally is able to learn the temporal structure of the sequence of frames as well as the sequence model of the generated sentences, i.e. a language model. We evaluate several variants of our model that exploit different visual features on a standard set of YouTube videos and two movie description datasets (M-VAD and MPII-MD).", "title": "Sequence to Sequence -- Video to Text"}, "0d8b85953c25c23c512d4522d5597c8e3e0bb8c7": {"paper_id": "0d8b85953c25c23c512d4522d5597c8e3e0bb8c7", "abstract": "Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.", "title": "Spatial Transformer Networks"}, "df01d2dede1a243b9b0eb26c27246bc13705d930": {"paper_id": "df01d2dede1a243b9b0eb26c27246bc13705d930", "abstract": "Generative probability models such as hidden ~larkov models provide a principled way of treating missing information and dealing with variable length sequences. On the other hand , discriminative methods such as support vector machines enable us to construct flexible decision boundaries and often result in classification performance superior to that of the model based approaches. An ideal classifier should combine these two complementary approaches. In this paper, we develop a natural way of achieving this combination by deriving kernel functions for use in discriminative methods such as support vector machines from generative probability models. We provide a theoretical justification for this combination as well as demonstrate a substantial improvement in the classification performance in the context of D~A and protein sequence analysis.", "title": "Exploiting Generative Models in Discriminative Classifiers"}, "0fbb184871bd7660bc579178848d58beb8288b7d": {"paper_id": "0fbb184871bd7660bc579178848d58beb8288b7d", "abstract": "We address the problem of image search on a very large scale, where three constraints have to be considered jointly: the accuracy of the search, its efficiency, and the memory usage of the representation. We first propose a simple yet efficient way of aggregating local image descriptors into a vector of limited dimension, which can be viewed as a simplification of the Fisher kernel representation. We then show how to jointly optimize the dimension reduction and the indexing algorithm, so that it best preserves the quality of vector comparison. The evaluation shows that our approach significantly outperforms the state of the art: the search accuracy is comparable to the bag-of-features approach for an image representation that fits in 20 bytes. Searching a 10 million image dataset takes about 50ms.", "title": "Aggregating local descriptors into a compact image representation"}, "1128a4f57148cec96c0ef4ae3b5a0fbf07efbad9": {"paper_id": "1128a4f57148cec96c0ef4ae3b5a0fbf07efbad9", "abstract": "Recognizing actions in videos is a challenging task as video is an information-intensive media with complex variations. Most existing methods have treated video as a flat data sequence while ignoring the intrinsic hierarchical structure of the video content. In particular, an action may span different granularities in this hierarchy including, from small to large, a single frame, consecutive frames (motion), a short clip, and the entire video. In this paper, we present a novel framework to boost action recognition by learning a deep spatio-temporal video representation at hierarchical multi-granularity. Specifically, we model each granularity as a single stream by 2D (for frame and motion streams) or 3D (for clip and video streams) convolutional neural networks (CNNs). The framework therefore consists of multi-stream 2D or 3D CNNs to learn both the spatial and temporal representations. Furthermore, we employ the Long Short-Term Memory (LSTM) networks on the frame, motion, and clip streams to exploit long-term temporal dynamics. With a softmax layer on the top of each stream, the classification scores can be predicted from all the streams, followed by a novel fusion scheme based on the multi-granular score distribution. Our networks are learned in an end-to-end fashion. On two video action benchmarks of UCF101 and HMDB51, our framework achieves promising performance compared with the state-of-the-art.", "title": "Action Recognition by Learning Deep Multi-Granular Spatio-Temporal Video Representation"}, "768cb0e32de3f1b5aebe04448aaec4c25586680c": {"paper_id": "768cb0e32de3f1b5aebe04448aaec4c25586680c", "abstract": "Automatically describing an image with a natural language has been an emerging challenge in both fields of computer vision and natural language processing. In this paper, we present Long Short-Term Memory with Attributes (LSTM-A) - a novel architecture that integrates attributes into the successful Convolutional Neural Networks (CNNs) plus Recurrent Neural Networks (RNNs) image captioning framework, by training them in an end-to-end manner. Particularly, the learning of attributes is strengthened by integrating inter-attribute correlations into Multiple Instance Learning (MIL). To incorporate attributes into captioning, we construct variants of architectures by feeding image representations and attributes into RNNs in different ways to explore the mutual but also fuzzy relationship between them. Extensive experiments are conducted on COCO image captioning dataset and our framework shows clear improvements when compared to state-of-the-art deep models. More remarkably, we obtain METEOR/CIDEr-D of 25.5%/100.2% on testing data of widely used and publicly available splits in [10] when extracting image representations by GoogleNet and achieve superior performance on COCO captioning Leaderboard.", "title": "Boosting Image Captioning with Attributes"}, "3d275a4e4f44d452f21e0e0ff6145a5e18e6cf87": {"paper_id": "3d275a4e4f44d452f21e0e0ff6145a5e18e6cf87", "abstract": "Automatically describing an image with a sentence is a long-standing challenge in computer vision and natural language processing. Due to recent progress in object detection, attribute classification, action recognition, etc., there is renewed interest in this area. However, evaluating the quality of descriptions has proven to be challenging. We propose a novel paradigm for evaluating image descriptions that uses human consensus. This paradigm consists of three main parts: a new triplet-based method of collecting human annotations to measure consensus, a new automated metric that captures consensus, and two new datasets: PASCAL-50S and ABSTRACT-50S that contain 50 sentences describing each image. Our simple metric captures human judgment of consensus better than existing metrics across sentences generated by various sources. We also evaluate five state-of-the-art image description approaches using this new protocol and provide a benchmark for future comparisons. A version of CIDEr named CIDEr-D is available as a part of MS COCO evaluation server to enable systematic evaluation and benchmarking.", "title": "CIDEr: Consensus-based image description evaluation"}, "665a311c538fc021c27acd3953f171924cc5905c": {"paper_id": "665a311c538fc021c27acd3953f171924cc5905c", "abstract": "In this paper, we propose a novel training procedure for image captioning models based on policy gradient methods. This allows us to directly optimize for the metrics of interest, rather than just maximizing likelihood of human generated captions. We show that by optimizing for standard metrics such as BLEU, CIDEr, METEOR and ROUGE, we can develop a system that improve on the metrics and ranks first on the MSCOCO image captioning leader board, even though our CNN-RNN model is much simpler than state of the art models. We further show that by also optimizing for the recently introduced SPICE metric, which measures semantic quality of captions, we can produce a system that significantly outperforms other methods as measured by human evaluation. Finally, we show how we can leverage extra sources of information, such as pre-trained image tagging models, to further improve quality.", "title": "Optimization of image description metrics using policy gradient methods"}, "7533d30329cfdbf04ee8ee82bfef792d08015ee5": {"paper_id": "7533d30329cfdbf04ee8ee82bfef792d08015ee5", "abstract": "We describe METEOR, an automatic metric for machine translation evaluation that is based on a generalized concept of unigram matching between the machineproduced translation and human-produced reference translations. Unigrams can be matched based on their surface forms, stemmed forms, and meanings; furthermore, METEOR can be easily extended to include more advanced matching strategies. Once all generalized unigram matches between the two strings have been found, METEOR computes a score for this matching using a combination of unigram-precision, unigram-recall, and a measure of fragmentation that is designed to directly capture how well-ordered the matched words in the machine translation are in relation to the reference. We evaluate METEOR by measuring the correlation between the metric scores and human judgments of translation quality. We compute the Pearson R correlation value between its scores and human quality assessments of the LDC TIDES 2003 Arabic-to-English and Chinese-to-English datasets. We perform segment-bysegment correlation, and show that METEOR gets an R correlation value of 0.347 on the Arabic data and 0.331 on the Chinese data. This is shown to be an improvement on using simply unigramprecision, unigram-recall and their harmonic F1 combination. We also perform experiments to show the relative contributions of the various mapping modules.", "title": "METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments"}, "11da2d589485685f792a8ac79d4c2e589e5f77bd": {"paper_id": "11da2d589485685f792a8ac79d4c2e589e5f77bd", "abstract": "Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.", "title": "Show and tell: A neural image caption generator"}, "10480a42957a8e08e4c543185e135d7c254583a5": {"paper_id": "10480a42957a8e08e4c543185e135d7c254583a5", "abstract": "Image captioning often requires a large set of training image-sentence pairs. In practice, however, acquiring sufficient training pairs is always expensive, making the recent captioning models limited in their ability to describe objects outside of training corpora (i.e., novel objects). In this paper, we present Long Short-Term Memory with Copying Mechanism (LSTM-C) &#x2014; a new architecture that incorporates copying into the Convolutional Neural Networks (CNN) plus Recurrent Neural Networks (RNN) image captioning framework, for describing novel objects in captions. Specifically, freely available object recognition datasets are leveraged to develop classifiers for novel objects. Our LSTM-C then nicely integrates the standard word-by-word sentence generation by a decoder RNN with copying mechanism which may instead select words from novel objects at proper places in the output sentence. Extensive experiments are conducted on both MSCOCO image captioning and ImageNet datasets, demonstrating the ability of our proposed LSTM-C architecture to describe novel objects. Furthermore, superior results are reported when compared to state-of-the-art deep models.", "title": "Incorporating Copying Mechanism in Image Captioning for Learning Novel Objects"}, "bea0bb77c0d75c3d70fefc274bfbff93a3eff015": {"paper_id": "bea0bb77c0d75c3d70fefc274bfbff93a3eff015", "abstract": "Automatically generating natural language descriptions of videos plays a fundamental challenge for computer vision community. Most recent progress in this problem has been achieved through employing 2-D and/or 3-D Convolutional Neural Networks (CNNs) to encode video content and Recurrent Neural Networks (RNNs) to decode a sentence. In this paper, we present Long Short-Term Memory with Transferred Semantic Attributes (LSTM-TSA)&#x2014;a novel deep architecture that incorporates the transferred semantic attributes learnt from images and videos into the CNN plus RNN framework, by training them in an end-to-end manner. The design of LSTM-TSA is highly inspired by the facts that 1) semantic attributes play a significant contribution to captioning, and 2) images and videos carry complementary semantics and thus can reinforce each other for captioning. To boost video captioning, we propose a novel transfer unit to model the mutually correlated attributes learnt from images and videos. Extensive experiments are conducted on three public datasets, i.e., MSVD, M-VAD and MPII-MD. Our proposed LSTM-TSA achieves to-date the best published performance in sentence generation on MSVD: 52.8% and 74.0% in terms of BLEU@4 and CIDEr-D. Superior results are also reported on M-VAD and MPII-MD when compared to state-of-the-art methods.", "title": "Video Captioning with Transferred Semantic Attributes"}, "3a0a839012575ba455f2b84c2d043a35133285f9": {"paper_id": "3a0a839012575ba455f2b84c2d043a35133285f9", "abstract": "We propose a sentence generation strategy that describes images by predicting the most likely nouns, verbs, scenes and prepositions that make up the core sentence structure. The input are initial noisy estimates of the objects and scenes detected in the image using state of the art trained detectors. As predicting actions from still images directly is unreliable, we use a language model trained from the English Gigaword corpus to obtain their estimates; together with probabilities of co-located nouns, scenes and prepositions. We use these estimates as parameters on a HMM that models the sentence generation process, with hidden nodes as sentence components and image detections as the emissions. Experimental results show that our strategy of combining vision and language produces readable and descriptive sentences compared to naive strategies that use vision alone.", "title": "Corpus-Guided Sentence Generation of Natural Images"}, "5a00e8b88d1012001c96f49dd18308132102bfb3": {"paper_id": "5a00e8b88d1012001c96f49dd18308132102bfb3", "abstract": "A common trend in object recognition is to detect and leverage the use of sparse, informative feature points. The use of such features makes the problem more manageable while providing increased robustness to noise and pose variation. In this work we develop an extension of these ideas to the spatio-temporal case. For this purpose, we show that the direct 3D counterparts to commonly used 2D interest point detectors are inadequate, and we propose an alternative. Anchoring off of these interest points, we devise a recognition algorithm based on spatio-temporally windowed data. We present recognition results on a variety of datasets including both human and rodent behavior.", "title": "Behavior recognition via sparse spatio-temporal features"}, "563e656203f29f0cbabc5cf0611355ba79ae4320": {"paper_id": "563e656203f29f0cbabc5cf0611355ba79ae4320", "abstract": "We study an energy functional for computing optical flow that combines three assumptions: a brightness constancy assumption, a gradient constancy assumption, and a discontinuity-preserving spatio-temporal smoothness constraint. In order to allow for large displacements, linearisations in the two data terms are strictly avoided. We present a consistent numerical scheme based on two nested fixed point iterations. By proving that this scheme implements a coarse-to-fine warping strategy, we give a theoretical foundation for warping which has been used on a mainly experimental basis so far. Our evaluation demonstrates that the novel method gives significantly smaller angular errors than previous techniques for optical flow estimation. We show that it is fairly insensitive to parameter variations, and we demonstrate its excellent robustness under noise. In Proc. 8th European Conference on Computer Vision, Springer LNCS 3024, T. Pajdla and J. Matas (Eds.), vol. 4, pp. 25-36, Prague, Czech Republic, May 2004 c \u00a9 Springer-Verlag Berlin Heidelberg 2004 Received The Longuet-Higgins Best Paper Award.", "title": "High Accuracy Optical Flow Estimation Based on a Theory for Warping"}, "025720574ef67672c44ba9e7065a83a5d6075c36": {"paper_id": "025720574ef67672c44ba9e7065a83a5d6075c36", "abstract": "We use multilayer Long Short Term Memory (LSTM) networks to learn representations of video sequences. Our model uses an encoder LSTM to map an input sequence into a fixed length representation. This representation is decoded using single or multiple decoder LSTMs to perform different tasks, such as reconstructing the input sequence, or predicting the future sequence. We experiment with two kinds of input sequences \u2013 patches of image pixels and high-level representations (\u201cpercepts\u201d) of video frames extracted using a pretrained convolutional net. We explore different design choices such as whether the decoder LSTMs should condition on the generated output. We analyze the outputs of the model qualitatively to see how well the model can extrapolate the learned video representation into the future and into the past. We try to visualize and interpret the learned features. We stress test the model by running it on longer time scales and on out-of-domain data. We further evaluate the representations by finetuning them for a supervised learning problem \u2013 human action recognition on the UCF-101 and HMDB-51 datasets. We show that the representations help improve classification accuracy, especially when there are only a few training examples. Even models pretrained on unrelated datasets (300 hours of YouTube videos) can help action recognition performance.", "title": "Unsupervised Learning of Video Representations using LSTMs"}, "c6241e6fc94192df2380d178c4c96cf071e7a3ac": {"paper_id": "c6241e6fc94192df2380d178c4c96cf071e7a3ac", "abstract": "Visual features are of vital importance for human action understanding in videos. This paper presents a new video representation, called trajectory-pooled deep-convolutional descriptor (TDD), which shares the merits of both hand-crafted features [31] and deep-learned features [24]. Specifically, we utilize deep architectures to learn discriminative convolutional feature maps, and conduct trajectory-constrained pooling to aggregate these convolutional features into effective descriptors. To enhance the robustness of TDDs, we design two normalization methods to transform convolutional feature maps, namely spatiotemporal normalization and channel normalization. The advantages of our features come from (i) TDDs are automatically learned and contain high discriminative capacity compared with those hand-crafted features; (ii) TDDs take account of the intrinsic characteristics of temporal dimension and introduce the strategies of trajectory-constrained sampling and pooling for aggregating deep-learned features. We conduct experiments on two challenging datasets: HMD-B51 and UCF101. Experimental results show that TDDs outperform previous hand-crafted features [31] and deep-learned features [24]. Our method also achieves superior performance to the state of the art on these datasets.", "title": "Action recognition with trajectory-pooled deep-convolutional descriptors"}, "b80f43b42b5320578d4c1e214fe1a8b6b45352ae": {"paper_id": "b80f43b42b5320578d4c1e214fe1a8b6b45352ae", "abstract": "While there has been increasing interest in the task of describing video with natural language, current computer vision algorithms are still severely limited in terms of the variability and complexity of the videos and their associated language that they can recognize. This is in part due to the simplicity of current benchmarks, which mostly focus on specific fine-grained domains with limited videos and simple descriptions. While researchers have provided several benchmark datasets for image captioning, we are not aware of any large-scale video description dataset with comprehensive categories yet diverse video content. In this paper we present MSR-VTT (standing for \"MSRVideo to Text\") which is a new large-scale video benchmark for video understanding, especially the emerging task of translating video to text. This is achieved by collecting 257 popular queries from a commercial video search engine, with 118 videos for each query. In its current version, MSR-VTT provides 10K web video clips with 41.2 hours and 200K clip-sentence pairs in total, covering the most comprehensive categories and diverse visual content, and representing the largest dataset in terms of sentence and vocabulary. Each clip is annotated with about 20 natural sentences by 1,327 AMT workers. We present a detailed analysis of MSR-VTT in comparison to a complete set of existing datasets, together with a summarization of different state-of-the-art video-to-text approaches. We also provide an extensive evaluation of these approaches on this dataset, showing that the hybrid Recurrent Neural Networkbased approach, which combines single-frame and motion representations with soft-attention pooling strategy, yields the best generalization capability on MSR-VTT.", "title": "MSR-VTT: A Large Video Description Dataset for Bridging Video and Language"}, "b79f3d9f8de4d1cc6679676146a40d2a8596f32d": {"paper_id": "b79f3d9f8de4d1cc6679676146a40d2a8596f32d", "abstract": "Studying natural language, and especially how people describe the world around them can help us better understand the visual world. In turn, it can also help us in the quest to generate natural language that describes this world in a human manner. We present a simple yet effective approach to automatically compose image descriptions given computer vision based inputs and using web-scale n-grams. Unlike most previous work that summarizes or retrieves pre-existing text relevant to an image, our method composes sentences entirely from scratch. Experimental results indicate that it is viable to generate simple textual descriptions that are pertinent to the specific content of an image, while permitting creativity in the description \u2013 making for more human-like annotations than previous approaches.", "title": "Composing Simple Image Descriptions using Web-scale N-grams"}, "968ef7f6ce0fdc4f7fef0bd51b06bbb139d5381f": {"paper_id": "968ef7f6ce0fdc4f7fef0bd51b06bbb139d5381f", "abstract": "Humans can prepare concise descriptions of pictures, focusing on what they find important. We demonstrate that automatic methods can do so too. We describe a system that can compute a score linking an image to a sentence. This score can be used to attach a descriptive sentence to a given image, or to obtain images that illustrate a given sentence. The score is obtained by comparing an estimate of meaning obtained from the image to one obtained from the sentence. Each estimate of meaning comes from a discriminative procedure that is learned using data. We evaluate on a novel dataset consisting of human-annotated images. While our underlying estimate of meaning is impoverished, it is sufficient to produce very good quantitative results, evaluated with a novel score that can account for synecdoche.", "title": "Every Picture Tells a Story: Generating Sentences from Images"}, "123b9de009865472c660192f8072493a48352dc2": {"paper_id": "123b9de009865472c660192f8072493a48352dc2", "abstract": "Generating a novel textual description of an image is an interesting problem that connects computer vision and natural language processing. In this paper, we present a simple model that is able to generate descriptive sentences given a sample image. This model has a strong focus on the syntax of the descriptions. We train a purely bilinear model that learns a metric between an image representation (generated from a previously trained Convolutional Neural Network) and phrases that are used to described them. The system is then able to infer phrases from a given image sample. Based on caption syntax statistics, we propose a simple language model that can produce relevant descriptions for a given test image using the phrases inferred. Our approach, which is considerably simpler than state-of-the-art models, achieves comparable results in two popular datasets for the task: Flickr30k and the recently proposed Microsoft COCO.", "title": "Phrase-based Image Captioning"}, "bbb6bef6b2e48f5088f9bc0fd7cf7c07d514ea2a": {"paper_id": "bbb6bef6b2e48f5088f9bc0fd7cf7c07d514ea2a", "abstract": "In this work, we introduce a dataset of video annotated with high quality natural language phrases describing the visual content in a given segment of time. Our dataset is based on the Descriptive Video Service (DVS) that is now encoded on many digital media products such as DVDs. DVS is an audio narration describing the visual elements and actions in a movie for the visually impaired. It is temporally aligned with the movie and mixed with the original movie soundtrack. We describe an automatic DVS segmentation and alignment method for movies, that enables us to scale up the collection of a DVS-derived dataset with minimal human intervention. Using this method, we have collected the largest DVS-derived dataset for video description of which we are aware. Our dataset currently includes over 84.6 hours of paired video/sentences from 92 DVDs and is growing.", "title": "Using Descriptive Video Services to Create a Large Data Source for Video Annotation Research"}, "579b2962ac567a39742601cafe3fc43cf7a7109c": {"paper_id": "579b2962ac567a39742601cafe3fc43cf7a7109c", "abstract": "We present an approach that exploits hierarchical Recurrent Neural Networks (RNNs) to tackle the video captioning problem, i.e., generating one or multiple sentences to describe a realistic video. Our hierarchical framework contains a sentence generator and a paragraph generator. The sentence generator produces one simple short sentence that describes a specific short video interval. It exploits both temporal-and spatial-attention mechanisms to selectively focus on visual elements during generation. The paragraph generator captures the inter-sentence dependency by taking as input the sentential embedding produced by the sentence generator, combining it with the paragraph history, and outputting the new initial state for the sentence generator. We evaluate our approach on two large-scale benchmark datasets: YouTubeClips and TACoS-MultiLevel. The experiments demonstrate that our approach significantly outperforms the current state-of-the-art methods with BLEU@4 scores 0.499 and 0.305 respectively.", "title": "Video Paragraph Captioning Using Hierarchical Recurrent Neural Networks"}, "1c30bb689a40a895bd089e55e0cad746e343d1e2": {"paper_id": "1c30bb689a40a895bd089e55e0cad746e343d1e2", "abstract": "We propose a simple, yet effective approach for spatiotemporal feature learning using deep 3-dimensional convolutional networks (3D ConvNets) trained on a large scale supervised video dataset. Our findings are three-fold: 1) 3D ConvNets are more suitable for spatiotemporal feature learning compared to 2D ConvNets, 2) A homogeneous architecture with small 3x3x3 convolution kernels in all layers is among the best performing architectures for 3D ConvNets, and 3) Our learned features, namely C3D (Convolutional 3D), with a simple linear classifier outperform state-of-the-art methods on 4 different benchmarks and are comparable with current best methods on the other 2 benchmarks. In addition, the features are compact: achieving 52.8% accuracy on UCF101 dataset with only 10 dimensions and also very efficient to compute due to the fast inference of ConvNets. Finally, they are conceptually very simple and easy to train and use.", "title": "Learning Spatiotemporal Features with 3D Convolutional Networks"}, "079edd5cf7968ac4759dfe72af2042cf6e990efc": {"paper_id": "079edd5cf7968ac4759dfe72af2042cf6e990efc", "abstract": "We propose an approach to learn spatio-temporal features in videos from intermediate visual representations we call \u201cpercepts\u201d using Gated-Recurrent-Unit Recurrent Networks (GRUs). Our method relies on percepts that are extracted from all levels of a deep convolutional network trained on the large ImageNet dataset. While high-level percepts contain highly discriminative information, they tend to have a low-spatial resolution. Low-level percepts, on the other hand, preserve a higher spatial resolution from which we can model finer motion patterns. Using low-level percepts, however, can lead to high-dimensionality video representations. To mitigate this effect and control the number of parameters, we introduce a variant of the GRU model that leverages the convolution operations to enforce sparse connectivity of the model units and share parameters across the input spatial locations. We empirically validate our approach on both Human Action Recognition and Video Captioning tasks. In particular, we achieve results equivalent to state-of-art on the YouTube2Text dataset using a simpler caption-decoder model and without extra 3D CNN features.", "title": "Delving Deeper into Convolutional Networks for Learning Video Representations"}, "e7eee4d652786a202ef9cda9a96359e7996f5eca": {"paper_id": "e7eee4d652786a202ef9cda9a96359e7996f5eca", "abstract": "A significant portion of the world\u2019s text is tagged by readers on social bookmarking websites. Credit attribution is an inherent problem in these corpora because most pages have multiple tags, but the tags do not always apply with equal specificity across the whole document. Solving the credit attribution problem requires associating each word in a document with the most appropriate tags and vice versa. This paper introduces Labeled LDA, a topic model that constrains Latent Dirichlet Allocation by defining a one-to-one correspondence between LDA\u2019s latent topics and user tags. This allows Labeled LDA to directly learn word-tag correspondences. We demonstrate Labeled LDA\u2019s improved expressiveness over traditional LDA with visualizations of a corpus of tagged web pages from del.icio.us. Labeled LDA outperforms SVMs by more than 3 to 1 when extracting tag-specific document snippets. As a multi-label text classifier, our model is competitive with a discriminative baseline on a variety of datasets.", "title": "Labeled LDA: A supervised topic model for credit attribution in multi-labeled corpora"}, "682988d3cc614c122745d0e87ad9df0f44c3e432": {"paper_id": "682988d3cc614c122745d0e87ad9df0f44c3e432", "abstract": "Multinomial distributions over words are frequently used to model topics in text collections. A common, major challenge in applying all such topic models to any text mining problem is to label a multinomial topic model accurately so that a user can interpret the discovered topic. So far, such labels have been generated manually in a subjective way. In this paper, we propose probabilistic approaches to automatically labeling multinomial topic models in an objective way. We cast this labeling problem as an optimization problem involving minimizing Kullback-Leibler divergence between word distributions and maximizing mutual information between a label and a topic model. Experiments with user study have been done on two text data sets with different genres.The results show that the proposed labeling methods are quite effective to generate labels that are meaningful and useful for interpreting the discovered topic models. Our methods are general and can be applied to labeling topics learned through all kinds of topic models such as PLSA, LDA, and their variations.", "title": "Automatic labeling of multinomial topic models"}, "7f8abf25ca24b48450b4e535f41e2b8a87df73f5": {"paper_id": "7f8abf25ca24b48450b4e535f41e2b8a87df73f5", "abstract": "This paper presents an LDA-style topic model that captures not only the low-dimensional structure of data, but also how the structure changes over time. Unlike other recent work that relies on Markov assumptions or discretization of time, here each topic is associated with a continuous distribution over timestamps, and for each generated document, the mixture distribution over topics is influenced by both word co-occurrences and the document's timestamp. Thus, the meaning of a particular topic can be relied upon as constant, but the topics' occurrence and correlations change significantly over time. We present results on nine months of personal email, 17 years of NIPS research papers and over 200 years of presidential state-of-the-union addresses, showing improved topics, better timestamp prediction, and interpretable trends.", "title": "Topics over time: a non-Markov continuous-time model of topical trends"}, "078fdc9d7dd7105dcc5e65aa19edefe3e48e8bc7": {"paper_id": "078fdc9d7dd7105dcc5e65aa19edefe3e48e8bc7", "abstract": "We propose a new unsupervised learning technique for extracting information from large text collections. We model documents as if they were generated by a two-stage stochastic process. Each author is represented by a probability distribution over topics, and each topic is represented as a probability distribution over words for that topic. The words in a multi-author paper are assumed to be the result of a mixture of each authors' topic mixture. The topic-word and author-topic distributions are learned from data in an unsupervised manner using a Markov chain Monte Carlo algorithm. We apply the methodology to a large corpus of 160,000 abstracts and 85,000 authors from the well-known CiteSeer digital library, and learn a model with 300 topics. We discuss in detail the interpretation of the results discovered by the system including specific topic and author models, ranking of authors by topic and topics by author, significant trends in the computer science literature between 1990 and 2002, parsing of abstracts by topics and authors and detection of unusual papers by specific authors. An online query interface to the model is also discussed that allows interactive exploration of author-topic models for corpora such as CiteSeer.", "title": "Probabilistic author-topic models for information discovery"}, "0621213a012d169cb7c2930354c6489d6a89baf8": {"paper_id": "0621213a012d169cb7c2930354c6489d6a89baf8", "abstract": "In this paper, we define and study a novel text mining problem, which we refer to as Comparative Text Mining (CTM). Given a set of comparable text collections, the task of comparative text mining is to discover any latent common themes across all collections as well as summarize the similarity and differences of these collections along each common theme. This general problem subsumes many interesting applications, including business intelligence and opinion summarization. We propose a generative probabilistic mixture model for comparative text mining. The model simultaneously performs cross-collection clustering and within-collection clustering, and can be applied to an arbitrary set of comparable text collections. The model can be estimated efficiently using the Expectation-Maximization (EM) algorithm. We evaluate the model on two different text data sets (i.e., a news article data set and a laptop review data set), and compare it with a baseline clustering method also based on a mixture model. Experiment results show that the model is quite effective in discovering the latent common themes across collections and performs significantly better than our baseline mixture model.", "title": "A cross-collection mixture model for comparative text mining"}, "cbe8f17f6a0069e613ddea3ca18476f6ec373309": {"paper_id": "cbe8f17f6a0069e613ddea3ca18476f6ec373309", "abstract": "Multi-label problems arise in various domains such as multi-topic document categorization and protein function prediction. One natural way to deal with such problems is to construct a binary classifier for each label, resulting in a set of independent binary classification problems. Since the multiple labels share the same input space, and the semantics conveyed by different labels are usually correlated, it is essential to exploit the correlation information contained in different labels. In this paper, we consider a general framework for extracting shared structures in multi-label classification. In this framework, a common subspace is assumed to be shared among multiple labels. We show that the optimal solution to the proposed formulation can be obtained by solving a generalized eigenvalue problem, though the problem is non-convex. For high-dimensional problems, direct computation of the solution is expensive, and we develop an efficient algorithm for this case. One appealing feature of the proposed framework is that it includes several well-known algorithms as special cases, thus elucidating their intrinsic relationships. We have conducted extensive experiments on eleven multi-topic web page categorization tasks, and results demonstrate the effectiveness of the proposed formulation in comparison with several representative algorithms.", "title": "Extracting shared subspace for multi-label classification"}, "6d43c41e19d994b802f5cff6fbe4e1feffd0d81f": {"paper_id": "6d43c41e19d994b802f5cff6fbe4e1feffd0d81f", "abstract": "We consider the problem of multiclass classification. Our main thesis is that a simple \u201cone-vs-all\u201d scheme is as accurate as any other approach, assuming that the underlying binary classifiers are well-tuned regularized classifiers such as support vector machines. This thesis is interesting in that it disagrees with a large body of recent published work on multiclass classification. We support our position by means of a critical review of the existing literature, a substantial collection of carefully controlled experimental work, and theoretical arguments.", "title": "In Defense of One-Vs-All Classification"}, "8bd682e46026b5fedae5727979fc76842072ac96": {"paper_id": "8bd682e46026b5fedae5727979fc76842072ac96", "abstract": "Common approaches to multi-label classification learn independent classifiers for each category, and employ ranking or thresholding schemes for classification. Because they do not exploit dependencies between labels, such techniques are only well-suited to problems in which categories are independent. However, in many domains labels are highly interdependent. This paper explores multi-label conditional random field (CRF)classification models that directly parameterize label co-occurrences in multi-label classification. Experiments show that the models outperform their single-label counterparts on standard text corpora. Even when multi-labels are sparse, the models improve subset classification error by as much as 40%.", "title": "Collective multi-label classification"}, "04ce064505b1635583fa0d9cc07cac7e9ea993cc": {"paper_id": "04ce064505b1635583fa0d9cc07cac7e9ea993cc", "abstract": "Recent approaches to text classification have used two different first-order probabilistic models for classification, both of which make the naive Bayes assumption. Some use a multi-variate Bernoulli model, that is, a Bayesian Network with no dependencies between words and binary word features (e.g. Larkey and Croft 1996; Koller and Sahami 1997). Others use a multinomial model, that is, a uni-gram language model with integer word counts (e.g. Lewis and Gale 1994; Mitchell 1997). This paper aims to clarify the confusion by describing the differences and details of these two models, and by empirically comparing their classification performance on five text corpora. We find that the multi-variate Bernoulli performs well with small vocabulary sizes, but that the multinomial performs usually performs even better at larger vocabulary sizes\u2014providing on average a 27% reduction in error over the multi-variate Bernoulli model at any vocabulary size.", "title": "A Comparison of Event Models for Naive Bayes Text Classification"}, "50287b978d60343ba3dd5225dffc86eb392722c8": {"paper_id": "50287b978d60343ba3dd5225dffc86eb392722c8", "abstract": "The simple Bayesian classifier is known to be optimal when attributes are independent given the class, but the question of whether other sufficient conditions for its optimality exist has so far not been explored. Empirical results showing that it performs surprisingly well in many domains containing clear attribute dependences suggest that the answer to this question may be positive. This article shows that, although the Bayesian classifier's probability estimates are only optimal under quadratic loss if the independence assumption holds, the classifier itself can be optimal under zero-one loss (misclassification rate) even when this assumption is violated by a wide margin. The region of quadratic-loss optimality of the Bayesian classifier is in fact a second-order infinitesimal fraction of the region of zero-one optimality. This implies that the Bayesian classifier has a much greater range of applicability than previously thought. For example, in this article it is shown to be optimal for learning conjunctions and disjunctions, even though they violate the independence assumption. Further, studies in artificial domains show that it will often outperform more powerful classifiers for common training set sizes and numbers of attributes, even if its bias is a priori much less appropriate to the domain. This article's results also imply that detecting attribute dependence is not necessarily the best way to extend the Bayesian classifier, and this is also verified empirically.", "title": "On the Optimality of the Simple Bayesian Classifier under Zero-One Loss"}, "44e915a220ce74badf755aae870fa0b69ee2b82a": {"paper_id": "44e915a220ce74badf755aae870fa0b69ee2b82a", "abstract": "The naive Bayes classiier, currently experiencing a renaissance in machine learning, has long been a core technique in information retrieval. We review some of the variations of naive Bayes models used for text retrieval and classiication, focusing on the distributional assumptions made about word occurrences in documents.", "title": "Naive (Bayes) at Forty: The Independence Assumption in Information Retrieval"}, "4f7cdae160ebb452ef8bb9b54877df782427709e": {"paper_id": "4f7cdae160ebb452ef8bb9b54877df782427709e", "abstract": "Recent work in supervised learning has shown that a surprisingly simple Bayesian classifier with strong assumptions of independence among features, called naive Bayes, is competitive with state-of-the-art classifiers such as C4.5. This fact raises the question of whether a classifier with less restrictive assumptions can perform even better. In this paper we evaluate approaches for inducing classifiers from data, based on the theory of learning Bayesian networks. These networks are factored representations of probability distributions that generalize the naive Bayesian classifier and explicitly represent statements about independence. Among these approaches we single out a method we call Tree Augmented Naive Bayes (TAN), which outperforms naive Bayes, yet at the same time maintains the computational simplicity (no search involved) and robustness that characterize naive Bayes. We experimentally tested these approaches, using problems from the University of California at Irvine repository, and compared them to C4.5, naive Bayes, and wrapper methods for feature selection.", "title": "Bayesian Network Classifiers"}, "0b98fd7bf704876e6d49eb5c9310b37b78989112": {"paper_id": "0b98fd7bf704876e6d49eb5c9310b37b78989112", "abstract": "When documents are organized in a large number of topic categories, the categories are often arranged in a hierarchy. The U.S. patent database and Yahoo are two examples. This paper shows that the accuracy of a naive Bayes text classi er can be signi cantly improved by taking advantage of a hierarchy of classes. We adopt an established statistical technique called shrinkage that smoothes parameter estimates of a data-sparse child with its parent in order to obtain more robust parameter estimates. The approach is also employed in deleted interpolation, a technique for smoothing n-grams in language modeling for speech recognition. Our method scales well to large data sets, with numerous categories in large hierarchies. Experimental results on three real-world data sets from UseNet, Yahoo, and corporate web pages show improved performance, with a reduction in error up to 29% over the traditional at classi er.", "title": "Improving Text Classification by Shrinkage in a Hierarchy of Classes"}, "094fc15bc058b0d62a661a1460885a9490bdb1bd": {"paper_id": "094fc15bc058b0d62a661a1460885a9490bdb1bd", "abstract": "The Rocchio relevance feedback algorithm is one of the most popular and widely applied learning methods from information retrieval. Here, a probabilistic analysis of this algorithm is presented in a text categorization framework. The analysis gives theoretical insight into the heuristics used in the Rocchio algorithm, particularly the word weighting scheme and the similarity metric. It also suggests improvements which lead to a probabilistic variant of the Rocchio classi er. The Rocchio classi er, its probabilistic variant, and a naive Bayes classi er are compared on six text categorization tasks. The results show that the probabilistic algorithms are preferable to the heuristic Rocchio classi er not only because they are more well-founded, but also because they achieve better performance.", "title": "A Probabilistic Analysis of the Rocchio Algorithm with TFIDF for Text Categorization"}, "17accbdd4aa3f9fad6af322bc3d7f4d5b648d9cd": {"paper_id": "17accbdd4aa3f9fad6af322bc3d7f4d5b648d9cd", "abstract": "This paper introduces Transductive Support Vector Machines (TSVMs) for text classi cation. While regular Support Vector Machines (SVMs) try to induce a general decision function for a learning task, Transductive Support Vector Machines take into account a particular test set and try to minimize misclassi cations of just those particular examples. The paper presents an analysis of why TSVMs are well suited for text classi cation. These theoretical ndings are supported by experiments on three test collections. The experiments show substantial improvements over inductive methods, especially for small training sets, cutting the number of labeled training examples down to a twentieth on some tasks. This work also proposes an algorithm for training TSVMs e ciently, handling 10,000 examples and more.", "title": "Transductive Inference for Text Classification using Support Vector Machines"}, "8c0031cd1df734ac224c8c1daf3ce858140c99d5": {"paper_id": "8c0031cd1df734ac224c8c1daf3ce858140c99d5", "abstract": "Topic models, such as latent Dirichlet allocation (LDA), can be useful tools for the statistical analysis of document collections and other discrete data. The LDA model assumes that the words of each document arise from a mixture of topics, each of which is a distribution over the vocabulary. A limitation of LDA is the inability to model topic correlation even though, for example, a document about genetics is more likely to also be about disease than x-ray astronomy. This limitation stems from the use of the Dirichlet distribution to model the variability among the topic proportions. In this paper we develop the correlated topic model (CTM), where the topic proportions exhibit correlation via the logistic normal distribution [1]. We derive a mean-field variational inference algorithm for approximate posterior inference in this model, which is complicated by the fact that the logistic normal is not conjugate to the multinomial. The CTM gives a better fit than LDA on a collection of OCRed articles from the journal Science. Furthermore, the CTM provides a natural way of visualizing and exploring this and other unstructured data sets.", "title": "Correlated Topic Models"}, "a32e74d41a066d3dad15b020cce36cc1e3170e49": {"paper_id": "a32e74d41a066d3dad15b020cce36cc1e3170e49", "abstract": "Graphical models bring together graph theory and probability theory in a powerful formalism for multivariate statistical modeling. In statistical signal processing\u2014 as well as in related fields such as communication theory, control theory and bioinformatics\u2014statistical models have long been formulated in terms of graphs, and algorithms for computing basic statistical quantities such as likelihoods and marginal probabilities have often been expressed in terms of recursions operating on these graphs. Examples include hidden Markov models, Markov random fields, the forward-backward algorithm and Kalman filtering [ Rabiner and Juang (1993); Pearl (1988); Kailath et al. (2000)]. These ideas can be understood, unified and generalized within the formalism of graphical models. Indeed, graphical models provide a natural framework for formulating variations on these classical architectures, and for exploring entirely new families of statistical models. The recursive algorithms cited above are all instances of a general recursive algorithm known as the junction tree algorithm [ Lauritzen and Spiegelhalter, 1988]. The junction tree algorithm takes advantage of factorization properties of the joint probability distribution that are encoded by the pattern of missing edges in a graphical model. For suitably sparse graphs, the junction tree algorithm provides a systematic and practical solution to the general problem of computing likelihoods and other statistical quantities associated with a graphical model. Unfortunately, many graphical models of practical interest are not \" suitably sparse, \" so that the junction tree algorithm no longer provides a viable computational solution to the problem of computing marginal probabilities and other expectations. One popular source of methods for attempting to cope with such cases is the Markov chain Monte Carlo (MCMC) framework, and indeed there is a significant literature on", "title": "A Variational Principle for Graphical Models"}, "52ef80e422dcd4650fac136820554033e54cf366": {"paper_id": "52ef80e422dcd4650fac136820554033e54cf366", "abstract": "Given a set of images containing multiple object categories, we seek to discover those categories and their image locations without supervision. We achieve this using generative models from the statistical text literature: probabilistic Latent Semantic Analysis (pLSA), and Latent Dirichlet Allocation (LDA). In text analysis these are used to discover topics in a corpus using the bag-of-words document representation. Here we discover topics as object categories, so that an image containing instances of several categories is modelled as a mixture of topics. The models are applied to images by using a visual analogue of a word, formed by vector quantizing SIFT like region descriptors. We investigate a set of increasingly demanding scenarios, starting with image sets containing only two object categories through to sets containing multiple categories (including airplanes, cars, faces, motorbikes, spotted cats) and background clutter. The object categories sample both intra-class and scale variation, and both the categories and their approximate spatial layout are found without supervision. We also demonstrate classification of unseen images and images containing multiple objects. Performance of the proposed unsupervised method is compared to the semi-supervised approach of [7]. 1 1This work was sponsored in part by the EU Project CogViSys, the University of Oxford, Shell Oil (grant #6896597), and the National GeospatialIntelligence Agency (grant #6896949).", "title": "Discovering object categories in image collections"}, "683a171a11fb8f2d4056aa5311ec8544381edfcc": {"paper_id": "683a171a11fb8f2d4056aa5311ec8544381edfcc", "abstract": "OBJECTIVE\nObesity increases risk of many adverse outcomes, but its early origins are obscure. Gestational diabetes mellitus (GDM) reflects a metabolically altered fetal environment associated with high birth weight, itself associated with later obesity. Previous studies of GDM and offspring obesity, however, have been few and conflicting. The objectives of this study were to examine associations of birth weight and GDM with adolescent body mass index (BMI) and to determine the extent to which the effect of GDM is explained by its influence on birth weight or by maternal adiposity.\n\n\nMETHODS\nWe conducted a survey of 7981 girls and 6900 boys, 9 to 14 years of age, who are participants in the Growing Up Today Study, a US nationwide study of diet, activity, and growth. In 1996, participants reported height, weight, diet, activity, and other variables by self-administered mailed questionnaire. We linked these data with information reported by their mothers, participants in the Nurses' Health Study II, including GDM, height, current weight, and child's birth weight. We excluded births <34 weeks' gestation and mothers who had preexisting diabetes. We defined overweight as BMI (kg/m(2)) >95th percentile, and at risk for overweight as 85th to 95th percentile, for age and gender from US national data.\n\n\nRESULTS\nMean birth weight was 3.4 kg for girls and 3.6 kg for boys. Among the 465 subjects whose mothers had GDM, 17.1% were at risk for overweight and 9.7% were overweight in early adolescence. In the group without maternal diabetes, these estimates were 14.2% and 6.6%, respectively. In multiple logistic regression analysis, controlling for age, gender, and Tanner stage, the odds ratio for adolescent overweight for each 1-kg increment in birth weight was 1.4 (95% confidence interval: 1.2-1.6). Adjustment for physical activity, television watching, energy intake, breastfeeding duration, mother's BMI, and other maternal and family variables reduced the estimate to 1.3 (1.1-1.5). For offspring of mothers with GDM versus no diabetes, the odds ratio for adolescent overweight was 1.4 (1.1-2.0), which was unchanged after controlling for energy balance and socioeconomic factors. Adjustment for birth weight slightly attenuated the estimate (1.3; 0.9-1.9); adjustment for maternal BMI reduced the odds ratio to 1.2 (0.8-1.7).\n\n\nCONCLUSIONS\nHigher birth weight predicted increased risk of overweight in adolescence. Having been born to a mother with GDM was also associated with increased adolescent overweight. However, the effect of GDM on offspring obesity seemed only partially explained by its influence on birth weight, and adjustment for mother's own BMI attenuated the GDM associations. Our results only modestly support a causal role of altered maternal-fetal glucose metabolism in the genesis of obesity in the offspring. Alternatively, GDM may program risk for a postnatal insult leading to obesity, or it may merely be a risk marker, not in the causal pathway.", "title": "Maternal gestational diabetes, birth weight, and adolescent obesity."}, "acedd1d4b7ab46a424dd9860557bd6b81c067c1d": {"paper_id": "acedd1d4b7ab46a424dd9860557bd6b81c067c1d", "abstract": "Gestational diabetes mellitus (GDM) is defined as glucose intolerance of various degrees that is first detected during pregnancy. GDM is detected through the screening of pregnant women for clinical risk factors and, among at-risk women, testing for abnormal glucose tolerance that is usually, but not invariably, mild and asymptomatic. GDM appears to result from the same broad spectrum of physiological and genetic abnormalities that characterize diabetes outside of pregnancy. Indeed, women with GDM are at high risk for having or developing diabetes when they are not pregnant. Thus, GDM provides a unique opportunity to study the early pathogenesis of diabetes and to develop interventions to prevent the disease.", "title": "Gestational diabetes mellitus."}, "44db12d5ca157b7efc982ed9e9d8a6cc8e4540c2": {"paper_id": "44db12d5ca157b7efc982ed9e9d8a6cc8e4540c2", "abstract": "Over fairly recent years the concept of an artificial sport trainer has been proposed in literature. This concept is based on computational intelligence algorithms. In this paper, we try to extend the artificial sports trainer by planning fitness training sessions that are suitable for athletes, especially during idle seasons when no competition takes place (e.g., winter). The bat algorithm was used for planning fitness training sessions and results showed promise for the proposed solution. Future directions for development are also outlined in the paper.", "title": "Planning Fitness Training Sessions Using the Bat Algorithm"}, "3a9b2fce277e474fb1570da2b4380bbf8c8ceb3f": {"paper_id": "3a9b2fce277e474fb1570da2b4380bbf8c8ceb3f", "abstract": "Data analysis plays an indispensable role for understanding various phenomena. Cluster analysis, primitive exploration with little or no prior knowledge, consists of research developed across a wide variety of communities. The diversity, on one hand, equips us with many tools. On the other hand, the profusion of options causes confusion. We survey clustering algorithms for data sets appearing in statistics, computer science, and machine learning, and illustrate their applications in some benchmark data sets, the traveling salesman problem, and bioinformatics, a new field attracting intensive efforts. Several tightly related topics, proximity measure, and cluster validation, are also discussed.", "title": "Survey of clustering algorithms"}, "9bc55cc4590caf827060fe677645e11242f28e4f": {"paper_id": "9bc55cc4590caf827060fe677645e11242f28e4f", "abstract": "leadership theory: An interactive perspective on leading in complex adaptive systems\" (2006). Management Department Faculty Publications. Paper 8. Traditional, hierarchical views of leadership are less and less useful given the complexities of our modern world. Leadership theory must transition to new perspectives that account for the complex adaptive needs of organizations. In this paper, we propose that leadership (as opposed to leaders) can be seen as a complex dynamic process that emerges in the interactive \" spaces between \" people and ideas. That is, leadership is a dynamic that transcends the capabilities of individuals alone; it is the product of interaction, tension, and exchange rules governing changes in perceptions and understanding. We label this a dynamic of adaptive leadership, and we show how this dynamic provides important insights about the nature of leadership and its outcomes in organizational fields. We define a leadership event as a perceived segment of action whose meaning is created by the interactions of actors involved in producing it, and we present a set of innovative methods for capturing and analyzing these contextually driven processes. We provide theoretical and practical implications of these ideas for organizational behavior and organization and management theory.", "title": "Complexity leadership theory : An interactive perspective on leading in complex adaptive systems"}, "82aa4715d6fde10c1b5b3a789ae8ed1dcaebd8b5": {"paper_id": "82aa4715d6fde10c1b5b3a789ae8ed1dcaebd8b5", "abstract": "A neonatal intensive care unit (NICU) provides critical services to preterm and high-risk infants. Over the years, many tools and techniques have been introduced to support the clinical decisions made by specialists in the NICU. This study systematically reviewed the different technologies used in neonatal decision support systems (DSS), including cognitive analysis, artificial neural networks, data mining techniques, multi-agent systems, and highlighted their role in patient diagnosis, prognosis, monitoring, and healthcare management. Articles on NICU DSS were surveyed, Searches were based on the PubMed, Science Direct, and IEEE databases and only English articles published after 1990 were included. The overall search strategy was to retrieve articles that included terms that were related to \u201cNICU Decision Support Systems\u201d or \u201cArtificial Intelligence\u201d and \u201cNeonatal\u201d. Different methods and artificial intelligence techniques used in NICU decision support systems were assessed and related outcomes, variables, methods and performance measures was reported and discussed. Because of the dynamic, heterogeneous, and real-time environment of the NICU, the processes and medical rules that are followed within a NICU are complicated, and the data records that are produced are complex and frequent. Therefore, a single tool or technology could not cover all the needs of a NICU. However, it is important to examine and deploy new temporal data mining approaches and system architectures, such as multi-agent systems, services, and sensors, to provide integrated real-time solutions for NICU.", "title": "Neonatal intensive care decision support systems using artificial intelligence techniques: a systematic review"}, "d7b97acf598d474d67af11afc83c286517e1c90c": {"paper_id": "d7b97acf598d474d67af11afc83c286517e1c90c", "abstract": "Today\u2019s companies face increased pressure regarding compliance to legal obligations. Regulations for the financial sector such as Basel II and III, Solvency II, or the Sarbanes-Oxley-Act explicitly demand various requirements. Many of those requirements address the governance and management of information assets, such as data. Companies need to report and track their information architecture, and furthermore have to provide accountability and responsibility information on their data to, e.g., supervisory authorities. Additionally, the tracking of processed data becomes increasingly difficult since the software systems and their interactions throughout the enterprise are highly complex. This paper argues for a consistent and comprehensive assignment mechanism on data governance roles. Based on logical inferences, we are able to show how accountability and responsibility can be assigned throughout processed data. Thereby, we analyze the limitations of traditional logic, such as propositional logic, and exemplarily show how non-monotonic defeasible logic can be used to keep the assignment of roles on information assets consistent.", "title": "Data Governance on EA Information Assets: Logical Reasoning for Derived Data"}, "d04fc13945f772ca41e15931e5ce9e0fe9687c00": {"paper_id": "d04fc13945f772ca41e15931e5ce9e0fe9687c00", "abstract": "This paper presents a new covert channel using smartphone magnetic sensors. We show that modern smartphones are capable to detect the magnetic field changes induced by different computer components during I/O operations. In particular, we are able to create a covert channel between a laptop and a mobile device without any additional equipment, firmware modifications or privileged access on either of the devices. We present two encoding schemes for the covert channel communication and evaluate their effectiveness.", "title": "Covert channels using mobile device's magnetic field sensors"}, "de7661a8df52b761d6f1cb73bcb4ad777939bfa7": {"paper_id": "de7661a8df52b761d6f1cb73bcb4ad777939bfa7", "abstract": "It has been assumed that the physical separation ('air-gap') of computers provides a reliable level of security, such that should two adjacent computers become compromised, the covert exchange of data between them would be impossible. In this paper, we demonstrate BitWhisper, a method of bridging the air-gap between adjacent compromised computers by using their heat emissions and built-in thermal sensors to create a covert communication channel. Our method is unique in two respects: it supports bidirectional communication, and it requires no additional dedicated peripheral hardware. We provide experimental results based on the implementation of the Bit-Whisper prototype, and examine the channel's properties and limitations. Our experiments included different layouts, with computers positioned at varying distances from one another, and several sensor types and CPU configurations (e.g., Virtual Machines). We also discuss signal modulation and communication protocols, showing how BitWhisper can be used for the exchange of data between two computers in a close proximity (positioned 0-40 cm apart) at an effective rate of 1-8 bits per hour, a rate which makes it possible to infiltrate brief commands and exfiltrate small amount of data (e.g., passwords) over the covert channel.", "title": "BitWhisper: Covert Signaling Channel between Air-Gapped Computers Using Thermal Manipulations"}, "4be6ff4c12e3f61021c6433031e1ccc0e864ebec": {"paper_id": "4be6ff4c12e3f61021c6433031e1ccc0e864ebec", "abstract": "A previously unknown form of compromising emanations has been discovered. LED status indicators on data communication equipment, under certain conditions, are shown to carry a modulated optical signal that is significantly correlated with information being processed by the device. Physical access is not required; the attacker gains access to all data going through the device, including plaintext in the case of data encryption systems. Experiments show that it is possible to intercept data under realistic conditions at a considerable distance. Many different sorts of devices, including modems and Internet Protocol routers, were found to be vulnerable. A taxonomy of compromising optical emanations is developed, and design changes are described that will successfully block this kind of \"Optical Tempest\" attack.", "title": "Information leakage from optical emanations"}, "3e69c1c84d4e1c83a081fe1293d08247e94f091a": {"paper_id": "3e69c1c84d4e1c83a081fe1293d08247e94f091a", "abstract": "Industrial systems consider only partially security, mostly relying on the basis of \u201cisolated\u201d networks, and controlled access environments. Monitoring and control systems such as SCADA/DCS are responsible for managing critical infrastructures operate in these environments, where a false sense of security assumptions is usually made. The Stuxnet worm attack demonstrated widely in mid 2010 that many of the security assumptions made about the operating environment, technological capabilities and potential threat risk analysis are far away from the reality and challenges modern industrial systems face. We investigate in this work the highly sophisticated aspects of Stuxnet, the impact that it may have on existing security considerations and pose some thoughts on the next generation SCADA/DCS systems from a security perspective.", "title": "Stuxnet worm impact on industrial cyber-physical system security"}, "21ddf1f7ab7e2cd2ae07073bf3238ce46314bac9": {"paper_id": "21ddf1f7ab7e2cd2ae07073bf3238ce46314bac9", "abstract": "As we increasingly rely on computers to process and manage our personal data, safeguarding sensitive information from malicious hackers is a fast growing concern. Among many forms of information leakage, covert timing channels operate by establishing an illegitimate communication channel between two processes and through transmitting information via timing modulation, thereby violating the underlying system's security policy. Recent studies have shown the vulnerability of popular computing environments, such as cloud computing, to these covert timing channels. In this work, we propose a new micro architecture-level framework, CC-Hunter, that detects the possible presence of covert timing channels on shared hardware. Our experiments demonstrate that Chanter is able to successfully detect different types of covert timing channels at varying bandwidths and message patterns.", "title": "CC-Hunter: Uncovering Covert Timing Channels on Shared Processor Hardware"}, "4175de877995760d590fc26b1af8dc01612c0644": {"paper_id": "4175de877995760d590fc26b1af8dc01612c0644", "abstract": "OBJECTIVE\nThis study was designed to investigate the revised and short version of the smartphone addiction scale and the proof of its validity in adolescents. In addition, it suggested cutting off the values by gender in order to determine smartphone addiction and elaborate the characteristics of smartphone usage in adolescents.\n\n\nMETHOD\nA set of questionnaires were provided to a total of 540 selected participants from April to May of 2013. The participants consisted of 343 boys and 197 girls, and their average age was 14.5 years old. The content validity was performed on a selection of shortened items, while an internal-consistency test was conducted for the verification of its reliability. The concurrent validity was confirmed using SAS, SAPS and KS-scale. Receiver operating characteristics analysis was conducted to suggest cut-off.\n\n\nRESULTS\nThe 10 final questions were selected using content validity. The internal consistency and concurrent validity of SAS were verified with a Cronbach's alpha of 0.911. The SAS-SV was significantly correlated with the SAS, SAPS and KS-scale. The SAS-SV scores of gender (p<.001) and self-evaluation of smartphone addiction (p<.001) showed significant difference. The ROC analysis results showed an area under a curve (AUC) value of 0.963(0.888-1.000), a cut-off value of 31, sensitivity value of 0.867 and specificity value of 0.893 in boys while an AUC value of 0.947(0.887-1.000), a cut-off value of 33, sensitivity value of 0.875, and a specificity value of 0.886 in girls.\n\n\nCONCLUSIONS\nThe SAS-SV showed good reliability and validity for the assessment of smartphone addiction. The smartphone addiction scale short version, which was developed and validated in this study, could be used efficiently for the evaluation of smartphone addiction in community and research areas.", "title": "The Smartphone Addiction Scale: Development and Validation of a Short Version for Adolescents"}, "537d5a0f09968979b4cf4e8b0213a8f39257b393": {"paper_id": "537d5a0f09968979b4cf4e8b0213a8f39257b393", "abstract": "Scale developers often provide evidence of content validity by computing a content validity index (CVI), using ratings of item relevance by content experts. We analyzed how nurse researchers have defined and calculated the CVI, and found considerable consistency for item-level CVIs (I-CVIs). However, there are two alternative, but unacknowledged, methods of computing the scale-level index (S-CVI). One method requires universal agreement among experts, but a less conservative method averages the item-level CVIs. Using backward inference with a purposive sample of scale development studies, we found that both methods are being used by nurse researchers, although it was not always possible to infer the calculation method. The two approaches can lead to different values, making it risky to draw conclusions about content validity. Scale developers should indicate which method was used to provide readers with interpretable content validity information.", "title": "The content validity index: are you sure you know what's being reported? Critique and recommendations."}, "fea84a81afb30758b29218dd44e54762fa881cd4": {"paper_id": "fea84a81afb30758b29218dd44e54762fa881cd4", "abstract": "Mobile phone use is banned or regulated in some circumstances. Despite recognized safety concerns and legal regulations, some people do not refrain from using mobile phones. Such problematic mobile phone use can be considered to be an addiction-like behavior. To find the potential predictors, we examined the correlation between problematic mobile phone use and personality traits reported in addiction literature, which indicated that problematic mobile phone use was a function of gender, self-monitoring, and approval motivation but not of loneliness. These findings suggest that the measurements of these addictive personality traits would be helpful in the screening and intervention of potential problematic users of mobile phones.", "title": "Addictive Personality and Problematic Mobile Phone Use"}, "d296c438bfb58581e9f8238707f8286496a551ea": {"paper_id": "d296c438bfb58581e9f8238707f8286496a551ea", "abstract": "OBJECTIVE\nThe aim of this study was to develop a self-diagnostic scale that could distinguish smartphone addicts based on the Korean self-diagnostic program for Internet addiction (K-scale) and the smartphone's own features. In addition, the reliability and validity of the smartphone addiction scale (SAS) was demonstrated.\n\n\nMETHODS\nA total of 197 participants were selected from Nov. 2011 to Jan. 2012 to accomplish a set of questionnaires, including SAS, K-scale, modified Kimberly Young Internet addiction test (Y-scale), visual analogue scale (VAS), and substance dependence and abuse diagnosis of DSM-IV. There were 64 males and 133 females, with ages ranging from 18 to 53 years (M\u200a=\u200a26.06; SD\u200a=\u200a5.96). Factor analysis, internal-consistency test, t-test, ANOVA, and correlation analysis were conducted to verify the reliability and validity of SAS.\n\n\nRESULTS\nBased on the factor analysis results, the subscale \"disturbance of reality testing\" was removed, and six factors were left. The internal consistency and concurrent validity of SAS were verified (Cronbach's alpha\u200a=\u200a0.967). SAS and its subscales were significantly correlated with K-scale and Y-scale. The VAS of each factor also showed a significant correlation with each subscale. In addition, differences were found in the job (p<0.05), education (p<0.05), and self-reported smartphone addiction scores (p<0.001) in SAS.\n\n\nCONCLUSIONS\nThis study developed the first scale of the smartphone addiction aspect of the diagnostic manual. This scale was proven to be relatively reliable and valid.", "title": "Development and Validation of a Smartphone Addiction Scale (SAS)"}, "9710c9d2177648ea5c4a4c6f09db7c85112a836e": {"paper_id": "9710c9d2177648ea5c4a4c6f09db7c85112a836e", "abstract": "This study developed a Smartphone Addiction Proneness Scale (SAPS) based on the existing internet and cellular phone addiction scales. For the development of this scale, 29 items (1.5 times the final number of items) were initially selected as preliminary items, based on the previous studies on internet/phone addiction as well as the clinical experience of involved experts. The preliminary scale was administered to a nationally representative sample of 795 students in elementary, middle, and high schools across South Korea. Then, final 15 items were selected according to the reliability test results. The final scale consisted of four subdomains: (1) disturbance of adaptive functions, (2) virtual life orientation, (3) withdrawal, and (4) tolerance. The final scale indicated a high reliability with Cronbach's \u03b1 of .880. Support for the scale's criterion validity has been demonstrated by its relationship to the internet addiction scale, KS-II (r \u200a=\u200a .49). For the analysis of construct validity, we tested the Structural Equation Model. The results showed the four-factor structure to be valid (NFI \u200a=\u200a .943, TLI \u200a=\u200a .902, CFI \u200a=\u200a .902, RMSEA \u200a=\u200a .034). Smartphone addiction is gaining a greater spotlight as possibly a new form of addiction along with internet addiction. The SAPS appears to be a reliable and valid diagnostic scale for screening adolescents who may be at risk of smartphone addiction. Further implications and limitations are discussed.", "title": "Development of Korean Smartphone Addiction Proneness Scale for Youth"}, "e199e9b0d143c11bf0ab71aaf389a3f38d40b127": {"paper_id": "e199e9b0d143c11bf0ab71aaf389a3f38d40b127", "abstract": "Technology offers great potential to reshape our relationship to work, but the form of that reshaping should not be allowed to happen haphazardly. As work and technology use become increasingly intertwined, a number of issues deserve re-examination. Some of these relate to work intensification and/or longer hours and possible exchange for flexibility. Recent research on use of employer-supplied smart phones offers some insight into employee perceptions of why the company supplies this technology and whether there is risk to declining the opportunity. Because dangers are more readily apparent, current limitations of technology use have been approached more often through laws related to driving than through general policies or regulation about the work itself. However, there are other concerns that may translate into employer liability beyond the possibility of car accidents. A variety of these concerns are covered in this article, along with related suggestion for actions by employers, their advisory groups, technology companies, government and employees themselves.", "title": "Alleviating the \u201cdark side\u201d of smart phone use"}, "04b309af262cf2643daa93a34c1ba177cd6e7a85": {"paper_id": "04b309af262cf2643daa93a34c1ba177cd6e7a85", "abstract": "Anecdotal reports indicated that some on-line users were becoming addicted to the Internet in much that same way that others became addicted to drugs or alcohol which resulted in academic, social, and occupational impairment. However, research among sociologists, psychologists, or psychiatrists has not formally identified addictive use of the Internet as a problematic behavior. This study investigated the existence of Internet addiction and the extent of problems caused by such potential misuse. This study utilized an adapted version of the criteria for pathological gambling defined by the DSM-IV (APA, 1994). On the basis of this criteria, case studies of 396 dependent Internet users (Dependents) and a control group of 100 non-dependent Internet users (Non-Dependents) were classified. Qualitative analyses suggests significant behavioral and functional usage differences between the two groups. Clinical and social implications of pathological Internet use and future directions for research are discussed.", "title": "Internet Addiction: The Emergence of a New Clinical Disorder"}, "340c62dd66352da11a8023ff6b0ac14ef3363153": {"paper_id": "340c62dd66352da11a8023ff6b0ac14ef3363153", "abstract": "Several authors have studied the risks arising from the growth in mobile phone use (e.g. large debts incurred by young people, banned or dangerous use of cellular phones). The aim of this study is to analyse whether impulsivity, which has often been related to various forms of addictive behaviours, is associated with massive use of and dependence on the mobile phone. In this study, 108 female undergraduate psychology students were screened using a questionnaire evaluating actual use of and perceived dependence on the mobile phone, and with the French adaptation of the UPPS Impulsive Behavior Scale. This scale identifies four distinct components associated with impulsive behaviour: Urgency, lack of Premeditation, lack of Perseverance, and Sensation Seeking. The results showed that a relationship can be established between the use of and perceived dependence on the cellular phone and two facets of impulsivity: Urgency and lack of Perseverance. Copyright # 2006 John Wiley & Sons, Ltd.", "title": "Does Impulsivity Relate to Perceived Dependence on and Actual Use of the Mobile Phone ?"}, "fe68e9ad91444dfaab12cdf47ed3359cc81ceee1": {"paper_id": "fe68e9ad91444dfaab12cdf47ed3359cc81ceee1", "abstract": "Mobile phone use is banned or illegal under certain circumstances and in some jurisdictions. Nevertheless, some people still use their mobile phones despite recognized safety concerns, legislation, and informal bans. Drawing potential predictors from the addiction literature, this study sought to predict usage and, specifically, problematic mobile phone use from extraversion, self-esteem, neuroticism, gender, and age. To measure problem use, the Mobile Phone Problem Use Scale was devised and validated as a reliable self-report instrument, against the Addiction Potential Scale and overall mobile phone usage levels. Problem use was a function of age, extraversion, and low self-esteem, but not neuroticism. As extraverts are more likely to take risks, and young drivers feature prominently in automobile accidents, this study supports community concerns about mobile phone use, and identifies groups that should be targeted in any intervention campaigns.", "title": "Psychological Predictors of Problem Mobile Phone Use"}, "bed359015324e4e105e95cce895cc79cae2bc2e7": {"paper_id": "bed359015324e4e105e95cce895cc79cae2bc2e7", "abstract": "Research from numerous corners of psychological inquiry suggests that self-assessments of skill and character are often flawed in substantive and systematic ways. We review empirical findings on the imperfect nature of self-assessment and discuss implications for three real-world domains: health, education, and the workplace. In general, people's self-views hold only a tenuous to modest relationship with their actual behavior and performance. The correlation between self-ratings of skill and actual performance in many domains is moderate to meager-indeed, at times, other people's predictions of a person's outcomes prove more accurate than that person's self-predictions. In addition, people overrate themselves. On average, people say that they are \"above average\" in skill (a conclusion that defies statistical possibility), overestimate the likelihood that they will engage in desirable behaviors and achieve favorable outcomes, furnish overly optimistic estimates of when they will complete future projects, and reach judgments with too much confidence. Several psychological processes conspire to produce flawed self-assessments. Research focusing on health echoes these findings. People are unrealistically optimistic about their own health risks compared with those of other people. They also overestimate how distinctive their opinions and preferences (e.g., discomfort with alcohol) are among their peers-a misperception that can have a deleterious impact on their health. Unable to anticipate how they would respond to emotion-laden situations, they mispredict the preferences of patients when asked to step in and make treatment decisions for them. Guided by mistaken but seemingly plausible theories of health and disease, people misdiagnose themselves-a phenomenon that can have severe consequences for their health and longevity. Similarly, research in education finds that students' assessments of their performance tend to agree only moderately with those of their teachers and mentors. Students seem largely unable to assess how well or poorly they have comprehended material they have just read. They also tend to be overconfident in newly learned skills, at times because the common educational practice of massed training appears to promote rapid acquisition of skill-as well as self-confidence-but not necessarily the retention of skill. Several interventions, however, can be introduced to prompt students to evaluate their skill and learning more accurately. In the workplace, flawed self-assessments arise all the way up the corporate ladder. Employees tend to overestimate their skill, making it difficult to give meaningful feedback. CEOs also display overconfidence in their judgments, particularly when stepping into new markets or novel projects-for example, proposing acquisitions that hurt, rather then help, the price of their company's stock. We discuss several interventions aimed at circumventing the consequences of such flawed assessments; these include training people to routinely make cognitive repairs correcting for biased self-assessments and requiring people to justify their decisions in front of their peers. The act of self-assessment is an intrinsically difficult task, and we enumerate several obstacles that prevent people from reaching truthful self-impressions. We also propose that researchers and practitioners should recognize self-assessment as a coherent and unified area of study spanning many subdisciplines of psychology and beyond. Finally, we suggest that policymakers and other people who makes real-world assessments should be wary of self-assessments of skill, expertise, and knowledge, and should consider ways of repairing self-assessments that may be flawed.", "title": "Flawed Self-Assessment: Implications for Health, Education, and the Workplace."}, "067a5859fd32d652a0fe9c5b7f8eaf6f643078cb": {"paper_id": "067a5859fd32d652a0fe9c5b7f8eaf6f643078cb", "abstract": "We propose a new multiscale image decomposition which offers a hierarchical, adaptive representation for the different features in general images. The starting point is a variational decomposition of an image, f = u0 + v0, where [u0, v0] is the minimizer of a J-functional, J(f, \u03bb0;X,Y ) = infu+v=f { \u2016u\u2016X + \u03bb0\u2016v\u2016pY } . Such minimizers are standard tools for image manipulations (e.g., denoising, deblurring, compression); see, for example, [M. Mumford and J. Shah, Proceedings of the IEEE Computer Vision Pattern Recognition Conference, San Francisco, CA, 1985] and [L. Rudin, S. Osher, and E. Fatemi, Phys. D, 60 (1992), pp. 259\u2013268]. Here, u0 should capture \u201cessential features\u201d of f which are to be separated from the spurious components absorbed by v0, and \u03bb0 is a fixed threshold which dictates separation of scales. To proceed, we iterate the refinement step [uj+1, vj+1] = arginf J(vj , \u03bb02 j), leading to the hierarchical decomposition, f = \u2211k j=0 uj + vk. We focus our attention on the particular case of (X,Y ) = (BV,L2) decomposition. The resulting hierarchical decomposition, f \u223c \u2211 j uj , is essentially nonlinear. The questions of convergence, energy decomposition, localization, and adaptivity are discussed. The decomposition is constructed by numerical solution of successive Euler\u2013Lagrange equations. Numerical results illustrate applications of the new decomposition to synthetic and real images. Both greyscale and color images are considered.", "title": "A Multiscale Image Representation Using Hierarchical (BV, L2 ) Decompositions"}, "3c6a0c8ff0ae51c434db5695a1a8775bc969f340": {"paper_id": "3c6a0c8ff0ae51c434db5695a1a8775bc969f340", "abstract": "The main contribution of this work is a new paradigm for image representation and image compression. We describe a new multilayered representation technique for images. An image is parsed into a superposition of coherent layers: piecewise smooth regions layer, textures layer, etc. The multilayered decomposition algorithm consists in a cascade of compressions applied successively to the image itself and to the residuals that resulted from the previous compressions. During each iteration of the algorithm, we code the residual part in a lossy way: we only retain the most significant structures of the residual part, which results in a sparse representation. Each layer is encoded independently with a different transform, or basis, at a different bitrate, and the combination of the compressed layers can always be reconstructed in a meaningful way. The strength of the multilayer approach comes from the fact that different sets of basis functions complement each others: some of the basis functions will give reasonable account of the large trend of the data, while others will catch the local transients, or the oscillatory patterns. This multilayered representation has a lot of beautiful applications in image understanding, and image and video coding. We have implemented the algorithm and we have studied its capabilities.", "title": "Multilayered image representation: application to image compression"}, "bf7687d96e68042e9c629019a9c3fef22623990a": {"paper_id": "bf7687d96e68042e9c629019a9c3fef22623990a", "abstract": "Traditional clustering algorithms consider all of the dimensions of an input data set equally. However, in the high dimensional data, a common property is that data points are highly clustered in subspaces, which means classes of objects are categorized in subspaces rather than the entire space. Subspace clustering is an extension of traditional clustering that seeks to find clusters in different subspaces categorical data and its corresponding time complexity is analyzed as well. In the proposed algorithm, an additional step is added to the k-modes clustering process to automatically compute the weight of all dimensions in each cluster by using complement entropy. Furthermore, the attribute weight can be used to identify the subsets of important dimensions that categorize different clusters. The effectiveness of the proposed algorithm is demonstrated with real data sets and synthetic data sets. & 2012 Elsevier B.V. All rights reserved.", "title": "A weighting k-modes algorithm for subspace clustering of categorical data"}, "21fb86020f68bf2dd57cd1b8a0e8adead5d9a9ae": {"paper_id": "21fb86020f68bf2dd57cd1b8a0e8adead5d9a9ae", "abstract": "Association rule mining was first proposed by Agrawal, Imielinski, and Swami [AIS93]. The Apriori algorithm discussed in Section 5.2.1 for frequent itemset mining was presented in Agrawal and Srikant [AS94b]. A variation of the algorithm using a similar pruning heuristic was developed independently by Mannila, Tiovonen, and Verkamo [MTV94]. A joint publication combining these works later appeared in Agrawal, Mannila, Srikant, Toivonen, and Verkamo [AMS96]. A method for generating association rules from frequent itemsets is described in Agrawal and Srikant [AS94a].", "title": "Data Mining : Concepts and Techniques"}, "288c67457f09c0c30cadd7439040114e9c377bc3": {"paper_id": "288c67457f09c0c30cadd7439040114e9c377bc3", "abstract": "Association rules, introduced by Agrawal, Imielinski, and Swami, are rules of the form \u201cfor 90% of the rows of the relation, if the row has value 1 in the columns in set W, then it has 1 also in column B\u201d. Efficient methods exist for discovering association rules from large collections of data. The number of discovered rules can, however, be so large that browsing the rule set and finding interesting rules from it can be quite difficult for the user. We show how a simple formalism of rule templates makes it possible to easily describe the structure of interesting rules. We also give examples of visualization of rules, and show how a visualization tool interfaces with rule templates.", "title": "Finding Interesting Rules from Large Sets of Discovered Association Rules"}, "4f640c1338840f3740187352531dfeca9381b5c3": {"paper_id": "4f640c1338840f3740187352531dfeca9381b5c3", "abstract": "The problem of mining sequential patterns was recently introduced in [AS95]. We are given a database of sequences, where each sequence is a list of transactions ordered by transaction-time, and each transaction is a set of items. The problem is to discover all sequential patterns with a user-speci ed minimum support, where the support of a pattern is the number of data-sequences that contain the pattern. An example of a sequential pattern is \\5% of customers bought `Foundation' and `Ringworld' in one transaction, followed by `Second Foundation' in a later transaction\". We generalize the problem as follows. First, we add time constraints that specify a minimum and/or maximum time period between adjacent elements in a pattern. Second, we relax the restriction that the items in an element of a sequential pattern must come from the same transaction, instead allowing the items to be present in a set of transactions whose transaction-times are within a user-speci ed time window. Third, given a user-de ned taxonomy (is-a hierarchy) on items, we allow sequential patterns to include items across all levels of the taxonomy. We present GSP, a new algorithm that discovers these generalized sequential patterns. Empirical evaluation using synthetic and real-life data indicates that GSP is much faster than the AprioriAll algorithm presented in [AS95]. GSP scales linearly with the number of data-sequences, and has very good scale-up properties with respect to the average datasequence size. Also, Department of Computer Science, University of Wisconsin, Madison.", "title": "Mining Sequential Patterns: Generalizations and Performance Improvements"}, "6e31ad98cf839424e152461e7044992604de0d9a": {"paper_id": "6e31ad98cf839424e152461e7044992604de0d9a", "abstract": "Mining frequent closed itemsets provides complete and non-redundant results for frequent pattern analysis. Extensive studies have proposed various strategies for efficient frequent closed itemset mining, such as depth-first search vs. breadthfirst search, vertical formats vs. horizontal formats, tree-structure vs. other data structures, top-down vs. bottom-up traversal, pseudo projection vs. physical projection of conditional database, etc. It is the right time to ask \"what are the pros and cons of the strategies?\" and \"what and how can we pick and integrate the best strategies to achieve higher performance in general cases?\"In this study, we answer the above questions by a systematic study of the search strategies and develop a winning algorithm CLOSET+. CLOSET+ integrates the advantages of the previously proposed effective strategies as well as some ones newly developed here. A thorough performance study on synthetic and real data sets has shown the advantages of the strategies and the improvement of CLOSET+ over existing mining algorithms, including CLOSET, CHARM and OP, in terms of runtime, memory usage and scalability.", "title": "CLOSET+: searching for the best strategies for mining frequent closed itemsets"}, "fec7ee742b091291492650ab5e4b4041648961b3": {"paper_id": "fec7ee742b091291492650ab5e4b4041648961b3", "abstract": "Sustainable electrification planning for remote locations especially in developing countries is very complex in nature while considering different traits such as social, economic, technical, and environmental. To address these issues related to current energy needs depending upon the end user requirements, a coherent, translucent, efficient, and rational energy planning framework has to be identified. This paper presents a comprehensive generalized methodological framework based on the synergies of decision analysis and optimization models for the design of a reliable, robust, and economic microgrid system based on locally available resources for rural communities in developing nations. The framework consists of three different stages. First, decision analysis considering various criterions (technical, social, economic, and environmental) for the selection of suitable energy alternative for designing the microgrid considering multiple scenarios are carried out. Second, the optimal sizing of the various energy resources in different microgrid structures is illustrated. Third, hybrid decision analysis methods are used for selection of the best sustainable microgrid energy system. Finally, the framework presented is then utilized for the design of a sustainable rural microgrid for a remote community located in the Himalayas in India to illustrate its effectiveness. The results obtained show that decision analysis tools provide a real-time solution for rural electrification by binding the synergy between various criteria considering different scenarios. The feasibility analysis using proposed multiyear scalable approach shows its competence not only in determining the suitable size of the microgrid, but also by reducing the net present cost and the cost of electricity significantly.", "title": "A Novel Methodological Framework for the Design of Sustainable Rural Microgrid for Developing Nations"}, "67d508bb13c2acdb879cbeecaf870360ac1a143c": {"paper_id": "67d508bb13c2acdb879cbeecaf870360ac1a143c", "abstract": "We present the design and experimental validation of a scalable dc microgrid for rural electrification in emerging regions. A salient property of the dc microgrid architecture is the distributed control of the grid voltage, which enables both instantaneous power sharing and a metric for determining the available grid power. A droop-voltage power-sharing scheme is implemented wherein the bus voltage droops in response to low supply/high demand. In addition, the architecture of the dc microgrid aims to minimize the losses associated with stored energy by distributing storage to individual households. In this way, the number of conversion steps and line losses are reduced. We calculate that the levelized cost of electricity of the proposed dc microgrid over a 15-year time horizon is $0.35/kWh. We also present the experimental results from a scaled-down experimental prototype that demonstrates the steady-state behavior, the perturbation response, and the overall efficiency of the system. Moreover, we present fault mitigation strategies for various faults that can be expected to occur in a microgrid distribution system. The experimental results demonstrate the suitability of the presented dc microgrid architecture as a technically advantageous and cost-effective method for electrifying emerging regions.", "title": "Scalable DC Microgrids for Rural Electrification in Emerging Regions"}, "6d06e485853d24f28855279bf7b13c45cb3cad31": {"paper_id": "6d06e485853d24f28855279bf7b13c45cb3cad31", "abstract": "The fundamental properties, such as passivity or incremental passivity, of the network elements making up a switched power converter are examined. The nominal open-loop operation of a broad class of such converters is shown to be stable in the large via a Lyapunov argument. The obtained Lyapunov function is then shown to be useful for designing globally stabilizing controls that include adaptive schemes for handling uncertain nominal parameters. Numerical simulations illustrate the application of this control approach in DC-DC converters.<<ETX>>", "title": "Lyapunov-based control for switched power converters"}, "0ef53fe5a5cfeb4cde46efe31378b93ae0cba328": {"paper_id": "0ef53fe5a5cfeb4cde46efe31378b93ae0cba328", "abstract": "Webpage is becoming a more and more important visual input to us. While there are few studies on saliency in webpage, we in this work make a focused study on how humans deploy their attention when viewing webpages and for the first time propose a computational model that is designed to predict webpage saliency. A dataset is built with 149 webpages and eye tracking data from 11 subjects who free-view the webpages. Inspired by the viewing patterns on webpages, multi-scale feature maps that contain object blob representation and text representation are integrated with explicit face maps and positional bias. We propose to use multiple kernel learning (MKL) to achieve a robust integration of various feature maps. Experimental results show that the proposed model outperforms its counterparts in predicting webpage saliency.", "title": "Webpage Saliency"}, "07f9592a78ff4f8301dafc93699a32e855da3275": {"paper_id": "07f9592a78ff4f8301dafc93699a32e855da3275", "abstract": "Five important trends have emerged from recent work on computational models of focal visual attention that emphasize the bottom-up, image-based control of attentional deployment. First, the perceptual saliency of stimuli critically depends on the surrounding context. Second, a unique 'saliency map' that topographically encodes for stimulus conspicuity over the visual scene has proved to be an efficient and plausible bottom-up control strategy. Third, inhibition of return, the process by which the currently attended location is prevented from being attended again, is a crucial element of attentional deployment. Fourth, attention and eye movements tightly interplay, posing computational challenges with respect to the coordinate system used to control attention. And last, scene understanding and object recognition strongly constrain the selection of attended locations. Insights from these five key areas provide a framework for a computational and neurobiological understanding of visual attention.", "title": "Computational modelling of visual attention"}, "2b5b7f45b44d28fe3d186db33df4b5a4c4f1caef": {"paper_id": "2b5b7f45b44d28fe3d186db33df4b5a4c4f1caef", "abstract": "The space around us is represented not once but many times in parietal cortex. These multiple representations encode locations and objects of interest in several egocentric reference frames. Stimulus representations are transformed from the coordinates of receptor surfaces, such as the retina or the cochlea, into the coordinates of effectors, such as the eye, head, or hand. The transformation is accomplished by dynamic updating of spatial representations in conjunction with voluntary movements. This direct sensory-to-motor coordinate transformation obviates the need for a single representation of space in environmental coordinates. In addition to representing object locations in motoric coordinates, parietal neurons exhibit strong modulation by attention. Both top-down and bottom-up mechanisms of attention contribute to the enhancement of visual responses. The saliance of a stimulus is the primary factor in determining the neural response to it. Although parietal neurons represent objects in motor coordinates, visual responses are independent of the intention to perform specific motor acts.", "title": "Space and attention in parietal cortex."}, "7b4b801de1ba8a4456adcde7b1a9c5bc5cf09fc3": {"paper_id": "7b4b801de1ba8a4456adcde7b1a9c5bc5cf09fc3", "abstract": "Most models of visual search, whether involving overt eye movements or covert shifts of attention, are based on the concept of a saliency map, that is, an explicit two-dimensional map that encodes the saliency or conspicuity of objects in the visual environment. Competition among neurons in this map gives rise to a single winning location that corresponds to the next attended target. Inhibiting this location automatically allows the system to attend to the next most salient location. We describe a detailed computer implementation of such a scheme, focusing on the problem of combining information across modalities, here orientation, intensity and color information, in a purely stimulus-driven manner. The model is applied to common psychophysical stimuli as well as to a very demanding visual search task. Its successful performance is used to address the extent to which the primate visual system carries out visual search via one or more such saliency maps and how this can be tested.", "title": "A saliency-based search mechanism for overt and covert shifts of visual attention"}, "48a0eba4bb1df2c391844aae409873942275ba09": {"paper_id": "48a0eba4bb1df2c391844aae409873942275ba09", "abstract": "Despite the voluminous evidence in support of the paradoxical finding that providing individuals with more options can be detrimental to choice, the question of whether and when large assortments impede choice remains open. Even though extant research has identified a variety of antecedents and consequences of choice overload, the findings of the individual studies fail to come together into a cohesive understanding of when large assortments can benefit choice and when they can be detrimental to choice. In a meta-analysis of 99 observations (N = 7202) reported by prior research, we identify four key factors\u2014choice set complexity, decision task difficulty, preference uncertainty, and decision goal\u2014that moderate the impact of assortment size on choice overload. We further show that each of these four factors has a reliable and significant impact on choice overload, whereby higher levels of decision task difficulty, greater choice set complexity, higher preference uncertainty, and a more prominent, effort-minimizing goal facilitate choice overload. We also find that four of the measures of choice overload used in prior research\u2014 satisfaction/confidence, regret, choice deferral, and switching likelihood\u2014are equally powerful measures of choice overload and can be used interchangeably. Finally, we document that when moderating variables are taken into account the overall effect of assortment size on choice overload is significant\u2014a finding counter to the data reported by prior meta-analytic research. \u00a9 2014 Society for Consumer Psychology. Published by Elsevier Inc. All rights reserved.", "title": "Choice overload : A conceptual review and meta-analysis"}, "6ffc86998c967d86293cbf9657977d37a8048894": {"paper_id": "6ffc86998c967d86293cbf9657977d37a8048894", "abstract": "This article describes what should typically be included in the introduction, method, results, and discussion sections of a meta-analytic review. Method sections include information on literature searches, criteria for inclusion of studies, and a listing of the characteristics recorded for each study. Results sections include information describing the distribution of obtained effect sizes, central tendencies, variability, tests of significance, confidence intervals, tests for heterogeneity, and contrasts (univariate or multivariate). The interpretation of meta-analytic results is often facilitated by the inclusion of the binomial effect size display procedure, the coefficient of robustness, file drawer analysis, and, where overall results are not significant, the counternull value of the obtained effect size and power analysis.", "title": "Writing Meta-Analytic Reviews"}, "549a5faace21d432a3bed00362c17d22720180d0": {"paper_id": "549a5faace21d432a3bed00362c17d22720180d0", "abstract": "After 4 decades of severe criticism, the ritual of null hypothesis significance testing\u2014mechanical dichotomous decisions around a sacred .05 criterion\u2014still persists. This article reviews the problems with this practice, including its near-universal misinterpretation ofp as the probability that Ho is false, the misinterpretation that its complement is the probability of successful replication, and the mistaken assumption that if one rejects Ho one thereby affirms the theory that led to the test. Exploratory data analysis and the use of graphic methods, a steady improvement in and a movement toward standardization in measurement, an emphasis on estimating effect sizes using confidence intervals, and the informed use of available statistical methods is suggested. For generalization, psychologists must finally rely, as has been done in all the older sciences, on replication.", "title": "The Earth Is Round ( p < . 05 )"}, "7cb5fe8c8b3a68cd725dd5522a3edd630c42204b": {"paper_id": "7cb5fe8c8b3a68cd725dd5522a3edd630c42204b", "abstract": "BACKGROUND\nApplication of novel machine learning approaches to electronic health record (EHR) data could provide valuable insights into disease processes. We utilized this approach to build predictive models for progression to prediabetes and type 2 diabetes (T2D).\n\n\nMETHODS\nUsing a novel analytical platform (Reverse Engineering and Forward Simulation [REFS]), we built prediction model ensembles for progression to prediabetes or T2D from an aggregated EHR data sample. REFS relies on a Bayesian scoring algorithm to explore a wide model space, and outputs a distribution of risk estimates from an ensemble of prediction models. We retrospectively followed 24 331 adults for transitions to prediabetes or T2D, 2007-2012. Accuracy of prediction models was assessed using an area under the curve (AUC) statistic, and validated in an independent data set.\n\n\nRESULTS\nOur primary ensemble of models accurately predicted progression to T2D (AUC = 0.76), and was validated out of sample (AUC = 0.78). Models of progression to T2D consisted primarily of established risk factors (blood glucose, blood pressure, triglycerides, hypertension, lipid disorders, socioeconomic factors), whereas models of progression to prediabetes included novel factors (high-density lipoprotein, alanine aminotransferase, C-reactive protein, body temperature; AUC = 0.70).\n\n\nCONCLUSIONS\nWe constructed accurate prediction models from EHR data using a hypothesis-free machine learning approach. Identification of established risk factors for T2D serves as proof of concept for this analytical approach, while novel factors selected by REFS represent emerging areas of T2D research. This methodology has potentially valuable downstream applications to personalized medicine and clinical research.", "title": "Reverse Engineering and Evaluation of Prediction Models for Progression to Type 2 Diabetes: An Application of Machine Learning Using Electronic Health Records."}, "646390e37e3a78e46817d9fefb62f929e21555f8": {"paper_id": "646390e37e3a78e46817d9fefb62f929e21555f8", "abstract": "This paper presents a method for Simultaneous Localization and Mapping (SLAM), relying on a monocular camera as the only sensor, which is able to build outdoor, closed-loop maps much larger than previously achieved with such input. Our system, based on the Hierarchical Map approach [1], builds independent local maps in real-time using the EKF-SLAM technique and the inverse depth representation proposed in [2]. The main novelty in the local mapping process is the use of a data association technique that greatly improves its robustness in dynamic and complex environments. A new visual map matching algorithm stitches these maps together and is able to detect large loops automatically, taking into account the unobservability of scale intrinsic to pure monocular SLAM. The loop closing constraint is applied at the upper level of the Hierarchical Map in near real-time. We present experimental results demonstrating monocular SLAM as a human carries a camera over long walked trajectories in outdoor areas with people and other clutter, even in the more difficult case of forward-looking camera, and show the closing of loops of several hundred meters.", "title": "Mapping Large Loops with a Single Hand-Held Camera"}, "0a202f1dfc6991a6a204eaa5e6b46d6223a4d98a": {"paper_id": "0a202f1dfc6991a6a204eaa5e6b46d6223a4d98a", "abstract": "No feature-based vision system can work unless good features can be identi ed and tracked from frame to frame. Although tracking itself is by and large a solved problem, selecting features that can be tracked well and correspond to physical points in the world is still hard. We propose a feature selection criterion that is optimal by construction because it is based on how the tracker works, and a feature monitoring method that can detect occlusions, disocclusions, and features that do not correspond to points in the world. These methods are based on a new tracking algorithm that extends previous Newton-Raphson style search methods to work under a ne image transformations. We test performance with several simulations and experiments.", "title": "Good features to track"}, "51fea461cf3724123c888cb9184474e176c12e61": {"paper_id": "51fea461cf3724123c888cb9184474e176c12e61", "abstract": "Image registration finds a variety of applications in computer vision. Unfortunately, traditional image registration techniques tend to be costly. We present a new image registration technique that makes use of the spatial intensity gradient of the images to find a good match using a type of Newton-Raphson iteration. Our technique is faster because it examines far fewer potential matches between the images than existing techniques. Furthermore, this registration technique can be generalized to handle rotation, scaling and shearing. We show show our technique can be adapted for use in a stereo vision system.", "title": "An Iterative Image Registration Technique with an Application to Stereo Vision"}, "96d18fd9fe19c242d445061d8bd007e18ad81b13": {"paper_id": "96d18fd9fe19c242d445061d8bd007e18ad81b13", "abstract": "Within the context of Simultaneous Localisation and Mapping (SLAM), \u201cloop closing\u201d is the task of deciding whether or not a vehicle has, after an excursion of arbitrary length, returned to a previously visited area. Reliable loop closing is both essential and hard. It is without doubt one of the greatest impediments to long term, robust SLAM. This paper illustrates how visual features, used in conjunction with scanning laser data, can be used to a great advantage. We use the notion of visual saliency to focus the selection of suitable (affine invariant) image-feature descriptors for storage in a database. When queried with a recently taken image the database returns the capture time of matching images. This time information is used to discover loop closing events. Crucially this is achieved independently of estimated map and vehicle location. We integrate the above technique into a SLAM algorithm using delayed vehicle states and scan matching to form interpose geometric constraints. We present initial results using this system to close loops (around 100m) in an indoor environment.", "title": "SLAM-Loop Closing with Visually Salient Features"}, "0dbc79cac96a0ea7c0553042ab1c782f2d665733": {"paper_id": "0dbc79cac96a0ea7c0553042ab1c782f2d665733", "abstract": "A key component of a mobile robot system is the ability to localize itself accurately and, simultaneously, to build a map of the environment. Most of the existing algorithms are based on laser range finders, sonar sensors or artificial landmarks. In this paper, we describe a vision-based mobile robot localization and mapping algorithm, which uses scale-invariant image features as natural landmarks in unmodified environments. The invariance of these features to image translation, scaling and rotation makes them suitable landmarks for mobile robot localization and map building. With our Triclops stereo vision system, these landmarks are localized and robot ego-motion is estimated by least-squares minimization of the matched landmarks. Feature viewpoint variation and occlusion are taken into account by maintaining a view direction for each landmark. Experiments show that these visual landmarks are robustly matched, robot pose is estimated and a consistent three-dimensional map is built. As image features are not noise-free, we carry out error analysis for the landmark positions and the robot pose. We use Kalman filters to track these landmarks in a dynamic environment, resulting in a database map with landmark positional uncertainty. KEY WORDS\u2014localization, mapping, visual landmarks, mobile robot", "title": "Mobile Robot Localization and Mapping with Uncertainty using Scale-Invariant Visual Landmarks"}, "e6b22a31bbab3a6217150b74b9c416cc084564d3": {"paper_id": "e6b22a31bbab3a6217150b74b9c416cc084564d3", "abstract": "S imultaneous localization and mapping (SLAM) is the process by which a mobile robot can build a map of the environment and, at the same time, use this map to compute its location. The past decade has seen rapid and exciting progress in solving the SLAM problem together with many compelling implementations of SLAM methods. The great majority of work has focused on improving computational efficiency while ensuring consistent and accurate estimates for the map and vehicle pose. However, there has also been much research on issues such as nonlinearity, data association , and landmark characterization, all of which are vital in achieving a practical and robust SLAM implementation. This tutorial focuses on the recursive Bayesian formulation of the SLAM problem in which probability distributions or estimates of absolute or relative locations of landmarks and vehicle pose are obtained. Part I of this tutorial (IEEE Robotics & Auomation Magazine, vol. 13, no. 2) surveyed the development of the essential SLAM algorithm in state-space and particle filter form, described a number of key implementations, and cited locations of source code and real-world data for evaluation of SLAM algorithms. Part II of this tutorial (this article), surveys the current state of the art in SLAM research with a focus on three key areas: computational complexity, data association, and environment representation. Much of the mathematical notation and essential concepts used in this article are defined in Part I of this tutorial and, therefore, are not repeated here. SLAM, in its naive form, scales quadratically with the number of landmarks in a map. For real-time implementation, this scaling is potentially a substantial limitation in the use of SLAM methods. The complexity section surveys the many approaches that have been developed to reduce this complexity. These include linear-time state augmentation, sparsifica-tion in information form, partitioned updates, and submapping methods. A second major hurdle to overcome in the implementation of SLAM methods is to correctly associate observations of landmarks with landmarks held in the map. Incorrect association can lead to catastrophic failure of the SLAM algorithm. Data association is particularly important when a vehicle returns to a previously mapped region after a long excursion, the so-called loop-closure problem. The data association section surveys current data association methods used in SLAM. These include batch-validation methods that exploit constraints inherent in the SLAM formulation, appearance based methods, and multihypothesis techniques. The third development discussed in this tutorial is \u2026", "title": "Simultaneous localization and mapping (SLAM): part II"}, "0501ae2b5c3ffa0779337865b2dbb670e88fe907": {"paper_id": "0501ae2b5c3ffa0779337865b2dbb670e88fe907", "abstract": "The problem of generating maps with mobile robots has received considerable attention over the past years. Most of the techniques developed so far have been designed for situations in which the environment is static during the mapping process. Dynamic objects, however, can lead to serious errors in the resulting maps such as spurious objects or misalignments due to localization errors. In this paper we consider the problem of creating maps with mobile robots in dynamic environments. We present a new approach that interleaves mapping and localization with a probabilistic technique to identify spurious measurements. In several experiments we demonstrate that our algorithm generates accurate 2d and 3d in different kinds of dynamic indoor and outdoor environments. We also use our algorithm to isolate the dynamic objects and to generate three-dimensional representation of them.", "title": "Map building with mobile robots in dynamic environments"}, "1f3e3872e1c28d76da663164c431929aeacc57a2": {"paper_id": "1f3e3872e1c28d76da663164c431929aeacc57a2", "abstract": "In this paper we describe a new technique for the creation of featurebased stochastic maps using standard Polaroid sonar sensors. The fundamental contributions of our proposal are: (1) a perceptual grouping process that permits the robust identification and localization of environmental features, such as straight segments and corners, from the sparse and noisy sonar data; (2) a map joining technique that allows the system to build a sequence of independent limited-size stochastic maps and join them in a globally consistent way; (3) a robust mechanism to determine which features in a stochastic map correspond to the same environment feature, allowing the system to update the stochastic map accordingly, and perform tasks such as revisiting and loop closing. We demonstrate the practicality of this approach by building a geometric map of a medium size, real indoor environment, with several people moving around the robot. Maps built from laser data for the same experiment are provided for comparison. KEY WORDS\u2014map building, local maps, data association, sonar sensors, Hough transform", "title": "Robust Mapping and Localization in Indoor Environments Using Sonar Data"}, "1099983c747a8773c1572a3226373ce4521107b1": {"paper_id": "1099983c747a8773c1572a3226373ce4521107b1", "abstract": "We present a system that estimates the motion of a stereo head or a single moving camera based on video input. The system operates in real-time with low delay and the motion estimates are used for navigational purposes. The front end of the system is a feature tracker. Point features are matched between pairs of frames and linked into image trajectories at video rate. Robust estimates of the camera motion are then produced from the feature tracks using a geometric hypothesize-and-test architecture. This generates what we call visual odometry, i.e. motion estimates from visual input alone. No prior knowledge of the scene nor the motion is necessary. The visual odometry can also be used in conjunction with information from other sources such as GPS, inertia sensors, wheel encoders, etc. The pose estimation method has been applied successfully to video from aerial, automotive and handheld platforms. We focus on results with an autonomous ground vehicle. We give examples of camera trajectories estimated purely from images over previously unseen distances and periods of time.", "title": "Visual odometry"}, "6b2e3c9b32e92dbbdd094d2bd88eb60a80c3083d": {"paper_id": "6b2e3c9b32e92dbbdd094d2bd88eb60a80c3083d", "abstract": "The problem we are addressing in Alvey Project MMI149 is that of using computer vision to understand the unconstrained 3D world, in which the viewed scenes will in general contain too wide a diversity of objects for topdown recognition techniques to work. For example, we desire to obtain an understanding of natural scenes, containing roads, buildings, trees, bushes, etc., as typified by the two frames from a sequence illustrated in Figure 1. The solution to this problem that we are pursuing is to use a computer vision system based upon motion analysis of a monocular image sequence from a mobile camera. By extraction and tracking of image features, representations of the 3D analogues of these features can be constructed.", "title": "A Combined Corner and Edge Detector"}, "6509b5e896c4891fd7f7a793359dfaaefe8c335b": {"paper_id": "6509b5e896c4891fd7f7a793359dfaaefe8c335b", "abstract": "This paper is concerned with the problem of how to better exploit 3D geometric information for dense semantic image labeling. Existing methods often treat the available 3D geometry information (e.g., 3D depth-map) simply as an additional image channel besides the R-G-B color channels, and apply the same technique for RGB image labeling. In this paper, we demonstrate that directly performing 3D convolution in the framework of a residual connected 3D voxel top-down modulation network can lead to superior results. Specifically, we propose a 3D semantic labeling method to label outdoor street scenes whenever a dense depth map is available. Experiments on the \u201cSynthia\u201d and \u201cCityscape\u201d datasets show our method outperforms the state-of-the-art methods, suggesting such a simple 3D representation is effective in incorporating 3D geometric information.", "title": "3D Geometry-Aware Semantic Labeling of Outdoor Street Scenes"}, "0cc22d1dab50d9bab9501008e9b359cd9e51872a": {"paper_id": "0cc22d1dab50d9bab9501008e9b359cd9e51872a", "abstract": "This paper presents a simple and effective nonparametric approach to the problem of image parsing, or labeling image regions (in our case, superpixels produced by bottom-up segmentation) with their categories. This approach requires no training, and it can easily scale to datasets with tens of thousands of images and hundreds of labels. It works by scene-level matching with global image descriptors, followed by superpixel-level matching with local features and efficient Markov random field (MRF) optimization for incorporating neighborhood context. Our MRF setup can also compute a simultaneous labeling of image regions into semantic classes (e.g., tree, building, car) and geometric classes (sky, vertical, ground). Our system outperforms the state-of-the-art nonparametric method based on SIFT Flow on a dataset of 2,688 images and 33 labels. In addition, we report per-pixel rates on a larger dataset of 15,150 images and 170 labels. To our knowledge, this is the first complete evaluation of image parsing on a dataset of this size, and it establishes a new benchmark for the problem.", "title": "SuperParsing: Scalable Nonparametric Image Parsing with Superpixels"}, "3070a1bd503c3767def898bbd50c7eea2bbf29c9": {"paper_id": "3070a1bd503c3767def898bbd50c7eea2bbf29c9", "abstract": "The trend towards increasingly deep neural networks has been driven by a general observation that increasing depth increases the performance of a network. Recently, however, evidence has been amassing that simply increasing depth may not be the best way to increase performance, particularly given other limitations. Investigations into deep residual networks have also suggested that they may not in fact be operating as a single deep network, but rather as an ensemble of many relatively shallow networks. We examine these issues, and in doing so arrive at a new interpretation of the unravelled view of deep residual networks which explains some of the behaviours that have been observed experimentally. As a result, we are able to derive a new, shallower, architecture of residual networks which significantly outperforms much deeper models such as ResNet-200 on the ImageNet classification dataset. We also show that this performance is transferable to other problem domains by developing a semantic segmentation approach which outperforms the state-of-the-art by a remarkable margin on datasets including PASCAL VOC, PASCAL Context, and Cityscapes. The architecture that we propose thus outperforms its comparators, including very deep ResNets, and yet is more efficient in memory use and sometimes also in training time. The code and models are available at https://github.com/itijyou/ademxapp.", "title": "Wider or Deeper: Revisiting the ResNet Model for Visual Recognition"}, "94419cbb932fa1ce9b5e89ece89ece48963dfb28": {"paper_id": "94419cbb932fa1ce9b5e89ece89ece48963dfb28", "abstract": "In this work we propose a novel interpretation of residual networks showing that they can be seen as a collection of many paths of differing length. Moreover, residual networks seem to enable very deep networks by leveraging only the short paths during training. To support this observation, we rewrite residual networks as an explicit collection of paths. Unlike traditional models, paths through residual networks vary in length. Further, a lesion study reveals that these paths show ensemble-like behavior in the sense that they do not strongly depend on each other. Finally, and most surprising, most paths are shorter than one might expect, and only the short paths are needed during training, as longer paths do not contribute any gradient. For example, most of the gradient in a residual network with 110 layers comes from paths that are only 10-34 layers deep. Our results reveal one of the key characteristics that seem to enable the training of very deep networks: Residual networks avoid the vanishing gradient problem by introducing short paths which can carry gradient throughout the extent of very deep networks.", "title": "Residual Networks Behave Like Ensembles of Relatively Shallow Networks"}, "39978ba7c83333475d6825d0ff897692933895fc": {"paper_id": "39978ba7c83333475d6825d0ff897692933895fc", "abstract": "Pixel-level labelling tasks, such as semantic segmentation, play a central role in image understanding. Recent approaches have attempted to harness the capabilities of deep learning techniques for image recognition to tackle pixel-level labelling tasks. One central issue in this methodology is the limited capacity of deep learning techniques to delineate visual objects. To solve this problem, we introduce a new form of convolutional neural network that combines the strengths of Convolutional Neural Networks (CNNs) and Conditional Random Fields (CRFs)-based probabilistic graphical modelling. To this end, we formulate Conditional Random Fields with Gaussian pairwise potentials and mean-field approximate inference as Recurrent Neural Networks. This network, called CRF-RNN, is then plugged in as a part of a CNN to obtain a deep network that has desirable properties of both CNNs and CRFs. Importantly, our system fully integrates CRF modelling with CNNs, making it possible to train the whole deep network end-to-end with the usual back-propagation algorithm, avoiding offline post-processing methods for object delineation. We apply the proposed method to the problem of semantic image segmentation, obtaining top results on the challenging Pascal VOC 2012 segmentation benchmark.", "title": "Conditional Random Fields as Recurrent Neural Networks"}, "12660f0defc6580e566c0fa2ac909971d6c6883b": {"paper_id": "12660f0defc6580e566c0fa2ac909971d6c6883b", "abstract": "Vision-based semantic segmentation in urban scenarios is a key functionality for autonomous driving. Recent revolutionary results of deep convolutional neural networks (DCNNs) foreshadow the advent of reliable classifiers to perform such visual tasks. However, DCNNs require learning of many parameters from raw images, thus, having a sufficient amount of diverse images with class annotations is needed. These annotations are obtained via cumbersome, human labour which is particularly challenging for semantic segmentation since pixel-level annotations are required. In this paper, we propose to use a virtual world to automatically generate realistic synthetic images with pixel-level annotations. Then, we address the question of how useful such data can be for semantic segmentation - in particular, when using a DCNN paradigm. In order to answer this question we have generated a synthetic collection of diverse urban images, named SYNTHIA, with automatically generated class annotations. We use SYNTHIA in combination with publicly available real-world urban images with manually provided annotations. Then, we conduct experiments with DCNNs that show how the inclusion of SYNTHIA in the training stage significantly improves performance on the semantic segmentation task.", "title": "The SYNTHIA Dataset: A Large Collection of Synthetic Images for Semantic Segmentation of Urban Scenes"}, "7e011eee579f3065edf99d780e18ac9f4f2c5f4a": {"paper_id": "7e011eee579f3065edf99d780e18ac9f4f2c5f4a", "abstract": "Crowdsourced 3D CAD models are easily accessible online, and can potentially generate an infinite number of training images for almost any object category. We show that augmenting the training data of contemporary Deep Convolutional Neural Net (DCNN) models with such synthetic data can be effective, especially when real training data is limited or not well matched to the target domain. Most freely available CAD models capture 3D shape but are often missing other low level cues, such as realistic object texture, pose, or background. In a detailed analysis, we use synthetic CAD images to probe the ability of DCNN to learn without these cues, with surprising findings. In particular, we show that when the DCNN is fine-tuned on the target detection task, it exhibits a large degree of invariance to missing low-level cues, but, when pretrained on generic ImageNet classification, it learns better when the low-level cues are simulated. We show that our synthetic DCNN training approach significantly outperforms previous methods on the benchmark PASCAL VOC2007 dataset when learning in the few-shot scenario and improves performance in a domain shift scenario on the Office benchmark.", "title": "Learning Deep Object Detectors from 3D Models"}, "069c40a8ca5305c9a0734c1f6134eb19a678f4ab": {"paper_id": "069c40a8ca5305c9a0734c1f6134eb19a678f4ab", "abstract": "We seek to build a large collection of images with ground truth labels to be used for object detection and recognition research. Such data is useful for supervised learning and quantitative evaluation. To achieve this, we developed a web-based tool that allows easy image annotation and instant sharing of such annotations. Using this annotation tool, we have collected a large dataset that spans many object categories, often containing multiple instances over a wide variety of images. We quantify the contents of the dataset and compare against existing state of the art datasets used for object recognition and detection. Also, we show how to extend the dataset to automatically enhance object labels with WordNet, discover object parts, recover a depth ordering of objects in a scene, and increase the number of labels using minimal user supervision and images from the web.", "title": "LabelMe: A Database and Web-Based Tool for Image Annotation"}, "bf5f67ebbe41f2fb1726a7c3c0be707366d5a4fb": {"paper_id": "bf5f67ebbe41f2fb1726a7c3c0be707366d5a4fb", "abstract": "We present an approach to interpret the major surfaces, objects, and support relations of an indoor scene from an RGBD image. Most existing work ignores physical interactions or is applied only to tidy rooms and hallways. Our goal is to parse typical, often messy, indoor scenes into floor, walls, supporting surfaces, and object regions, and to recover support relationships. One of our main interests is to better understand how 3D cues can best inform a structured 3D interpretation. We also contribute a novel integer programming formulation to infer physical support relations. We offer a new dataset of 1449 RGBD images, capturing 464 diverse indoor scenes, with detailed annotations. Our experiments demonstrate our ability to infer support relations in complex scenes and verify that our 3D scene cues and inferred support lead to better object segmentation.", "title": "Indoor Segmentation and Support Inference from RGBD Images"}, "81b1991bfc9e32a3af7bd00bb84a3b4a60007f19": {"paper_id": "81b1991bfc9e32a3af7bd00bb84a3b4a60007f19", "abstract": "Scrum Teams use lightweight tools like Story Points, the Burn down chart, and Team Velocity. While essential, these tools alone provide insufficient information to maintain a high energy state that yields Hyper productivity. More data is required, but data collection itself can slow Teams. This effect must be avoided when productivity is the primary marker of success. Here we describe nine metrics that can develop and sustain Hyper productive Teams -- Velocity, Work Capacity, Focus Factor, Percentage of Adopted Work, Percentage of Found Work, Accuracy of Estimation, Accuracy of Forecast, Targeted Value Increase, Success at Scale, and the Win/Loss Record of the Team. The unique contribution of this paper is to demonstrate how a light touch and lightweight strategy can be used to compare Teams with different Story Point reference scales.", "title": "Scrum Metrics for Hyperproductive Teams: How They Fly like Fighter Aircraft"}, "cc925389cae2efac5ddfed32ebcabd37d2c7bcb4": {"paper_id": "cc925389cae2efac5ddfed32ebcabd37d2c7bcb4", "abstract": "The Scrum software development framework was designed for the hyperproductive state where productivity increases by 5-10 times over waterfall teams and many co-located teams have achieved this effect. In 2006, Xebia (The Netherlands) started localized projects with half Dutch and half Indian team members. After establishing a localized velocity of five times their waterfall competitors on the same project, they moved the Indian members of the team to India and showed stable velocity with fully distributed teams. The ability to achieve hyperproductivity with distributed, outsourced teams was shown to be a repeatable process and a fully distributed model is now the recommended standard when organizations have disciplined Scrum teams with full implementation of XP engineering practices inside the Scrum. Previous studies used overlapping time zones to ease communication and create a single distributed team. The goal of this report is to go one step further and show the same results with team members separated by the 12.5 hour time difference between India and San Francisco. If Scrum works without overlapping time zones then applying it to the mainstream offshoring practice in North America will be possible. In 2008, Xebia India started engagements with partners like TBD.com, a social networking site in San Francisco. TBD has an existing core team of developers doing Scrum with an established local velocity. Adding Xebia India developers to the San Francisco team with a Fully Distributed Scrum model achieved linear scalability with a globally distributed outsourced team.", "title": "Fully Distributed Scrum: Linear Scalability of Production between San Francisco and India"}, "d4065b447872f7f77676adf51adf8abe6c8b3e5d": {"paper_id": "d4065b447872f7f77676adf51adf8abe6c8b3e5d", "abstract": "Scrum was designed to achieve a hyperproductive state where productivity increases 5-10 times over industry averages and many collocated teams have achieved this effect. The question for this paper is whether distributed, offshore teams can consistently achieve the hyperproductive state. In particular, can a team establish a localized velocity and then maintain or increase that velocity when distributing teams across continents. Since 2006, Xebia started projects with half Dutch and half Indian team members. After establishing localized hyperproductivity, they move the Indian members of the team to India and show increasing velocity with fully distributed teams. After running XP engineering practices inside many distributed Scrum projects, Xebia has systematically productized a model very similar to the SirsiDynix model (J. Sutherland, 2006) for high performance, distributed, offshore teams with outstanding quality.", "title": "Fully Distributed Scrum: The Secret Sauce for Hyperproductive Offshored Development Teams"}, "82ce5d6862e726c9221104fe67b0e3c8fe890b9a": {"paper_id": "82ce5d6862e726c9221104fe67b0e3c8fe890b9a", "abstract": "The purpose of this review was to examine published research on small-group development done in the last ten years that would constitute an empirical test of Tuckman\u2019s (1965) hypothesis that groups go through these stages of \u201cforming,\u201d \u201cstorming,\u201d \u201cnorming,\u201d and \u201cperforming.\u201d Of the twenty-two studies reviewed, only one set out to directly test this hypothesis, although many of the others could be related to it. Following a review of these studies, a fifth stage, \u201cadjourning.\u201d was added to the hypothesis, and more empirical work was recommended.", "title": "Stages of Small-Group Development Revisited"}, "3044b2590d7f8a64f4b3e9c3de64ea532591748b": {"paper_id": "3044b2590d7f8a64f4b3e9c3de64ea532591748b", "abstract": "Projects combining agile methods with CMMI combine adaptability with predictability to better serve large customer needs. The introduction of Scrum at Systematic, a CMMI Level 5 company, doubled productivity and cut defects by 40% compared to waterfall projects in 2006 by focusing on early testing and time to fix builds. Systematic institutionalized Scrum across all projects and used data driven tools like story process efficiency to surface Product Backlog impediments. This allowed them to systematically develop a strategy for a second doubling in productivity. Two teams have achieved a sustainable quadrupling of productivity compared to waterfall projects. We discuss here the strategy to bring the entire company to that level. Our experiences shows that Scrum and CMMI together bring a more powerful combination of adaptability and predictability than either one alone and suggest how other companies can combine them to achieve Toyota level performance \u2013 4 times the productivity and 12 times the quality of waterfall teams.", "title": "Scrum and CMMI Going from Good to Great"}, "1270508ef54c8857a3910ee9719dfdf85e477916": {"paper_id": "1270508ef54c8857a3910ee9719dfdf85e477916", "abstract": "Near-infrared (NIR) solid-state micro/nanolasers are important building blocks for true integration of optoelectronic circuitry. Although significant progress has been made in III-V nanowire lasers with achieving NIR lasing at room temperature, challenges remain including low quantum efficiencies and high Auger losses. Importantly, the obstacles toward integrating one-dimensional nanowires on the planar ubiquitous Si platform need to be effectively tackled. Here we demonstrate a new family of planar room-temperature NIR nanolasers based on organic-inorganic perovskite CH3NH3PbI(3-a)X(a) (X = I, Br, Cl) nanoplatelets. Their large exciton binding energies, long diffusion lengths, and naturally formed high-quality planar whispering-gallery mode cavities ensure adequate gain and efficient optical feedback for low-threshold optically pumped in-plane lasing. We show that these remarkable wavelength tunable whispering-gallery nanolasers can be easily integrated onto conductive platforms (Si, Au, indium tin oxide, and so forth). Our findings open up a new class of wavelength tunable planar nanomaterials potentially suitable for on-chip integration.", "title": "Room-temperature near-infrared high-Q perovskite whispering-gallery planar nanolasers."}, "1fa9f48ad9c32d3ed48e22f3e3a62176031e9828": {"paper_id": "1fa9f48ad9c32d3ed48e22f3e3a62176031e9828", "abstract": "Room-temperature ultraviolet lasing in semiconductor nanowire arrays has been demonstrated. The self-organized, <0001> oriented zinc oxide nanowires grown on sapphire substrates were synthesized with a simple vapor transport and condensation process. These wide band-gap semiconductor nanowires form natural laser cavities with diameters varying from 20 to 150 nanometers and lengths up to 10 micrometers. Under optical excitation, surface-emitting lasing action was observed at 385 nanometers, with an emission linewidth less than 0.3 nanometer. The chemical flexibility and the one-dimensionality of the nanowires make them ideal miniaturized laser light sources. These short-wavelength nanolasers could have myriad applications, including optical computing, information storage, and microanalysis.", "title": "Room-temperature ultraviolet nanowire nanolasers."}, "497d6a4888f65034dbec6097d2e79bff770d7231": {"paper_id": "497d6a4888f65034dbec6097d2e79bff770d7231", "abstract": "Many different photovoltaic technologies are being developed for large-scale solar energy conversion. The wafer-based first-generation photovoltaic devices have been followed by thin-film solid semiconductor absorber layers sandwiched between two charge-selective contacts and nanostructured (or mesostructured) solar cells that rely on a distributed heterojunction to generate charge and to transport positive and negative charges in spatially separated phases. Although many materials have been used in nanostructured devices, the goal of attaining high-efficiency thin-film solar cells in such a way has yet to be achieved. Organometal halide perovskites have recently emerged as a promising material for high-efficiency nanostructured devices. Here we show that nanostructuring is not necessary to achieve high efficiencies with this material: a simple planar heterojunction solar cell incorporating vapour-deposited perovskite as the absorbing layer can have solar-to-electrical power conversion efficiencies of over 15 per cent (as measured under simulated full sunlight). This demonstrates that perovskite absorbers can function at the highest efficiencies in simplified device architectures, without the need for complex nanostructures.", "title": "Efficient planar heterojunction perovskite solar cells by vapour deposition"}, "fdbf8709b80dc1d337050bfef0213ff1ac6890ef": {"paper_id": "fdbf8709b80dc1d337050bfef0213ff1ac6890ef", "abstract": "The energy costs associated with separating tightly bound excitons (photoinduced electron-hole pairs) and extracting free charges from highly disordered low-mobility networks represent fundamental losses for many low-cost photovoltaic technologies. We report a low-cost, solution-processable solar cell, based on a highly crystalline perovskite absorber with intense visible to near-infrared absorptivity, that has a power conversion efficiency of 10.9% in a single-junction device under simulated full sunlight. This \"meso-superstructured solar cell\" exhibits exceptionally few fundamental energy losses; it can generate open-circuit photovoltages of more than 1.1 volts, despite the relatively narrow absorber band gap of 1.55 electron volts. The functionality arises from the use of mesoporous alumina as an inert scaffold that structures the absorber and forces electrons to reside in and be transported through the perovskite.", "title": "Efficient hybrid solar cells based on meso-superstructured organometal halide perovskites."}, "4758be6e11dee2030eaec91b56c8ed960887e34e": {"paper_id": "4758be6e11dee2030eaec91b56c8ed960887e34e", "abstract": "We report on solid-state mesoscopic heterojunction solar cells employing nanoparticles (NPs) of methyl ammonium lead iodide (CH(3)NH(3))PbI(3) as light harvesters. The perovskite NPs were produced by reaction of methylammonium iodide with PbI(2) and deposited onto a submicron-thick mesoscopic TiO(2) film, whose pores were infiltrated with the hole-conductor spiro-MeOTAD. Illumination with standard AM-1.5 sunlight generated large photocurrents (J(SC)) exceeding 17 mA/cm(2), an open circuit photovoltage (V(OC)) of 0.888 V and a fill factor (FF) of 0.62 yielding a power conversion efficiency (PCE) of 9.7%, the highest reported to date for such cells. Femto second laser studies combined with photo-induced absorption measurements showed charge separation to proceed via hole injection from the excited (CH(3)NH(3))PbI(3) NPs into the spiro-MeOTAD followed by electron transfer to the mesoscopic TiO(2) film. The use of a solid hole conductor dramatically improved the device stability compared to (CH(3)NH(3))PbI(3) -sensitized liquid junction cells.", "title": "Lead Iodide Perovskite Sensitized All-Solid-State Submicron Thin Film Mesoscopic Solar Cell with Efficiency Exceeding 9%"}, "af1fdb800cf316aa7c66343f5db261f14ea41641": {"paper_id": "af1fdb800cf316aa7c66343f5db261f14ea41641", "abstract": "Consolidated tables showing an extensive listing of the highest independently confirmed efficiencies for solar cells and modules are presented. Guidelines for inclusion of results into these tables are outlined and new entries since January 2010 are reviewed. Copyright # 2010 John Wiley & Sons, Ltd.", "title": "Solar cell efficiency tables ( version 36 )"}, "01523e9ee2ef484fd74c21c26db3761fee37e1ee": {"paper_id": "01523e9ee2ef484fd74c21c26db3761fee37e1ee", "abstract": "The use of TLS by malware poses new challenges to network threat detection because traditional pattern-matching techniques can no longer be applied to its messages. However, TLS also introduces a complex set of observable data features that allow many inferences to be made about both the client and the server. We show that these features can be used to detect and understand malware communication, while at the same time preserving the privacy of the benign uses of encryption. These data features also allow for accurate malware family attribution of network communication, even when restricted to a single, encrypted flow. To demonstrate this, we performed a detailed study of how TLS is used by malware and enterprise applications. We provide a general analysis on millions of TLS encrypted flows, and a targeted study on 18 malware families composed of thousands of unique malware samples and tens-of-thousands of malicious TLS flows. Importantly, we identify and accommodate for the bias introduced by the use of a malware sandbox. We show that the performance of a malware classifier is correlated with a malware family\u2019s use of TLS, i.e., malware families that actively evolve their use of cryptography are more difficult to classify. We conclude that malware\u2019s usage of TLS is distinct in an enterprise setting, and that these differences can be effectively used in rules and machine learning classifiers.", "title": "Deciphering malware\u2019s use of TLS (without decryption)"}, "6c5d03568e012a95c5a663309c8c21ff1e07e53f": {"paper_id": "6c5d03568e012a95c5a663309c8c21ff1e07e53f", "abstract": "Many botnet detection systems employ a blacklist of known command and control (C&C) domains to detect bots and block their traffic. Similar to signature-based virus detection, such a botnet detection approach is static because the blacklist is updated only after running an external (and often manual) process of domain discovery. As a response, botmasters have begun employing domain generation algorithms (DGAs) to dynamically produce a large number of random domain names and select a small subset for actual C&C use. That is, a C&C domain is randomly generated and used for a very short period of time, thus rendering detection approaches that rely on static domain lists ineffective. Naturally, if we know how a domain generation algorithm works, we can generate the domains ahead of time and still identify and block botnet C&C traffic. The existing solutions are largely based on reverse engineering of the bot malware executables, which is not always feasible. In this paper we present a new technique to detect randomly generated domains without reversing. Our insight is that most of the DGA-generated (random) domains that a bot queries would result in Non-Existent Domain (NXDomain) responses, and that bots from the same botnet (with the same DGA algorithm) would generate similar NXDomain traffic. Our approach uses a combination of clustering and classification algorithms. The clustering algorithm clusters domains based on the similarity in the make-ups of domain names as well as the groups of machines that queried these domains. The classification algorithm is used to assign the generated clusters to models of known DGAs. If a cluster cannot be assigned to a known model, then a new model is produced, indicating a new DGA variant or family. We implemented a prototype system and evaluated it on real-world DNS traffic obtained from large ISPs in North America. We report the discovery of twelve DGAs. Half of them are variants of known (botnet) DGAs, and the other half are brand new DGAs that have never been reported before.", "title": "From Throw-Away Traffic to Bots: Detecting the Rise of DGA-Based Malware"}, "31c51d22abeef7a071920a38b56a232a888a95e3": {"paper_id": "31c51d22abeef7a071920a38b56a232a888a95e3", "abstract": "The domain name service (DNS) plays an important role in the operation of the Internet, providing a two-way mapping between domain names and their numerical identifiers. Given its fundamental role, it is not surprising that a wide variety of malicious activities involve the domain name service in one way or another. For example, bots resolve DNS names to locate their command and control servers, and spam mails contain URLs that link to domains that resolve to scam servers. Thus, it seems beneficial to monitor the use of the DNS system for signs that indicate that a certain name is used as part of a malicious operation. In this paper, we introduce EXPOSURE, a system that employs large-scale, passive DNS analysis techniques to detect domains that are involved in malicious activity. We use 15 features that we extract from the DNS traffic that allow us to characterize different properties of DNS names and the ways that they are queried. Our experiments with a large, real-world data set consisting of 100 billion DNS requests, and a real-life deployment for two weeks in an ISP show that our approach is scalable and that we are able to automatically identify unknown malicious domains that are misused in a variety of malicious activity (such as for botnet command and control, spamming, and phishing).", "title": "EXPOSURE: Finding Malicious Domains Using Passive DNS Analysis"}, "122560e02003d5c0f1de3e0083c7a0474c8f1a53": {"paper_id": "122560e02003d5c0f1de3e0083c7a0474c8f1a53", "abstract": "Botnets are now the key platform for many Internet attacks, such as spam, distributed denial-of-service (DDoS), identity theft, and phishing. Most of the current botnet detection approaches work only on specific botnet command and control (C&C) protocols (e.g., IRC) and structures (e.g., centralized), and can become ineffective as botnets change their C&C techniques. In this paper, we present a general detection framework that is independent of botnet C&C protocol and structure, and requires noa priori knowledge of botnets (such as captured bot binaries and hence the botnet signatures, and C&C server names/addresses). We start from the definition and essential properties of botnets. We define a botnet as acoordinated groupof malware instances that arecontrolled via C&C communication channels. The essential properties of a botnet are that the bots communicate with some C&C servers/peers, perform malicious activities, and do so in a similar or correlated way. Accordingly, our detection framework clusters similar communication traffic and similar malicious traffic, and performs cross cluster correlation to identify the hosts that share both similar communication patterns and similar malicious activity patterns. These hosts are thus bots in the monitored network. We have implemented our BotMiner prototype system and evaluated it using many real network traces. The results show that it can detect real-world botnets ( IRC-based, HTTP-based, and P2P botnets including Nugache and Storm worm), and has a very low false positive rate.", "title": "BotMiner: Clustering Analysis of Network Traffic for Protocol- and Structure-Independent Botnet Detection"}, "afbc27feb9bd1c9cea7e9a1938283e0fe2fcac38": {"paper_id": "afbc27feb9bd1c9cea7e9a1938283e0fe2fcac38", "abstract": "Botnets such as Conficker and Torpig utilize high entropy domains for fluxing and evasion. Bots may query a large number of domains, some of which may fail. In this paper, we present techniques where the failed domain queries (NXDOMAIN) may be utilized for: (i) Speeding up the present detection strategies which rely only on successful DNS domains. (ii) Detecting Command and Control (C&C) server addresses through features such as temporal correlation and information entropy of both successful and failed domains. We apply our technique to a Tier-1 ISP dataset obtained from South Asia, and a campus DNS trace, and thus validate our methods by detecting Conficker botnet IPs and other anomalies with a false positive rate as low as 0.02%. Our technique can be applied at the edge of an autonomous system for real-time detection.", "title": "Winning with DNS Failures: Strategies for Faster Botnet Detection"}, "37453d54ee3d8f65f7b201215b264060456ede10": {"paper_id": "37453d54ee3d8f65f7b201215b264060456ede10", "abstract": "The application of machine learning for the detection of malicious network traffic has been well researched over the past several decades; it is particularly appealing when the traffic is encrypted because traditional pattern-matching approaches cannot be used. Unfortunately, the promise of machine learning has been slow to materialize in the network security domain. In this paper, we highlight two primary reasons why this is the case: inaccurate ground truth and a highly non-stationary data distribution. To demonstrate and understand the effect that these pitfalls have on popular machine learning algorithms, we design and carry out experiments that show how six common algorithms perform when confronted with real network data. With our experimental results, we identify the situations in which certain classes of algorithms underperform on the task of encrypted malware traffic classification. We offer concrete recommendations for practitioners given the real-world constraints outlined. From an algorithmic perspective, we find that the random forest ensemble method outperformed competing methods. More importantly, feature engineering was decisive; we found that iterating on the initial feature set, and including features suggested by domain experts, had a much greater impact on the performance of the classification system. For example, linear regression using the more expressive feature set easily outperformed the random forest method using a standard network traffic representation on all criteria considered. Our analysis is based on millions of TLS encrypted sessions collected over 12 months from a commercial malware sandbox and two geographically distinct, large enterprise networks.", "title": "Machine Learning for Encrypted Malware Traffic Classification: Accounting for Noisy Labels and Non-Stationarity"}, "6a74a8573cb1bd15c5f4fa4e047613d2340e61b9": {"paper_id": "6a74a8573cb1bd15c5f4fa4e047613d2340e61b9", "abstract": "This document specifies an Internet standards track protocol for the Internet community, and requests discussion and suggestions for improvements. Please refer to the current edition of the \"Internet Official Protocol Standards\" (STD 1) for the standardization state and status of this protocol. Distribution of this memo is unlimited. Abstract This document specifies Version 1.2 of the Transport Layer Security (TLS) protocol. The TLS protocol provides communications security over the Internet. The protocol allows client/server applications to communicate in a way that is designed to prevent eavesdropping, tampering, or message forgery.", "title": "The Transport Layer Security (TLS) Protocol Version 1.2"}, "7fd394ecd939d0f3c898d265fc366dd689e88857": {"paper_id": "7fd394ecd939d0f3c898d265fc366dd689e88857", "abstract": "We develop fast algorithms for estimation of generalized linear models with convex penalties. The models include linear regression, two-class logistic regression, and multinomial regression problems while the penalties include \u2113(1) (the lasso), \u2113(2) (ridge regression) and mixtures of the two (the elastic net). The algorithms use cyclical coordinate descent, computed along a regularization path. The methods can handle large problems and can also deal efficiently with sparse features. In comparative timings we find that the new algorithms are considerably faster than competing methods.", "title": "Regularization Paths for Generalized Linear Models via Coordinate Descent."}, "165e4c6467eeae8974739a731045bdff07d906fc": {"paper_id": "165e4c6467eeae8974739a731045bdff07d906fc", "abstract": "Many classification tasks, such as spam filtering, intrusion detection, and terrorism detection, are complicated by an adversary who wishes to avoid detection. Previous work on adversarial classification has made the unrealistic assumption that the attacker has perfect knowledge of the classifier [2]. In this paper, we introduce the adversarial classifier reverse engineering (ACRE) learning problem, the task of learning sufficient information about a classifier to construct adversarial attacks. We present efficient algorithms for reverse engineering linear classifiers with either continuous or Boolean features and demonstrate their effectiveness using real data from the domain of spam filtering.", "title": "Adversarial learning"}, "d43ef06299691af9f1e3acfca863cee8881c2e8a": {"paper_id": "d43ef06299691af9f1e3acfca863cee8881c2e8a", "abstract": "Battista Biggio battista.biggio@diee.unica.it Dept. of Electrical and Electronic Engineering University of Cagliari Piazza d\u2019Armi, 09123, Cagliari, Italy and Blaine Nelson blaine.nelson@wsii.uni-tuebingen.de Dept. of Mathematics and Natural Sciences Eberhard-Karls-Universit\u00e4t T\u00fcbingen Sand 1, 72076, T\u00fcbingen, Germany and Pavel Laskov pavel.laskov@uni-tuebingen.de Dept. of Mathematics and Natural Sciences Eberhard-Karls-Universit\u00e4t T\u00fcbingen Sand 1, 72076, T\u00fcbingen, Germany", "title": "Support Vector Machines Under Adversarial Label Noise"}, "7da323e7103245eeaed32367c46abe3f4913df86": {"paper_id": "7da323e7103245eeaed32367c46abe3f4913df86", "abstract": "The research community has begun looking for IP traffic classification techniques that do not rely on `well known\u00bf TCP or UDP port numbers, or interpreting the contents of packet payloads. New work is emerging on the use of statistical traffic characteristics to assist in the identification and classification process. This survey paper looks at emerging research into the application of Machine Learning (ML) techniques to IP traffic classification - an inter-disciplinary blend of IP networking and data mining techniques. We provide context and motivation for the application of ML techniques to IP traffic classification, and review 18 significant works that cover the dominant period from 2004 to early 2007. These works are categorized and reviewed according to their choice of ML strategies and primary contributions to the literature. We also discuss a number of key requirements for the employment of ML-based traffic classifiers in operational IP networks, and qualitatively critique the extent to which the reviewed works meet these requirements. Open issues and challenges in the field are also discussed.", "title": "A survey of techniques for internet traffic classification using machine learning"}, "be1347aac950979aa86b27d1b3316cb1301f65b1": {"paper_id": "be1347aac950979aa86b27d1b3316cb1301f65b1", "abstract": "Literature on the use of Machine Learning (ML) algorithms for classifying IP traffic has relied on bi-directional full-flow statistics while assuming that flows have explicit directionality implied by the first packet captured or the Client-toServer direction. In contrast, many real-world classifiers may miss an arbitrary number of packets from the start of a flow, and be unsure in which direction the flow started. This would lead to degradation in classification performance for application with asymmetric traffic characteristics. We propose a novel approach to train the ML classifier using statistical features calculated over multiple short sub-flows extracted from full-flow generated by the target application and their mirror-imaged replicas as if the flow is in the reverse direction. We demonstrate our optimisation when applied to the Naive Bayes and Decision Tree algorithms. Our approach results in excellent performance even when classification is initiated mid-way through a flow, without prior knowledge of the flow\u2019s direction and using windows as small as 25 packets long.", "title": "Synthetic sub-flow pairs for timely and stable IP traffic identification"}, "240755995d26483ebc9d3d7e2fb61aef82323137": {"paper_id": "240755995d26483ebc9d3d7e2fb61aef82323137", "abstract": "Policy search can in principle acquire complex strategies for control of robots and other autonomous systems. When the policy is trained to process raw sensory inputs, such as images and depth maps, it can also acquire a strategy that combines perception and control. However, effectively processing such complex inputs requires an expressive policy class, such as a large neural network. These high-dimensional policies are difficult to train, especially when learning to control safety-critical systems. We propose PLATO, a continuous, reset-free reinforcement learning algorithm that trains complex control policies with supervised learning, using model-predictive control (MPC) to generate the supervision, hence never in need of running a partially trained and potentially unsafe policy. PLATO uses an adaptive training method to modify the behavior of MPC to gradually match the learned policy in order to generate training samples at states that are likely to be visited by the learned policy. PLATO also maintains the MPC cost as an objective to avoid highly undesirable actions that would result from strictly following the learned policy before it has been fully trained. We prove that this type of adaptive MPC expert produces supervision that leads to good long-horizon performance of the resulting policy. We also empirically demonstrate that MPC can still avoid dangerous on-policy actions in unexpected situations during training. Our empirical results on a set of challenging simulated aerial vehicle tasks demonstrate that, compared to prior methods, PLATO learns faster, experiences substantially fewer catastrophic failures (crashes) during training, and often converges to a better policy.", "title": "PLATO: Policy learning using adaptive trajectory optimization"}, "667fb84bfca10bec165f9e2cca3e21f5e4829ca7": {"paper_id": "667fb84bfca10bec165f9e2cca3e21f5e4829ca7", "abstract": "We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.", "title": "Playing Atari with Deep Reinforcement Learning"}, "b55dcba008184f55741abc3ab99eeff111d00151": {"paper_id": "b55dcba008184f55741abc3ab99eeff111d00151", "abstract": "We consider reinforcement learning in Markov decision processes with high dimensional state and action spaces. We parametrize policies using energy-based models (particularly restricted Boltzmann machines), and train them using policy gradient learning. Our approach builds upon Sallans and Hinton (2004), who parameterized value functions using energy-based models, trained using a non-linear variant of temporal-difference (TD) learning. Unfortunately, non-linear TD is known to diverge in theory and practice. We introduce the first sound and efficient algorithm for training energy-based policies, based on an actorcritic architecture. Our algorithm is computationally efficient, converges close to a local optimum, and outperforms Sallans and Hinton (2004) in several high dimensional domains.", "title": "Actor-Critic Reinforcement Learning with Energy-Based Policies"}, "1b8e45c10238f261a468171374f0c515be790650": {"paper_id": "1b8e45c10238f261a468171374f0c515be790650", "abstract": "A number of reinforcement learning algorithms have been developed that are guaranteed to converge to the optimal solution when used with lookup tables. It is shown, however, that these algorithms can easily become unstable when implemented directly with a general function-approximation system, such as a sigmoidal multilayer perceptron, a radial-basisfunction system, a memory-based learning system, or even a linear function-approximation system. A new class of algorithms, residual gradient algorithms, is proposed, which perform gradient descent on the mean squared Bellman residual, guaranteeing convergence. It is shown, however, that they may learn very slowly in some cases. A larger class of algorithms, residual algorithms, is proposed that has the guaranteed convergence of the residual gradient algorithms, yet can retain the fast learning speed of direct algorithms. In fact, both direct and residual gradient algorithms are shown to be special cases of residual algorithms, and it is shown that residual algorithms can combine the advantages of each approach. The direct, residual gradient, and residual forms of value iteration, Qlearning, and advantage learning are all presented. Theoretical analysis is given explaining the properties these algorithms have, and simulation results are given that demonstrate these properties.", "title": "Residual Algorithms: Reinforcement Learning with Function Approximation"}, "1f26c41f9d637f1e056355341d06472ad65a9a44": {"paper_id": "1f26c41f9d637f1e056355341d06472ad65a9a44", "abstract": "Although TD-Gammon is one of the major successes in machine learning, it has not led to similar impressive breakthroughs in temporal difference learning for other applications or even other games. We were able to replicate some of the success of TD-Gammon, developing a competitive evaluation function on a 4000 parameter feed-forward neural network, without using back-propagation, reinforcement or temporal difference learning methods. Instead we apply simple hill-climbing in a relative fitness environment. These results and further analysis suggest that the surprising success of Tesauro\u2019s program had more to do with the co-evolutionary structure of the learning task and the dynamics of the backgammon game itself.", "title": "Why did TD-Gammon Work?"}, "69de011693b70a3da088c08b7bdc6bb20c0a4b37": {"paper_id": "69de011693b70a3da088c08b7bdc6bb20c0a4b37", "abstract": "We discuss the temporal-difference learning algorithm, as applied to approximating the cost-to-go function of an infinite-horizon discounted Markov chain, using a function approximator involving linear combinations of fixed basis functions. The algorithm we analyze performs on-line updating of a parameter vector during a single endless trajectory of an ergodic Markov chain with a finite or infinite state space. We present a proof of convergence (with probability 1), a characterization of the limit of convergence, and a bound on the resulting approximation error. In addition to proving new and stronger results than those previously available, our analysis is based on a new line of reasoning that provides new intuition about the dynamics of temporal-difference learning. Finally, we prove that on-line updates, based on entire trajectories of the Markov chain, are in a certain sense necessary for convergence. This fact reconciles positive and negative results that have been discussed in the literature, regarding the soundness of temporal-difference learning.", "title": "An Analysis of Temporal-Difference Learning with Function Approximation 1"}, "08ac954ed1628d97548a125b4d95d871efff219c": {"paper_id": "08ac954ed1628d97548a125b4d95d871efff219c", "abstract": "We introduce the first temporal-difference learning algorithms that converge with smooth value function approximators, such as neural networks. Conventional temporal-difference (TD) methods, such as TD(\u03bb), Q-learning and Sarsa have been used successfully with function approximation in many applications. However, it is well known that off-policy sampling, as well as nonlinear function approximation, can cause these algorithms to become unstable (i.e., the parameters of the approximator may diverge). Sutton et al. (2009a, 2009b) solved the problem of off-policy learning with linear TD algorithms by introducing a new objective function, related to the Bellman error, and algorithms that perform stochastic gradient-descent on this function. These methods can be viewed as natural generalizations to previous TD methods, as they converge to the same limit points when used with linear function approximation methods. We generalize this work to nonlinear function approximation. We present a Bellman error objective function and two gradient-descent TD algorithms that optimize it. We prove the asymptotic almost-sure convergence of both algorithms, for any finite Markov decision process and any smooth value function approximator, to a locally optimal solution. The algorithms are incremental and the computational complexity per time step scales linearly with the number of parameters of the approximator. Empirical results obtained in the game of Go demonstrate the algorithms\u2019 effectiveness.", "title": "Convergent Temporal-Difference Learning with Arbitrary Smooth Function Approximation"}, "178631e0f0e624b1607c7a7a2507ed30d4e83a42": {"paper_id": "178631e0f0e624b1607c7a7a2507ed30d4e83a42", "abstract": "Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates deep recurrent neural networks, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.", "title": "Speech recognition with deep recurrent neural networks"}, "7bc9ba75c75c190c3ccb4f6899b0efd31cb0266c": {"paper_id": "7bc9ba75c75c190c3ccb4f6899b0efd31cb0266c", "abstract": "Model predictive control (MPC) is an effective method for controlling robotic systems, particularly autonomous aerial vehicles such as quadcopters. However, application of MPC can be computationally demanding, and typically requires estimating the state of the system, which can be challenging in complex, unstructured environments. Reinforcement learning can in principle forego the need for explicit state estimation and acquire a policy that directly maps sensor readings to actions, but is difficult to apply to unstable systems that are liable to fail catastrophically during training before an effective policy has been found. We propose to combine MPC with reinforcement learning in the framework of guided policy search, where MPC is used to generate data at training time, under full state observations provided by an instrumented training environment. This data is used to train a deep neural network policy, which is allowed to access only the raw observations from the vehicle's onboard sensors. After training, the neural network policy can successfully control the robot without knowledge of the full state, and at a fraction of the computational cost of MPC. We evaluate our method by learning obstacle avoidance policies for a simulated quadrotor, using simulated onboard sensors and no explicit state estimation at test time.", "title": "Learning deep control policies for autonomous aerial vehicles with MPC-guided policy search"}, "97b8ef88008d2075f9d7cb96185cf7bb7f1ef299": {"paper_id": "97b8ef88008d2075f9d7cb96185cf7bb7f1ef299", "abstract": "ALVINN (Autonomous Land Vehicle In a Neural Network) is a 3-layer back-propagation network designed for the task of road following. Currently ALVINN takes images from a camera and a laser range finder as input and produces as output the direction the vehicle should travel in order to follow the road. Training has been conducted using simulated road images. Successful tests on the Carnegie Mellon autonomous navigation test vehicle indicate that the network can effectively follow real roads under certain field conditions. The representation developed to perform the task differs dramatically when the network is trained under various conditions, suggesting the possibility of a novel adaptive autonomous navigation system capable of tailoring its processing to the conditions at hand. This research was^supported by the Office of Naval Research under Contracts N00014-87K-0385 and N00014-87-K-0533, by National Science Foundation Grant EET-8716324, by the Defense Advanced Research Projects Agency (DOD) monitored by the Space and Naval Warfare Systems Command under Contract N0O039-87-C-0251, and by the Strategic Computing Initiative of DARPA, through ARPA Order 5351, and monitored by the U.S. Army Engineer Topographic Laboratories under contract DACA76-85-C-0003 titled \"Road Following\".", "title": "ALVINN: An Autonomous Land Vehicle in a Neural Network"}, "1407ae8725ccef768d634b2f6ec2baa2197b0bbb": {"paper_id": "1407ae8725ccef768d634b2f6ec2baa2197b0bbb", "abstract": "Policy search methods can allow robots to learn control policies for a wide range of tasks, but practical applications of policy search often require hand-engineered components for perception, state estimation, and low-level control. In this paper, we aim to answer the following question: does training the perception and control systems jointly end-toend provide better performance than training each component separately? To this end, we develop a method that can be used to learn policies that map raw image observations directly to torques at the robot\u2019s motors. The policies are represented by deep convolutional neural networks (CNNs) with 92,000 parameters, and are trained using a partially observed guided policy search method, which transforms policy search into supervised learning, with supervision provided by a simple trajectory-centric reinforcement learning method. We evaluate our method on a range of real-world manipulation tasks that require close coordination between vision and control, such as screwing a cap onto a bottle, and present simulated comparisons to a range of prior policy search methods.", "title": "End-to-End Training of Deep Visuomotor Policies"}, "3d54efedc99c3c8eb7e073761f8b210408c8cfee": {"paper_id": "3d54efedc99c3c8eb7e073761f8b210408c8cfee", "abstract": "Over the last years, there has been substantial progress in robust manipulation in unstructured environments. The long-term goal of our work is to get away from precise, but very expensive robotic systems and to develop affordable, potentially imprecise, self-adaptive manipulator systems that can interactively perform tasks such as playing with children. In this paper, we demonstrate how a low-cost off-the-shelf robotic system can learn closed-loop policies for a stacking task in only a handful of trials\u2014from scratch. Our manipulator is inaccurate and provides no pose feedback. For learning a controller in the work space of a Kinect-style depth camera, we use a model-based reinforcement learning technique. Our learning method is data efficient, reduces model bias, and deals with several noise sources in a principled way during long-term planning. We present a way of incorporating state-space constraints into the learning process and analyze the learning gain by exploiting the sequential structure of the stacking task.", "title": "Learning to Control a Low-Cost Manipulator using Data-Efficient Reinforcement Learning"}, "1c78d3c154fabf1bc42b0f8a2b563de238dd49a4": {"paper_id": "1c78d3c154fabf1bc42b0f8a2b563de238dd49a4", "abstract": "Reinforcement learning offers to robotics a framework and set of tools for the design of sophisticated and hard-to-engineer behaviors. Conversely, the challenges of robotic problems provide both inspiration, impact, and validation for developments in reinforcement learning. The relationship between disciplines has sufficient promise to be likened to that between physics and mathematics. In this article, we attempt to strengthen the links between the two research communities by providing a survey of work in reinforcement learning for behavior generation in robots. We highlight both key challenges in robot reinforcement learning as well as notable successes. We discuss how contributions tamed the complexity of the domain and study the role of algorithms, representations, and prior knowledge in achieving these successes. As a result, a particular focus of our paper lies on the choice between model-based and model-free as well as between value function-based and policy search methods. By analyzing a simple problem in some detail we demonstrate how reinforcement learning approaches may be profitably applied, and we note throughout open questions and the tremendous potential for future research. keywords: reinforcement learning, learning control, robot, survey", "title": "Reinforcement learning in robotics: A survey"}, "1dc697ae0d6a1e90dc8ff061e36441b6efdcff7e": {"paper_id": "1dc697ae0d6a1e90dc8ff061e36441b6efdcff7e", "abstract": "We present an iterative linear-quadratic-Gaussian method for locally-optimal feedback control of nonlinear stochastic systems subject to control constraints. Previously, similar methods have been restricted to deterministic unconstrained problems with quadratic costs. The new method constructs an affine feedback control law, obtained by minimizing a novel quadratic approximation to the optimal cost-to-go function. Global convergence is guaranteed through a Levenberg-Marquardt method; convergence in the vicinity of a local minimum is quadratic. Performance is illustrated on a limited-torque inverted pendulum problem, as well as a complex biomechanical control problem involving a stochastic model of the human arm, with 10 state dimensions and 6 muscle actuators. A Matlab implementation of the new algorithm is availabe at www.cogsci.ucsd.edu//spl sim/todorov.", "title": "A generalized iterative LQG method for locally-optimal feedback control of constrained nonlinear stochastic systems"}, "48230ed0c3fa53ef1d43d79e1f6b113f13e83b9b": {"paper_id": "48230ed0c3fa53ef1d43d79e1f6b113f13e83b9b", "abstract": "This paper presents an Iterative Linear Quadratic Regulator (ILQR) me thod for locally-optimal feedback control of nonlinear dynamical systems. The method is applied to a musculo-s ke etal arm model with 10 state dimensions and 6 controls, and is used to compute energy-optimal reach ing movements. Numerical comparisons with three existing methods demonstrate that the new method converge s substantially faster and finds slightly better solutions.", "title": "Iterative Linear Quadratic Regulator Design for Nonlinear Biological Movement Systems"}, "759a3b3821d9f0e08e0b0a62c8b693230afc3f8d": {"paper_id": "759a3b3821d9f0e08e0b0a62c8b693230afc3f8d", "abstract": "We present two novel methods for face verification. Our first method - \u201cattribute\u201d classifiers - uses binary classifiers trained to recognize the presence or absence of describable aspects of visual appearance (e.g., gender, race, and age). Our second method - \u201csimile\u201d classifiers - removes the manual labeling required for attribute classification and instead learns the similarity of faces, or regions of faces, to specific reference people. Neither method requires costly, often brittle, alignment between image pairs; yet, both methods produce compact visual descriptions, and work on real-world images. Furthermore, both the attribute and simile classifiers improve on the current state-of-the-art for the LFW data set, reducing the error rates compared to the current best by 23.92% and 26.34%, respectively, and 31.68% when combined. For further testing across pose, illumination, and expression, we introduce a new data set - termed PubFig - of real-world images of public figures (celebrities and politicians) acquired from the internet. This data set is both larger (60,000 images) and deeper (300 images per individual) than existing data sets of its kind. Finally, we present an evaluation of human performance.", "title": "Attribute and simile classifiers for face verification"}, "31e362dee2355e9fef8b8b5dbb14dc74abebb80e": {"paper_id": "31e362dee2355e9fef8b8b5dbb14dc74abebb80e", "abstract": "We introduce a two-layer undirected graphical model, calle d a \u201cReplicated Softmax\u201d, that can be used to model and automatically extract low -dimensional latent semantic representations from a large unstructured collec ti n of documents. We present efficient learning and inference algorithms for thi s model, and show how a Monte-Carlo based method, Annealed Importance Sampling, c an be used to produce an accurate estimate of the log-probability the model a ssigns to test data. This allows us to demonstrate that the proposed model is able to g neralize much better compared to Latent Dirichlet Allocation in terms of b th the log-probability of held-out documents and the retrieval accuracy.", "title": "Replicated Softmax: an Undirected Topic Model"}, "497a80b2813cffb17f46af50e621a71505094528": {"paper_id": "497a80b2813cffb17f46af50e621a71505094528", "abstract": "We propose a non-linear generative model for human motion data that uses an undirected model with binary latent variables and real-valued \u201cvisible\u201d variables that represent joint angles. The latent and visible variables at each time step receive directed connections from the visible variables at the last few time-steps. Such an architecture makes on-line inference efficient and allows us to use a simple approximate learning procedure. After training, the model finds a single set of parameters that simultaneously capture several different kinds of motion. We demonstrate the power of our approach by synthesizing various motion sequences and by performing on-line filling in of data lost during motion capture. Website: http://www.cs.toronto.edu/ \u223cgwtaylor/publications/nips2006mhmublv/", "title": "Modeling Human Motion Using Binary Latent Variables"}, "0f0d11429e5aaecbc9fce8445afaa3bad7a74888": {"paper_id": "0f0d11429e5aaecbc9fce8445afaa3bad7a74888", "abstract": "Autonomous navigation for large Unmanned Aerial Vehicles (UAVs) is fairly straight-forward, as expensive sensors and monitoring devices can be employed. In contrast, obstacle avoidance remains a challenging task for Micro Aerial Vehicles (MAVs) which operate at low altitude in cluttered environments. Unlike large vehicles, MAVs can only carry very light sensors, such as cameras, making autonomous navigation through obstacles much more challenging. In this paper, we describe a system that navigates a small quadrotor helicopter autonomously at low altitude through natural forest environments. Using only a single cheap camera to perceive the environment, we are able to maintain a constant velocity of up to 1.5m/s. Given a small set of human pilot demonstrations, we use recent state-of-the-art imitation learning techniques to train a controller that can avoid trees by adapting the MAVs heading. We demonstrate the performance of our system in a more controlled environment indoors, and in real natural forest environments outdoors.", "title": "Learning monocular reactive UAV control in cluttered natural environments"}, "374858657ac7f1906cace0eda68539600f7040a8": {"paper_id": "374858657ac7f1906cace0eda68539600f7040a8", "abstract": "State-of-the-art motion estimation algorithms suffer from three major problems: Poorly textured regions, occlusions and small scale image structures. Based on the Gestalt principles of grouping we propose to incorporate a low level image segmentation process in order to tackle these problems. Our new motion estimation algorithm is based on non-local total variation regularization which allows us to integrate the low level image segmentation process in a unified variational framework. Numerical results on the Mid-dlebury optical flow benchmark data set demonstrate that we can cope with the aforementioned problems.", "title": "Motion estimation with non-local total variation regularization"}, "5fa41c242fd273b08baf029cc73437ad92d5ea4f": {"paper_id": "5fa41c242fd273b08baf029cc73437ad92d5ea4f", "abstract": "The Maximum Margin Planning (MMP) (Ratliff et al., 2006) algorithm solves imitation learning problems by learning linear mappings from features to cost functions in a planning domain. The learned policy is the result of minimum-cost planning using these cost functions. These mappings are chosen so that example policies (or trajectories) given by a teacher appear to be lower cost (with a lossscaled margin) than any other policy for a given planning domain. We provide a novel approach, MMPBOOST , based on the functional gradient descent view of boosting (Mason et al., 1999; Friedman, 1999a) that extends MMP by \u201cboosting\u201d in new features. This approach uses simple binary classification or regression to improve performance of MMP imitation learning, and naturally extends to the class of structured maximum margin prediction problems. (Taskar et al., 2005) Our technique is applied to navigation and planning problems for outdoor mobile robots and robotic legged locomotion.", "title": "Boosting Structured Prediction for Imitation Learning"}, "421e6c7247f41c419a46212477d7b29540cbf7b1": {"paper_id": "421e6c7247f41c419a46212477d7b29540cbf7b1", "abstract": "We consider the task of driving a remote control car at high speeds through unstructured outdoor environments. We present an approach in which supervised learning is first used to estimate depths from single monocular images. The learning algorithm can be trained either on real camera images labeled with ground-truth distances to the closest obstacles, or on a training set consisting of synthetic graphics images. The resulting algorithm is able to learn monocular vision cues that accurately estimate the relative depths of obstacles in a scene. Reinforcement learning/policy search is then applied within a simulator that renders synthetic scenes. This learns a control policy that selects a steering direction as a function of the vision system's output. We present results evaluating the predictive ability of the algorithm both on held out test data, and in actual autonomous driving experiments.", "title": "High speed obstacle avoidance using monocular vision and reinforcement learning"}, "408019366a17e18c04083ab785daf44b3d907f84": {"paper_id": "408019366a17e18c04083ab785daf44b3d907f84", "abstract": "We present a novel system that is capable of generating live dense volumetric reconstructions based on input from a micro aerial vehicle. The distributed reconstruction pipeline is based on state-of-the-art approaches to visual SLAM and variational depth map fusion, and is designed to exploit the individual capabilities of the system components. Results are visualized in real-time on a tablet interface, which gives the user the opportunity to interact. We demonstrate the performance of our approach by capturing several indoor and outdoor scenes on-the-fly and by evaluating our results with respect to a ground-truth model.", "title": "Dense reconstruction on-the-fly"}, "17eddf33b513ae1134abadab728bdbf6abab2a05": {"paper_id": "17eddf33b513ae1134abadab728bdbf6abab2a05", "abstract": "Sequential prediction problems such as imitation learning, where future observations depend on previous predictions (actions), violate the common i.i.d. assumptions made in statistical learning. This leads to poor performance in theory and often in practice. Some recent approaches (Daum\u00e9 III et al., 2009; Ross and Bagnell, 2010) provide stronger guarantees in this setting, but remain somewhat unsatisfactory as they train either non-stationary or stochastic policies and require a large number of iterations. In this paper, we propose a new iterative algorithm, which trains a stationary deterministic policy, that can be seen as a no regret algorithm in an online learning setting. We show that any such no regret algorithm, combined with additional reduction assumptions, must find a policy with good performance under the distribution of observations it induces in such sequential settings. We demonstrate that this new approach outperforms previous approaches on two challenging imitation learning problems and a benchmark sequence labeling problem.", "title": "A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning"}, "68102f37d6da41530b63dfd232984f93d8685693": {"paper_id": "68102f37d6da41530b63dfd232984f93d8685693", "abstract": "This paper examines the generalization properties of online convex programming algorithms when the loss function is Lipschitz and strongly convex. Our main result is a sharp bound, that holds with high probability, on the excess risk of the output of an online algorithm in terms of the average regret. This allows one to use recent algorithms with logarithmic cumulative regret guarantees to achieve fast convergence rates for the excess risk with high probability. As a corollary, we characterize the convergence rate of P EGASOS(with high probability), a recently proposed method for solving the SVM optimization problem.", "title": "On the Generalization Ability of Online Strongly Convex Programming Algorithms"}, "248040fa359a9f18527e28687822cf67d6adaf16": {"paper_id": "248040fa359a9f18527e28687822cf67d6adaf16", "abstract": "We present a comprehensive survey of robot Learning from Demonstration (LfD), a technique that develops policies from example state to action mappings. We introduce the LfD design choices in terms of demonstrator, problem space, policy derivation and performance, and contribute the foundations for a structure in which to categorize LfD research. Specifically, we analyze and categorize the multiple ways in which examples are gathered, ranging from teleoperation to imitation, as well as the various techniques for policy derivation, including matching functions, dynamics models and plans. To conclude we discuss LfD limitations and related promising areas for future research.", "title": "A survey of robot learning from demonstration"}, "220bdd265e6721e1d7ec1c4252aa41825147e61b": {"paper_id": "220bdd265e6721e1d7ec1c4252aa41825147e61b", "abstract": "We consider learning in a Markov decision process where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform. This setting is useful in applications (such as the task of driving) where it may be difficult to write down an explicit reward function specifying exactly how different desiderata should be traded off. We think of the expert as trying to maximize a reward function that is expressible as a linear combination of known features, and give an algorithm for learning the task demonstrated by the expert. Our algorithm is based on using \"inverse reinforcement learning\" to try to recover the unknown reward function. We show that our algorithm terminates in a small number of iterations, and that even though we may never recover the expert's reward function, the policy output by the algorithm will attain performance close to that of the expert, where here performance is measured with respect to the expert's unknown reward function.", "title": "Apprenticeship learning via inverse reinforcement learning"}, "0104063400e6d69294edc95fb14c7e8fac347f6a": {"paper_id": "0104063400e6d69294edc95fb14c7e8fac347f6a", "abstract": "In typical classification tasks, we seek a function which assigns a label to a single object. Kernel-based approaches, such as support vector machines (SVMs), which maximize the margin of confidence of the classifier, are the method of choice for many such tasks. Their popularity stems both from the ability to use high-dimensional feature spaces, and from their strong theoretical guarantees. However, many real-world tasks involve sequential, spatial, or structured data, where multiple labels must be assigned. Existing kernel-based methods ignore structure in the problem, assigning labels independently to each object, losing much useful information. Conversely, probabilistic graphical models, such as Markov networks, can represent correlations between labels, by exploiting problem structure, but cannot handle high-dimensional feature spaces, and lack strong theoretical generalization guarantees. In this paper, we present a new framework that combines the advantages of both approaches: Maximum margin Markov (M ) networks incorporate both kernels, which efficiently deal with highdimensional features, and the ability to capture correlations in structured data. We present an efficient algorithm for learning M networks based on a compact quadratic program formulation. We provide a new theoretical bound for generalization in structured domains. Experiments on the task of handwritten character recognition, demonstrate very significant gains over previous approaches.", "title": "Max-Margin Markov Networks"}, "a026179217a2a578a9cfaa4730f8ae57de190920": {"paper_id": "a026179217a2a578a9cfaa4730f8ae57de190920", "abstract": "Mobile malware threats (e.g., on Android) have recently become a real concern. In this paper, we evaluate the state-of-the-art commercial mobile anti-malware products for Android and test how resistant they are against various common obfuscation techniques (even with known malware). Such an evaluation is important for not only measuring the available defense against mobile malware threats, but also proposing effective, next-generation solutions. We developed DroidChameleon, a systematic framework with various transformation techniques, and used it for our study. Our results on 10 popular commercial anti-malware applications for Android are worrisome: none of these tools is resistant against common malware transformation techniques. In addition, a majority of them can be trivially defeated by applying slight transformation over known malware with little effort for malware authors. Finally, in light of our results, we propose possible remedies for improving the current state of malware detection on mobile devices.", "title": "Catch Me If You Can: Evaluating Android Anti-Malware Against Transformation Attacks"}, "12ef153d9c7ccc374d56acf34b59fb2eaec6f755": {"paper_id": "12ef153d9c7ccc374d56acf34b59fb2eaec6f755", "abstract": "The popularity and adoption of smart phones has greatly stimulated the spread of mobile malware, especially on the popular platforms such as Android. In light of their rapid growth, there is a pressing need to develop effective solutions. However, our defense capability is largely constrained by the limited understanding of these emerging mobile malware and the lack of timely access to related samples. In this paper, we focus on the Android platform and aim to systematize or characterize existing Android malware. Particularly, with more than one year effort, we have managed to collect more than 1,200 malware samples that cover the majority of existing Android malware families, ranging from their debut in August 2010 to recent ones in October 2011. In addition, we systematically characterize them from various aspects, including their installation methods, activation mechanisms as well as the nature of carried malicious payloads. The characterization and a subsequent evolution-based study of representative families reveal that they are evolving rapidly to circumvent the detection from existing mobile anti-virus software. Based on the evaluation with four representative mobile security software, our experiments show that the best case detects 79.6% of them while the worst case detects only 20.2% in our dataset. These results clearly call for the need to better develop next-generation anti-mobile-malware solutions.", "title": "Dissecting Android Malware: Characterization and Evolution"}, "451f72230e607cb59d60f996299c578623a19294": {"paper_id": "451f72230e607cb59d60f996299c578623a19294", "abstract": "Modern browsers and smartphone operating systems treat applications as mutually untrusting, potentially malicious principals. Applications are (1) isolated except for explicit IPC or inter-application communication channels and (2) unprivileged by default, requiring user permission for additional privileges. Although inter-application communication supports useful collaboration, it also introduces the risk of permission redelegation. Permission re-delegation occurs when an application with permissions performs a privileged task for an application without permissions. This undermines the requirement that the user approve each application\u2019s access to privileged devices and data. We discuss permission re-delegation and demonstrate its risk by launching real-world attacks on Android system applications; several of the vulnerabilities have been confirmed as bugs. We discuss possible ways to address permission redelegation and present IPC Inspection, a new OS mechanism for defending against permission re-delegation. IPC Inspection prevents opportunities for permission redelegation by reducing an application\u2019s permissions after it receives communication from a less privileged application. We have implemented IPC Inspection for a browser and Android, and we show that it prevents the attacks we found in the Android system applications.", "title": "Permission Re-Delegation: Attacks and Defenses"}, "603d1ea62463a665702ad0d3e2dc25a322a26145": {"paper_id": "603d1ea62463a665702ad0d3e2dc25a322a26145", "abstract": "MockDroid is a modified version of the Android operating system which allows a user to 'mock' an application's access to a resource. This resource is subsequently reported as empty or unavailable whenever the application requests access. This approach allows users to revoke access to particular resources at run-time, encouraging users to consider the trade-off between functionality and the disclosure of personal information whilst they use an application. Existing applications continue to work on MockDroid, possibly with reduced functionality, since existing applications are already written to tolerate resource failure, such as network unavailability or lack of a GPS signal. We demonstrate the practicality of our approach by successfully running a random sample of twenty-three popular applications from the Android Market.", "title": "MockDroid: trading privacy for application functionality on smartphones"}, "1ba779d5a5c9553ee8ecee5cf6bafb4b494ea7bc": {"paper_id": "1ba779d5a5c9553ee8ecee5cf6bafb4b494ea7bc", "abstract": "Users have begun downloading an increasingly large number of mobile phone applications in response to advancements in handsets and wireless networks. The increased number of applications results in a greater chance of installing Trojans and similar malware. In this paper, we propose the Kirin security service for Android, which performs lightweight certification of applications to mitigate malware at install time. Kirin certification uses security rules, which are templates designed to conservatively match undesirable properties in security configuration bundled with applications. We use a variant of security requirements engineering techniques to perform an in-depth security analysis of Android to produce a set of rules that match malware characteristics. In a sample of 311 of the most popular applications downloaded from the official Android Market, Kirin and our rules found 5 applications that implement dangerous functionality and therefore should be installed with extreme caution. Upon close inspection, another five applications asserted dangerous rights, but were within the scope of reasonable functional needs. These results indicate that security configuration bundled with Android applications provides practical means of detecting malware.", "title": "On lightweight mobile phone application certification"}, "265b6313093a3c5ea4a5c75096592739f2999f05": {"paper_id": "265b6313093a3c5ea4a5c75096592739f2999f05", "abstract": "A malware detector is a system that attempts to determine whether a program has malicious intent. In order to evade detection, malware writers (hackers) frequently use obfuscation to morph malware. Malware detectors that use a pattern-matching approach (such as commercial virus scanners) are susceptible to obfuscations used by hackers. The fundamental deficiency in the pattern-matching approach to malware detection is that it is purely syntactic and ignores the semantics of instructions. In this paper, we present a malware-detection algorithm that addresses this deficiency by incorporating instruction semantics to detect malicious program traits. Experimental evaluation demonstrates that our malware-detection algorithm can detect variants of malware with a relatively low run-time overhead. Moreover our semantics-aware malware detection algorithm is resilient to common obfuscations used by hackers.", "title": "Semantics-aware malware detection"}, "0a5a866958424aee2c0ed185e007b12892d89726": {"paper_id": "0a5a866958424aee2c0ed185e007b12892d89726", "abstract": "Malicious code detection is a crucial component of any defense mechanism. In this paper, we present a unique viewpoint on malicious code detection. We regard malicious code detection as an obfuscation-deobfuscation game between malicious code writers and researchers working on malicious code detection. Malicious code writers attempt to obfuscate the malicious code to subvert the malicious code detectors, such as anti-virus software. We tested the resilience of three commercial virus scanners against code-obfuscation attacks. The results were surprising: the three commercial virus scanners could be subverted by very simple obfuscation transformations! We present an architecture for detecting malicious patterns in executables that is resilient to common obfuscation transformations. Experimental results demonstrate the efficacy of our prototype tool, SAFE (a static analyzer for executables).", "title": "Static Analysis of Executables to Detect Malicious Patterns"}, "5de7a8df56456d8c16401cf322bf9e3d4f26c668": {"paper_id": "5de7a8df56456d8c16401cf322bf9e3d4f26c668", "abstract": "A rootkit is a collection of tools used by intruders to keep the legitimate users and administrators of a compromised machine unaware of their presence. Originally, root-kits mainly included modified versions of system auditing programs (e.g., ps or netstat on a Unix system). However, for operating systems that support loadable kernel modules (e.g., Linux and Solaris), a new type of rootkit has recently emerged. These rootkits are implemented as kernel modules, and they do not require modification of user-space binaries to conceal malicious activity. Instead, these rootkits operate within the kernel, modifying critical data structures such as the system call table or the list of currently-loaded kernel modules. This paper presents a technique that exploits binary analysis to ascertain, at load time, if a module's behavior resembles the behavior of a rootkit. Through this method, it is possible to provide additional protection against this type of malicious modification of the kernel. Our technique relies on an abstract model of module behavior that is not affected by small changes in the binary image of the module. Therefore, the technique is resistant to attempts to conceal the malicious nature of a kernel module.", "title": "Detecting kernel-level rootkits through binary analysis"}, "2f35c2bf57242f5a755ac82635605100c14319da": {"paper_id": "2f35c2bf57242f5a755ac82635605100c14319da", "abstract": "Today's smartphone application markets host an ever increasing number of applications. The sheer number of applications makes their review a daunting task. We propose AppsPlayground for Android, a framework that automates the analysis smartphone applications. AppsPlayground integrates multiple components comprising different detection and automatic exploration techniques for this purpose. We evaluated the system using multiple large scale and small scale experiments involving real benign and malicious applications. Our evaluation shows that AppsPlayground is quite effective at automatically detecting privacy leaks and malicious functionality in applications.", "title": "AppsPlayground: automatic security analysis of smartphone applications"}, "3b798a146409aea990a323f1124d284ada244112": {"paper_id": "3b798a146409aea990a323f1124d284ada244112", "abstract": "With the introduction of Apple\u2019s iOS and Google\u2019s Android operating systems, the sales of smartphones have exploded. These smartphones have become powerful devices that are basically miniature versions of personal computers. However, the growing popularity and sophistication of smartphones have also increased concerns about the privacy of users who operate these devices. These concerns have been exacerbated by the fact that it has become increasingly easy for users to install and execute third-party applications. To protect its users from malicious applications, Apple has introduced a vetting process. This vetting process should ensure that all applications conform to Apple\u2019s (privacy) rules before they can be offered via the App Store. Unfortunately, this vetting process is not welldocumented, and there have been cases where malicious applications had to be removed from the App Store after", "title": "PiOS: Detecting Privacy Leaks in iOS Applications"}, "79136e5a1eb8799b068a8d59fddaf670b2a82cb8": {"paper_id": "79136e5a1eb8799b068a8d59fddaf670b2a82cb8", "abstract": "As a large and complex application platform, the World Wide Web is capable of delivering a broad range of sophisticated applications. However, many Web applications go through rapid development phases with extremely short turnaround time, making it difficult to eliminate vulnerabilities. Here we analyze the design of Web application security assessment mechanisms in order to identify poor coding practices that render Web applications vulnerable to attacks such as SQL injection and cross-site scripting. We describe the use of a number of software-testing techniques (including dynamic analysis, black-box testing, fault injection, and behavior monitoring), and suggest mechanisms for applying these techniques to Web applications. Real-world situations are used to test a tool we named the Web Application Vulnerability and Error Scanner (WAVES, an open-source project available at http://waves.sourceforge.net) and to compare it with other tools. Our results show that WAVES is a feasible platform for assessing Web application security.", "title": "Web application security assessment by fault injection and behavior monitoring"}, "7eaaac1bbda1944879c8a2e8808886cb352236a4": {"paper_id": "7eaaac1bbda1944879c8a2e8808886cb352236a4", "abstract": "Users increasingly rely on mobile applications for computational needs. Google Android is a popular mobile platform, hence the reliability of Android applications is becoming increasingly important. Many Android correctness issues, however, fall outside the scope of traditional verification techniques, as they are due to the novelty of the platform and its GUI-oriented application construction paradigm. In this paper we present an approach for automating the testing process for Android applications, with a focus on GUI bugs. We first conduct a bug mining study to understand the nature and frequency of bugs affecting Android applications; our study finds that GUI bugs are quite numerous. Next, we present techniques for detecting GUI bugs by automatic generation of test cases, feeding the application random events, instrumenting the VM, producing log/trace files and analyzing them post-run. We show how these techniques helped to re-discover existing bugs and find new bugs, and how they could be used to prevent certain bug categories. We believe our study and techniques have the potential to help developers increase the quality of Android applications.", "title": "Automating GUI testing for Android applications"}, "abf8b53346f688011514da540cc48e3499cb4015": {"paper_id": "abf8b53346f688011514da540cc48e3499cb4015", "abstract": "We examine two privacy controls for Android smartphones that empower users to run permission-hungry applications while protecting private data from being exfiltrated: (1) covertly substituting shadow data in place of data that the user wants to keep private, and (2) blocking network transmissions that contain data the user made available to the application for on-device use only. We retrofit the Android operating system to implement these two controls for use with unmodified applications. A key challenge of imposing shadowing and exfiltration blocking on existing applications is that these controls could cause side effects that interfere with user-desired functionality. To measure the impact of side effects, we develop an automated testing methodology that records screenshots of application executions both with and without privacy controls, then automatically highlights the visual differences between the different executions. We evaluate our privacy controls on 50 applications from the Android Market, selected from those that were both popular and permission-hungry. We find that our privacy controls can successfully reduce the effective permissions of the application without causing side effects for 66% of the tested applications. The remaining 34% of applications implemented user-desired functionality that required violating the privacy requirements our controls were designed to enforce; there was an unavoidable choice between privacy and user-desired functionality.", "title": "These aren't the droids you're looking for: retrofitting android to protect data from imperious applications"}, "c0b8f78df8dc6b21384979e065153187e6abaab8": {"paper_id": "c0b8f78df8dc6b21384979e065153187e6abaab8", "abstract": "GUNWOONG LEE is a Ph.D. candidate of Information Systems at the W. P. Carey School of Business, Arizona State University. His research interests include digital content management in mobile platforms, information and communication technology for development, and technologydriven healthcare innovations. His research has appeared in major conferences and journals including Decision Support Systems, International Conference on Information Systems, and America Conference on Information Systems. He has consulting experience with Korea Association of Game Industry, Korea Stock Exchange, and other companies and government agencies.", "title": "Determinants of Mobile Apps' Success: Evidence from the App Store Market"}, "d06fef6e6e7862c22a393f1801f9fdea392ad2c1": {"paper_id": "d06fef6e6e7862c22a393f1801f9fdea392ad2c1", "abstract": "Motivations to engage in retail shopping include both utilitarian and hedonic dimensions. Business to consumer e-commerce conducted via the mechanism of web-shopping provides an expanded opportunity for companies to create a cognitively and esthetically rich shopping environment in ways not readily imitable in the nonelectronic shopping world. In this article an attitudinal model is developed and empirically tested integrating constructs from technology acceptance research and constructs derived from models of web behavior. Results of two studies from two distinct categories of the interactive shopping environment support the differential importance of immersive, hedonic aspects of the new media as well as the more traditional utilitarian motivations. In addition, navigation, convenience, and the substitutability of the electronic environment to personally examining products were found to be important predictors of online shopping attitudes. Results are discussed in terms of insights for the creation of the online shopping webmosphere through more effective design of interactive retail shopping environments. \u00a9 2001 by New York University. All rights reserved.", "title": "Hedonic and utilitarian motivations for online retail shopping behavior"}, "1460a86302668eb1be7e32055c58e57e2f2a2f24": {"paper_id": "1460a86302668eb1be7e32055c58e57e2f2a2f24", "abstract": "Information systems can serve as intermediaries between the buyers and the sellers in a market, creating an \u201celectronic marketplace\u201d that lowers the buyers' cost to acquire information about seller prices and product offerings. As a result, electronic marketplaces reduce the inefficiencies caused by buyer search costs, in the process reducing the ability of sellers to extract monopolistic profits while increasing the ability of markets to optimally allocate productive resources. This article models the role of buyer search costs in markets with differentiated product offerings. The impact of reducing these search costs is analyzed in the context of an electronic marketplace, and the allocational efficiencies such a reduction can bring to a differentiated market are formalized. The resulting implications for the incentives of buyers, sellers and independent intermediaries to invest in electronic marketplaces are explored. Finally, the possibility to separate price information from product attribute information is introduced, and the implications of designing markets promoting competition along each of these dimensions are discussed. Copyright \u00a9 1997 by The Institute of Management Sciences", "title": "Reducing Buyer Search Costs : Implications for Electronic Marketplaces"}, "25395d80fd037b23cfcb8d72b2b549a2603d27ae": {"paper_id": "25395d80fd037b23cfcb8d72b2b549a2603d27ae", "abstract": "How can firms profitably give away free products? This paper provides a novel answer and articulates tradeoffs in a space of information product design. We introduce a formal model of two-sided network externalities based in textbook economics\u2014a mix of Katz & Shapiro network effects, price discrimination, and product differentiation. Externality-based complements, however, exploit a different mechanism than either tying or lock-in even as they help to explain many recent strategies such as those of firms selling operating systems, Internet browsers, games, music, and video. The model presented here argues for three simple but useful results. First, even in the absence of competition, a firm can rationally invest in a product it intends to give away into perpetuity. Second, we identify distinct markets for content providers and end consumers and show that either can be a candidate for a free good. Third, product coupling across markets can increase consumer welfare even as it increases firm profits. The model also generates testable hypotheses on the size and direction of network effects while offering insights to regulators seeking to apply antitrust law to network markets. ACKNOWLEDGMENTS: We are grateful to participants of the 1999 Workshop on Information Systems and Economics, the 2000 Association for Computing Machinery SIG E-Commerce, the 2000 International Conference on Information Systems, the 2002 Stanford Institute for Theoretical Economics (SITE) workshop on Internet Economics, the 2003 Insitut D\u2019Economie Industrielle second conference on \u201cThe Economics of the Software and Internet Industries,\u201d as well as numerous participants at university seminars. We wish to thank Tom Noe for helpful observations on oligopoly markets, Lones Smith, Kai-Uwe Kuhn, and Jovan Grahovac for corrections and model generalizations, Jeff MacKie-Mason for valuable feedback on model design and bundling, and Hal Varian for helpful comments on firm strategy and model implications. Frank Fisher provided helpful advice on and knowledge of the Microsoft trial. Jean Tirole provided useful suggestions and examples, particularly in regard to credit card markets. Paul Resnick proposed the descriptive term \u201cinternetwork\u201d externality to describe two-sided network externalities. Tom Eisenmann provided useful feedback and examples. We also thank Robert Gazzale, Moti Levi, and Craig Newmark for their many helpful observations. This research has been supported by NSF Career Award #IIS 9876233. For an earlier version of the paper that also addresses bundling and competition, please see \u201cInformation Complements, Substitutes, and Strategic Product Design,\u201d November 2000, http://ssrn.com/abstract=249585.", "title": "Two-Sided Network Effects: A Theory of Information Product Design"}, "5eae830442c129596ab7d9f5316f1aad2e8c178f": {"paper_id": "5eae830442c129596ab7d9f5316f1aad2e8c178f", "abstract": "Many if not most markets with network externalities are two-sided. To succeed, platforms in industries such as software, portals and media, payment systems and the Internet, must \u201cget both sides of the market on board \u201d. Accordingly, platforms devote much attention to their business model, that is to how they court each side while making money overall. The paper builds a model of platform competition with two-sided markets. It unveils the determinants of price allocation and enduser surplus for different governance structures (profit-maximizing platforms and not-for-profit joint undertakings), and compares the outcomes with those under an integrated monopolist and a Ramsey planner.", "title": "Platform Competition in Two-Sided Markets"}, "70b2879bf06b72faddb28923b136a8c3862d0c48": {"paper_id": "70b2879bf06b72faddb28923b136a8c3862d0c48", "abstract": "Internet-based technologies such as micropayments increasingly enable the sale and delivery of small units of information. This paper draws attention to the opposite strategy of bundling a large number of information goods, such as those increasingly available on the Internet, for a fixed price that does not depend on how many goods are actually used by the buyer. We analyze the optimal bundling strategies for a multiproduct monopolist, and we find that bundling very large numbers of unrelated information goods can be surprisingly profitable. The reason is that the law of large numbers makes it much easier to predict consumers' valuations for a bundle of goods than their valuations for the individual goods when sold separately. As a result, this \"predictive value of bundling\" makes it possible to achieve greater sales, greater economic efficiency and greater profits per good from a bundle of information goods than can be attained when the same goods are sold separately. Our results do not extend to most physical goods, as the marginal costs of production typically negate any benefits from the predictive value of bundling. While determining optimal bundling strategies for more than two goods is a notoriously difficult problem, we use statistical techniques to provide strong asymptotic results and bounds on profits for bundles of any arbitrary size. We show how our model can be used to analyze the bundling of complements and substitutes, bundling in the presence of budget constraints and bundling of goods with various types of correlations. We find that when different market segments of consumers differ systematically in their valuations for goods, simple bundling will no longer be optimal. However, by offering a menu of different bundles aimed at each market segment, a monopolist can generally earn substantially higher profits than would be possible without bundling. The predictions of our analysis appear to be consistent with empirical observations of the markets for Internet and on-line content, cable television programming, and copyrighted music. ________________________________________ We thank Timothy Bresnahan, Hung-Ken Chien, Frank Fisher, Michael Harrison, Paul Kleindorfer, Thomas Malone, Robert Pindyck, Nancy Rose, Richard Schmalensee, John Tsitsiklis, Hal Varian, Albert Wenger, Birger Wernerfelt, four anonymous reviewers and seminar participants at the University of California at Berkeley, MIT, New York University, Stanford University, University of Rochester, the Wharton School, the 1995 Workshop on Information Systems and Economics and the 1998 Workshop on Marketing Science and the Internet for many helpful suggestions. Any errors that remain are only our responsibility. BUNDLING INFORMATION GOODS Page 1", "title": "Bundling Information Goods : Pricing , Profits and Efficiency"}, "1537360a3e673a0eb28135279e97d462e5a792a4": {"paper_id": "1537360a3e673a0eb28135279e97d462e5a792a4", "abstract": "Social networks have the surprising property of being \"searchable\": Ordinary people are capable of directing messages through their network of acquaintances to reach a specific but distant target person in only a few steps. We present a model that offers an explanation of social network searchability in terms of recognizable personal identities: sets of characteristics measured along a number of social dimensions. Our model defines a class of searchable networks and a method for searching them that may be applicable to many network search problems, including the location of data files in peer-to-peer networks, pages on the World Wide Web, and information in distributed databases.", "title": "Identity and search in social networks."}, "40bbbaf3cb1459fd05fca6771f3a29d3fc9cfa19": {"paper_id": "40bbbaf3cb1459fd05fca6771f3a29d3fc9cfa19", "abstract": "This paper presents a camera prototype for 2D/3D image capture in low illumination conditions based on single-photon avalanche-diode (SPAD) image sensor for direct time-offlight (d-ToF). The imager is a 64\u00d764 array with in-pixel TDC for high frame rate acquisition. Circuit design techniques are combined to ensure successful 3D image capturing under low sensitivity conditions and high level of uncorrelated noise such as dark count and background illumination. Among them an innovative time gated front-end for the SPAD detector, a reverse start-stop scheme and real-time image reconstruction at Ikfps are incorporated by the imager. To the best of our knowledge, this is the first ToF camera based on a SPAD sensor fabricated and proved for 3D image reconstruction in a standard CMOS process without any opto-flavor or high voltage option. It has a depth resolution of 1cm at an illumination power from less than 6nW/mm2 down to 0.1nW/mm2.", "title": "Photon counting and direct ToF camera prototype based on CMOS SPADs"}, "f54a638733c1e278825dc8b5fd477514eb656f16": {"paper_id": "f54a638733c1e278825dc8b5fd477514eb656f16", "abstract": "Recent technology surveys identified flash light detection and ranging technology as the best choice for the navigation and landing of spacecrafts in extraplanetary missions, working from single-point altimeter to range-imaging camera mode. Among all available technologies for a 2D array of direct time-of-flight (DTOF) pixels, CMOS single-photon avalanche diodes (SPADs) represent the ideal candidate due to their rugged design and electronics integration. However, state-of-the-art SPAD imagers are not designed for operation over a wide variety of scenarios, including variable background light, very long to short range, or fast relative movement.", "title": "6.5 A 64\u00d764-pixel digital silicon photomultiplier direct ToF sensor with 100Mphotons/s/pixel background rejection and imaging/altimeter mode with 0.14% precision up to 6km for spacecraft navigation and landing"}, "7dee8be2a8eccb892253a94d2fcaa0aa9971cc54": {"paper_id": "7dee8be2a8eccb892253a94d2fcaa0aa9971cc54", "abstract": "This paper discusses the construction principles and performance of a pulsed time-of-flight (TOF) laser radar based on high-speed (FWHM ~100 ps) and high-energy (~1 nJ) optical transmitter pulses produced with a specific laser diode working in an \u201cenhanced gain-switching\u201d regime and based on single-photon detection in the receiver. It is shown by analysis and experiments that single-shot precision at the level of 2W3 cm is achievable. The effective measurement rate can exceed 10 kHz to a noncooperative target (20% reflectivity) at a distance of > 50 m, with an effective receiver aperture size of 2.5 cm2. The effect of background illumination is analyzed. It is shown that the gating of the SPAD detector is an effective means to avoid the blocking of the receiver in a high-level background illumination case. A brief comparison with pulsed TOF laser radars employing linear detection techniques is also made.", "title": "On Laser Ranging Based on High-Speed/Energy Laser Diode Pulses and Single-Photon Detection Techniques"}, "0e431052b86ba408622cbcf9728cf8423c6892fa": {"paper_id": "0e431052b86ba408622cbcf9728cf8423c6892fa", "abstract": "We report on the design and characterization of a multipurpose 64 \u00d7 32 CMOS single-photon avalanche diode (SPAD) array. The chip is fabricated in a high-voltage 0.35-\u03bcm CMOS technology and consists of 2048 pixels, each combining a very low noise (100 cps at 5-V excess bias) 30-\u03bcm SPAD, a prompt avalanche sensing circuit, and digital processing electronics. The array not only delivers two-dimensional intensity information through photon counting in either free-running (down to 10-\u03bcs integration time) or time-gated mode, but can also perform smart light demodulation with in-pixel background suppression. The latter feature enables phase-resolved imaging for extracting either three-dimensional depth-resolved images or decay lifetime maps, by measuring the phase shift between a modulated excitation light and the reflected photons. Pixel-level memories enable fully parallel processing and global-shutter readout, preventing motion artifacts (e.g., skew, wobble, motion blur) and partial exposure effects. The array is able to acquire very fast optical events at high frame-rate (up to 100 000 fps) and at single-photon level. Low-noise SPADs ensure high dynamic range (up to 110 dB at 100 fps) with peak photon detection efficiency of almost 50% at 410 nm. The SPAD imager provides different operating modes, thus, enabling both time-domain applications, like fluorescence lifetime imaging (FLIM) and fluorescence correlation spectroscopy, as well as frequency-domain FLIM and lock-in 3-D ranging for automotive vision and lidar.", "title": "100 000 Frames/s 64 \u00d7 32 Single-Photon Detector Array for 2-D Imaging and 3-D Ranging"}, "8f5460254f75f98f4171c6b6e53102ce2b160c79": {"paper_id": "8f5460254f75f98f4171c6b6e53102ce2b160c79", "abstract": "Edge detection plays a significant role in image processing and performance of high-level tasks such as image segmentation and object recognition depends on its efficiency. It is clear that accurate edge map generation is more difficult when images are corrupted with noise. Moreover, most of edge detection methods have parameters which must be set manually. Here we propose a new color edge detector based on a statistical test, which is robust to noise. Also, the parameters of this method will be set automatically based on image content. To show the effectiveness of the proposed method, four state-of-the-art edge detectors are implemented and the results are compared. Experimental results on five of the most well-known edge detection benchmarks show that the proposed method is robust to noise. The performance of our method for lower levels of noise is very comparable to the existing approaches, whose performances highly depend on their parameter tuning stage. However, for higher levels of noise, the observed results significantly highlight the superiority of the proposed method over the existing edge detection methods, both quantitatively and qualitatively.", "title": "A Robust Statistical Color Edge Detection for Noisy Images"}, "31864e13a9b3473ebb07b4f991f0ae3363517244": {"paper_id": "31864e13a9b3473ebb07b4f991f0ae3363517244", "abstract": "This paper describes a computational approach to edge detection. The success of the approach depends on the definition of a comprehensive set of goals for the computation of edge points. These goals must be precise enough to delimit the desired behavior of the detector while making minimal assumptions about the form of the solution. We define detection and localization criteria for a class of edges, and present mathematical forms for these criteria as functionals on the operator impulse response. A third criterion is then added to ensure that the detector has only one response to a single edge. We use the criteria in numerical optimization to derive detectors for several common image features, including step edges. On specializing the analysis to step edges, we find that there is a natural uncertainty principle between detection and localization performance, which are the two main goals. With this principle we derive a single operator shape which is optimal at any scale. The optimal detector has a simple approximate implementation in which edges are marked at maxima in gradient magnitude of a Gaussian-smoothed image. We extend this simple detector using operators of several widths to cope with different signal-to-noise ratios in the image. We present a general method, called feature synthesis, for the fine-to-coarse integration of information from operators at different scales. Finally we show that step edge detector performance improves considerably as the operator point spread function is extended along the edge.", "title": "A Computational Approach to Edge Detection"}, "f62fbec04ed3a4625a1bc4e83e6766e9b74673cc": {"paper_id": "f62fbec04ed3a4625a1bc4e83e6766e9b74673cc", "abstract": "A theory of edge detection is presented. The analysis proceeds in two parts. (1) Intensity changes, which occur in a natural image over a wide range of scales, are detected separately at different scales. An appropriate filter for this purpose at a given scale is found to be the second derivative of a Gaussian, and it is shown that, provided some simple conditions are satisfied, these primary filters need not be orientation-dependent. Thus, intensity changes at a given scale are best detected by finding the zero values of delta 2G(x,y)*I(x,y) for image I, where G(x,y) is a two-dimensional Gaussian distribution and delta 2 is the Laplacian. The intensity changes thus discovered in each of the channels are then represented by oriented primitives called zero-crossing segments, and evidence is given that this representation is complete. (2) Intensity changes in images arise from surface discontinuities or from reflectance or illumination boundaries, and these all have the property that they are spatially. Because of this, the zero-crossing segments from the different channels are not independent, and rules are deduced for combining them into a description of the image. This description is called the raw primal sketch. The theory explains several basic psychophysical findings, and the operation of forming oriented zero-crossing segments from the output of centre-surround delta 2G filters acting on the image forms the basis for a physiological model of simple cells (see Marr & Ullman 1979).", "title": "Theory of edge detection."}, "4834ef7df6b94ad2d5ccf5c4714be1f98c54b69e": {"paper_id": "4834ef7df6b94ad2d5ccf5c4714be1f98c54b69e", "abstract": "The Gaussian filter has been used extensively in image processing and computer vision for many years. In this survey paper, we discuss the various features of this operator that make it the filter of choice in the area of edge detection. Despite these desirable features of the Gaussian filter, edge detection algorithms which use it suffer from many problems. We will review several linear and nonlinear Gaussian-based edge detection methods.", "title": "Gaussian-based edge-detection methods - a survey"}, "3dbe40033c570f4ea9c88ffbee1ad26a6b7d0df6": {"paper_id": "3dbe40033c570f4ea9c88ffbee1ad26a6b7d0df6", "abstract": "In practice the relevant details of images exist only over a restricted range of scale. Hence it is important to study the dependence of image structure on the level of resolution. It seems clear enough that visual perception treats images on several levels of resolution simultaneously and that this fact must be important for the study of perception. However, no applicable mathematically formulated theory to deal with such problems appers to exist. In this paper it is shown that any image can be embedded in a one-parameter family of derived images (with resolution as the parameter) in essentially only one unique way if the constraint that no spurious detail should be generated when the resolution is diminished, is applied. The structure of this family is governed by the well known diffusion equation (a parabolic, linear, partial differential equation of the second order). As such the structure fits into existing theories that treat the front end of the visual system as a continuous tack of homogeneous layer, characterized by iterated local processing schemes. When resolution is decreased the images becomes less articulated because the extrem (\u201clight and dark blobs\u201d) disappear one after the other. This erosion of structure is a simple process that is similar in every case. As a result any image can be described as a juxtaposed and nested set of light and dark blobs, wherein each blod has a limited range of resolution in which it manifests itself. The structure of the family of derived images permits a derivation of the sampling density required to sample the image at multiple scales of resolution. The natural scale along the resolution axis (leading to an informationally uniform sampling density) is logarithmic, thus the structure is apt for the description of size invariances.", "title": "The structure of images"}, "6890cd507b8052d99d287563161e05bb447d6802": {"paper_id": "6890cd507b8052d99d287563161e05bb447d6802", "abstract": "When computing descriptors of image data, the type of information that can be extracted may be strongly dependent on the scales at which the image operators are applied. This article presents a systematic methodology for addressing this problem. A mechanism is presented for automatic selection of scale levels when detecting one-dimensional image features, such as edges and ridges. A novel concept of a scale-space edge is introduced, defined as a connected set of points in scale-space at which: (i) the gradient magnitude assumes a local maximum in the gradient direction, and (ii) a normalized measure of the strength of the edge response is locally maximal over scales. An important consequence of this definition is that it allows the scale levels to vary along the edge. Two specific measures of edge strength are analyzed in detail, the gradient magnitude and a differential expression derived from the third-order derivative in the gradient direction. For a certain way of normalizing these differential descriptors, by expressing them in terms of so-called \u03b3-normalized derivatives, an immediate consequence of this definition is that the edge detector will adapt its scale levels to the local image structure. Specifically, sharp edges will be detected at fine scales so as to reduce the shape distortions due to scale-space smoothing, whereas sufficiently coarse scales will be selected at diffuse edges, such that an edge model is a valid abstraction of the intensity profile across the edge. Since the scale-space edge is defined from the intersection of two zero-crossing surfaces in scale-space, the edges will by definition form closed curves. This simplifies selection of salient edges, and a novel significance measure is proposed, by integrating the edge strength along the edge. Moreover, the scale information associated with each edge provides useful clues to the physical nature of the edge. With just slight modifications, similar ideas can be used for formulating ridge detectors with automatic selection, having the characteristic property that the selected scales on a scale-space ridge instead reflect the width of the ridge. It is shown how the methodology can be implemented in terms of straightforward visual front-end operations, and the validity of the approach is supported by theoretical analysis as well as experiments on real-world and synthetic data.", "title": "Edge Detection and Ridge Detection with Automatic Scale Selection"}, "a26f893c224ed6e3df1f37479de0e774a4cae237": {"paper_id": "a26f893c224ed6e3df1f37479de0e774a4cae237", "abstract": "The extrema in a signal and its first few derivatives provide a useful general-purpose qualitative description for many kinds of signals. A fundamental problem in computing such descriptions is scale: a derivative must be taken over some neighborhood, but there is seldom a principled basis for choosing its size. Scale-space filtering is a method that describes signals qualitatively, managing the ambiguity of scale in an organized and natural way. The signal is first expanded by convolution with gaussian masks over a continuum of sizes. This \"scale-space\" image is then collapsed, using its qualitative structure, into a tree providing a concise but complete qualitative description covering all scales of observation. The description is further refined by applying a stability criterion, to identify events that persist of large changes in scale.", "title": "Scale-Space Filtering"}, "09c5b100f289a3993d91a66116e35ee95e99acc0": {"paper_id": "09c5b100f289a3993d91a66116e35ee95e99acc0", "abstract": "t\u2014This paper describes a new method for the segmentation and extraction of cardiac MRI s. Our method is based on the novel use of a 2D bank. By convolving the tagged input image with ilters, the tagging lines are automatically enhanced ted out. We design the Gabor filter bank based on age\u2019s spatial and frequency characteristics. The is a combination of each filter\u2019s response in the bank. We demonstrate that compared to bandpass ds such as HARP, this method results in robust and mentation of the tagging lines.", "title": "Segmenting cardiac MRI tagging lines using Gabor filter banks"}, "3c28462bd9996c18d6ad54fc6e3780eb7b88fb71": {"paper_id": "3c28462bd9996c18d6ad54fc6e3780eb7b88fb71", "abstract": "Gabor filters have been successfully applied to a broad range of image processing tasks. The present paper considers the design of a single filter to segment a two-texture image. A new efficient algorithm for Gabor-filter design is presented, along with methods for estimating filter output statistics. The algorithm draws upon previous results that showed that the output of a Gabor-filtered texture is modeled well by a Rician distribution. A measure of the total output power is used to select the center frequency of the filter and is used to estimate the Rician statistics of the Gabor-filtered image. The method is further generalized to include the statistics of postfiltered outputs that are generated by a Gaussian filtering operation following the Gabor filter. The new method typically requires an order of magnitude less computation to design a filter than a previously proposed method. Experimental results demonstrate the efficacy of the method. Image segmentation Texture Gabor filters Rician statistics Texture segmentation Image statistics Texture analysis", "title": "Efficient Gabor filter design for texture segmentation"}, "b1cfe7f8b8557b03fa38036030f09b448d925041": {"paper_id": "b1cfe7f8b8557b03fa38036030f09b448d925041", "abstract": "-This paper presents a texture segmentation algorithm inspired by the multi-channel filtering theory for visual information processing in the early stages of human visual system. The channels are characterized by a bank of Gabor filters that nearly uniformly covers the spatial-frequency domain, and a systematic filter selection scheme is proposed, which is based on reconstruction of the input image from the filtered images. Texture features are obtained by subjecting each (selected) filtered image to a nonlinear transformation and computing a measure of \"energy\" in a window around each pixel. A square-error clustering algorithm is then used to integrate the feature images and produce a segmentation. A simple procedure to incorporate spatial information in the clustering process is proposed. A relative index is used to estimate the \"'true\" number of texture categories. Texture segmentation Multi-channel filtering Clustering Clustering index Gabor filters Wavelet transform I . I N T R O D U C T I O N Image segmentation is a difficult yet very important task in many image analysis or computer vision applications. Differences in the mean gray level or in color in small neighborhoods alone are not always sufficient for image segmentation. Rather, one has to rely on differences in the spatial arrangement of gray values of neighboring pixels-that is, on differences in texture. The problem of segmenting an image based on textural cues is referred to as the texture segmentation problem. Texture segmentation involves identifying regions with \"uniform\" textures in a given image. Appropriate measures of texture are needed in order to decide whether a given region has uniform texture. Sklansky (o has suggested the following definition of texture which is appropriate in the segmentation context: \"A region in an image has a constant texture if a set of local statistics or other local properties of the picture are constant, slowly varying, or approximately periodic\". Texture, therefore, has both local and global connotations--i t is characterized by invariance of certain local measures or properties over an image region. The diversity of natural and artificial textures makes it impossible to give a universal definition of texture. A large number of techniques for analyzing image texture has been proposed in the past two decades/2,3) In this paper, we focus on a particular approach to texture analysis which is referred to as \u00b0 This work was supported in part by the National Science Foundation infrastructure grant CDA-8806599, and by a grant from E. I. Du Pont De Nemours & Company Inc. the multi-channel filtering approach. This approach is inspired by a multi-channel filtering theory for processing visual information in the early stages of the human visual system. First proposed by Campbell and Robson (4) the theory holds that the visual system decomposes the retinal image into a number of filtered images, each of which contains intensity variations over a narrow range of frequency (size) and orientation. The psychophysical experiments that suggested such a decomposition used various grating patterns as stimuli and were based on adaptation techniques. I~l Subsequent psychophysiological experiments provided additional evidence supporting the theory. De Valois et al. ,(5) for example, recorded the response of simple cells in the visual cortex of the Macaque monkey to sinusoidal gratings with different frequencies and orientations. It was observed that each cell responds to a narrow range of frequency and orientation only. Therefore, it appears that there are mechanisms in the visual cortex of mammals that are tuned to combinations of frequency and orientation in a narrow range. These mechanisms are often referred to as channels, and are appropriately interpreted as band-pass filters. The multi-channel filtering approach to texture analysis is intuitively appealing because it allows us to exploit differences in dominant sizes and orientations of different textures. Today, the need for a multi-resolution approach to texture analysis is well recognized. While other approaches to texture analysis have had to be extended to accommodate this paradigm, the multi-channel filtering approach, is inherently multi-resolutional. Another important", "title": "Unsupervised texture segmentation using Gabor filters"}, "9ecacb83f18dd9c520a66c60fef5fd3dd02dd5a1": {"paper_id": "9ecacb83f18dd9c520a66c60fef5fd3dd02dd5a1", "abstract": "A computational approach for analyzing visible textures is described. Textures are modeled as irradiance patterns containing a limited range of spatial frequencies, where mutually distinct textures differ significantly in their dominant characterizing frequencies. By encoding images into multiple narrow spatial frequency and orientation channels, the slowly-varying channel envelopes (amplitude and phase) are used to segregate textural regions of different spatial frequency, orientation, or phase characteristics. Thus, an interpretation of image texture as a region code, or currier of region information, is", "title": "Multichannel Texture Analysis Using Localized Spatial Filters"}, "6b125801726b464b9fcc5095e1b31527370ae0fb": {"paper_id": "6b125801726b464b9fcc5095e1b31527370ae0fb", "abstract": "Base stations represent the main contributor to the energy consumption of a mobile cellular network. Since traffic load in mobile networks significantly varies during a working or weekend day, it is important to quantify the influence of these variations on the base station power consumption. Therefore, this paper investigates changes in the instantaneous power consumption of GSM (Global System for Mobile Communications) and UMTS (Universal Mobile Telecommunications System) base stations according to their respective traffic load. The real data in terms of the power consumption and traffic load have been obtained from continuous measurements performed on a fully operated base station site. Measurements show the existence of a direct relationship between base station traffic load and power consumption. According to this relationship, we develop a linear power consumption model for base stations of both technologies. This paper also gives an overview of the most important concepts which are being proposed to make cellular networks more energy-efficient.", "title": "Measurements and Modelling of Base Station Power Consumption under Real Traffic Loads \u2020"}, "30235fb06030f78e83868c04ca0fb9d9e03d964a": {"paper_id": "30235fb06030f78e83868c04ca0fb9d9e03d964a", "abstract": "The advanced development in wireless sensor networks can be used in monitoring various parameters in agriculture. Due to uneven natural distribution of rain water it is very difficult for farmers to monitor and control the distribution of water to agriculture field in the whole farm or as per the requirement of the crop. There is no ideal irrigation method for all weather conditions, soil structure and variety of crops cultures. Farmers suffer large financial losses because of wrong prediciton of weather and incorrect irrigation methods. In this context, with the evolution of miniaturized sensor devices coupled with wireless technologies, it is possible remotely monitor parameters such as moisure, temperature and humidity. In this paper it is proposed to design, develop and implement a wireless sensor network connected to a central node using ZigBee, which in turn is connected to a Central Monitoring Station (CMS) through General Packet Radio Service (GPRS) or Global System for Mobile (GSM) technologies. The system also obtains Global Positionting System (GPS) parameters related to the field and sends them to a central monitoring station. This system is expected to help farmers in evaluating soil conditions and act accordingly.", "title": "Wireless Sensor Based Remote Monitoring System for Agriculture Using ZigBee and GPS"}, "b7b741d9677cd4176b6ec9af9a94b4fe9a25eea7": {"paper_id": "b7b741d9677cd4176b6ec9af9a94b4fe9a25eea7", "abstract": "This paper proposes an agricultural environment monitoring server system for monitoring information concerning an outdoors agricultural production environment utilizing Wireless Sensor Network (WSN) technology. The proposed agricultural environment monitoring server system collects environmental and soil information on the outdoors through WSN-based environmental and soil sensors, collects image information through CCTVs, and collects location information using GPS modules. This collected information is converted into a database through the agricultural environment monitoring server consisting of a sensor manager, which manages information collected from the WSN sensors, an image information manager, which manages image information collected from CCTVs, and a GPS manager, which processes location information of the agricultural environment monitoring server system, and provides it to producers. In addition, a solar cell-based power supply is implemented for the server system so that it could be used in agricultural environments with insufficient power infrastructure. This agricultural environment monitoring server system could even monitor the environmental information on the outdoors remotely, and it could be expected that the use of such a system could contribute to increasing crop yields and improving quality in the agricultural field by supporting the decision making of crop producers through analysis of the collected information.", "title": "Study on an Agricultural Environment Monitoring Server System using Wireless Sensor Networks"}, "8a9a4cf24ec45d6d7c9339ffb81031f7677568e0": {"paper_id": "8a9a4cf24ec45d6d7c9339ffb81031f7677568e0", "abstract": "Using ethnographic research methods, the authors studied the structure of the needs and priorities of people working in a vineyard to gain a better understanding of the potential for sensor networks in agriculture. We discuss an extended study of vineyard workers and their work practices to assess the potential for sensor network systems to aid work in this environment. The major purpose is to find new directions and new topics that pervasive computing and sensor networks might address in designing technologies to support a broader range of users and activities.", "title": "Vineyard computing: sensor networks in agricultural production"}, "7765ba63bd5f9b8c46edbfe492a18e974cd87db6": {"paper_id": "7765ba63bd5f9b8c46edbfe492a18e974cd87db6", "abstract": "SVM is extensively used in pattern recognition because of its capability to classify future unseen data and its\u2019 good generalization performance. Several algorithms and models have been proposed for pattern recognition that uses SVM for classification. These models proved the efficiency of SVM in pattern recognition. Researchers have compared their results for SVM with other traditional empirical risk minimization techniques, such as Artificial Neural Network, Decision tree, etc. Comparison results show that SVM is superior to these techniques. Also, different variants of SVM are developed for enhancing the performance. In this paper, SVM is briefed and some of the pattern recognition applications of SVM are surveyed and briefly summarized. Keyword Hyperplane, Pattern Recognition, Quadratic Programming Problem, Support Vector Machines.", "title": "Review : Support Vector Machines in Pattern Recognition"}, "a8cd4bc012e08a2c6b2ca7618ec411a8d9e523e7": {"paper_id": "a8cd4bc012e08a2c6b2ca7618ec411a8d9e523e7", "abstract": "In this paper we describe the application of mixtures of experts on gender and ethnic classification of human faces, and pose classification, and show their feasibility on the FERET database of facial images. The FERET database allows us to demonstrate performance on hundreds or thousands of images. The mixture of experts is implemented using the \"divide and conquer\" modularity principle with respect to the granularity and/or the locality of information. The mixture of experts consists of ensembles of radial basis functions (RBFs). Inductive decision trees (DTs) and support vector machines (SVMs) implement the \"gating network\" components for deciding which of the experts should be used to determine the classification output and to restrict the support of the input space. Both the ensemble of RBF's (ERBF) and SVM use the RBF kernel (\"expert\") for gating the inputs. Our experimental results yield an average accuracy rate of 96% on gender classification and 92% on ethnic classification using the ERBF/DT approach from frontal face images, while the SVM yield 100% on pose classification.", "title": "Mixture of experts for classification of gender, ethnic origin, and pose of human faces"}, "595640253ffdfd12e04ac57bd78753f936a7cfad": {"paper_id": "595640253ffdfd12e04ac57bd78753f936a7cfad", "abstract": "This paper presents a general theoretical framework for ensemble methods of constructing signiicantly improved regression estimates. Given a population of regression estimators, we construct a hybrid estimator which is as good or better in the MSE sense than any estimator in the population. We argue that the ensemble method presented has several properties: 1) It eeciently uses all the networks of a population-none of the networks need be discarded. 2) It eeciently uses all the available data for training without over-tting. 3) It inherently performs regularization by smoothing in functional space which helps to avoid over-tting. 4) It utilizes local minima to construct improved estimates whereas other neural network algorithms are hindered by local minima. 5) It is ideally suited for parallel computation. 6) It leads to a very useful and natural measure of the number of distinct estimators in a population. 7) The optimal parameters of the ensemble estimator are given in closed form. Experimental results are provided which show that the ensemble method dramatically improves neural network performance on diicult real-world optical character recognition tasks.", "title": "When Networks Disagree : Ensemble Methods for Hybrid Neural Networks"}, "31fd53c2942135e35f83566d58e693ffedec2369": {"paper_id": "31fd53c2942135e35f83566d58e693ffedec2369", "abstract": "This paper describes an approach for the problem of face pose discrimination using Support Vector Machines (SVM). Face pose discrimination means that one can label the face image as one of several known poses. Face images are drawn from the standard FERET data base. The training set consists of 150 images equally distributed among frontal, approximately 33.75 rotated left and right poses, respectively, and the test set consists of 450 images again equally distributed among the three different types of poses. SVM achieved perfect accuracy 100% discriminating between the three possible face poses on unseen test data, using either polynomials of degree 3 or Radial Basis Functions (RBFs) as kernel approximation functions.", "title": "Face pose discrimination using support vector machines (SVM)"}, "d1ee87290fa827f1217b8fa2bccb3485da1a300e": {"paper_id": "d1ee87290fa827f1217b8fa2bccb3485da1a300e", "abstract": "Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy.", "title": "Bagging predictors"}, "0be360a2964c4bb91aaad0cc6d1baa6639746028": {"paper_id": "0be360a2964c4bb91aaad0cc6d1baa6639746028", "abstract": "Humans detect and identify faces in a scene with little or no effort . However, building an automated system that accomplishes this task is very difficult . There are several related Subproblems : detection of a pattern as a face . identification of the face, analysis of facial expressions, and classification based on physical features of the face . A system that performs these operations will find many applications, e .g . criminal identification, authentication in secure systems, etc . Most of the work to date has been in identification . This paper surveys the past work in solving these problems . The capability of the human visual system with respect to these problems is also discussed . It is meant to serve as a guide for an automated system . Some new approaches to these problems are also briefly discussed . Face detection Face identification Facial expressions Classification Facial features", "title": "Automatic recognition and analysis of human faces and facial expressions: a survey"}, "1f4b7d91f1f41b950c23a7a3c0dea58728879b26": {"paper_id": "1f4b7d91f1f41b950c23a7a3c0dea58728879b26", "abstract": "The objective of this paper is to provide a comparison among permanent magnet (PM) wind generators of different topologies. Seven configurations are chosen for the comparison, consisting of both radial-flux and axial-flux machines. The comparison is done at seven power levels ranging from 1 to 200 kW. The basis for the comparison is discussed and implemented in detail in the design procedure. The criteria used for comparison are considered to be critical for the efficient deployment of PM wind generators. The design data are optimized and verified by finite-element analysis and commercial generator test results. For a given application, the results provide an indication of the best-suited machine.", "title": "PM wind generator topologies"}, "442423b5d855d4ce19ffbd67168ee38c245e05d8": {"paper_id": "442423b5d855d4ce19ffbd67168ee38c245e05d8", "abstract": "Supervised learning with large scale labeled datasets and deep layered models has made a paradigm shift in diverse areas in learning and recognition. However, this approach still suffers generalization issues under the presence of a domain shift between the training and the test data distribution. In this regard, unsupervised domain adaptation algorithms have been proposed to directly address the domain shift problem. In this paper, we approach the problem from a transductive perspective. We incorporate the domain shift and the transductive target inference into our framework by jointly solving for an asymmetric similarity metric and the optimal transductive target label assignment. We also show that our model can easily be extended for deep feature learning in order to learn features which are discriminative in the target domain. Our experiments show that the proposed method significantly outperforms state-of-the-art algorithms in both object recognition and digit classification experiments by a large margin.", "title": "Unsupervised Transductive Domain Adaptation"}, "16fc3294f7d089e4e2388112d404a2168bbe00d0": {"paper_id": "16fc3294f7d089e4e2388112d404a2168bbe00d0", "abstract": "Learning domain-invariant features is of vital importance to unsupervised domain adaptation, where classifiers trained on the source domain need to be adapted to a different target domain for which no labeled examples are available. In this paper, we propose a novel approach for learning such features. The central idea is to exploit the existence of landmarks, which are a subset of labeled data instances in the source domain that are distributed most similarly to the target domain. Our approach automatically discovers the landmarks and use them to bridge the source to the target by constructing provably easier auxiliary domain adaptation tasks. The solutions of those auxiliary tasks form the basis to compose invariant features for the original task. We show how this composition can be optimized discriminatively without requiring labels from the target domain. We validate the method on standard benchmark datasets for visual object recognition and sentiment analysis of text. Empirical results show the proposed method outperforms the state-ofthe-art significantly.", "title": "Connecting the Dots with Landmarks: Discriminatively Learning Domain-Invariant Features for Unsupervised Domain Adaptation"}, "091c0a0a785f638ecf10d216258708eb9677beb0": {"paper_id": "091c0a0a785f638ecf10d216258708eb9677beb0", "abstract": "We propose a framework for analyzing and comparing distributions, allowing us to design statistical tests to determine if two samples are drawn from different distributions. Our test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert space (RKHS). We present two tests based on large deviation bounds for the test statistic, while a third is based on the asymptotic distribution of this statistic. The test statistic can be computed in quadratic time, although efficient linear time approximations are available. Several classical metrics on distributions are recovered when the function space used to compute the difference in expectations is allowed to be more general (eg. a Banach space). We apply our two-sample tests to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where they perform strongly. Excellent performance is also obtained when comparing distributions over graphs, for which these are the first such tests.", "title": "A Kernel Method for the Two-Sample Problem"}, "7af5a5eb1e472ab23c6d4c3dd9b37640533ec89c": {"paper_id": "7af5a5eb1e472ab23c6d4c3dd9b37640533ec89c", "abstract": "Discriminative learning methods for classification perfor m well when training and test data are drawn from the same distribution. In many situa tions, though, we have labeled training data for a sourcedomain, and we wish to learn a classifier which performs well on atargetdomain with a different distribution. Under what conditions can we adapt a classifier trained on the source dom ain for use in the target domain? Intuitively, a good feature representation is a crucial factor in the success of domain adaptation. We formalize this intuition t heoretically with a generalization bound for domain adaption. Our theory illus trates the tradeoffs inherent in designing a representation for domain adaptation nd gives a new justification for a recently proposed model. It also points toward a promising new model for domain adaptation: one which explicitly minimizes the d ifference between the source and target domains, while at the same time maximizing the margin of the training set.", "title": "Analysis of Representations for Domain Adaptation"}, "94db635f54d25bdb95edb42185aca93ba53b051b": {"paper_id": "94db635f54d25bdb95edb42185aca93ba53b051b", "abstract": "Automatic sentiment classification has been extensively studied and applied in recent years. However, sentiment is expressed differently in different domains, and annotating corpora for every possible domain of interest is impractical. We investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products. First, we extend to sentiment classification the recently-proposed structural correspondence learning (SCL) algorithm, reducing the relative error due to adaptation between domains by an average of 30% over the original SCL algorithm and 46% over a supervised baseline. Second, we identify a measure of domain similarity that correlates well with the potential for adaptation of a classifier from one domain to another. This measure could for instance be used to select a small set of domains to annotate whose trained classifiers would transfer well to many other domains.", "title": "Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification"}, "02227c94dd41fe0b439e050d377b0beb5d427cda": {"paper_id": "02227c94dd41fe0b439e050d377b0beb5d427cda", "abstract": "Detecting and reading text from natural images is a hard computer vision task that is central to a variety of emerging applications. Related problems like document character recognition have been widely studied by computer vision and machine learning researchers and are virtually solved for practical applications like reading handwritten digits. Reliably recognizing characters in more complex scenes like photographs, however, is far more difficult: the best existing methods lag well behind human performance on the same tasks. In this paper we attack the problem of recognizing digits in a real application using unsupervised feature learning methods: reading house numbers from street level photos. To this end, we introduce a new benchmark dataset for research use containing over 600,000 labeled digits cropped from Street View images. We then demonstrate the difficulty of recognizing these digits when the problem is approached with hand-designed features. Finally, we employ variants of two recently proposed unsupervised feature learning methods and find that they are convincingly superior on our benchmarks.", "title": "Reading Digits in Natural Images with Unsupervised Feature Learning"}, "32b8f58a038df83138435b12a499c8bf0de13811": {"paper_id": "32b8f58a038df83138435b12a499c8bf0de13811", "abstract": "This paper focuses on the problem of word detection and recognition in natural images. The problem is significantly more challenging than reading text in scanned documents, and has only recently gained attention from the computer vision community. Sub-components of the problem, such as text detection and cropped image word recognition, have been studied in isolation [7, 4, 20]. However, what is unclear is how these recent approaches contribute to solving the end-to-end problem of word recognition. We fill this gap by constructing and evaluating two systems. The first, representing the de facto state-of-the-art, is a two stage pipeline consisting of text detection followed by a leading OCR engine. The second is a system rooted in generic object recognition, an extension of our previous work in [20]. We show that the latter approach achieves superior performance. While scene text recognition has generally been treated with highly domain-specific methods, our results demonstrate the suitability of applying generic computer vision methods. Adopting this approach opens the door for real world scene text recognition to benefit from the rapid advances that have been taking place in object recognition.", "title": "End-to-end scene text recognition"}, "31b58ced31f22eab10bd3ee2d9174e7c14c27c01": {"paper_id": "31b58ced31f22eab10bd3ee2d9174e7c14c27c01", "abstract": "With the advent of the Internet, billions of images are now freely available online and constitute a dense sampling of the visual world. Using a variety of non-parametric methods, we explore this world with the aid of a large dataset of 79,302,017 images collected from the Internet. Motivated by psychophysical results showing the remarkable tolerance of the human visual system to degradations in image resolution, the images in the dataset are stored as 32 x 32 color images. Each image is loosely labeled with one of the 75,062 non-abstract nouns in English, as listed in the Wordnet lexical database. Hence the image database gives a comprehensive coverage of all object categories and scenes. The semantic information from Wordnet can be used in conjunction with nearest-neighbor methods to perform object classification over a range of semantic levels minimizing the effects of labeling noise. For certain classes that are particularly prevalent in the dataset, such as people, we are able to demonstrate a recognition performance comparable to class-specific Viola-Jones style detectors.", "title": "80 Million Tiny Images: A Large Data Set for Nonparametric Object and Scene Recognition"}, "0a59337568cbf74e7371fb543f7ca34bbc2153ac": {"paper_id": "0a59337568cbf74e7371fb543f7ca34bbc2153ac", "abstract": "In real-world applications of visual recognition, many factors - such as pose, illumination, or image quality - can cause a significant mismatch between the source domain on which classifiers are trained and the target domain to which those classifiers are applied. As such, the classifiers often perform poorly on the target domain. Domain adaptation techniques aim to correct the mismatch. Existing approaches have concentrated on learning feature representations that are invariant across domains, and they often do not directly exploit low-dimensional structures that are intrinsic to many vision datasets. In this paper, we propose a new kernel-based method that takes advantage of such structures. Our geodesic flow kernel models domain shift by integrating an infinite number of subspaces that characterize changes in geometric and statistical properties from the source to the target domain. Our approach is computationally advantageous, automatically inferring important algorithmic parameters without requiring extensive cross-validation or labeled data from either domain. We also introduce a metric that reliably measures the adaptability between a pair of source and target domains. For a given target domain and several source domains, the metric can be used to automatically select the optimal source domain to adapt and avoid less desirable ones. Empirical studies on standard datasets demonstrate the advantages of our approach over competing methods.", "title": "Geodesic flow kernel for unsupervised domain adaptation"}, "490020c0d4fa1eb85fe353add5713e49f08c628d": {"paper_id": "490020c0d4fa1eb85fe353add5713e49f08c628d", "abstract": "We demonstrate the performance of our interest point detector/descriptor scheme SURF \u2013 Speeded Up Robust Features \u2013 in an application that finds correspondences to a reference image in realtime. The user takes a reference image with a handheld video camera and then moves the camera around the object. The system identifies interest points in every newly acquired image and matches them with the ones in the reference image. The matches are then displayed on a computer screen. This is repeated subsequently for every frame. This application shows the speed and accuracy of SURF. We don\u2019t exploit tracking. For every new image an independent set of interest points is computed and matched with the ones in the reference image, using standard wide baseline matching techniques.", "title": "SURF: Speeded Up Robust Features"}, "632a3e8971f6a26cf127a03689c28399d2fce7d8": {"paper_id": "632a3e8971f6a26cf127a03689c28399d2fce7d8", "abstract": "This paper addresses pattern classification in the framework of domain adaptation by considering methods that solve problems in which training data are assumed to be available only for a source domain different (even if related) from the target domain of (unlabeled) test data. Two main novel contributions are proposed: 1) a domain adaptation support vector machine (DASVM) technique which extends the formulation of support vector machines (SVMs) to the domain adaptation framework and 2) a circular indirect accuracy assessment strategy for validating the learning of domain adaptation classifiers when no true labels for the target--domain instances are available. Experimental results, obtained on a series of two-dimensional toy problems and on two real data sets related to brain computer interface and remote sensing applications, confirmed the effectiveness and the reliability of both the DASVM technique and the proposed circular validation strategy.", "title": "Domain Adaptation Problems: A DASVM Classification Technique and a Circular Validation Strategy"}, "235723a15c86c369c99a42e7b666dfe156ad2cba": {"paper_id": "235723a15c86c369c99a42e7b666dfe156ad2cba", "abstract": "A class of predictive densities is derived by weighting the observed samples in maximizing the log-likelihood function. This approach is e ective in cases such as sample surveys or design of experiments, where the observed covariate follows a di erent distribution than that in the whole population. Under misspeci cation of the parametric model, the optimal choice of the weight function is asymptotically shown to be the ratio of the density function of the covariate in the population to that in the observations. This is the pseudo-maximum likelihood estimation of sample surveys. The optimality is de ned by the expected Kullback\u2013Leibler loss, and the optimal weight is obtained by considering the importance sampling identity. Under correct speci cation of the model, however, the ordinary maximum likelihood estimate (i.e. the uniform weight) is shown to be optimal asymptotically. For moderate sample size, the situation is in between the two extreme cases, and the weight function is selected by minimizing a variant of the information criterion derived as an estimate of the expected loss. The method is also applied to a weighted version of the Bayesian predictive density. Numerical examples as well as Monte-Carlo simulations are shown for polynomial regression. A connection with the robust parametric estimation is discussed. c \u00a9 2000 Elsevier Science B.V. All rights reserved.", "title": "Improving predictive inference under covariate shift by weighting the log-likelihood function"}, "0fca9a022f4910dda7f8bdc92bbbe8a9c6e35303": {"paper_id": "0fca9a022f4910dda7f8bdc92bbbe8a9c6e35303", "abstract": "The paper investigates the acceleration of t-SNE\u2014an embedding technique that is commonly used for the visualization of high-dimensional data in scatter plots\u2014using two treebased algorithms. In particular, the paper develops variants of the Barnes-Hut algorithm and of the dual-tree algorithm that approximate the gradient used for learning t-SNE embeddings in O(N logN). Our experiments show that the resulting algorithms substantially accelerate t-SNE, and that they make it possible to learn embeddings of data sets with millions of objects. Somewhat counterintuitively, the Barnes-Hut variant of t-SNE appears to outperform the dual-tree variant.", "title": "Accelerating t-SNE using tree-based algorithms"}, "20cf48240b89bd522beff22a0cf0c8cd5b2f8abf": {"paper_id": "20cf48240b89bd522beff22a0cf0c8cd5b2f8abf", "abstract": "We present a tree data structure for fast nearest neighbor operations in general <i>n</i>-point metric spaces (where the data set consists of <i>n</i> points). The data structure requires <i>O</i>(<i>n</i>) space <i>regardless</i> of the metric's structure yet maintains all performance properties of a navigating net (Krauthgamer & Lee, 2004b). If the point set has a bounded expansion constant <i>c</i>, which is a measure of the intrinsic dimensionality, as defined in (Karger & Ruhl, 2002), the cover tree data structure can be constructed in <i>O</i> (<i>c</i><sup>6</sup><i>n</i> log <i>n</i>) time. Furthermore, nearest neighbor queries require time only logarithmic in <i>n</i>, in particular <i>O</i> (<i>c</i><sup>12</sup> log <i>n</i>) time. Our experimental results show speedups over the brute force search varying between one and several orders of magnitude on natural machine learning datasets.", "title": "Cover trees for nearest neighbor"}, "02d17701dd346311197c3f1553ae9e0d6376fd43": {"paper_id": "02d17701dd346311197c3f1553ae9e0d6376fd43", "abstract": "Unlike human learning, machine learning often fails to handle changes between training (source) and test (target) input distributions. Such domain shifts, common in practical scenarios, severely damage the performance of conventional machine learning methods. Supervised domain adaptation methods have been proposed for the case when the target data have labels, including some that perform very well despite being \u201cfrustratingly easy\u201d to implement. However, in practice, the target domain is often unlabeled, requiring unsupervised adaptation. We propose a simple, effective, and efficient method for unsupervised domain adaptation called CORrelation ALignment (CORAL). CORAL minimizes domain shift by aligning the second-order statistics of source and target distributions, without requiring any target labels. Even though it is extraordinarily simple\u2013it can be implemented in four lines of Matlab code\u2013CORAL performs remarkably well in extensive evaluations on standard benchmark datasets. \u201cEverything should be made as simple as possible, but not simpler.\u201d", "title": "Return of Frustratingly Easy Domain Adaptation"}, "136b9952f29632ab3fa2bbf43fed277204e13cb5": {"paper_id": "136b9952f29632ab3fa2bbf43fed277204e13cb5", "abstract": "Scene categorization is a fundamental problem in computer vision. However, scene understanding research has been constrained by the limited scope of currently-used databases which do not capture the full variety of scene categories. Whereas standard databases for object categorization contain hundreds of different classes of objects, the largest available dataset of scene categories contains only 15 classes. In this paper we propose the extensive Scene UNderstanding (SUN) database that contains 899 categories and 130,519 images. We use 397 well-sampled categories to evaluate numerous state-of-the-art algorithms for scene recognition and establish new bounds of performance. We measure human scene classification performance on the SUN database and compare this with computational methods. Additionally, we study a finer-grained scene representation to detect scenes embedded inside of larger scenes.", "title": "SUN database: Large-scale scene recognition from abbey to zoo"}, "05bd81538cbf475f47e1a2a8f416b271ae494bbb": {"paper_id": "05bd81538cbf475f47e1a2a8f416b271ae494bbb", "abstract": "Discriminative learning methods are widely used in natural language processing. These methods work best when their training and test data are drawn from the same distribution. For many NLP tasks, however, we are confronted with new domains in which labeled data is scarce or non-existent. In such cases, we seek to adapt existing models from a resourcerich source domain to a resource-poor target domain. We introduce structural correspondence learning to automatically induce correspondences among features from different domains. We test our technique on part of speech tagging and show performance gains for varying amounts of source and target training data, as well as improvements in target domain parsing accuracy using our improved tagger.", "title": "Domain Adaptation with Structural Correspondence Learning"}, "023f6fc69fe1f6498e35dbf85932ecb549d36ca4": {"paper_id": "023f6fc69fe1f6498e35dbf85932ecb549d36ca4", "abstract": "This paper introduces a novel algorithm to approximate the matrix with minimum nuclear norm among all matrices obeying a set of convex constraints. This problem may be understood as the convex relaxation of a rank minimization problem and arises in many important applications as in the task of recovering a large matrix from a small subset of its entries (the famous Netflix problem). Off-the-shelf algorithms such as interior point methods are not directly amenable to large problems of this kind with over a million unknown entries. This paper develops a simple first-order and easy-to-implement algorithm that is extremely efficient at addressing problems in which the optimal solution has low rank. The algorithm is iterative, produces a sequence of matrices {Xk ,Y k}, and at each step mainly performs a soft-thresholding operation on the singular values of the matrix Y k. There are two remarkable features making this attractive for low-rank matrix completion problems. The first is that the soft-thresholding operation is applied to a sparse matrix; the second is that the rank of the iterates {Xk} is empirically nondecreasing. Both these facts allow the algorithm to make use of very minimal storage space and keep the computational cost of each iteration low. On the theoretical side, we provide a convergence analysis showing that the sequence of iterates converges. On the practical side, we provide numerical examples in which 1, 000 \u00d7 1, 000 matrices are recovered in less than a minute on a modest desktop computer. We also demonstrate that our approach is amenable to very large scale problems by recovering matrices of rank about 10 with nearly a billion unknowns from just about 0.4% of their sampled entries. Our methods are connected with the recent literature on linearized Bregman iterations for 1 minimization, and we develop a framework in which one can understand these algorithms in terms of well-known Lagrange multiplier algorithms.", "title": "A Singular Value Thresholding Algorithm for Matrix Completion"}, "abf805ce186d86e900c704c6920da89e590aa854": {"paper_id": "abf805ce186d86e900c704c6920da89e590aa854", "abstract": "MOTIVATION\nMany problems in data integration in bioinformatics can be posed as one common question: Are two sets of observations generated by the same distribution? We propose a kernel-based statistical test for this problem, based on the fact that two distributions are different if and only if there exists at least one function having different expectation on the two distributions. Consequently we use the maximum discrepancy between function means as the basis of a test statistic. The Maximum Mean Discrepancy (MMD) can take advantage of the kernel trick, which allows us to apply it not only to vectors, but strings, sequences, graphs, and other common structured data types arising in molecular biology.\n\n\nRESULTS\nWe study the practical feasibility of an MMD-based test on three central data integration tasks: Testing cross-platform comparability of microarray data, cancer diagnosis, and data-content based schema matching for two different protein function classification schemas. In all of these experiments, including high-dimensional ones, MMD is very accurate in finding samples that were generated from the same distribution, and outperforms its best competitors.\n\n\nCONCLUSIONS\nWe have defined a novel statistical test of whether two samples are from the same distribution, compatible with both multivariate and structured data, that is fast, easy to implement, and works well, as confirmed by our experiments.\n\n\nAVAILABILITY\nhttp://www.dbs.ifi.lmu.de/~borgward/MMD.", "title": "Integrating structured biological data by Kernel Maximum Mean Discrepancy"}, "5d9a3036181676e187c9c0ff995d8bed1db3557d": {"paper_id": "5d9a3036181676e187c9c0ff995d8bed1db3557d", "abstract": "Domain adaptation is an important emerging topic in computer vision. In this paper, we present one of the first studies of domain shift in the context of object recognition. We introduce a method that adapts object models acquired in a particular visual domain to new imaging conditions by learning a transformation that minimizes the effect of domain-induced changes in the feature distribution. The transformation is learned in a supervised manner and can be applied to categories for which there are no labeled examples in the new domain. While we focus our evaluation on object recognition tasks, the transform-based adaptation technique we develop is general and could be applied to non-image data. Another contribution is a new multi-domain object database, freely available for download. We experimentally demonstrate the ability of our method to improve recognition on categories with few or no target domain labels and moderate to large changes in the imaging conditions.", "title": "Adapting Visual Category Models to New Domains"}, "4b7c8063892ca57131bd71b9c839d970ee8b0226": {"paper_id": "4b7c8063892ca57131bd71b9c839d970ee8b0226", "abstract": "We describe an approach to domain adaptation that is appropriate exactly in the case when one has enough \u201ctarget\u201d data to do slightly better than just using only \u201csource\u201d data. Our approach is incredibly simple, easy to implement as a preprocessing step (10 lines of Perl!) and outperforms stateof-the-art approaches on a range of datasets. Moreover, it is trivially extended to a multidomain adaptation problem, where one has data from a variety of different domains.", "title": "Frustratingly Easy Domain Adaptation"}, "08ae74a8967f7439a3df8733e268caa6294b7e5b": {"paper_id": "08ae74a8967f7439a3df8733e268caa6294b7e5b", "abstract": "We present a new machine learning framework called \"self-taught learning\" for using unlabeled data in supervised classification tasks. We do not assume that the unlabeled data follows the same class labels or generative distribution as the labeled data. Thus, we would like to use a large number of unlabeled images (or audio samples, or text documents) randomly downloaded from the Internet to improve performance on a given image (or audio, or text) classification task. Such unlabeled data is significantly easier to obtain than in typical semi-supervised or transfer learning settings, making self-taught learning widely applicable to many practical learning problems. We describe an approach to self-taught learning that uses sparse coding to construct higher-level features using the unlabeled data. These features form a succinct input representation and significantly improve classification performance. When using an SVM for classification, we further show how a Fisher kernel can be learned for this representation.", "title": "Self-taught learning: transfer learning from unlabeled data"}, "18ca2837d280a6b2250024b6b0e59345601064a7": {"paper_id": "18ca2837d280a6b2250024b6b0e59345601064a7", "abstract": "Many areas of science depend on exploratory data analysis and visualization. The need to analyze large amounts of multivariate data raises the fundamental problem of dimensionality reduction: how to discover compact representations of high-dimensional data. Here, we introduce locally linear embedding (LLE), an unsupervised learning algorithm that computes low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs. Unlike clustering methods for local dimensionality reduction, LLE maps its inputs into a single global coordinate system of lower dimensionality, and its optimizations do not involve local minima. By exploiting the local symmetries of linear reconstructions, LLE is able to learn the global structure of nonlinear manifolds, such as those generated by images of faces or documents of text.", "title": "Nonlinear dimensionality reduction by locally linear embedding."}, "20739c96ed44ccfdc5352ea38e1a2a15137363f4": {"paper_id": "20739c96ed44ccfdc5352ea38e1a2a15137363f4", "abstract": "Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem of dimensionality reduction: finding meaningful low-dimensional structures hidden in their high-dimensional observations. The human brain confronts the same problem in everyday perception, extracting from its high-dimensional sensory inputs-30,000 auditory nerve fibers or 10(6) optic nerve fibers-a manageably small number of perceptually relevant features. Here we describe an approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set. Unlike classical techniques such as principal component analysis (PCA) and multidimensional scaling (MDS), our approach is capable of discovering the nonlinear degrees of freedom that underlie complex natural observations, such as human handwriting or images of a face under different viewing conditions. In contrast to previous algorithms for nonlinear dimensionality reduction, ours efficiently computes a globally optimal solution, and, for an important class of data manifolds, is guaranteed to converge asymptotically to the true structure.", "title": "A global geometric framework for nonlinear dimensionality reduction."}, "223319a93dcf3912bbc1e5f949e5ab4d53906e62": {"paper_id": "223319a93dcf3912bbc1e5f949e5ab4d53906e62", "abstract": "Top-performing deep architectures are trained on massive amounts of labeled data. In the absence of labeled data for a certain task, domain adaptation often provides an attractive option given that labeled data of similar nature but from a different domain (e.g. synthetic images) are available. Here, we propose a new approach to domain adaptation in deep architectures that can be trained on large amount of labeled data from the source domain and large amount of unlabeled data from the target domain (no labeled targetdomain data is necessary). As the training progresses, the approach promotes the emergence of \u201cdeep\u201d features that are (i) discriminative for the main learning task on the source domain and (ii) invariant with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a simple new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation. Overall, the approach can be implemented with little effort using any of the deep-learning packages. The method performs very well in a series of image classification experiments, achieving adaptation effect in the presence of big domain shifts and outperforming previous state-ofthe-art on Office datasets.", "title": "Unsupervised Domain Adaptation by Backpropagation"}, "e6bef595cb78bcad4880aea6a3a73ecd32fbfe06": {"paper_id": "e6bef595cb78bcad4880aea6a3a73ecd32fbfe06", "abstract": "The exponential increase in the availability of online reviews and recommendations makes sentiment classification an interesting topic in academic and industrial research. Reviews can span so many different domains that it is difficult to gather annotated training data for all of them. Hence, this paper studies the problem of domain adaptation for sentiment classifiers, hereby a system is trained on labeled reviews from one source domain but is meant to be deployed on another. We propose a deep learning approach which learns to extract a meaningful representation for each review in an unsupervised fashion. Sentiment classifiers trained with this high-level feature representation clearly outperform state-of-the-art methods on a benchmark composed of reviews of 4 types of Amazon products. Furthermore, this method scales well and allowed us to successfully perform domain adaptation on a larger industrial-strength dataset of 22 domains.", "title": "Domain Adaptation for Large-Scale Sentiment Classification: A Deep Learning Approach"}, "97f85a67b95af12b80e3d241bcaed25b117f653a": {"paper_id": "97f85a67b95af12b80e3d241bcaed25b117f653a", "abstract": "Consider the multicommodity flow problem in which the object is to maximize the sum of commodities routed. We prove the following approximate max-flow min-multicut theorem: min multicut < max flow < min multicut, O(log k) where k is the number of commodities. Our proof is constructive; it enables us to find a multicut within O(logk) of the max flow (and hence also the optimal multicut). In addition, the proof technique provides a unified framework in which one can also analyse the case of flows with specified demands of Leighton and Rao and Klein et al. and thereby obtain an improved bound for the latter problem. Key words, approximation algorithm, multicommodity flow, minimum multicut AMS subject classifications. 68Q25, 90B10", "title": "Approximate max-flow min-(multi)cut theorems and their applications"}, "94a36fb291ed72462ba2abb117723608ec2f8fa8": {"paper_id": "94a36fb291ed72462ba2abb117723608ec2f8fa8", "abstract": "A resonant snubber is described for voltage-source inverters, current-source inverters, and self-commutated frequency changers. The main self-turn-off devices have shunt capacitors directly across them. The lossless resonant snubber described avoids trapping energy in a converter circuit where high dynamic stresses at both turn-on and turn-off are normally encountered. This is achieved by providing a temporary parallel path through a small ordinary thyristor (or other device operating in a similar node) to take over the high-stress turn-on duty from the main gate turn-off (GTO) or power transistor, in a manner that leaves no energy trapped after switching.<<ETX>>", "title": "Resonant snubbers with auxiliary switches"}, "22b6bd1980ea40d0f095a151101ea880fae33049": {"paper_id": "22b6bd1980ea40d0f095a151101ea880fae33049", "abstract": "The evaluation of machine translation (MT) systems is a vital field of research, both for determining the effectiveness of existing MT systems and for optimizing the performance of MT systems. This part describes a range of different evaluation approaches used in the GALE community and introduces evaluation protocols and methodologies used in the program. We discuss the development and use of automatic, human, task-based and semi-automatic (human-in-the-loop) methods of evaluating machine translation, focusing on the use of a human-mediated translation error rate HTER as the evaluation standard used in GALE. We discuss the workflow associated with the use of this measure, including post editing, quality control, and scoring. We document the evaluation tasks, data, protocols, and results of recent GALE MT Evaluations. In addition, we present a range of different approaches for optimizing MT systems on the basis of different measures. We outline the requirements and specific problems when using different optimization approaches and describe how the characteristics of different MT metrics affect the optimization. Finally, we describe novel recent and ongoing work on the development of fully automatic MT evaluation metrics that can have the potential to substantially improve the effectiveness of evaluation and optimization of MT systems. Progress in the field of machine translation relies on assessing the quality of a new system through systematic evaluation, such that the new system can be shown to perform better than pre-existing systems. The difficulty arises in the definition of a better system. When assessing the quality of a translation, there is no single correct answer; rather, there may be any number of possible correct translations. In addition, when two translations are only partially correct but in different ways it is difficult to distinguish quality. Moreover, quality assessments may be dependent on the intended use for the translation, e.g., the tone of a translation may be crucial in some applications, but irrelevant in other applications. Traditionally, there are two paradigms of machine translation evaluation: (1) Glass Box evaluation, which measures the quality of a system based upon internal system properties, and (2) Black Box evaluation, which measures the quality of a system based solely upon its output, without respect to the internal mechanisms of the translation system. Glass Box evaluation focuses upon an examination of the system's linguistic", "title": "Part 5: Machine Translation Evaluation Chapter 5.1 Introduction"}, "3d07b5087e53c6f7c228b3c7e769494527be228e": {"paper_id": "3d07b5087e53c6f7c228b3c7e769494527be228e", "abstract": "We examine a new, intuitive measure for evaluating machine-translation output that avoids the knowledge intensiveness of more meaning-based approaches, and the labor-intensiveness of human judgments. Translation Edit Rate (TER) measures the amount of editing that a human would have to perform to change a system output so it exactly matches a reference translation. We show that the single-reference variant of TER correlates as well with human judgments of MT quality as the four-reference variant of BLEU. We also define a human-targeted TER (or HTER) and show that it yields higher correlations with human judgments than BLEU\u2014even when BLEU is given human-targeted references. Our results indicate that HTER correlates with human judgments better than HMETEOR and that the four-reference variants of TER and HTER correlate with human judgments as well as\u2014or better than\u2014a second human judgment does.", "title": "A Study of Translation Edit Rate with Targeted Human Annotation"}, "8fe1f4f904242bb92576992b4f0386f99108f9b0": {"paper_id": "8fe1f4f904242bb92576992b4f0386f99108f9b0", "abstract": "Evaluation of MT evaluation measures is limited by inconsistent human judgment data. Nonetheless, machine translation can be evaluated using the well-known measures precision, recall, and their average, the F-measure. The unigrambased F-measure has significantly higher correlation with human judgments than recently proposed alternatives. More importantly, this standard measure has an intuitive graphical interpretation, which can facilitate insight into how MT systems might be improved. The relevant software is publicly available from http://nlp.cs.nyu.edu/GTM/.", "title": "Evaluation of Machine Translation and its Evaluation"}, "32f1efb0183c99e80cf1b15465a7c21b4b9738af": {"paper_id": "32f1efb0183c99e80cf1b15465a7c21b4b9738af", "abstract": "Computer-generated texts, whether from Natural Language Generation (NLG) or Machine Translation (MT) systems, are often post-edited by humans before being released to users. The frequency and type of post-edits is a measure of how well the system works, and can be used for evaluation. We describe how we have used post-edit data to evaluate SUMTIME-MOUSAM, an NLG system that produces weather forecasts.", "title": "Evaluating an NLG System using Post-Editing"}, "d61c0b9d86f60ce2220be9ee2baf5009c5ce8841": {"paper_id": "d61c0b9d86f60ce2220be9ee2baf5009c5ce8841", "abstract": "The TIPSTER Text Summarization Evaluation (SUMMAC) has developed several new extrinsic and intrinsic methods for evaluating summaries. It has established definitively that automatic text summarization is very effective in relevance assessment tasks on news articles. Summaries as short as 17% of full text length sped up decision-making by almost a factor of 2 with no statistically significant degradation in accuracy. Analysis of feedback forms filled in after each decision indicated that the intelligibility of present-day machine-generated summaries is high. Systems that performed most accurately in the production of indicative and informative topic-related summaries used term frequency and co-occurrence statistics, and vocabulary overlap comparisons between text passages. However, in the absence of a topic, these statistical methods do not appear to provide any additional leverage: in the case of generic summaries, the systems were indistinguishable in accuracy. The paper discusses some of the tradeoffs and challenges faced by the evaluation, and also lists some of the lessons learned, impacts, and possible future directions. The evaluation methods used in the SUMMAC evaluation are of interest to both summarization evaluation as well as evaluation of other 'output-related' NLP technologies, where there may be many potentially acceptable outputs, with no automatic way to compare them.", "title": "SUMMAC: a text summarization evaluation"}, "9bac73b9b952d8292287dc4dc22a6ba9b908f314": {"paper_id": "9bac73b9b952d8292287dc4dc22a6ba9b908f314", "abstract": "The coccyx has been relatively neglected in anatomical research which is surprising given the population prevalence of coccydynia and our inadequate understanding of its etiology. This systematic review analyzes available information on the clinical anatomy of the coccyx. A literature search using five electronic databases and standard anatomy reference texts was conducted yielding 61 primary and 7 secondary English-language sources. This was supplemented by a manual search of selected historical foreign language articles. The coccygeal vertebrae, associated joints, ligaments and muscles, coccygeal movements, nerves, and blood supply were analyzed in detail. Although the musculoskeletal aspects of the coccyx are reasonably well described, the precise anatomy of the coccygeal plexus and its distribution, the function of the coccygeal body, and the anatomy of the sacrococcygeal zygapophyseal joints are poorly documented. Further research into the anatomy of the coccyx may clarify the etiopathogenesis of coccydynia which remains uncertain in one-third of affected patients.", "title": "Clinical anatomy of the coccyx: A systematic review."}, "a5b28f5ebb81f6dcc5c5a8068b147aa8820c89cb": {"paper_id": "a5b28f5ebb81f6dcc5c5a8068b147aa8820c89cb", "abstract": "This paper proposes a method to measure the junction temperatures of insulated-gate bipolar transistors (IGBTs) during the converter operation for prototype evaluation. The IGBT short-circuit current is employed as the temperature-sensitive electrical parameter (TSEP). The calibration experiments show that the short-circuit current has an adequate temperature sensitivity of 0.35 A/\u00b0C. The parameter also has good selectivity and linearity, which makes it suitable to be used as a TSEP. Test circuit and hardware design are proposed for the IGBT junction temperature measurement in various power electronics dc-dc and ac-dc converter applications. By connecting a temperature measurement unit to the converter and giving a short-circuit pulse during the converter operation, the short-circuit current is measured, and the IGBT junction temperature can be derived from the calibration curve. The proposed temperature measurement method is a valuable tool for prototype evaluation and avoids the unnecessary safety margin regarding device operating temperatures, which is significant particularly for high-temperature/high-density converter applications.", "title": "Junction Temperature Measurement of IGBTs Using Short-Circuit Current as a Temperature-Sensitive Electrical Parameter for Converter Prototype Evaluation"}, "1af7290aa89965a0ec06bce76c881024b87cf338": {"paper_id": "1af7290aa89965a0ec06bce76c881024b87cf338", "abstract": "Cyber security remains one of the most serious challenges to national security and the economy that we face today. Systems employing well known but static defenses are increasingly vulnerable to penetration from determined, diverse, and well resourced adversaries launching targeted attacks such as Advanced Persistent Threats (APTs). Due to the heavy focus on cyber security technologies in both commercial and government environments over the last decade, an overwhelming array of cyber defense technologies have become available for cyber defenders to use. As the number and complexity of these defenses increase, cyber defenders face the problem of selecting, composing, and configuring them, a process which to date is performed manually and without a clear understanding of integration points and risks associated with each defense or combination of defenses. As shown in Figure 1, the current state-of-the-art approach for selecting and configuring cyber defenses is manual in nature and is often done without a clear understanding of security metrics associated with attack surfaces. Due to the talent Unknown& Security& Metrics Manual&Selection&and& Configuration&of&Cyber&Defenses", "title": "Using Ontologies to Quantify Attack Surfaces"}, "11469352326248de37b299d6f49b0b5b107a083b": {"paper_id": "11469352326248de37b299d6f49b0b5b107a083b", "abstract": "Situation awareness depends on a reliable perception of the environment and comprehension of its semantic structures. In this respect, cyberspace presents a unique challenge to the situation awareness of users and analysts, since it is a unique combination of human and machine elements, whose complex interactions occur in a global communication network. Accordingly, we outline the underpinnings of an ontology of secure operations in cyberspace, presenting the ontology framework and providing two modeling examples. We make the case for adopting a rigorous semantic model of cyber security to overcome the current limits of the state of the art. Keywords\u2014 cyber security, ontology, situation awareness,", "title": "Building an Ontology of Cyber Security"}, "860d3d4114711fa4ce9a5a4ccf362b80281cc981": {"paper_id": "860d3d4114711fa4ce9a5a4ccf362b80281cc981", "abstract": "This paper reports on a trade study we performed to support the development of a Cyber ontology from an initial malware ontology. The goals of the Cyber ontology effort are first described, followed by a discussion of the ontology development methodology used. The main body of the paper then follows, which is a description of the potential ontologies and standards that could be utilized to extend the Cyber ontology from its initially constrained malware focus. These resources include, in particular, Cyber and malware standards, schemas, and terminologies that directly contributed to the initial malware ontology effort. Other resources are upper (sometimes called 'foundational') ontologies. Core concepts that any Cyber ontology will extend have already been identified and rigorously defined in these foundational ontologies. However, for lack of space, this section is profoundly reduced. In addition, utility ontologies that are focused on time, geospatial, person, events, and network operations are briefly described. These utility ontologies can be viewed as specialized super-domain or even mid-level ontologies, since they span many, if not most, ontologies -including any Cyber ontology. An overall view of the ontological architecture used by the trade study is also given. The report on the trade study concludes with some proposed next steps in the iterative evolution of the", "title": "Developing an Ontology of the Cyber Security Domain"}, "af359fbcfc6d58a741ac0d597cd20eb9a68dfa51": {"paper_id": "af359fbcfc6d58a741ac0d597cd20eb9a68dfa51", "abstract": "This paper describes a method for maintaining the relationships between temporal intervals in a hierarchical manner using constraint propagation techniques. The representation includes a notion of the present moment (i.e., \"now\") , and allows one to represent intervals that may extend indefinitely into the past/future. This research was supported in part by the National Science Foundation under Grant Number IST-80-12418, and in part by the Office of Naval Research under Grant Number N00014-80-O0197.", "title": "An Interval-Based Representation of Temporal Knowledge"}, "26a1223b1beb72a12c7944668ca26cf78ec12d8d": {"paper_id": "26a1223b1beb72a12c7944668ca26cf78ec12d8d", "abstract": "Many binary tools, such as disassemblers, dynamic code generation systems, and executable code rewriters, need to understand how machine instructions are encoded. Unfortunately, specifying such encodings is tedious and error-prone. Users must typically specify thousands of details of instruction layout, such as opcode and field locations values, legal operands, and jump offset encodings. We have built a tool called DERIVE that extracts these details from existing software: the system assembler. Users need only provide the assembly syntax for the instructions for which they want encodings. DERIVE automatically reverse-engineers instruction encoding knowledge from the assembler by feeding it permutations of instructions and doing equation solving on the output.\nDERIVE is robust and general. It derives instruction encodings for SPARC, MIPS, Alpha, PowerPC, ARM, and x86. In the last case, it handles variable-sized instructions, large instructions, instruction encodings determined by operand size, and other CISC features. DERIVE is also remarkably simple: it is a factor of 6 smaller than equivalent, more traditional systems. Finally, its declarative specifications eliminate the mis-specification errors that plague previous approaches, such as illegal registers used as operands or incorrect field offsets and sizes. This paper discusses our current DERIVE prototype, explains how it computes instruction encodings, and also discusses the more general implications of the ability to extract functionality from installed software.", "title": "Derive: a tool that automatically reverse-engineers instruction encodings"}, "049b30d8aaedc87c9b66dac2e607ea0cf4e87b56": {"paper_id": "049b30d8aaedc87c9b66dac2e607ea0cf4e87b56", "abstract": "Dynamic code generation allows programmers to use run-time information in order to achieve performance and expressiveness superior to those of static code. The 'C(Tick C) language is a superset of ANSI C that supports efficient and high-level use of dynamic code generation. 'C provides dynamic code generation at the level of C expressions and statements and supports the composition of dynamic code at run time. These features enable programmers to add dynamic code generation to existing C code incrementally and to write important applications (such as \u201cjust-in-time\u201d compilers) easily. The article presents many examples of how 'C can be used to solve practical problems. The tcc compiler is an efficient, portable, and freely available implementation of 'C. tcc allows programmers to trade dynamic compilation speed for dynamic code quality: in some aplications, it is most important to generate code quickly, while in others code quality matters more than compilation speed. The overhead of dynamic compilation is on the order of 100 to 600 cycles per generated instruction, depending on the level of dynamic optimizaton. Measurements show that the use of dynamic code generation can improve performance by almost an order of magnitude; two- to four-fold speedups are common. In most cases, the overhead of dynamic compilation is recovered in under 100 uses of the dynamic code; sometimes it can be recovered within one use.", "title": "'C and tcc: A Language and Compiler for Dynamic Code Generation"}, "69b91877c70c6111d44dfb927e3402eeb718364d": {"paper_id": "69b91877c70c6111d44dfb927e3402eeb718364d", "abstract": "Atdr..r-R.gin4 alqrioi r.t bc vi.y.d s I g.ph otoinS Fobld. Ea.h rod. in rh. srapn n..th for I cohp{cd qusrry !n.r rBjd6 in r h&hh;,.9't.r, \"\"d l;;;; ;;: conner.d by &..rB.irrh. quurls,nk.rs. w,rh ach odd. rr,.r n-rr ir,iy * iii,\"iii.i\"liJ r. ' shr pohr n ,h. ooFd p,osah rhu rppro.dr lnou8h ,.\",i_.a i\"ir,i, [iiiiiii]iiii ncvc hprchdr.d b.ror. pEuh,n.ry rc$hi of&.rpcri,.nhl inDlcnar! oD in I pLi oFidDrS compil.! !u!i6r rh.r 8lob.t E8lrcr.lt@fio. rpDr@chinr rh.t oI h.nd{od; emDry u.&rl! mrv bc .rrajngl.", "title": "Register Allocation Via Coloring"}, "5f55f327c7fb90cf1831aa54c789d420628191ee": {"paper_id": "5f55f327c7fb90cf1831aa54c789d420628191ee", "abstract": "Remote procedure calls (RPC) appear to be a useful paradig m for providing communication across a network between programs written in a high-level language. This paper describes a package providing a remote procedure call facility, the options that face the designer of such a package, and the decisions ~we made. We describe the overall structure of our RPC mechanism, our facilities for binding RPC clients, the transport level communication protocol, and some performance measurements. We include descriptioro~ of some optimizations used to achieve high performance and to minimize the load on server machines that have many clients.", "title": "Implementing Remote Procedure Calls"}, "a3f1fe3a1e81e4c8da25a3ca81e27390e62388e9": {"paper_id": "a3f1fe3a1e81e4c8da25a3ca81e27390e62388e9", "abstract": "Clouds is a native operating system for a distributed environment. The Clouds operating system is built on top of a kernel called Ra. Ra \u00eds a second generation kernel derived from our experience with the first version of the Clouds operating system. R\u00f8 is a minimal, flexible kernel that provides a framework for implementing a variety of distributed operating systems. This paper presents the Clouds paradigm and a brief overview of its first implementation. We then present the details of the R\u00f8 kernel, the rationale for its design, and the system services that constitute the Clouds operating system. This work was funded by NSF grant CCR-8619886. @ Computing Systems, Vol. 3 . No. I . Winter 1990 11", "title": "The Design and Implementation of the Clouds Distributed Operating System"}, "f2e714d1146d1a4900b47031b93612a811833cce": {"paper_id": "f2e714d1146d1a4900b47031b93612a811833cce", "abstract": "Recent developments reveal that memories relying on the hippocampus are relatively resistant to interference, but sensitive to decay. The hippocampus is vital to recollection, a form of memory involving reinstatement of a studied item within its spatial-temporal context. An additional form of memory known as familiarity does not involve contextual reinstatement, but a feeling of acquaintance with the studied items. Familiarity depends more on extrahippocampal structures that do not have the properties promoting resistance to interference. These notions led to the novel hypothesis that the causes of forgetting depend on the memories' nature: memories depending on recollection are more vulnerable to decay than interference, whereas for memories depending on familiarity, the reverse is true. This review provides comprehensive evidence for this hypothesis.", "title": "How we forget may depend on how we remember"}, "4c20d2a5a4b29e401042e65dee05558811102cf9": {"paper_id": "4c20d2a5a4b29e401042e65dee05558811102cf9", "abstract": "Online, reverse auctions are increasingly being utilized in industrial sourcing activities. This phenomenon represents a novel, emerging area of inquiry with significant implications for sourcing strategies. However, there is little systematic thinking or empirical evidence on the topic. In this paper, the use of these auctions in sourcing activities is reviewed and four key aspects are highlighted: (i) the differences from physical auctions or those of the theoretical literature, (ii) the conditions for using online, reverse auctions, (iii) methods for structuring the auctions, and (iv) evaluations of auction performance. Some empirical evidence on these issues is also provided. ONLINE, REVERSE AUCTIONS: ISSUES, THEMES, AND PROSPECTS FOR THE FUTURE INTRODUCTION For nearly the past decade, managers, analysts, researchers, and the business press have been remarking that, \u201cThe Internet will change everything.\u201d And since the advent of the Internet, we have seen it challenge nearly every aspect of marketing practice. This raises the obligation to consider the consequences of the Internet to management practices, the theme of this special issue. Yet, it may take decades to fully understand the impact of the Internet on marketing practice, in general. This paper is one step in that direction. Specifically, I consider the impact of the Internet in a business-to-business context, the sourcing of direct and indirect materials from a supply base. It has been predicted that the Internet will bring about $1 trillion in efficiencies to the annual $7 trillion that is spent on the procurement of goods and services worldwide (USA Today, 2/7/00, B1). How and when this will happen remains an open question. However, one trend that is showing increasing promise is the use of online, reverse auctions. Virtually every major industry has begun to use and adopt these auctions on a regular basis (Smith 2002). During the late 1990s, slow-growth, manufacturing firms such as Boeing, SPX/Eaton, United Technologies, and branches of the United States military, utilized these auctions. Since then, consumer product companies such as Emerson Electronics, Nestle, and Quaker have followed suit. Even high-tech firms such as Dell, Hewlett-Packard, Intel, and Sun Microsystems have increased their usage of auctions in sourcing activities. And the intention and potential for the use of these auctions to continue to grow in the future is clear. In their annual survey of purchasing managers, Purchasing magazine found that 25% of its respondents expected to use reverse auctions in their sourcing efforts. Currently, the annual throughput in these auctions is estimated to be $40 billion; however, the addressable spend of the Global 500 firms is potentially $6.3 trillion.", "title": "ONLINE, REVERSE AUCTIONS: ISSUES, THEMES, AND PROSPECTS FOR THE FUTURE* Invited manuscript forthcoming in the MSI/JAMS Special Issue on Marketing to and Serving Customers Through the Internet: Conceptual Frameworks, Practical Insights and Research Directions"}, "2ca8332a4b64659f41e02d89081223d6eac9ccad": {"paper_id": "2ca8332a4b64659f41e02d89081223d6eac9ccad", "abstract": "This document describes an extension of the One-Time Password (OTP) algorithm, namely the HMAC-based One-Time Password (HOTP) algorithm, as defined in RFC 4226, to support the time-based moving factor. The HOTP algorithm specifies an event-based OTP algorithm, where the moving factor is an event counter. The present work bases the moving factor on a time value. A time-based variant of the OTP algorithm provides short-lived OTP values, which are desirable for enhanced security. The proposed algorithm can be used across a wide range of network applications, from remote Virtual Private Network (VPN) access and Wi-Fi network logon to transaction-oriented Web applications. The authors believe that a common and shared algorithm will facilitate adoption of two-factor authentication on the Internet by enabling interoperability across commercial and open-source implementations. (IETF). It represents the consensus of the IETF community. It has received public review and has been approved for publication by the Internet Engineering Steering Group (IESG). Not all documents approved by the IESG are a candidate for any level of Internet Standard; see Section 2 of RFC 5741. Information about the current status of this document, any errata, and how to provide feedback on it may be obtained at in effect on the date of publication of this document. Please review these documents carefully, as they describe your rights and restrictions with respect to this document. Code Components extracted from this document must include Simplified BSD License text as described in Section 4.e of the Trust Legal Provisions and are provided without warranty as described in the Simplified BSD License.", "title": "TOTP: Time-Based One-Time Password Algorithm"}, "e8e2c3d884bba807bcf7fbfa2c27f864b20ceb80": {"paper_id": "e8e2c3d884bba807bcf7fbfa2c27f864b20ceb80", "abstract": "This memo provides information for the Internet community. This memo does not specify an Internet standard of any kind. Distribution of this memo is unlimited. Abstract This document describes HMAC, a mechanism for message authentication using cryptographic hash functions. HMAC can be used with any iterative cryptographic hash function, e.g., MD5, SHA-1, in combination with a secret shared key. The cryptographic strength of HMAC depends on the properties of the underlying hash function.", "title": "HMAC: Keyed-Hashing for Message Authentication"}, "96f23d87015bdb919cde92f038ca5a76c7b6bb8c": {"paper_id": "96f23d87015bdb919cde92f038ca5a76c7b6bb8c", "abstract": "Effective design of concurrent tree implementation plays a major role in improving the scalability of applications in a multicore environment. We introduce a concurrent binary search tree with fatnodes (FatCBST) and present algorithms to perform basic operations on it. Unlike a simple node with single value, a fatnode consists of a set of values. FatCBST concept allows a thread to perform update operations on an existing fatnode without changing the tree structure, making it less disruptive to other threads' operations. Fatnodes help to take advantage of the spatial locality in the cache hierarchy and also reduce the height of the concurrent binary search tree. Our FatCBST implementation allows multiple threads to perform update operations on the same existing fatnode at the same time. Experimental results show that the FatCBST implementations that have small fatnode sizes provide better throughput for high and medium contention workloads; and large fatnode sizes provide better throughput for low contention workloads, as compared to the current state-of-the-art implementations.", "title": "FatCBST: Concurrent Binary Search Tree with Fatnodes"}, "a24b1525f5385836231c32867d871ffe2effe002": {"paper_id": "a24b1525f5385836231c32867d871ffe2effe002", "abstract": "The advent of multicore processors as the standard computing platform will force major changes in software design.", "title": "Data structures in the multicore age"}, "377177bb82105c35e6e26ebad1698a20688473bd": {"paper_id": "377177bb82105c35e6e26ebad1698a20688473bd", "abstract": "The non-blocking work-stealing algorithm of Arora, Blumofe, and Plaxton (henceforth ABP work-stealing) is on its way to becoming the multiprocessor load balancing technology of choice in both industry and academia. This highly efficient scheme is based on a collection of array-based double-ended queues (deques) with low cost synchronization among local and stealing processes. Unfortunately, the algorithm's synchronization protocol is strongly based on the use of fixed size arrays, which are prone to overflows, especially in the multiprogrammed environments for which they are designed. We present a work-stealing deque that does not have the overflow problem.The only ABP-style work-stealing algorithm that eliminates the overflow problem is the list-based one presented by Hendler, Lev and Shavit. Their algorithm indeed deals with the overflow problem, but it is complicated, and introduces a trade-off between the space and time complexity, due to the extra work required to maintain the list.Our new algorithm presents a simple lock-free work-stealing deque, which stores the elements in a cyclic array that can grow when it overflows. The algorithm has no limit other than integer overflow (and the system's memory size) on the number of elements that may be on the deque, and the total memory required is linear in the number of elements in the deque.", "title": "Dynamic circular work-stealing deque"}, "42142c121b2dbe48d55e81c2ce198a5639645030": {"paper_id": "42142c121b2dbe48d55e81c2ce198a5639645030", "abstract": "A concurrent object is a data object shared by concurrent processes. Linearizability is a correctness condition for concurrent objects that exploits the semantics of abstract data types. It permits a high degree of concurrency, yet it permits programmers to specify and reason about concurrent objects using known techniques from the sequential domain. Linearizability provides the illusion that each operation applied by concurrent processes takes effect instantaneously at some point between its invocation and its response, implying that the meaning of a concurrent object's operations can be given by pre- and post-conditions. This paper defines linearizability, compares it to other correctness conditions, presents and demonstrates a method for proving the correctness of implementations, and shows how to reason about concurrent objects, given they are linearizable.", "title": "Linearizability: A Correctness Condition for Concurrent Objects"}, "9043d0e94ab9c4884682098e73dd61c1e289934a": {"paper_id": "9043d0e94ab9c4884682098e73dd61c1e289934a", "abstract": "One approach to achieving high performance in a database management system is to store the database in main memorv rather than on disk. -One can then design new data structures aid algorithms oriented towards making eflicient use of CPU cycles and memory space rather than minimizing disk accesses and &ing disk space efliciently. In this paper we present some results on index structures from an ongoing study of main memory database management systems. We propose a new index structure, the T Tree, and we compare it to existing index structures in a main memory database environment. Our results indicate that the T Tree provides good overall performance in main memory.", "title": "A Study of Index Structures for Main Memory Database Management Systems"}, "3449592f90c5f5329cf541f027925db085ea33fd": {"paper_id": "3449592f90c5f5329cf541f027925db085ea33fd", "abstract": "Extendible hashing is a new access technique, in which the user is guaranteed no more than two page faults to locate the data associated with a given unique identifier, or key. Unlike conventional hashing, extendible hashing has a dynamic structure that grows and shrinks gracefully as the database grows and shrinks. This approach simultaneously solves the problem of making hash tables that are extendible and of making radix search trees that are balanced. We study, by analysis and simulation, the performance of extendible hashing. The results indicate that extendible hashing provides an attractive alternative to other access methods, such as balanced trees.", "title": "Extendible Hashing - A Fast Access Method for Dynamic Files"}, "03a00248b7d5e2d89f5337e62c39fad277c66102": {"paper_id": "03a00248b7d5e2d89f5337e62c39fad277c66102", "abstract": "problems To understand the class of polynomial-time solvable proble ms, we must first have a formal notion of what a \u201cproblem\u201d is. We define anbstract problemQ to be a binary relation on a set I of probleminstancesand a setS of problemsolutions. For example, an instance for SHORTEST-PATH is a triple consi sting of a graph and two vertices. A solution is a sequence of vertices in the g raph, with perhaps the empty sequence denoting that no path exists. The problem SHORTEST-PATH itself is the relation that associates each instance of a gra ph and two vertices with a shortest path in the graph that connects the two vertices. S ince shortest paths are not necessarily unique, a given problem instance may have mo r than one solution. This formulation of an abstract problem is more general than is required for our purposes. As we saw above, the theory of NP-completeness res tricts attention to decision problems : those having a yes/no solution. In this case, we can view an abstract decision problem as a function that maps the instan ce setI to the solution set {0, 1}. For example, a decision problem related to SHORTEST-PATH i s the problem PATH that we saw earlier. If i = \u3008G,u, v,k\u3009 is an instance of the decision problem PATH, then PATH(i ) = 1 (yes) if a shortest path fromu to v has at mostk edges, and PATH (i ) = 0 (no) otherwise. Many abstract problems are not decision problems, but rather optimization problems , in which some value must be minimized or maximized. As we saw above, however, it is usual ly a simple matter to recast an optimization problem as a decision problem that is no harder. 1See Hopcroft and Ullman [156] or Lewis and Papadimitriou [20 4] for a thorough treatment of the Turing-machine model. 34.1 Polynomial time 973", "title": "Introduction to Algorithms"}, "208125449f697c46d02a98eceb18b8c4622384c5": {"paper_id": "208125449f697c46d02a98eceb18b8c4622384c5", "abstract": "Several polynomial time algorithms finding \"good,\" but not necessarily optimal, tours for the traveling salesman problem are considered. We measure the closeness of a tour by the ratio of the obtained tour length to the minimal tour length. For the nearest neighbor method, we show the ratio is bounded above by a logarithmic function of the number of nodes. We also provide a logarithmic lower bound on the worst case. A class of approximation methods we call insertion methods are studied, and these are also shown to have a logarithmic upper bound. For two specific insertion methods, which we call nearest insertion and cheapest insertion, the ratio is shown to have a constant upper bound of 2, and examples are provided that come arbitrarily close to this upper bound. It is also shown that for any n => 8, there are traveling salesman problems with n nodes having tours which cannot be improved by making n/4 edge changes, but for which the ratio is 2(1l/n). Key words, traveling salesman problem, approximation algorithm, k-optimal, minimal spanning tree, triangle inequality", "title": "An Analysis of Several Heuristics for the Traveling Salesman Problem"}, "acc37c78d1840109e056261640c1a6434421841e": {"paper_id": "acc37c78d1840109e056261640c1a6434421841e", "abstract": "We present a data structure, based upon a stratified binary tree, which enables us to manipulate on-line a priority queue whose priorities are selected from the interval 1...n, with an average and worst case processing time of O(log log n) per instruction. The structure is used to obtain a mergeable heap whose time requirements are about as good.", "title": "Preserving order in a forest in less than logarithmic time"}, "05c34e5fc12aadcbb309b36dc9f0ed309fd2dd50": {"paper_id": "05c34e5fc12aadcbb309b36dc9f0ed309fd2dd50", "abstract": "In this paper an attempt is made to explore the logical foundations of computer programming by use of techniques which were first applied in the study of geometry and have later been extended to other branches of mathematics. This involves the elucidation of sets of axioms and rules of inference which can be used in proofs of the properties of computer programs. Examples are given of such axioms and rules, and a formal proof of a simple theorem is displayed. Finally, it is argued that important advantage, both theoretical and practical, may follow from a pursuance of these topics.", "title": "An Axiomatic Basis for Computer Programming"}, "47fa40daa68278183e95cc7a7aca6762d20bd9de": {"paper_id": "47fa40daa68278183e95cc7a7aca6762d20bd9de", "abstract": "The B-tree and its variants have been found to be highly useful (both theoretically and in practice) for storing large amounts of information, especially on secondary storage devices. We examine the problem of overcoming the inherent difficulty of concurrent operations on such structures, using a practical storage model. A single additional \u201clink\u201d pointer in each node allows a process to easily recover from tree modifications performed by other concurrent processes. Our solution compares favorably with earlier solutions in that the locking scheme is simpler (no read-locks are used) and only a (small) constant number of nodes are locked by any update process at any given time. An informal correctness proof for our system is given.", "title": "Efficient Locking for Concurrent Operations on B-Trees"}, "dfe3731d484dba672b3b6ad5f94e58aa7b3e7f2e": {"paper_id": "dfe3731d484dba672b3b6ad5f94e58aa7b3e7f2e", "abstract": "Atomic actions (or transactions) are useful for coping with concurrency and failures. One way of ensuring atomicity of actions is to implement applications in terms of atomic data types: abstract data types whose objects ensure serializability and recoverability of actions using them. Many atomic types can be implemented to provide high levels of concurrency by taking advantage of algebraic properties of the type's operations, for example, that certain operations commute. In this paper we analyze the level of concurrency permitted by an atomic type. We introduce several local constraints on individual objects that suffice to ensure global atomicity of actions; we call these constraints local atomicity properties. We present three local atomicity properties, each of which is optimal: no strictly weaker local constraint on objects suffices to ensure global atomicity for actions. Thus, the local atomicity properties define precise limits on the amount of concurrency that can be permitted by an atomic type.", "title": "Local Atomicity Properties: Modular Concurrency Control for Abstract Data Types"}, "e518e0e7d10a71e299b73e7274a93d5db1f2edad": {"paper_id": "e518e0e7d10a71e299b73e7274a93d5db1f2edad", "abstract": "We present ongoing work on a gold standard annotation of German terminology in an inhomogeneous domain. The text basis is thematically broad and contains various registers, from expert text to user-generated data taken from an online discussion forum. We identify issues related with these properties, and show our approach how to model the domain. Futhermore, we present our approach to handle multiword terms, including discontinuous ones. Finally, we evaluate the annotation quality.", "title": "Creating a gold standard corpus for terminological annotation from online forum data"}, "7e7343a5608fff1c68c5259db0c77b9193f1546d": {"paper_id": "7e7343a5608fff1c68c5259db0c77b9193f1546d", "abstract": "This paper presents a general statistical methodology for the analysis of multivariate categorical data arising from observer reliability studies. The procedure essentially involves the construction of functions of the observed proportions which are directed at the extent to which the observers agree among themselves and the construction of test statistics for hypotheses involving these functions. Tests for interobserver bias are presented in terms of first-order marginal homogeneity and measures of interobserver agreement are developed as generalized kappa-type statistics. These procedures are illustrated with a clinical diagnosis example from the epidemiological literature.", "title": "The measurement of observer agreement for categorical data."}, "aee71bfa28ec1bad78d4bd4aadcab168aa6b3b13": {"paper_id": "aee71bfa28ec1bad78d4bd4aadcab168aa6b3b13", "abstract": "We introduce the third major release of WebAnno, a generic web-based annotation tool for distributed teams. New features in this release focus on semantic annotation tasks (e.g. semantic role labelling or event annotation) and allow the tight integration of semantic annotations with syntactic annotations. In particular, we introduce the concept of slot features, a novel constraint mechanism that allows modelling the interaction between semantic and syntactic annotations, as well as a new annotation user interface. The new features were developed and used in an annotation project for semantic roles on German texts. The paper briefly introduces this project and reports on experiences performing annotations with the new tool. On a comparative evaluation, our tool reaches significant speedups over WebAnno 2 for a semantic annotation task.", "title": "A Web-based Tool for the Integrated Annotation of Semantic and Syntactic Structures"}, "837c2bf28887fc1a00e1b1148e5df748809da242": {"paper_id": "837c2bf28887fc1a00e1b1148e5df748809da242", "abstract": "The Proposition Bank project takes a practical approach to semantic representation, adding a layer of predicate-argument information, or semantic role labels, to the syntactic structures of the Penn Treebank. The resulting resource can be thought of as shallow, in that it does not represent coreference, quantification, and many other higher-order phenomena, but also broad, in that it covers every instance of every verb in the corpus and allows representative statistics to be calculated. We discuss the criteria used to define the sets of semantic roles used in the annotation process and to analyze the frequency of syntactic/semantic alternations in the corpus. We describe an automatic system for semantic role tagging trained on the corpus and discuss the effect on its performance of various types of information, including a comparison of full syntactic parsing with a flat representation and the contribution of the empty trace categories of the treebank.", "title": "The Proposition Bank: An Annotated Corpus of Semantic Roles"}, "24ff26b9cf71d9f9a8e7dedf0cfe56d105363cd3": {"paper_id": "24ff26b9cf71d9f9a8e7dedf0cfe56d105363cd3", "abstract": "FrameNet is a three-year NSF-supported project in corpus-based computational lexicography, now in its second year (NSF IRI-9618838, \"Tools for Lexicon Building\"). The project's key features are (a) a commitment to corpus evidence for semantic and syntactic generalizations, and (b) the representation of the valences of its target words (mostly nouns, adjectives, and verbs) in which the semantic portion makes use of frame semantics. The resulting database will contain (a) descriptions of the semantic frames underlying the meanings of the words described, and (b) the valence representation (semantic and syntactic) of several thousand words and phrases, each accompanied by (c) a representative collection of annotated corpus attestations, which jointly exemplify the observed linkings between \"frame elements\" and their syntactic realizations (e.g. grammatical function, phrase type, and other syntactic traits). This report will present the project's goals and workflow, and information about the computational tools that have been adapted or created in-house for this work. 1 I n t r o d u c t i o n The Berkeley FrameNet project 1 is producing frame-semantic descriptions of several thousand English lexical items and backing up these descriptions with semantically annotated attestations from contemporary English corpora 2. 1The project is based at the International Computer Science Institute (1947 Center Street, Berkeley, CA). A fuller bibliography may be found in (Lowe et ai., 1997) 2Our main corpus is the British National Corpus. We have access to it through the courtesy of Oxford University Press; the POS-tagged and lemmatized version we use was prepared by the Institut flit Maschinelle Sprachverarbeitung of the University of Stuttgart). The These descriptions are based on hand-tagged semantic annotations of example sentences extracted from large text corpora and systematic analysis of the semantic patterns they exemplify by lexicographers and linguists. The primary emphasis of the project therefore is the encoding, by humans, of semantic knowledge in machine-readable form. The intuition of the lexicographers is guided by and constrained by the results of corpus-based research using highperformance software tools. The semantic domains to be covered are\" HEALTH CARE, CHANCE, PERCEPTION, COMMUNICATION, TRANSACTION, TIME, SPACE, BODY (parts and functions of the body), MOTION, LIFE STAGES, SOCIAL CONTEXT, EMOTION and COGNITION. 1.1 Scope of t h e P r o j e c t The results of the project are (a) a lexical resource, called the FrameNet database 3, and (b) associated software tools. The database has three major components (described in more detail below: \u2022 Lexicon containing entries which are composed of: (a) some conventional dictionary-type data, mainly for the sake of human readers; (b) FORMULAS which capture the morphosyntactic ways in which elements of the semantic frame can be realized within the phrases or sentences built up around the word; (c) links to semantically ANNOTATED EXAMEuropean collaborators whose participation has made this possible are Sue Atkins, Oxford University Press, and Ulrich Held, IMS-Stuttgart. SThe database will ultimately contain at least 5,000 lexical entries together with a parallel annotated corpus, these in formats suitable for integration into applications which use other lexical resources such as WordNet and COMLEX. The final design of the database will be selected in consultation with colleagues at Princeton (WordNet), ICSI, and IMS, and with other members of the NLP community.", "title": "The Berkeley FrameNet Project"}, "2b5aadb2586707433beb21121677289d196e6144": {"paper_id": "2b5aadb2586707433beb21121677289d196e6144", "abstract": "We introduce the brat rapid annotation tool (BRAT), an intuitive web-based tool for text annotation supported by Natural Language Processing (NLP) technology. BRAT has been developed for rich structured annotation for a variety of NLP tasks and aims to support manual curation efforts and increase annotator productivity using NLP techniques. We discuss several case studies of real-world annotation projects using pre-release versions of BRAT and present an evaluation of annotation assisted by semantic class disambiguation on a multicategory entity mention annotation task, showing a 15% decrease in total annotation time. BRAT is available under an opensource license from: http://brat.nlplab.org", "title": "brat: a Web-based Tool for NLP-Assisted Text Annotation"}, "9d6312e318215361fd6a1e87cf2cfe1e44429bbd": {"paper_id": "9d6312e318215361fd6a1e87cf2cfe1e44429bbd", "abstract": "In this paper, we present a flexible approach to the efficient and exhaustive manual annotation of text documents. For this purpose, we extend WebAnno (Yimam et al., 2013) an open-source web-based annotation tool.1 While it was previously limited to specific annotation layers, our extension allows adding and configuring an arbitrary number of layers through a web-based UI. These layers can be annotated separately or simultaneously, and support most types of linguistic annotations such as spans, semantic classes, dependency relations, lexical chains, and morphology. Further, we tightly integrate a generic machine learning component for automatic annotation suggestions of span annotations. In two case studies, we show that automatic annotation suggestions, combined with our split-pane UI concept, significantly reduces annotation time.", "title": "Automatic Annotation Suggestions and Custom Annotation Layers in WebAnno"}, "da6c3fdf8ef9aae979a5dd156e074ba6691b2e2c": {"paper_id": "da6c3fdf8ef9aae979a5dd156e074ba6691b2e2c", "abstract": "MOTIVATION\nNatural language processing (NLP) methods are regarded as being useful to raise the potential of text mining from biological literature. The lack of an extensively annotated corpus of this literature, however, causes a major bottleneck for applying NLP techniques. GENIA corpus is being developed to provide reference materials to let NLP techniques work for bio-textmining.\n\n\nRESULTS\nGENIA corpus version 3.0 consisting of 2000 MEDLINE abstracts has been released with more than 400,000 words and almost 100,000 annotations for biological terms.", "title": "GENIA corpus - a semantically annotated corpus for bio-textmining"}, "8dda0b84e6a42863ca520e3d02e90ca77826a052": {"paper_id": "8dda0b84e6a42863ca520e3d02e90ca77826a052", "abstract": "In this paper, we describe our investigation of traces of naturally occurring emotions in electrical brain signals, that can be used to build interfaces that respond to our emotional state. This study confirms a number of known affective correlates in a realistic, uncontrolled environment for the emotions of valence (or pleasure), arousal and dominance: (1) a significant decrease in frontal power in the theta range is found for increasingly positive valence, (2) a significant frontal increase in power in the alpha range is associated with increasing emotional arousal, (3) a significant right posterior power increase in the delta range correlates with increasing arousal and (4) asymmetry in power in the lower alpha bands correlates with self-reported valence. Furthermore, asymmetry in the higher alpha bands correlates with self-reported dominance. These last two effects provide a simple measure for subjective feelings of pleasure and feelings of control.", "title": "Valence, arousal and dominance in the EEG during game play"}, "6529ed8a4a5e5b457339d4c67ae6a0a092e3c1d5": {"paper_id": "6529ed8a4a5e5b457339d4c67ae6a0a092e3c1d5", "abstract": "While artificial neural networks are regularly employed in modeling the perception of facial and vocal emotion expression as well as in automatic expression decoding by artificial agents, this approach is yet to be extended to the modeling of emotion elicitation and differentiation. In part, this may be due to the dominance of discrete and dimensional emotion models, which have not encouraged computational modeling. This situation has changed with the advent of appraisal theories of emotion and a number of attempts to develop rule-based models can be found in the literature. However, most of these models operate at a high level of conceptual abstraction and rarely include the underlying neural architecture. In this contribution, an appraisal-based emotion theory, the Component Process Model (CPM), is described that seems particularly suited to modeling with the help of artificial neural network approaches. This is due to its high degree of specificity in postulating underlying mechanisms including efferent physiological and behavioral manifestations as well as to the possibility of linking the theoretical assumptions to underlying neural architectures and dynamic processes. This paper provides a brief overview of the model, suggests constraints imposed by neural circuits, and provides examples on how the temporal unfolding of emotion can be conceptualized and experimentally tested. In addition, it is shown that the specific characteristics of emotion episodes can be profitably explored with the help of non-linear dynamic systems theory.", "title": "A systems approach to appraisal mechanisms in emotion"}, "25c937ed643e7ecf79f2d3c5376a54224fc16d0c": {"paper_id": "25c937ed643e7ecf79f2d3c5376a54224fc16d0c", "abstract": "Functional magnetic resonance imaging (fMRI) of the human brain was used to study whether the amygdala is activated in response to emotional stimuli, even in the absence of explicit knowledge that such stimuli were presented. Pictures of human faces bearing fearful or happy expressions were presented to 10 normal, healthy subjects by using a backward masking procedure that resulted in 8 of 10 subjects reporting that they had not seen these facial expressions. The backward masking procedure consisted of 33 msec presentations of fearful or happy facial expressions, their offset coincident with the onset of 167 msec presentations of neutral facial expressions. Although subjects reported seeing only neutral faces, blood oxygen level-dependent (BOLD) fMRI signal in the amygdala was significantly higher during viewing of masked fearful faces than during the viewing of masked happy faces. This difference was composed of significant signal increases in the amygdala to masked fearful faces as well as significant signal decreases to masked happy faces, consistent with the notion that the level of amygdala activation is affected differentially by the emotional valence of external stimuli. In addition, these facial expressions activated the sublenticular substantia innominata (SI), where signal increases were observed to both fearful and happy faces--suggesting a spatial dissociation of territories that respond to emotional valence versus salience or arousal value. This study, using fMRI in conjunction with masked stimulus presentations, represents an initial step toward determining the role of the amygdala in nonconscious processing.", "title": "Masked presentations of emotional facial expressions modulate amygdala activity without explicit knowledge."}, "6d75df4360a3d56514dcb775c832fdc572bab64b": {"paper_id": "6d75df4360a3d56514dcb775c832fdc572bab64b", "abstract": "We present here new evidence of cross-cultural agreement in the judgement of facial expression. Subjects in 10 cultures performed a more complex judgment task than has been used in previous cross-cultural studies. Instead of limiting the subjects to selecting only one emotion term for each expression, this task allowed them to indicate that multiple emotions were evident and the intensity of each emotion. Agreement was very high across cultures about which emotion was the most intense. The 10 cultures also agreed about the second most intense emotion signaled by an expression and about the relative intensity among expressions of the same emotion. However, cultural differences were found in judgments of the absolute level of emotional intensity.", "title": "Universals and cultural differences in the judgments of facial expressions of emotion."}, "1de0b6d3c8897aec5307f1d4937aeef75cc7ca37": {"paper_id": "1de0b6d3c8897aec5307f1d4937aeef75cc7ca37", "abstract": "OBJECTIVE\nThe present study aimed at examining the time course and topography of oscillatory brain activity and event-related potentials (ERPs) in response to laterally presented affective pictures.\n\n\nMETHODS\nElectroencephalography was recorded from 129 electrodes in 10 healthy university students during presentation of pictures from the international affective picture system. Frequency measures and ERPs were obtained for pleasant, neutral, and unpleasant pictures.\n\n\nRESULTS\nIn accordance with previous reports, a modulation of the late positive ERP wave at parietal recording sites was found as a function of emotional arousal. Early mid gamma band activity (GBA; 30-45 Hz) at 80 ms post-stimulus was enhanced in response to aversive stimuli only, whereas the higher GBA (46-65 Hz) at 500 ms showed an enhancement of arousing, compared to neutral pictures. ERP and late gamma effects showed a pronounced right-hemisphere preponderance, but differed in terms of topographical distribution.\n\n\nCONCLUSIONS\nLate gamma activity may represent a correlate of widespread cortical networks processing different aspects of emotionally arousing visual objects. In contrast, differences between affective categories in early gamma activity might reflect fast detection of aversive stimulus features.", "title": "Effects of emotional arousal in the cerebral hemispheres: a study of oscillatory brain activity and event-related potentials"}, "6313edb4136e8edc6a23c1512734c175b7affb63": {"paper_id": "6313edb4136e8edc6a23c1512734c175b7affb63", "abstract": "If the cortex is an associative memory, strongly connected cell assemblies will form when neurons in different cortical areas are frequently active at the same time. The cortical distributions of these assemblies must be a consequence of where in the cortex correlated neuronal activity occurred during learning. An assembly can be considered a functional unit exhibiting activity states such as full activation (\"ignition\") after appropriate sensory stimulation (possibly related to perception) and continuous reverberation of excitation within the assembly (a putative memory process). This has implications for cortical topographies and activity dynamics of cell assemblies forming during language acquisition, in particular for those representing words. Cortical topographies of assemblies should be related to aspects of the meaning of the words they represent, and physiological signs of cell assembly ignition should be followed by possible indicators of reverberation. The following postulates are discussed in detail: (1) assemblies representing phonological word forms are strongly lateralized and distributed over perisylvian cortices; (2) assemblies representing highly abstract words such as grammatical function words are also strongly lateralized and restricted to these perisylvian regions; (3) assemblies representing concrete content words include additional neurons in both hemispheres; (4) assemblies representing words referring to visual stimuli include neurons in visual cortices; and (5) assemblies representing words referring to actions include neurons in motor cortices. Two main sources of evidence are used to evaluate these proposals: (a) imaging studies focusing on localizing word processing in the brain, based on stimulus-triggered event-related potentials (ERPs), positron emission tomography (PET), and functional magnetic resonance imaging (fMRI), and (b) studies of the temporal dynamics of fast activity changes in the brain, as revealed by high-frequency responses recorded in the electroencephalogram (EEG) and magnetoencephalogram (MEG). These data provide evidence for processing differences between words and matched meaningless pseudowords, and between word classes, such as concrete content and abstract function words, and words evoking visual or motor associations. There is evidence for early word class-specific spreading of neuronal activity and for equally specific high-frequency responses occurring later. These results support a neurobiological model of language in the Hebbian tradition. Competing large-scale neuronal theories of language are discussed in light of the data summarized. Neurobiological perspectives on the problem of serial order of words in syntactic strings are considered in closing.", "title": "Words in the brain's language."}, "0d91ad0ad88fc925e1c5a63ddf7fba6d9399698b": {"paper_id": "0d91ad0ad88fc925e1c5a63ddf7fba6d9399698b", "abstract": "The present study was designed to test differential hemispheric activation induced by emotional stimuli in the gamma band range (30-90 Hz). Subjects viewed slides with differing emotional content (from the International Affective Picture System). A significant valence by hemisphere interaction emerged in the gamma band from 30-50 Hz. Other bands, including alpha and beta, did not show such an interaction. Previous hypotheses suggested that the left hemisphere is more involved in positive affective processing as compared to the right hemisphere, while the latter dominates during negative emotions. Contrary to this expectation, the 30-50 Hz band showed relatively more power for negative valence over the left temporal region as compared to the right and a laterality shift towards the right hemisphere for positive valence. In addition, emotional processing enhanced gamma band power at right frontal electrodes regardless of the particular valence as compared to processing neutral pictures. The extended distribution of specific activity in the gamma band may be the signature of cell assemblies with members in limbic, temporal and frontal neocortical structures that differ in spatial distribution depending on the particular type of emotional processing.", "title": "Processing of affective pictures modulates right-hemispheric gamma band EEG activity"}, "9576d1573688a05e45ad30e783e7d0c10a10d6d9": {"paper_id": "9576d1573688a05e45ad30e783e7d0c10a10d6d9", "abstract": "The Self-Assessment Manikin (SAM) is a non-verbal pictorial assessment technique that directly measures the pleasure, arousal, and dominance associated with a person's affective reaction to a wide variety of stimuli. In this experiment, we compare reports of affective experience obtained using SAM, which requires only three simple judgments, to the Semantic Differential scale devised by Mehrabian and Russell (An approach to environmental psychology, 1974) which requires 18 different ratings. Subjective reports were measured to a series of pictures that varied in both affective valence and intensity. Correlations across the two rating methods were high both for reports of experienced pleasure and felt arousal. Differences obtained in the dominance dimension of the two instruments suggest that SAM may better track the personal response to an affective stimulus. SAM is an inexpensive, easy method for quickly assessing reports of affective response in many contexts.", "title": "Measuring emotion: the Self-Assessment Manikin and the Semantic Differential."}, "66ecbdb79ce324a4e1c6f44353e5aaee0f164ad9": {"paper_id": "66ecbdb79ce324a4e1c6f44353e5aaee0f164ad9", "abstract": "The concept of attention as central to human performance extends back to the start of experimental psychology (James 1890), yet even a few years ago, it would not have been possible to outline in even a preliminary form a functional anatomy of the human attentional system. New developments in neuroscience (Hillyard & Picton 1987, Raichle 1983, Wurtz et al 1980) have opened the study of higher cognition to physiological analysis, and have revealed a system of anatomical areas that appear to be basic to the selection of information for focal (conscious) processing. The importance of attention is its unique role in connecting the mental level of description of processes used in cognitive science with the anatomical level common i neuroscience. Sperry (1988, p. 609) describes the central role that mental concepts play in understanding brain function as follows:", "title": "The attention system of the human brain."}, "78e005aef1fee015b9c39c12aee53046890d8481": {"paper_id": "78e005aef1fee015b9c39c12aee53046890d8481", "abstract": "Epileptic seizures are manifestations of epilepsy, a serious brain dynamical disorder second only to strokes. Of the world's /spl sim/50 million people with epilepsy, fully 1/3 have seizures that are not controlled by anti-convulsant medication. The field of seizure prediction, in which engineering technologies are used to decode brain signals and search for precursors of impending epileptic seizures, holds great promise to elucidate the dynamical mechanisms underlying the disorder, as well as to enable implantable devices to intervene in time to treat epilepsy. There is currently an explosion of interest in this field in academic centers and medical industry with clinical trials underway to test potential prediction and intervention methodology and devices for Food and Drug Administration (FDA) approval. This invited paper presents an overview of the application of signal processing methodologies based upon the theory of nonlinear dynamics to the problem of seizure prediction. Broader application of these developments to a variety of systems requiring monitoring, forecasting and control is a natural outgrowth of this field.", "title": "Epileptic seizure prediction and control"}, "95fe18cb21b5f380bb3ee341ee6d5008d869fef0": {"paper_id": "95fe18cb21b5f380bb3ee341ee6d5008d869fef0", "abstract": "The emergence of a unified cognitive moment relies on the coordination of scattered mosaics of functionally specialized brain regions. Here we review the mechanisms of large-scale integration that counterbalance the distributed anatomical and functional organization of brain activity to enable the emergence of coherent behaviour and cognition. Although the mechanisms involved in large-scale integration are still largely unknown, we argue that the most plausible candidate is the formation of dynamic links mediated by synchrony over multiple frequency bands.", "title": "The brainweb: Phase synchronization and large-scale integration"}, "e306fa4ee8035f48f1bee70bdad0c94e1e1e8803": {"paper_id": "e306fa4ee8035f48f1bee70bdad0c94e1e1e8803", "abstract": "This paper presents an average current mode buck dimmable light-emitting diode (LED) driver for large-scale single-string LED backlighting applications. The proposed integrated current control technique can provide exact current control signals by using an autozeroed integrator to enhance the accuracy of the average current of LEDs while driving a large number of LEDs. Adoption of discontinuous low-side current sensing leads to power loss reduction. Adoption of a fast-settling technique allows the LED driver to enter into the steady state within three switching cycles after the dimming signal is triggered. Implemented in a 0.35-\u03bcm HV CMOS process, the proposed LED driver achieves 1.7% LED current error and 98.16% peak efficiency over an input voltage range of 110 to 200 V while driving 30 to 50 LEDs.", "title": "A 200-V 98.16%-Efficiency Buck LED Driver Using Integrated Current Control to Improve Current Accuracy for Large-Scale Single-String LED Backlighting Applications"}, "feea8c02428b9fdab94281b0c7c062739db3165c": {"paper_id": "feea8c02428b9fdab94281b0c7c062739db3165c", "abstract": "Object tracking under complex circumstances is a challenging task because of background interference, obstacle occlusion, object deformation, etc. Given such conditions, robustly detecting, locating, and analyzing a target through single-feature representation are difficult tasks. Global features, such as color, are widely used in tracking, but may cause the object to drift under complex circumstances. Local features, such as HOG and SIFT, can precisely represent rigid targets, but these features lack the robustness of an object in motion. An effective method is adaptive fusion of multiple features in representing targets. The process of adaptively fusing different features is the key to robust object tracking. This study uses a multi-feature joint descriptor (MFJD) and the distance between joint histograms to measure the similarity between a target and its candidate patches. Color and HOG features are fused as the tracked object of the joint representation. This study also proposes a self-adaptive multi-feature fusion strategy that can adaptively adjust the joint weight of the fused features based on their stability and contrast measure scores. The mean shift process is adopted as the object tracking framework with multi-feature representation. The experimental results demonstrate that the proposed MFJD tracking method effectively handles background clutter, partial occlusion by obstacles, scale changes, and deformations. The novel method performs better than several state-of-the-art methods in real surveillance scenarios.", "title": "Robust object tracking via multi-feature adaptive fusion based on stability: contrast analysis"}, "469f5b07c8927438b79a081efacea82449b338f8": {"paper_id": "469f5b07c8927438b79a081efacea82449b338f8", "abstract": "This paper presents an e cient shape-based object detection method based on Distance Transforms and describes its use for real-time vision on-board vehicles. The method uses a template hierarchy to capture the variety of object shapes; e cient hierarchies can be generated o ine for given shape distributions using stochastic optimization techniques (i.e. simulated annealing). Online, matching involves a simultaneous coarse-tone approach over the shape hierarchy and over the transformation parameters. Very large speedup factors are typically obtained when comparing this approach with the equivalent brute-force formulation; we have measured gains of several orders of magnitudes. We present experimental results on the real-time detection of tra c signs and pedestrians from a moving vehicle. Because of the highly time sensitive nature of these vision tasks, we also discuss some hardwarespeci c implementations of the proposed method as far as SIMD parallelism is concerned.", "title": "Real-Time Object Detection for \"Smart\" Vehicles"}, "5010f30b0e6a71a16b49cbc2134450bb9e3a2659": {"paper_id": "5010f30b0e6a71a16b49cbc2134450bb9e3a2659", "abstract": "The ability to recognize humans and their activities by vision is key for a machine to interact intelligently and effortlessly with a human-inhabited environment. Because of many potentially important applications, \u201clooking at people\u201d is currently one of the most active application domains in computer vision. This survey identifies a number of promising applications and provides an overview of recent developments in this domain. The scope of this survey is limited to work on whole-body or hand motion; it does not include work on human faces. The emphasis is on discussing the various methodologies; they are grouped in 2-D approaches with or without explicit shape models and 3-D approaches. Where appropriate, systems are reviewed. We conclude with some thoughts about future directions. c \u00a9 1999 Academic Press", "title": "The Visual Analysis of Human Movement: A Survey"}, "36cd88ed2c17a596001e9c7d89533ac46c28dec0": {"paper_id": "36cd88ed2c17a596001e9c7d89533ac46c28dec0", "abstract": "In this paper we describe a trainable object detector and its instantiations for detecting faces and cars at any size, location, and pose. To cope with variation in object orientation, the detector uses multiple classifiers, each spanning a different range of orientation. Each of these classifiers determines whether the object is present at a specified size within a fixed-size image window. To find the object at any location and size, these classifiers scan the image exhaustively. Each classifier is based on the statistics of localized parts. Each part is a transform from a subset of wavelet coefficients to a discrete set of values. Such parts are designed to capture various combinations of locality in space, frequency, and orientation. In building each classifier, we gathered the class-conditional statistics of these part values from representative samples of object and non-object images. We trained each classifier to minimize classification error on the training set by using Adaboost with Confidence-Weighted Predictions (Shapire and Singer, 1999). In detection, each classifier computes the part values within the image window and looks up their associated class-conditional probabilities. The classifier then makes a decision by applying a likelihood ratio test. For efficiency, the classifier evaluates this likelihood ratio in stages. At each stage, the classifier compares the partial likelihood ratio to a threshold and makes a decision about whether to cease evaluation\u2014labeling the input as non-object\u2014or to continue further evaluation. The detector orders these stages of evaluation from a low-resolution to a high-resolution search of the image. Our trainable object detector achieves reliable and efficient detection of human faces and passenger cars with out-of-plane rotation.", "title": "Object Detection Using the Statistics of Parts"}, "1bfe26fac93ad96c81cf1a580b9e7744477f56aa": {"paper_id": "1bfe26fac93ad96c81cf1a580b9e7744477f56aa", "abstract": "\u00d0Our goal is to develop a visual monitoring system that passively observes moving objects in a site and learns patterns of activity from those observations. For extended sites, the system will require multiple cameras. Thus, key elements of the system are motion tracking, camera coordination, activity classification, and event detection. In this paper, we focus on motion tracking and show how one can use observed motion to learn patterns of activity in a site. Motion segmentation is based on an adaptive background subtraction method that models each pixel as a mixture of Gaussians and uses an on-line approximation to update the model. The Gaussian distributions are then evaluated to determine which are most likely to result from a background process. This yields a stable, real-time outdoor tracker that reliably deals with lighting changes, repetitive motions from clutter, and long-term scene changes. While a tracking system is unaware of the identity of any object it tracks, the identity remains the same for the entire tracking sequence. Our system leverages this information by accumulating joint co-occurrences of the representations within a sequence. These joint cooccurrence statistics are then used to create a hierarchical binary-tree classification of the representations. This method is useful for classifying sequences, as well as individual instances of activities in a site. Index Terms\u00d0Real-time visual tracking, adaptive background estimation, activity modeling, co-occurrence clustering, object recognition, video surveillance and", "title": "Learning Patterns of Activity Using Real-Time Tracking"}, "b94c7ff9532ab26c3aedbee3988ec4c7a237c173": {"paper_id": "b94c7ff9532ab26c3aedbee3988ec4c7a237c173", "abstract": "w e propose Q novel approach for solving the perceptual grouping problem in vision. Rather than focusing on local features and their consistencies in the amage data, our approach aims a t extracting the global impression of an image. We treat image segmentation QS (I graph partitioning problem and propose Q novel global criterion, the normalized cut, for segmenting the graph. The normalized cut craterion measures both the total dissimilarity between the different groups QS well as the total similarity within the groups. We show that an eficient computational technique based on a generaked eigenvalue problem can be used to optimize this criterion. w e have applied this approach to segmenting static images and found results very enco u raging.", "title": "Normalized Cuts and Image Segmentation"}, "1790ae874429c5e033677c358cb43a77e4486f55": {"paper_id": "1790ae874429c5e033677c358cb43a77e4486f55", "abstract": "A common method for real-time segmentation of moving regions in image sequences involves \u201cbackground subtraction,\u201d or thresholding the error between an estimate of the image without moving objects and the current image. The numerous approaches to this problem differ in the type of background model used and the procedure used to update the model. This paper discusses modeling each pixel as a mixture of Gaussians and using an on-line approximation to update the model. The Gaussian distributions of the adaptive mixture model are then evaluated to determine which are most likely to result from a background process. Each pixel is classified based on whether the Gaussian distribution which represents it most effectively is considered part of the background model. This results in a stable, real-time outdoor tracker which reliably deals with lighting changes, repetitive motions from clutter, and long-term scene changes. This system has been run almost continuously for 16 months, 24 hours a day, through rain and snow.", "title": "Adaptive Background Mixture Models for Real-Time Tracking"}, "0ce9ad941f6da90068759344abf07a4e15cf4ccf": {"paper_id": "0ce9ad941f6da90068759344abf07a4e15cf4ccf", "abstract": "The Robotics Institute at Carnegie Mellon University (CMU) and the Sarnoff Corporation are developing a system for autonomous Video Surveillance and Monitoring. The technical objective is to use multiple, cooperative video sensors to provide continuous coverage of people and vehicles in cluttered environments. This paper presents an overview of the system and significant results achieved to date.", "title": "A System for Video Surveillance and Monitoring"}, "16404d38775e2e8ddcd28e2ca6e79cf8a82d0d9f": {"paper_id": "16404d38775e2e8ddcd28e2ca6e79cf8a82d0d9f", "abstract": "By combining Histograms of Oriented Gradients (HOG) and Local Binary Pattern (LBP) as the feature set, we propose a novel human detection approach capable of handling partial occlusion. Two kinds of detectors, i.e., global detector for whole scanning windows and part detectors for local regions, are learned from the training data using linear SVM. For each ambiguous scanning window, we construct an occlusion likelihood map by using the response of each block of the HOG feature to the global detector. The occlusion likelihood map is then segmented by Mean-shift approach. The segmented portion of the window with a majority of negative response is inferred as an occluded region. If partial occlusion is indicated with high likelihood in a certain scanning window, part detectors are applied on the unoccluded regions to achieve the final classification on the current scanning window. With the help of the augmented HOG-LBP feature and the global-part occlusion handling method, we achieve a detection rate of 91.3% with FPPW= 10\u22126, 94.7% with FPPW= 10\u22125, and 97.9% with FPPW= 10\u22124 on the INRIA dataset, which, to our best knowledge, is the best human detection performance on the INRIA dataset. The global-part occlusion handling method is further validated using synthesized occlusion data constructed from the INRIA and Pascal dataset.", "title": "An HOG-LBP human detector with partial occlusion handling"}, "1c168275c59ba382588350ee1443537f59978183": {"paper_id": "1c168275c59ba382588350ee1443537f59978183", "abstract": "Mean shift, a simple iterative procedure that shifts each data point to the average of data points in its neighborhood, is generalized and analyzed in this paper. This generalization makes some k-means like clustering algorithms its special cases. It is shown that mean shift is a mode-seeking process on a surface constructed with a \u201cshadow\u201d kernel. For Gaussian kernels, mean shift is a gradient mapping. Convergence is studied for mean shift iterations. Cluster analysis is treated as a deterministic problem of finding a fixed point of mean shift that characterizes the data. Applications in clustering and Hough transform are demonstrated. Mean shift is also considered as an evolutionary strategy that performs multistart global optimization.", "title": "Mean Shift, Mode Seeking, and Clustering"}, "0fb1b0ce8b93abcfd30a4bb41d4d9b266b1c0f64": {"paper_id": "0fb1b0ce8b93abcfd30a4bb41d4d9b266b1c0f64", "abstract": "This paper describes a visual object detection framework that is capable of processing images extremely rapidly while achieving high detection rates. There are three key contributions. The first is the introduction of a new image representation called the \u201cIntegral Image\u201d which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features and yields extremely efficient classifiers [6]. The third contribution is a method for combining classifiers in a \u201ccascade\u201d which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. A set of experiments in the domain of face detection are presented. The system yields face detection performace comparable to the best previous systems [18, 13, 16, 12, 1]. Implemented on a conventional desktop, face detection proceeds at 15 frames per second.", "title": "Robust Real-time Object Detection"}, "28bfb9f0e16cb66ba4422bee5902e79c5e89e765": {"paper_id": "28bfb9f0e16cb66ba4422bee5902e79c5e89e765", "abstract": "Detecting humans in films and videos is a challenging problem owing to the motion of the subjects, the camera and the background and to variations in pose, appearance, clothing, illumination and background clutter. We develop a detector for standing and moving people in videos with possibly moving cameras and backgrounds, testing several different motion coding schemes and showing empirically that orientated histograms of differential optical flow give the best overall performance. These motion-based descriptors are combined with our Histogram of Oriented Gradient appearance descriptors. The resulting detector is tested on several databases including a challenging test set taken from feature films and containing wide ranges of pose, motion and background variations, including moving cameras and backgrounds. We validate our results on two challenging test sets containing more than 4400 human examples. The combined detector reduces the false alarm rate by a factor of 10 relative to the best appearance-based detector, for example giving false alarm rates of 1 per 20,000 windows tested at 8% miss rate on our Test Set 1.", "title": "Human Detection Using Oriented Histograms of Flow and Appearance"}, "b623b8d6282b78e7f894a6a0a390fc05500f9534": {"paper_id": "b623b8d6282b78e7f894a6a0a390fc05500f9534", "abstract": "Detecting people in images is key for several important application domains in computer vision. This paper presents an in-depth experimental study on pedestrian classification; multiple feature-classifier combinations are examined with respect to their ROC performance and efficiency. We investigate global versus local and adaptive versus nonadaptive features, as exemplified by PCA coefficients, Haar wavelets, and local receptive fields (LRFs). In terms of classifiers, we consider the popular support vector machines (SVMs), feedforward neural networks, and k-nearest neighbor classifier. Experiments are performed on a large data set consisting of 4,000 pedestrian and more than 25,000 nonpedestrian (labeled) images captured in outdoor urban environments. Statistically meaningful results are obtained by analyzing performance variances caused by varying training and test sets. Furthermore, we investigate how classification performance and training sample size are correlated. Sample size is adjusted by increasing the number of manually labeled training data or by employing automatic bootstrapping or cascade techniques. Our experiments show that the novel combination of SVMs with LRF features performs best. A boosted cascade of Haar wavelets can, however, reach quite competitive results, at a fraction of computational cost. The data set used in this paper is made public, establishing a benchmark for this important problem", "title": "An Experimental Study on Pedestrian Classification"}, "bec9dc98201a898f3afbcb14c7f8ef51ea2137b5": {"paper_id": "bec9dc98201a898f3afbcb14c7f8ef51ea2137b5", "abstract": "Visual prior from generic real-world images can be learned and transferred for representing objects in a scene. Motivated by this, we propose an algorithm that transfers visual prior learned offline for online object tracking. From a collection of real-world images, we learn an overcomplete dictionary to represent visual prior. The prior knowledge of objects is generic, and the training image set does not necessarily contain any observation of the target object. During the tracking process, the learned visual prior is transferred to construct an object representation by sparse coding and multiscale max pooling. With this representation, a linear classifier is learned online to distinguish the target from the background and to account for the target and background appearance variations over time. Tracking is then carried out within a Bayesian inference framework, in which the learned classifier is used to construct the observation model and a particle filter is used to estimate the tracking result sequentially. Experiments on a variety of challenging sequences with comparisons to several state-of-the-art methods demonstrate that more robust object tracking can be achieved by transferring visual prior.", "title": "Transferring Visual Prior for Online Object Tracking"}, "caa0fd34e50bb417fae3ee32f667e78fe5b198bc": {"paper_id": "caa0fd34e50bb417fae3ee32f667e78fe5b198bc", "abstract": "The goal of this article is to review the state-of-the-art tracking methods, classify them into different categories, and identify new trends. Object tracking, in general, is a challenging problem. Difficulties in tracking objects can arise due to abrupt object motion, changing appearance patterns of both the object and the scene, nonrigid object structures, object-to-object and object-to-scene occlusions, and camera motion. Tracking is usually performed in the context of higher-level applications that require the location and/or shape of the object in every frame. Typically, assumptions are made to constrain the tracking problem in the context of a particular application. In this survey, we categorize the tracking methods on the basis of the object and motion representations used, provide detailed descriptions of representative methods in each category, and examine their pros and cons. Moreover, we discuss the important issues related to tracking including the use of appropriate image features, selection of motion models, and detection of objects.", "title": "Object tracking: A survey"}, "102ed1e9b785caec1cb69c043dbda7b2cfa2d57d": {"paper_id": "102ed1e9b785caec1cb69c043dbda7b2cfa2d57d", "abstract": "This paper proposes a new tree-based ensemble method for supervised classification and regression problems. It essentially consists of randomizing strongly both attribute and cut-point choice while splitting a tree node. In the extreme case, it builds totally randomized trees whose structures are independent of the output values of the learning sample. The strength of the randomization can be tuned to problem specifics by the appropriate choice of a parameter. We evaluate the robustness of the default choice of this parameter, and we also provide insight on how to adjust it in particular situations. Besides accuracy, the main strength of the resulting algorithm is computational efficiency. A bias/variance analysis of the Extra-Trees algorithm is also provided as well as a geometrical and a kernel characterization of the models induced.", "title": "Extremely randomized trees"}, "1427fc2aace877b91e43aefd1fe0b2a19b01d78b": {"paper_id": "1427fc2aace877b91e43aefd1fe0b2a19b01d78b", "abstract": "Recently, on-line adaptation of binary classifiers for tracking has been investigated. On-line learning allows for simple classifiers since only the current view of the object from its surrounding background needs to be discriminated. However, on-line adaptation faces one key problem: Each update of the tracker may introduce an error which, finally, can lead to tracking failure (drifting). The contribution of this paper is a novel on-line semi-supervised boosting method which significantly alleviates the drifting problem in tracking applications. This allows to limit the drifting problem while still staying adaptive to appearance changes. The main idea is to formulate the update process in a semi-supervised fashion as combined decision of a given prior and an on-line classifier. This comes without any parameter tuning. In the experiments, we demonstrate real-time tracking of our SemiBoost tracker on several challenging test sequences where our tracker outperforms other on-line tracking methods. Helmut Grabner1,2 Christian Leistner1 Horst Bischof1 1 Institute for Computer Graphics and Vision, Graz University of Technology, Austria 2 Computer Vision Laboratory, ETH-Zurich, Switzerland Motivation Robust Tracking Loop search Region actual object position create confidence map update classifier (tracker) evaluate classifier", "title": "Semi-supervised On-Line Boosting for Robust Tracking"}, "e427ec49dd2b70bbfa89250b5fec0751abc2b9a8": {"paper_id": "e427ec49dd2b70bbfa89250b5fec0751abc2b9a8", "abstract": "Human adenoviruses (HAdV) are responsible for a wide spectrum of diseases. The neutralization epsilon determinant (loops 1 and 2) and the hemagglutination gamma determinant are relevant for the taxonomy of HAdV. Precise type identification of HAdV prototypes is crucial for detection of infection chains and epidemiology. epsilon and gamma determinant sequences of all 51 HAdV were generated to propose molecular classification criteria. Phylogenetic analysis of epsilon determinant sequences demonstrated sufficient genetic divergence for molecular classification, with the exception of HAdV-15 and HAdV-29, which also cannot be differentiated by classical cross-neutralization. Precise sequence divergence criteria for typing (<2.5% from loop 2 prototype sequence and <2.4% from loop 1 sequence) were deduced from phylogenetic analysis. These criteria may also facilitate identification of new HAdV prototypes. Fiber knob (gamma determinant) phylogeny indicated a two-step model of species evolution and multiple intraspecies recombination events in the origin of HAdV prototypes. HAdV-29 was identified as a recombination variant of HAdV-15 (epsilon determinant) and a speculative, not-yet-isolated HAdV prototype (gamma determinant). Subanalysis of molecular evolution in hypervariable regions 1 to 6 of the epsilon determinant indicated different selective pressures in subclusters of species HAdV-D. Additionally, gamma determinant phylogenetic analysis demonstrated that HAdV-8 did not cluster with -19 and -37 in spite of their having the same tissue tropism. The phylogeny of HAdV-E4 suggested origination by interspecies recombination between HAdV-B (hexon) and HAdV-C (fiber), as in simian adenovirus 25, indicating additional zoonotic transfer. In conclusion, molecular classification by systematic sequence analysis of immunogenic determinants yields new insights into HAdV phylogeny and evolution.", "title": "Phylogenetic analysis of the main neutralization and hemagglutination determinants of all human adenovirus prototypes as a basis for molecular classification and taxonomy."}, "8b7773160b2e892c1b042565bbdc747996e4bcef": {"paper_id": "8b7773160b2e892c1b042565bbdc747996e4bcef", "abstract": "The question whether preemptive systems are better than non-preemptive systems has been debated for a long time, but only partial answers have been provided in the real-time literature and still some issues remain open. In fact, each approach has advantages and disadvantages, and no one dominates the other when both predictability and efficiency have to be taken into account in the system design. In particular, limiting preemptions allows increasing program locality, making timing analysis more predictable with respect to the fully preemptive case. In this paper, we integrate the features of both preemptive and non-preemptive scheduling by considering that each task can switch to non-preemptive mode, at any time, for a bounded interval. Three methods (with different complexity and performance) are presented to calculate the longest non-preemptive interval that can be executed by each task, under fixed priorities, without degrading the schedulability of the task set, with respect to the fully preemptive case. The methods are also compared by simulations to evaluate their effectiveness in reducing the number of preemptions.", "title": "Bounding the Maximum Length of Non-preemptive Regions under Fixed Priority Scheduling"}, "5bb7da9b268c9a066e2be2e5142e5e2c11630437": {"paper_id": "5bb7da9b268c9a066e2be2e5142e5e2c11630437", "abstract": "Feasibility analysis of fixed priority systems has been widely studied in the real-time literature and several acceptance tests have been proposed to guarantee a set of periodic tasks. They can be divided in two main classes: polynomial time tests and exact tests. Polynomial time tests can efficiently be used for online guarantee of real-time applications, where tasks are activated at runtime. These tests introduce a negligible overhead, when executed upon a new task arrival, however provide only a sufficient schedulability condition, which may cause a poor processor utilization. On the other hand, exact tests, which are based on response time analysis, provide a necessary and sufficient schedulability condition, but are too complex to be executed on line for large task sets. As a consequence, for large task sets, they are often executed off line. This paper proposes a novel approach for analyzing the schedulability of periodic task sets on a single processor under an arbitrary fixed priority assignment: Using this approach, we derive a new schedulability test which can be tuned through a parameter to balance complexity versus acceptance ratio, so that it can be used on line to better exploit the processor, based on the available computational power. Extensive simulations show that our test, when used in its exact form, is significantly faster than the current response time analysis methods. Moreover the proposed approach, for its elegance and compactness, offers an explanation of some known phenomena of fixed priority scheduling and could be helpful for further work on schedulability analysis.", "title": "Schedulability analysis of periodic fixed priority systems"}, "36f8f7b0d30f58e925969e227c8847eddfedf22a": {"paper_id": "36f8f7b0d30f58e925969e227c8847eddfedf22a", "abstract": "This paper considers the problem of fixed priority scheduling of periodic tasks where each task\u2019s execution priority may vary. Periodic tasks are decomposed into serially executed subtasks. where each subtask is characterized by an execution time and a fixed priority, and is permitted to have a deadline. A method for determining the schedulability of each task is presented along with its theoretical underpinnings. This method can be used to analyze the schedulability of complex task sets which involve interrupts, certain synchronization protocols, nonpreemptible sections and, in general, any mechanism that contributes to a complex priority structure. The method is illustrated with a realistic example.", "title": "Fixed priority scheduling periodic tasks with varying execution priority"}, "a9244fcfe729d8e81bad99e93279cfe90613e45f": {"paper_id": "a9244fcfe729d8e81bad99e93279cfe90613e45f", "abstract": "This paper considers the problem of fixed priority scheduling of periodic tasks with arbitrary deadlines. A general criterion for the schedulability of such a task set is given. Worst case bounds are given which generalize the Liu and Layland bound. The results are shown to provide a basis for developing predictable distributed real-time systems.", "title": "Fixed Priority Scheduling of Periodic Task Sets with Arbitrary Deadlines"}, "1bbb2c63c35e1d0836feb4a133f00c56cb52b761": {"paper_id": "1bbb2c63c35e1d0836feb4a133f00c56cb52b761", "abstract": "We present a new algorithm for near-interactive simulation of skeleton driven, high resolution elasticity models. Our methodology is used for soft tissue deformation in character animation. The algorithm is based on a novel discretization of corotational elasticity over a hexahedral lattice. Within this framework we enforce positive definiteness of the stiffness matrix to allow efficient quasistatics and dynamics. In addition, we present a multigrid method that converges with very high efficiency. Our design targets performance through parallelism using a fully vectorized and branch-free SVD algorithm as well as a stable one-point quadrature scheme. Since body collisions, self collisions and soft-constraints are necessary for real-world examples, we present a simple framework for enforcing them. The whole approach is demonstrated in an end-to-end production-level character skinning system.", "title": "Efficient elasticity for character skinning with contact and collisions"}, "7bf497df069796e1ef152244f262c67a6139cd2a": {"paper_id": "7bf497df069796e1ef152244f262c67a6139cd2a", "abstract": "Skinning of skeletally deformable models is extensively used for real-time animation of characters, creatures and similar objects. The standard solution, linear blend skinning, has some serious drawbacks that require artist intervention. Therefore, a number of alternatives have been proposed in recent years. All of them successfully combat some of the artifacts, but none challenge the simplicity and efficiency of linear blend skinning. As a result, linear blend skinning is still the number one choice for the majority of developers. In this article, we present a novel skinning algorithm based on linear combination of dual quaternions. Even though our proposed method is approximate, it does not exhibit any of the artifacts inherent in previous methods and still permits an efficient GPU implementation. Upgrading an existing animation system from linear to dual quaternion skinning is very easy and has a relatively minor impact on runtime performance.", "title": "Geometric skinning with approximate dual quaternion blending"}, "3c46b3845b599b229a24f44859d9b271f286bd51": {"paper_id": "3c46b3845b599b229a24f44859d9b271f286bd51", "abstract": "Pose space deformation generalizes and improves upon both shape interpolation and common skeleton-driven deformation techniques. This deformation approach proceeds from the observation that several types of deformation can be uniformly represented as mappings from a pose space, defined by either an underlying skeleton or a more abstract system of parameters, to displacements in the object local coordinate frames. Once this uniform representation is identified, previously disparate deformation types can be accomplished within a single unified approach. The advantages of this algorithm include improved expressive power and direct manipulation of the desired shapes yet the performance associated with traditional shape interpolation is achievable. Appropriate applications include animation of facial and body deformation for entertainment, telepresence, computer gaming, and other applications where direct sculpting of deformations is desired or where real-time synthesis of a deforming model is required.", "title": "Pose space deformation: a unified approach to shape interpolation and skeleton-driven deformation"}, "614d54e97c100f13bfcbe393968e16be16f88288": {"paper_id": "614d54e97c100f13bfcbe393968e16be16f88288", "abstract": "We present a tagged corpus for English noun compound interpretation and describe the method used to generate them. In order to collect noun compounds, we extracted binary noun compounds (i.e. noun-noun pairs) by looking for sequences of two nouns in the POS tag data of the Wall Street Journal. We then manually filtered out all noun compounds which were incorrectly tagged or included proper nouns. This left us with a data set of 2,169 noun compounds, which we annotated using a set of 20 semantic relations defined by Barker and Szpakowicz (1998) allowing the annotators to assign multiple semantic relations if necessary. The initial agreement was 52.31%. The final data set contains 1,081 test noun compounds and 1,088 training noun compounds.", "title": "Standardised Evaluation of English Noun Compound Interpretation"}, "50ad73ad256cae97a75d12ac58894c52067bb174": {"paper_id": "50ad73ad256cae97a75d12ac58894c52067bb174", "abstract": "This paper presents an approach for detecting semantic relations in noun phrases. A learning algorithm, called semantic scattering, is used to automatically label complex nominals, genitives and adjectival noun phrases with the corresponding semantic relation. 1 Problem description This paper is about the automatic labeling of semantic relations in noun phrases (NPs). The semantic relations are the underlying relations between two concepts expressed by words or phrases. We distinguish here between semantic relations and semantic roles. Semantic roles are always between verbs (or nouns derived from verbs) and other constituents (run quickly, went to the store, computer maker), whereas semantic relations can occur between any constituents, for example in complex nominals (malaria mosquito (CAUSE)), genitives (girl\u2019s mouth (PART-WHOLE)), prepositional phrases attached to nouns (man at the store (LOCATIVE)), or discourse level (The bus was late. As a result, I missed my appointment (CAUSE)). Thus, in a sense, semantic relations are more general than semantic roles and many semantic role types will appear on our list of semantic relations. The following NP level constructions are considered here (cf. the classifications provided by (Quirk et al.1985) and (Semmelmeyer and Bolander 1992)): (1) Compound Nominals consisting of two consecutive nouns (eg night club a TEMPORAL relation indicating that club functions at night), (2) Adjective Noun constructions where the adjectival modifier is derived from a noun (eg musical clock a MAKE/PRODUCE relation), (3) Genitives (eg the door of the car a PART-WHOLE relation), and (4) Adjective phrases (cf. (Semmelmeyer and Bolander 1992)) in which the modifier noun is expressed by a prepositional phrase which functions as an adjective (eg toy in the box a LOCATION relation). Example: \u201cSaturday\u2019s snowfall topped a one-day record in Hartford, Connecticut, with the total of 12.5 inches, the weather service said. The storm claimed its fatality Thursday, when a car which was driven by a college student skidded on an interstate overpass in the mountains of Virginia and hit a concrete barrier, police said\u201d. (www.cnn.com \u201cRecord-setting Northeast snowstorm winding down\u201d, Sunday, December 7, 2003). There are several semantic relations at the noun phrase level: (1) Saturday\u2019s snowfall is a genitive encoding a TEMPORAL relation, (2) one-day record is a TOPIC noun compound indicating that record is about one-day snowing an ellipsis here, (3) record in Hartford is an adjective phrase in a LOCATION relation, (4) total of 12.5 inches is an of-genitive that expresses MEASURE, (5) weather service is a noun compound in a TOPIC relation, (6) car which was driven by a college student encodes a THEME semantic role in an adjectival clause, (7) college student is a compound nominal in a PART-WHOLE/MEMBER-OF relation, (8) interstate overpass is a LOCATION noun compound, (9) mountains of Virginia is an of-genitive showing a PART-WHOLE/PLACE-AREA and LOCATION relation, (10) concrete barrier is a noun compound encoding PART-WHOLE/STUFF-OF. 1.1 List of Semantic Relations After many iterations over a period of time we identified a set of semantic relations that cover a large majority of text semantics. Table 1 lists these relations, their definitions, examples, and some references. Most of the time, the semantic relations are encoded by lexico-syntactic patterns that are highly ambiguous. One pattern can express a number of semantic relations, its disambiguation being provided by the context or world knowledge. Often semantic relations are not disjoint or mutually exclusive, two or more appearing in the same lexical construct. This is called semantic blend (Quirk et al.1985). For example, the expression \u201cTexas city\u201d contains both a LOCATION as well as a PART-WHOLE relation. Other researchers have identified other sets of semantic relations (Levi 1979), (Vanderwende 1994), (Sowa 1994), (Baker, Fillmore, and Lowe 1998), (Rosario and Hearst 2001), (Kingsbury, et al. 2002), (Blaheta and Charniak 2000), (Gildea and Jurafsky 2002), (Gildea and Palmer 2002). Our list contains the most frequently used semantic relations we have observed on a large corpus. Besides the work on semantic roles, considerable interest has been shown in the automatic interpretation of complex nominals, and especially of compound nominals. The focus here is to determine the semantic relations that hold between different concepts within the same phrase, and to analyze the meaning of these compounds. Several approaches have been proposed for empirical noun-compound interpretation, such as syntactic analysis based on statistical techniques (Lauer and Dras 1994), (Pustejovsky et al. 1993). Another popular approach focuses on the interpretation of the underlying semantics. Many researchers that followed this approach relied mostly on hand-coded rules (Finin 1980), (Vanderwende 1994). More recently, (Rosario and Hearst 2001), (Rosario, Hearst, and Fillmore 2002), (Lapata 2002) have proposed automatic methods that analyze and detect noun compounds relations from text. (Rosario and Hearst 2001) focused on the medical domain making use of a lexical ontology and standard machine learning techniques.", "title": "Models For The Semantic Classification Of Noun Phrases"}, "16c43db3f50e714d4a6d773cd8b709a34b338167": {"paper_id": "16c43db3f50e714d4a6d773cd8b709a34b338167", "abstract": "We are developing corpus-based techniques for identifying semantic relations at an intermediate level of description (more specific than those used in case frames, but more general than those used in traditional knowledge representation systems). In this paper we describe a classification algorithm for identifying relationships between two-word noun compounds. We find that a very simple approach using a machine learning algorithm and a domain-specific lexical hierarchy successfully generalizes from training instances, performing better on previously unseen words than a baseline consisting of training on the words themselves.", "title": "Classifying the Semantic Relations in Noun Compounds via a Domain-Specific Lexical Hierarchy"}, "621566b223a73c0a7d8cf918297ac02c5e58af38": {"paper_id": "621566b223a73c0a7d8cf918297ac02c5e58af38", "abstract": "This paper presents our basic approach to creating Proposition Bank, which involves adding a layer of semantic annotation to the Penn English TreeBank. Without attempting to confirm or disconfirm any particular semantic theory, our goal is to provide consistent argument labeling that will facilitate the automatic extraction of relational data. An argument such asthe window in John broke the window and in The window brokewould receive the same label in both sentences. In order to ensure reliable human annotation, we provide our annotators with explicit guidelines for labeling all of the syntactic and semantic frames of each particular verb. We give several examples of these guidelines and discuss the inter\u2212annotator agreement figures. We also discuss our current experiments on the automatic expansion of our verb guidelines based on verb class membership. Our current rate of progress and our consistency of annotation demonstrate the feasibility of the task.", "title": "Adding Semantic Annotation to the Penn TreeBank"}, "9295ec336f4235f2680c87031191cf722aaf6e29": {"paper_id": "9295ec336f4235f2680c87031191cf722aaf6e29", "abstract": "This paper briefly introduces an approach to the problem of building semantic interpretations of nominal ComDounds, i.e. sequences of two or more nouns related through modification. Examples of the kinds of nominal compounds dealt with are: \"engine repairs\", \"aircraft flight arrival\", ~aluminum water pump\", and \"noun noun modification\".", "title": "The Semantic Interpretation of Nominal Compounds"}, "35471fa1234fb7f7dc9586df0c5b23371f806a04": {"paper_id": "35471fa1234fb7f7dc9586df0c5b23371f806a04", "abstract": "We study the performance of two representations of word meaning in learning noun-modifier semantic relations. One representation is based on lexical resources, in particular WordNet, the other \u2013 on a corpus. We experimented with decision trees, instance-based learning and Support Vector Machines. All these methods work well in this learning task. We report high precision, recall and F-score, and small variation in performance across several 10-fold cross-validation runs. The corpus-based method has the advantage of working with data without word-sense annotations and performs well over the baseline. The WordNet-based method, requiring wordsense annotated data, has higher precision.", "title": "Learning Noun-Modifier Semantic Relations with Corpus-based and WordNet-based Features"}, "11157109b8f3a098c5c3f801ba9acbffd2aa49b1": {"paper_id": "11157109b8f3a098c5c3f801ba9acbffd2aa49b1", "abstract": "Bootstrapping semantics from text is one of the greatest challenges in natural language learning. We first define a word similarity measure based on the distributional pattern of words. The similarity measure allows us to construct a thesaurus using a parsed corpus. We then present a new evaluation methodology for the automatically constructed thesaurus. The evaluation results show that the thesaurus is significantly closer to WordNet than Roget Thesaurus is.", "title": "Automatic Retrieval and Clustering of Similar Words"}, "20ad0ba7e187e6f335a08c59a4e53da4e4b027ec": {"paper_id": "20ad0ba7e187e6f335a08c59a4e53da4e4b027ec", "abstract": "We describe a method for the automatic acquisition of the hyponymy lexical relation from unrestricted text. Two goals motivate the approach: (i) avoidance of the need for pre-encoded knowledge and (ii) applicability across a wide range of text. We identify a set of lexicosyntactic patterns that are easily recognizable, that occur frequently and across text genre boundaries, and that indisputably indicate the lexical relation of interest. We describe a method for discovering these patterns and suggest that other lexical relations will also be acquirable in this way. A subset of the acquisition algorithm is implemented and the results are used to augment and critique the structure of a large hand-built thesaurus. Extensions and applications to areas such as information retrieval are suggested.", "title": "Automatic Acquisition of Hyponyms from Large Text Corpora"}, "031b4656032aad6699a642f62d993173b378b772": {"paper_id": "031b4656032aad6699a642f62d993173b378b772", "abstract": "We present an approach to building a verb lexicon compatible with WordNet but with explicitly stated syntactic and semantic information, using Levin verb classes to systematically construct lexical entries. By using verb classes we capture generalizations about verb behavior and reduce the effort needed to construct the lexicon. The syntactic frames for the verb classes are represented by a Lexicalized Tree Adjoining Grammar augmented with semantic predicates, which allows a compositional interpretation.", "title": "Class-Based Construction of a Verb Lexicon"}, "4e7dd05b75add873d618bc2c7ea5003829e449b6": {"paper_id": "4e7dd05b75add873d618bc2c7ea5003829e449b6", "abstract": "Entity relation detection is a form of information extraction that finds predefined relations between pairs of entities in text. This paper describes a relation detection approach that combines clues from different levels of syntactic processing using kernel methods. Information from three different levels of processing is considered: tokenization, sentence parsing and deep dependency analysis. Each source of information is represented by kernel functions. Then composite kernels are developed to integrate and extend individual kernels so that processing errors occurring at one level can be overcome by information from other levels. We present an evaluation of these methods on the 2004 ACE relation detection task, using Support Vector Machines, and show that each level of syntactic processing contributes useful information for this task. When evaluated on the official test data, our approach produced very competitive ACE value scores. We also compare the SVM with KNN on different kernels.", "title": "Extracting Relations with Integrated Information Using Kernel Methods"}, "497ce53f8f12f2117d36b5a61b3fc142f0cb05ee": {"paper_id": "497ce53f8f12f2117d36b5a61b3fc142f0cb05ee", "abstract": "We propose a novel method for automatically interpreting compound nouns based on a predefined set of semantic relations. First we map verb tokens in sentential contexts to a fixed set of seed verbs using WordNet::Similarity and Moby\u2019s Thesaurus. We then match the sentences with semantic relations based on the semantics of the seed verbs and grammatical roles of the head noun and modifier. Based on the semantics of the matched sentences, we then build a classifier using TiMBL. The performance of our final system at interpreting NCs is 52.6%.", "title": "Interpreting Semantic Relations in Noun Compounds via Verb Semantics"}, "0e3e3c3d8ae5cb7c4636870d69967c197484d3bb": {"paper_id": "0e3e3c3d8ae5cb7c4636870d69967c197484d3bb", "abstract": "This paper will focus on the semantic representation of verbs in computer systems and its impact on lexical selection problems in machine translation (MT). Two groups of English and Chinese verbs are examined to show that lexical selection must be based on interpretation of the sentence as well as selection restrictions placed on the verb arguments. A novel representation scheme is suggested, and is compared to representations with selection restrictions used in transfer-based MT. We see our approach as closely aligned with knowledge-based MT approaches (KBMT), and as a separate component that could be incorporated into existing systems. Examples and experimental results will show that, using this scheme, inexact matches can achieve correct lexical selection.", "title": "Verb Semantics and Lexical Selection"}, "011ffa226a58b6711ce509609b8336911325b0e0": {"paper_id": "011ffa226a58b6711ce509609b8336911325b0e0", "abstract": "This paper presents a new approach for measuring semantic similarity/distance between words and concepts. It combines a lexical taxonomy structure with corpus statistical information so that the semantic distance between nodes in the semantic space constructed by the taxonomy can be better quantified with the computational evidence derived from a distributional analysis of corpus data. Specifically, the proposed measure is a combined approach that inherits the edge-based approach of the edge counting scheme, which is then enhanced by the node-based approach of the information content calculation. When tested on a common data set of word pair similarity ratings, the proposed approach outperforms other computational models. It gives the highest correlation value (r = 0.828) with a benchmark based on human similarity judgements, whereas an upper bound (r = 0.885) is observed when human subjects replicate the same task.", "title": "Semantic Similarity Based on Corpus Statistics and Lexical Taxonomy"}, "68f2a8b62fc248c3b0a37fdfa33d96bb628d6146": {"paper_id": "68f2a8b62fc248c3b0a37fdfa33d96bb628d6146", "abstract": "We examine learning in all experiments we could locate involving 100 periods or more of games with a unique equilibn'um in mixed strategies, and in a new experiment. We study both the ex post (\" best fit \") akscrtptive power of learning models, and their ex ante predictive power, by simulating each experiment using parameters estimated from the other experiments. Even a one-parameter reinforcement learning model robustly outpelfonns the equilibrium predictions. Predictive power is improved by adding ' yorgemng \" and \" experimentation, \" or by allowing greater ra-tionality as in probabilistic jictitious play. Implications for developing a low-rationality, cognitive game theory are discussed (JEL C72. C92) i Game theory has traditionally been developed as a theory of strategic interaction among players who are perfectly rational, and who (consequently) exhibit equilibrium behavior. This approach has been complemented by evolutionary game theory, which, motivated by biological evolution, seeks to understand how equilibria could arise in the long term by selection among generations of players who need not be rational or even conscious decision makers. Somewhere in between are models of learning, which consider the adaptive behavior of goal-oriented players who may not be highly rational, both to provide foundations also contributed to the design and programming of the new experiment. We are indebted to Barry O'Neill, Jack Ochs, and Amnon Rapoport for access to unpublished parts of their data. The present version reflects numerous comments by three anonymous referees on several earlier drafts. for theories of equilibrium and to model em-: pirically observed behavior. The present paper considers how well ~im-.~'! ple learning models, motivated by the pay-' chology of learning, can model the interaction :. of players who must learn about the game and. C each other in the course of playing the game, over time spans that may not be long enough to lead to equilibrium. Our goal will be to model observed behavior, starting with behavior observed in experimental settings. (In the conclusion we will also consider the implications of this approach for applied economics in naturally occurring, nonexperimental settings .) We will show that a wide range of experimental data can be both well described ex post and robustly predicted ex ante by a very simple family of learning theories. Economists have traditionally avoided explaining behavior as less than rational for fear of developing many fragmented theories of mistakes. Part of the attraction of highly rational models \u2026", "title": "Predicting How People Play Games : Reinforcement Learning i Experimental Games with Unique , Mixed Strategy Equilibria"}, "f3566be263583b136c51451cba7ea4ace6947f83": {"paper_id": "f3566be263583b136c51451cba7ea4ace6947f83", "abstract": "Unstructured human environments present a substantial challenge to effective robotic operation. Mobile manipulation in human environments requires dealing with novel unknown objects, cluttered workspaces, and noisy sensor data. We present an approach to mobile pick and place in such environments using a combination of two-dimensional (2-D) and three-dimensional (3-D) visual processing, tactile and proprioceptive sensor data, fast motion planning, reactive control and monitoring, and reactive grasping. We demonstrate our approach by using a two-arm mobile manipulation system to pick and place objects. Reactive components allow our system to account for uncertainty arising from noisy sensors, inaccurate perception (e.g., object detection or registration), or dynamic changes in the environment. We also present a set of tools that allows our system to be easily configured within a short time for a new robotic system.", "title": "Mobile Manipulation in Unstructured Environments: Perception, Planning, and Execution"}, "f3fa1f32535a1725ca3074482aa8304bdd385064": {"paper_id": "f3fa1f32535a1725ca3074482aa8304bdd385064", "abstract": "In this paper, we present an approach for modeling 3D environments based on octrees using a probabilistic occupancy estimation. Our technique is able to represent full 3D models including free and unknown areas. It is available as an open-source library to facilitate the development of 3D mapping systems. We also provide a detailed review of existing approaches to 3D modeling. Our approach was thoroughly evaluated using different real-world and simulated datasets. The results demonstrate that our approach is able to model the data probabilistically while, at the same time, keeping the memory requirement at a minimum.", "title": "OctoMap : A Probabilistic , Flexible , and Compact 3 D Map Representation for Robotic Systems"}, "7b9b7335941b65a7dc03fdb17f0403ec57822c58": {"paper_id": "7b9b7335941b65a7dc03fdb17f0403ec57822c58", "abstract": "Accurate terrain estimation is critical for autonomous offroad navigation. Reconstruction of a 3D surface allows rough and hilly ground to be represented, yielding faster driving and better planning and control. However, data from a 3D sensor samples the terrain unevenly, quickly becoming sparse at longer ranges and containing large voids because of occlusions and inclines. The proposed approach uses online kernel-based learning to estimate a continuous surface over the area of interest while providing upper and lower bounds on that surface. Unlike other approaches, visibility information is exploited to constrain the terrain surface and increase precision, and an efficient gradient-based optimization allows for realtime implementation.", "title": "Accurate rough terrain estimation with space-carving kernels"}, "8a9cf53fa232d912272e143801c88c57072015fb": {"paper_id": "8a9cf53fa232d912272e143801c88c57072015fb", "abstract": "We present a complete software architecture for reliable grasping of household objects. Our work combines aspects such as scene interpretation from 3D range data, grasp planning, motion planning, and grasp failure identification and recovery using tactile sensors. We build upon, and add several new contributions to the significant prior work in these areas. A salient feature of our work is the tight coupling between perception (both visual and tactile) and manipulation, aiming to address the uncertainty due to sensor and execution errors. This integration effort has revealed new challenges, some of which can be addressed through system and software engineering, and some of which present opportunities for future research. Our approach is aimed at typical indoor environments, and is validated by long running experiments where the PR2 robotic platform was able to consistently grasp a large variety of known and unknown objects. The set of tools and algorithms for object grasping presented here have been integrated into the open-source Robot Operating System (ROS).", "title": "Towards Reliable Grasping and Manipulation in Household Environments"}, "47c3c0273c010115cd1d5ee90210937f47658d4e": {"paper_id": "47c3c0273c010115cd1d5ee90210937f47658d4e", "abstract": "In our recent work [1], [2], we proposed Point Feature Histograms (PFH) as robust multi-dimensional features which describe the local geometry around a point p for 3D point cloud datasets. In this paper, we modify their mathematical expressions and perform a rigorous analysis on their robustness and complexity for the problem of 3D registration for overlapping point cloud views. More concretely, we present several optimizations that reduce their computation times drastically by either caching previously computed values or by revising their theoretical formulations. The latter results in a new type of local features, called Fast Point Feature Histograms (FPFH), which retain most of the discriminative power of the PFH. Moreover, we propose an algorithm for the online computation of FPFH features for realtime applications. To validate our results we demonstrate their efficiency for 3D registration and propose a new sample consensus based method for bringing two datasets into the convergence basin of a local non-linear optimizer: SAC-IA (SAmple Consensus Initial Alignment).", "title": "Fast Point Feature Histograms (FPFH) for 3D registration"}, "b8669c47d280b16e14dea28eb59a60acd6e7a6fa": {"paper_id": "b8669c47d280b16e14dea28eb59a60acd6e7a6fa", "abstract": "Assistive mobile robots that autonomously manipulate objects within everyday settings have the potential to improve the lives of the elderly, injured, and disabled. Within this paper, we present the most recent version of the assistive mobile manipulator EL-E with a focus on the subsystem that enables the robot to retrieve objects from and deliver objects to flat surfaces. Once provided with a 3D location via brief illumination with a laser pointer, the robot autonomously approaches the location and then either grasps the nearest object or places an object. We describe our implementation in detail, while highlighting design principles and themes, including the use of specialized behaviors, task-relevant features, and low-dimensional representations. We also present evaluations of EL-E\u2019s performance relative to common forms of variation. We tested EL-E\u2019s ability to approach and grasp objects from the 25 object categories that were ranked most important for robotic retrieval by motor-impaired patients from the Emory ALS Center. Although reliability varied, EL-E succeeded at least once with objects from 21 out of 25 of these categories. EL-E also approached and grasped a cordless telephone on 12 different surfaces including floors, tables, and counter tops with 100% success. The same test using a vitamin pill (ca. 15mm \u00d75mm \u00d75mm) resulted in 58% success.", "title": "EL-E: an assistive mobile manipulator that autonomously fetches objects from flat surfaces"}, "03a283ca72dd7fb9f544e3a049efba6bf4203520": {"paper_id": "03a283ca72dd7fb9f544e3a049efba6bf4203520", "abstract": "A robotic grasping simulator, called Graspit!, is presented as versatile tool for the grasping community. The focus of the grasp analysis has been on force-closure grasps, which are useful for pick-and-place type tasks. This work discusses the different types of world elements and the general robot definition, and presented the robot library. The paper also describes the user interface of Graspit! and present the collision detection and contact determination system. The grasp analysis and visualization method were also presented that allow a user to evaluate a grasp and compute optimal grasping forces. A brief overview of the dynamic simulation system was provided.", "title": "Graspit! A versatile simulator for robotic grasping"}, "e90ebe299803ed32656ce4fbaed0393fad51f02c": {"paper_id": "e90ebe299803ed32656ce4fbaed0393fad51f02c", "abstract": "The attention of software vendors has moved recently to SMEs (smallto medium-sized enterprises), offering them a vast range of enterprise systems (ES), which were formerly adopted by large firms only. From reviewing information technology innovation adoption literature, it can be argued that IT innovations are highly differentiated technologies for which there is not necessarily a single adoption model. Additionally, the question of why one SME adopts an ES while another does not is still understudied. This study intends to fill this gap by investigating the factors impacting SME adoption of ES. A qualitative approach was adopted in this study involving key decision makers in nine SMEs in the Northwest of England. The contribution of this study is twofold: it provides a framework that can be used as a theoretical basis for studying SME adoption of ES, and it empirically examines the impact of the factors within this framework on SME adoption of ES. The findings of this study confirm that factors impacting the adoption of ES are different from factors impacting SME adoption of other previously studied IT innovations. Contrary to large companies that are mainly affected by organizational factors, this study shows that SMEs are not only affected by environmental factors as previously established, but also affected by technological and organizational factors.", "title": "SME Adoption of Enterprise Systems in the Northwest of England - An Environmental, Technological, and Organizational Perspective"}, "ba408fb0ab6ae53f9be40d107859ebec78034df9": {"paper_id": "ba408fb0ab6ae53f9be40d107859ebec78034df9", "abstract": "Many EDI researchers and practitioners have recognized the importance of high penetration levels for the success of EDI. Unfortunately, such penetration is partly impeded by the resistance of small companies to become EDI capable. To investigate this issue, three major factors are identified that influence the EDI adoption practices of small firms. These factors are: organizational readiness (because of the low levels of IT sophistication and resource availability of small firms), external pressures to adopt (because of the weak market positions of small firms and the network nature of the technology), and perceived benefits (because of the limited impact that IT has on small firms due to under-utilization and lack of integration). By combining the anticipated effects of these factom, we developed a framework of EDI adoption by small businesses. The applicability of this framework is empirically demonstrated using the results of seven case studies. Finally, recommendations are made for the development of successful EDI partner expansion plans. These include the development of a long-term EDI partner expansion plan from the very beginning, the individual assessment of each partner\u2019s EDI preparedness level, and the selection of appropriate influence tactics to expedite adoption by small partners. Specifically, it is suggested that EDI initiators pursue promotional efforts to improve partners\u2019 perceptions of EDI benefits, provide financial and technological assistance to partners with low organizational readiness, and carefully select and enact influence strategies to reduce resistance.", "title": "Electronic Data Interchange and Small Organizations: Adoption and Impact of Technology"}, "838ec45eeb7f63875742a76aa1080563f44af619": {"paper_id": "838ec45eeb7f63875742a76aa1080563f44af619", "abstract": "This article defines and discusses one of these qualitative methods--the case research strategy. Suggestions are provided for researchers who wish to undertake research employing this approach. Criteria for the evaluation of case research are established and several characteristics useful for categorizing the studies are identified. A sample of papers drawn from information systems journals is reviewed. The paper concludes with examples of research areas that are particularly wellsuited to investigation using the case research approach. ACM categories: H.O., J.O.", "title": "The Case Research Strategy in Studies of Information Systems"}, "222717d12ef311906161096bc5e5e325f0bd5fe5": {"paper_id": "222717d12ef311906161096bc5e5e325f0bd5fe5", "abstract": "The present research develops and tests a theoretical extension of the Technology Acceptance Model (TAM) that explains perceived usefulness and usage intentions in terms of social influence and cognitive instrumental processes. The extended model, referred to as TAM2, was tested using longitudinal data collected regarding four different systems at four organizations (N 156), two involving voluntary usage and two involving mandatory usage. Model constructs were measured at three points in time at each organization: preimplementation, one month postimplementation, and three months postimplementation. The extended model was strongly supported for all four organizations at all three points of measurement, accounting for 40%\u201360% of the variance in usefulness perceptions and 34%\u201352% of the variance in usage intentions. Both social influence processes (subjective norm, voluntariness, and image) and cognitive instrumental processes (job relevance, output quality, result demonstrability, and perceived ease of use) significantly influenced user acceptance. These findings advance theory and contribute to the foundation for future research aimed at improving our understanding of user adoption behavior. (Adoption of Information Technology; Technology Acceptance Model; Social Influence; Perceived Usefulness)", "title": "A Theoretical Extension of the Technology Acceptance Model : Four Longitudinal Field Studies"}, "1beddcd2cee2a18e6875d0a624541f1c378cdde8": {"paper_id": "1beddcd2cee2a18e6875d0a624541f1c378cdde8", "abstract": "By NOW, many experimental studies (e.g., 1, 3, 6) have demonstrated that individual psychological processes are subject to social influences. Most investigators, however, have not distinguished among different kinds of social influences; rather, they have carelessly used the term \"group\" influence to characterize the impact of many different kinds of social factors. In fact, a review of the major experiments in this area\u2014e.g., those by Sherif (6), Asch (1), Bovard (3)\u2014would indicate that the subjects (5s) in these experiments as they made their judgments were not functioning as members of a group in any simple or obvious manner. The S, in the usual experiment in this area, made perceptual judgments hi the physical presence of others after hearing their judgments. Typically, the S was not given experimental instructions which made him feel that he was a member of a group faced with a common task requiring cooperative effort for its most effective solution. If \"group\" influences were at work in the foregoing experiments, they were subtly and indirectly created rather than purposefully created by the experimenter.", "title": "A study of normative and informational social influences upon individual judgement."}, "ed6d9295d9fedb35701ee548deeb13edd29b8829": {"paper_id": "ed6d9295d9fedb35701ee548deeb13edd29b8829", "abstract": "This paper reports on the development of an instrument designed to measure the various perceptions that an individual may have of adopting an information technology (IT) innovation. This instrument is intended to be a tool for the study of the initial adoption and eventual diffusion of IT innovations within organizations. While the adoption of information technologies by individuals and organizations has been an area of substantial research interest since the early days of computerization, research efforts to date have led to mixed and inconclusive outcomes. The lack of a theoretical foundation for such research and inadequate definition and measurement of constructs have been identified as major causes for such outcomes. In a recent study examining the diffusion of new end-user IT, we decided to focus on measuring the potential adopters' perceptions of the technology. Measuring such perceptions has been termed a \"classic issue\" in the innovation diffusion literature, and a key to integrating the various findings of diffusion research. The perceptions of adopting were initially based on the five characteristics of innovations derived by Rogers (1983) from the diffusion of innovations literature, plus two developed specifically within this study. Of the existing scales for measuring these characteristics, very few had the requisite levels of validity and reliability. For this study, both newly created and existing items were placed in a common pool and subjected to four rounds of sorting by judges to establish which items should be in the various scales. The objective was to verify the convergent and discriminant validity of the scales by examining how the items were sorted into various construct categories. Analysis of interjudge agreement about item placement identified both bad items as well as weaknesses in some of the constructs' original definitions. These were subsequently redefined. Scales for the resulting constructs were subjected to three separate field tests. Following the final test, the scales all demonstrated acceptable levels of reliability. Their validity was further checked using factor analysis, as well as conducting discriminant analysis comparing responses between adopters and nonadopters of the innovation. The result is a parsimonious, 38-item instrument comprising eight scales which provides a useful tool for", "title": "Development of an Instrument to Measure the Perceptions of Adopting an Information Technology Innovation"}, "96eea63c856d7ac166cf890ac99828bf63fa41b8": {"paper_id": "96eea63c856d7ac166cf890ac99828bf63fa41b8", "abstract": "Interactive TV research encompasses a rather diverse body of work (e.g. multimedia, HCI, CSCW, UIST, user modeling, media studies) that has accumulated over the past 20 years. In this article, we highlight the state-of-the-art and consider two basic issues: What is interactive TV research? Can it help us reinvent the practices of creating, sharing and watching TV? We survey the literature and identify three concepts that have been inherent in interactive TV research: 1) interactive TV as content creation, 2) interactive TV as a content and experience sharing process, and 3) interactive TV as control of audiovisual content. We propose this simple taxonomy (create-share-control) as an evolutionary step over the traditional hierarchical produce-distribute-consume paradigm. Moreover, we highlight the importance of sociability in all phases of the create-share-control model.", "title": "Interactivity and user participation in the television lifecycle: creating, sharing, and controlling content"}, "38317e7aa925e6d907e79d5f288993e1ed08485e": {"paper_id": "38317e7aa925e6d907e79d5f288993e1ed08485e", "abstract": "Watching video online is becoming increasingly popular, and new video streaming technologies have the potential to transform video watching from a passive, isolating experience into an active, socially engaging experience. However, the viability of an active social experience is unclear: both chatting and watching video require attention, and may interfere with one another and detract from the experience. In this paper, we empirically examine the activity of chatting while watching video online. We examine how groups of friends and strangers interact, and find that chat has a positive influence on social relationships, and people chat despite being distracted. We discuss the benefits and opportunities provided by mixing chat and video, uncover some of the attentional and social challenges inherent in this combination of media, and provide guidance for structuring the viewing experience.", "title": "Watching together: integrating text chat with video"}, "467948fa466e8e1c332356abb70a28e8afda8290": {"paper_id": "467948fa466e8e1c332356abb70a28e8afda8290", "abstract": "This paper describes the origins and history of multiple resource theory in accounting for di\u0080 erences in dual task interference. One particular application of the theory, the 4-dimensional multiple resources model, is described in detail, positing that there will be greater interference between two tasks to the extent that they share stages (perceptual/cognitive vs response) sensory modalities (auditory vs visual), codes (visual vs spatial) and channels of visual information (focal vs ambient). A computational rendering of this model is then presented. Examples are given of how the model predicts interference di\u0080 erences in operational environments. Finally, three challenges to the model are outlined regarding task demand coding, task allocation and visual resource competition.", "title": "Multiple resources and performance prediction"}, "8120ad44fd5115e4808f1cfda30da624d5ecbdc8": {"paper_id": "8120ad44fd5115e4808f1cfda30da624d5ecbdc8", "abstract": "We explore the generation of interactive computer graphics at digital set-top boxes in place of the fixed graphics that were embedded to the television video before the broadcast. This direction raises new requirements for user interface development, since the graphics are merged with video at each set-top box dynamically, without the traditional quality control from the television producers. Besides the technical issues, interactive computer graphics for television should be evaluated by television viewers. We employ an animated character in an interactive music television application that was evaluated by consumers, and was developed using the Virtual Channel Control Library, a custom high-level API, that was built using Microsoft Windows and TV technologies. r 2004 Elsevier Ltd. All rights reserved.", "title": "User interface development for interactive television: extending a commercial DTV platform to the virtual channel API"}, "245061d94c228abdc9bda2d9a10679e897526465": {"paper_id": "245061d94c228abdc9bda2d9a10679e897526465", "abstract": "For 11 studies, we find that the detection of usability problems as a function of number of users tested or heuristic evaluators employed is well modeled as a Poisson process. The model can be used to plan the amount of evaluation required to achieve desired levels of thoroughness or benefits. Results of early tests can provide estimates of the number of problems left to be found and the number of additional evaluations needed to find a given fraction. With quantitative evaluation costs and detection values, the model can estimate the numbers of evaluations at which optimal cost/benefit ratios are obtained and at which marginal utility vanishes. For a \u201cmedium\u201d example, we estimate that 16 evaluations would be worth their cost, with maximum benefit/cost ratio at four.", "title": "A mathematical model of the finding of usability problems"}, "5163a44c47682be31b2a6b138413569a1c233f49": {"paper_id": "5163a44c47682be31b2a6b138413569a1c233f49", "abstract": "This paper presents an improvement of a previously proposed pitch determination algorithm (PDA). Particularly aiming at handling alternate cycles in speech signal, the algorithm estimates pitch through spectrum shifting on logarithmic frequency scale and calculating the Subharmonic-to-Harmonic Ratio (SHR). The evaluation results on two databases show that this algorithm performs considerably better than other PDAs compared. Application of SHR to voice quality analysis task is also presented. The implementation and evaluation routines are available from &#60;http://mel.speech.nwu.edu/sunxj/pda.htm>.", "title": "Pitch determination and voice quality analysis using Subharmonic-to-Harmonic Ratio"}, "b97f4bcfe9c9b240689855ce83c1412e2db98393": {"paper_id": "b97f4bcfe9c9b240689855ce83c1412e2db98393", "abstract": "We model a degraded image as an original image that has been subject to linear frequency distortion and additive noise injection. Since the psychovisual effects of frequency distortion and noise injection are independent, we decouple these two sources of degradation and measure their effect on the human visual system. We develop a distortion measure (DM) of the effect of frequency distortion, and a noise quality measure (NQM) of the effect of additive noise. The NQM, which is based on Peli's (1990) contrast pyramid, takes into account the following: 1) variation in contrast sensitivity with distance, image dimensions, and spatial frequency; 2) variation in the local luminance mean; 3) contrast interaction between spatial frequencies; 4) contrast masking effects. For additive noise, we demonstrate that the nonlinear NQM is a better measure of visual quality than peak signal-to noise ratio (PSNR) and linear quality measures. We compute the DM in three steps. First, we find the frequency distortion in the degraded image. Second, we compute the deviation of this frequency distortion from an allpass response of unity gain (no distortion). Finally, we weight the deviation by a model of the frequency response of the human visual system and integrate over the visible frequencies. We demonstrate how to decouple distortion and additive noise degradation in a practical image restoration system.", "title": "Image quality assessment based on a degradation model"}, "34cd060fccf4c52a3a7426cbc077710a1bf7b904": {"paper_id": "34cd060fccf4c52a3a7426cbc077710a1bf7b904", "abstract": "Spin-Transfer Torque RAM (STT-RAM) has emerged as a potential candidate for Universal memory. However, there are two challenges to using STT-RAM in memory system design: (1) the intrinsic variation in the storage element, the Magnetic Tunnel Junction (MTJ), and (2) the high write energy. In this paper, we present a physically based thermal noise model for simulating the statistical variations of MTJs. We have implemented it in HSPICE and validated it against analytical results. We demonstrate its use in setting the write pulse width for a given write error rate. We then propose two write-energy reduction techniques. At the device level, we propose the use of a low-MS ferromagnetic material that can reduce the write energy without sacrificing retention time. At the architecture level, we show that Invert Coding provides a 7% average reduction in the total write energy for the SPEC CPU2006 benchmark suite without any performance overhead.", "title": "Delivering on the promise of universal memory for spin-transfer torque RAM (STT-RAM)"}, "0b0a8fb95e3331cacfe58f8938c3f7134a4c70e1": {"paper_id": "0b0a8fb95e3331cacfe58f8938c3f7134a4c70e1", "abstract": "The dream of replacing rotating mechanical storage, the disk drive, with solid-state, nonvolatile RAM may become a reality in the near future. Approximately ten new technologies\u2014collectively called storage-class memory (SCM)\u2014are currently under development and promise to be fast, inexpensive, and power efficient. Using SCM as a disk drive replacement, storage system products will have random and sequential I/O performance that is orders of magnitude better than that of comparable disk-based systems and require much less space and power in the data center. In this paper, we extrapolate disk and SCM technology trends to 2020 and analyze the impact on storage systems. The result is a 100to 1,000-fold advantage for SCM in terms of the data center space and power required.", "title": "Storage-class memory: The next storage system technology"}, "f3e47327c8ae8238d7539439c3cbc67b856edaeb": {"paper_id": "f3e47327c8ae8238d7539439c3cbc67b856edaeb", "abstract": "Linux scheduling is based on the time-sharing technique already introduced in the section \"CPU's Time Sharing\" in Chapter 5, Timing Measurements: several processes are allowed to run \"concurrently,\" which means that the CPU time is roughly divided into \"slices,\" one for each runnable process.[1] Of course, a single processor can run only one process at any given instant. If a currently running process is not terminated when its time slice or quantum expires, a process switch may take place. Time-sharing relies on timer interrupts and is thus transparent to processes. No additional code needs to be inserted in the programs in order to ensure CPU time-sharing.", "title": "Understanding the Linux Kernel"}, "69b8c7f168fdb610471c473b9c2f20daaffe052c": {"paper_id": "69b8c7f168fdb610471c473b9c2f20daaffe052c", "abstract": "Phase change memory (PCM) is one of the most promising technology among emerging non-volatile random access memory technologies. Implementing a cache memory using PCM provides many benefits such as high density, non-volatility, low leakage power, and high immunity to soft error. However, its disadvantages such as high write latency, high write energy, and limited write endurance prevent it from being used as a drop-in replacement of an SRAM cache. In this paper, we study a set of techniques to design an energy- and endurance-aware PCM cache. We also modeled the timing, energy, endurance, and area of PCM caches and integrated them into a PCM cache simulator to evaluate the techniques. Experiments show that our PCM cache design can achieve 8% of energy saving and 3.8 years of lifetime compared with a baseline PCM cache having less than a hour of lifetime.", "title": "Energy- and endurance-aware design of phase change memory caches"}, "a95436fb5417f16497d90cd2aeb11a0e2873f55f": {"paper_id": "a95436fb5417f16497d90cd2aeb11a0e2873f55f", "abstract": "Memory scaling is in jeopardy as charge storage and sensing mechanisms become less reliable for prevalent memory technologies, such as DRAM. In contrast, phase change memory (PCM) storage relies on scalable current and thermal mechanisms. To exploit PCM's scalability as a DRAM alternative, PCM must be architected to address relatively long latencies, high energy writes, and finite endurance.\n We propose, crafted from a fundamental understanding of PCM technology parameters, area-neutral architectural enhancements that address these limitations and make PCM competitive with DRAM. A baseline PCM system is 1.6x slower and requires 2.2x more energy than a DRAM system. Buffer reorganizations reduce this delay and energy gap to 1.2x and 1.0x, using narrow rows to mitigate write energy and multiple rows to improve locality and write coalescing. Partial writes enhance memory endurance, providing 5.6 years of lifetime. Process scaling will further reduce PCM energy costs and improve endurance.", "title": "Architecting phase change memory as a scalable dram alternative"}, "2aca2e2c5bb4f680ba0f4a601564f0d1a4b3dbc3": {"paper_id": "2aca2e2c5bb4f680ba0f4a601564f0d1a4b3dbc3", "abstract": "NAND flash-based storage is widely used in embedded systems due to its numerous benefits: low cost, high density, small form factor and so on. However, NAND flash-based storage is still suffering from serious performance degradation for random or small size write access. This degradation mainly comes from the physical constraints of NAND flash: erase-before-program and different unit size of erase and program operations. To overcome these constraints, we propose to use PRAM (Phase-change RAM) which supports advanced features: fast byte access capability and no requirement for erase-before-program.\n In this paper, we focus on developing a high-performance NAND flash-based storage system by maximally exploiting the advanced feature of PRAM, in terms of performance and wearing out. To do this, we first propose a new hybrid storage architecture which consists of PRAM and NAND flash. Second, we devise two novel software schemes for the proposed hybrid storage architecture; FSMS (File System Metadata Separation) and hFTL (hybrid Flash Translation Layer). Finally, we demonstrate that our hybrid architecture increases the performance up to 290% and doubles the lifespan compared to the existing NAND flash only storage systems.", "title": "A PRAM and NAND flash hybrid architecture for high-performance embedded storage subsystems"}, "4b6140af0b418d1efc00baa4f14fa15c6a68f764": {"paper_id": "4b6140af0b418d1efc00baa4f14fa15c6a68f764", "abstract": "This paper introduces an approach for enabling existing multi-view stereo methods to operate on extremely large unstructured photo collections. The main idea is to decompose the collection into a set of overlapping sets of photos that can be processed in parallel, and to merge the resulting reconstructions. This overlapping clustering problem is formulated as a constrained optimization and solved iteratively. The merging algorithm, designed to be parallel and out-of-core, incorporates robust filtering steps to eliminate low-quality reconstructions and enforce global visibility constraints. The approach has been tested on several large datasets downloaded from Flickr.com, including one with over ten thousand images, yielding a 3D reconstruction with nearly thirty million points.", "title": "Towards Internet-scale multi-view stereo"}, "438997f5eb454e8f55023f402ee237314f5f7dda": {"paper_id": "438997f5eb454e8f55023f402ee237314f5f7dda", "abstract": "We show that surface reconstruction from oriented points can be cast as a spatial Poisson problem. This Poisson formulation considers all the points at once, without resorting to heuristic spatial partitioning or blending, and is therefore highly resilient to data noise. Unlike radial basis function schemes, our Poisson approach allows a hierarchy of locally supported basis functions, and therefore the solution reduces to a well conditioned sparse linear system. We describe a spatially adaptive multiscale algorithm whose time and space complexities are proportional to the size of the reconstructed model. Experimenting with publicly available scan data, we demonstrate reconstruction of surfaces with greater detail than previously achievable.", "title": "Poisson surface reconstruction"}, "481a68ca9993ec3f606c03d219f4a23a68f3489f": {"paper_id": "481a68ca9993ec3f606c03d219f4a23a68f3489f", "abstract": "Different geometric structures are investigated in the context of discrete surface representation. It is shown that minimal representations (i.e., polyhedra) can be provided by a surface-based method using nearest neighbors structures or by a volume-based method using the Delaunay triangulation. Both approaches are compared with respect to various criteria, such as space requirements, computation time, constraints on the distribution of the points, facilities for further calculations, and agreement with the actual shape of the object.", "title": "Geometric Structures for Three-Dimensional Shape Representation"}, "45f3d40a84efd42e8f44a611846dec82081ce0ea": {"paper_id": "45f3d40a84efd42e8f44a611846dec82081ce0ea", "abstract": "We use polyharmonic Radial Basis Functions (RBFs) to reconstruct smooth, manifold surfaces from point-cloud data and to repair incomplete meshes. An object's surface is defined implicitly as the zero set of an RBF fitted to the given surface data. Fast methods for fitting and evaluating RBFs allow us to model large data sets, consisting of millions of surface points, by a single RBF \u2014 previously an impossible task. A greedy algorithm in the fitting process reduces the number of RBF centers required to represent a surface and results in significant compression and further computational advantages. The energy-minimisation characterisation of polyharmonic splines result in a \u201csmoothest\u201d interpolant. This scale-independent characterisation is well-suited to reconstructing surfaces from non-uniformly sampled data. Holes are smoothly filled and surfaces smoothly extrapolated. We use a non-interpolating approximation when the data is noisy. The functional representation is in effect a solid model, which means that gradients and surface normals can be determined analytically. This helps generate uniform meshes and we show that the RBF representation has advantages for mesh simplification and remeshing applications. Results are presented for real-world rangefinder data.", "title": "Reconstruction and representation of 3D objects with radial basis functions"}, "55062ce658f7828876c058c5e96c5071525a1b5e": {"paper_id": "55062ce658f7828876c058c5e96c5071525a1b5e", "abstract": "The paper presents a system for automatic, geo-registered, real-time 3D reconstruction from video of urban scenes. The system collects video streams, as well as GPS and inertia measurements in order to place the reconstructed models in geo-registered coordinates. It is designed using current state of the art real-time modules for all processing steps. It employs commodity graphics hardware and standard CPU\u2019s to achieve real-time performance. We present the main considerations in designing the system and the steps of the processing pipeline. Our system extends existing algorithms to meet the robustness and variability necessary to operate out of the lab. To account for the large dynamic range of outdoor videos the processing pipeline estimates global camera gain changes in the feature tracking stage and efficiently compensates for these in stereo estimation without impacting the real-time performance. The required accuracy for many applications is achieved with a two-step stereo reconstruction process exploiting the redundancy across frames. We show results on real video sequences comprising hundreds of thousands of frames.", "title": "Detailed Real-Time Urban 3D Reconstruction from Video"}, "52cba0165f7aa498854dd07415cb4b1ee8a9fd5f": {"paper_id": "52cba0165f7aa498854dd07415cb4b1ee8a9fd5f", "abstract": "Edges in man-made environments, grouped according to vanishing point directions, provide single-view constraints that have been exploited before as a precursor to both scene understanding and camera calibration. A Bayesian approach to edge grouping was proposed in the \"Manhattan World\" paper by Coughlan and Yuille, where they assume the existence of three mutually orthogonal vanishing directions in the scene. We extend the thread of work spawned by Coughlan and Yuille in several significant ways. We propose to use the expectation maximization (EM) algorithm to perform the search over all continuous parameters that influence the location of the vanishing points in a scene. Because EM behaves well in high-dimensional spaces, our method can optimize over many more parameters than the exhaustive and stochastic algorithms used previously for this task. Among other things, this lets us optimize over multiple groups of orthogonal vanishing directions, each of which induces one additional degree of freedom. EM is also well suited to recursive estimation of the kind needed for image sequences and/or in mobile robotics. We present experimental results on images of \"Atlanta worlds\", complex urban scenes with multiple orthogonal edge-groups, that validate our approach. We also show results for continuous relative orientation estimation on a mobile robot.", "title": "Atlanta world: an expectation maximization framework for simultaneous low-level edge grouping and camera calibration in complex man-made environments"}, "250b1eb62ef3188e7172b63b64b7c9b133b370f9": {"paper_id": "250b1eb62ef3188e7172b63b64b7c9b133b370f9", "abstract": "In this paper we address the problem of minimizing a large class of energy functions that occur in early vision. The major restriction is that the energy function\u2019s smoothness term must only involve pairs of pixels. We propose two algorithms that use graph cuts to compute a local minimum even when very large moves are allowed. The first move we consider is an \u03b1-\u03b2swap: for a pair of labels \u03b1, \u03b2, this move exchanges the labels between an arbitrary set of pixels labeled \u03b1 and another arbitrary set labeled \u03b2. Our first algorithm generates a labeling such that there is no swap move that decreases the energy. The second move we consider is an \u03b1-expansion: for a label \u03b1, this move assigns an arbitrary set of pixels the label \u03b1. Our second algorithm, which requires the smoothness term to be a metric, generates a labeling such that there is no expansion move that decreases the energy. Moreover, this solution is within a known factor of the global minimum. We experimentally demonstrate the effectiveness of our approach on image restoration, stereo and motion. 1 Energy minimization in early vision Many early vision problems require estimating some spatially varying quantity (such as intensity or disparity) from noisy measurements. Such quantities tend to be piecewise smooth; they vary smoothly at most points, but change dramatically at object boundaries. Every pixel p \u2208 P must be assigned a label in some set L; for motion or stereo, the labels are disparities, while for image restoration they represent intensities. The goal is to find a labeling f that assigns each pixel p \u2208 P a label fp \u2208 L, where f is both piecewise smooth and consistent with the observed data. These vision problems can be naturally formulated in terms of energy minimization. In this framework, one seeks the labeling f that minimizes the energy E(f) = Esmooth(f) + Edata(f). Here Esmooth measures the extent to which f is not piecewise smooth, while Edata measures the disagreement between f and the observed data. Many different energy functions have been proposed in the literature. The form of Edata is typically", "title": "Fast Approximate Energy Minimization via Graph Cuts"}, "0e09033f6636217d34fc0222de46a87c108e1c06": {"paper_id": "0e09033f6636217d34fc0222de46a87c108e1c06", "abstract": "In this paper we consider the problem of computing the 3D shape of an unknown, arbitrarily-shaped scene from multiple photographs taken at known but arbitrarily-distributed viewpoints. By studying the equivalence class of all 3D shapes that reproduce the input photographs, we prove the existence of a special member of this class, the photo hull, that (1) can be computed directly from photographs of the scene, and (2) subsumes all other members of this class. We then give a provably-correct algorithm, called Space Carving, for computing this shape and present experimental results on complex real-world scenes. The approach is designed to (1) capture photorealistic shapes that accurately model scene appearance from a wide range of viewpoints, and (2) account for the complex interactions between occlusion, parallax, shading, and their view-dependent effects on scene-appearance.", "title": "A Theory of Shape by Space Carving"}, "ad695519aa31c61b47050e4c27e7c9f78754906f": {"paper_id": "ad695519aa31c61b47050e4c27e7c9f78754906f", "abstract": "In this paper we present a new approach to high quality 3D object reconstruction. Starting from a calibrated sequence of color images, the algorithm is able to reconstruct both the 3D geometry and the texture. The core of the method is based on a deformable model, which defines the framework where texture and silhouette information can be fused. This is achieved by defining two external forces based on the images: a texture driven force and a silhouette driven force. The texture force is computed in two steps: a multi-stereo correlation voting approach and a gradient vector flow diffusion. Due to the high resolution of the voting approach, a multi-grid version of the gradient vector flow has been developed. Concerning the silhouette force, a new formulation of the silhouette constraint is derived. It provides a robust way to integrate the silhouettes in the evolution algorithm. As a consequence, we are able to recover the apparent contours of the model at the end of the iteration process. Finally, a texture map is computed from the original images for the reconstructed 3D model.", "title": "Silhouette and Stereo Fusion for 3D Object Modeling"}, "45c911ffca5da80fff69361072cb347511582822": {"paper_id": "45c911ffca5da80fff69361072cb347511582822", "abstract": "A novel scene reconstruction technique is presented, different from previous approaches in its ability to cope with large changes in visibility and its modeling of intrinsic scene color and texture information. The method avoids image correspondence problems by working in a discretized scene space whose voxels are traversed in a fixed visibility ordering. This strategy takes full account of occlusions and allows the input cameras to be far apart and widely distributed about the environment. The algorithm identifies a special set of invariant voxels which together form a spatial and photometric reconstruction of the scene, fully consistent with the input images. The approach is evaluated with images from both inward-facing and outward-facing cameras.", "title": "Photorealistic Scene Reconstruction by Voxel Coloring"}, "033cc3784a60115d758a11a765e764b86aca336c": {"paper_id": "033cc3784a60115d758a11a765e764b86aca336c", "abstract": "Many algorithms for both identifying and reconstructing a 3-D object are based on the 2-D silhouettes of the object. In general, identifying a nonconvex object using a silhouettebased approach implies neglecting some features of its surface as identification clues. The same features cannot be reconstructed by volume intersection techniques using multiple silhouettes of the object. This paper addresses the problem of finding which parts of a nonconvex object are relevant for silhouette-based image understanding. For this purpose, the geometric concept of visual hull of a 3-D object is introduced. The visual hull of a 3-D object S is the closest approximation of S that can be obtained with the volume intersection approach. An equivalent statement, relative to object identification, is that the visual hull of S is the maximal object silhouette-equivalent to S, i.e., which can be substituted for S without affecting any silhouette. Only the parts of the surface of S that also lie on the surface of the visual hull can be reconstructed or identified using silhouette-based algorithms. The visual hull of an object depends not only on the object itself but also on the region allowed to the viewpoint. Two main viewing regions can be considered, resulting in the external and internal visual hull. In the former case the viewing region is related to the convex hull of S, in the latter it is bounded by S itself. The internal visual hull also admits an interpretation not related to silhouettes: the features of the surface of S that is not coincident with the surface of the internal visual hull cannot be observed from any viewpoint lying outside the convex hull. After a general discussion of the visual hull and its properties, algorithms for computing the internal and external visual hulls of 2-D objects and 3-D planar face objects are presented and their complexity analyzed. In general, the visual hull of a 3-D planar face object turns out to be bounded by planar and curved patches. A precise statement of the concept of visual hull appears to be novel, as is the problem of its computation.", "title": "The Visual Hull Concept for Silhouette-Based Image Understanding"}, "0a3381f0432c5cfe491c718349d7a44e5814592c": {"paper_id": "0a3381f0432c5cfe491c718349d7a44e5814592c", "abstract": "The task: Detect errors in learner writing (spelling, grammar, word usage, etc). Examples: I want to travel on July and because of it is more suitable for me. We don\u2019t need to wear clothes like layer and layer. The restaurant was closed because unknown reasons. Applications: \u2022 Immediate feedback in self-tutoring systems for language learning. \u2022Automated exam grading for language testing. \u2022Providing language checking in general writing applications.", "title": "Compositional Sequence Labeling Models for Error Detection in Learner Writing"}, "7c5bac84a0ef1701047bd70ae33b3e89008260db": {"paper_id": "7c5bac84a0ef1701047bd70ae33b3e89008260db", "abstract": "We present freely available open-source toolkit for training recurrent neural network based language models. I t can be easily used to improve existing speech recognition and ma chine translation systems. Also, it can be used as a baseline for fu ture research of advanced language modeling techniques. In the p a er, we discuss optimal parameter selection and different modes of functionality. The toolkit, example scripts and basic setups are freely available at http://rnnlm.sourceforge.net/. I. I NTRODUCTION, MOTIVATION AND GOALS Statistical language modeling attracts a lot of attention, as models of natural languages are important part of many practical systems today. Moreover, it can be estimated that with further research progress, language models will becom e closer to human understanding [1] [2], and completely new applications will become practically realizable. Immedia tely, any significant progress in language modeling can be utilize d in the esisting speech recognition and statistical machine translation systems. However, the whole research field struggled for decades to overcome very simple, but also effective models based on ngram frequencies [3] [4]. Many techniques were developed to beat n-grams, but the improvements came at the cost of computational complexity. Moreover, the improvements wer e often reported on very basic systems, and after application to state-of-the-art setups and comparison to n-gram models trained on large amounts of data, improvements provided by many techniques vanished. This has lead to scepticism among speech recognition researchers. In our previous work, we have compared many major advanced language modeling techniques, and found that neur al network based language models (NNLM) perform the best on several standard setups [5]. Models of this type were introduced by Bengio in [6], about ten years ago. Their main weaknesses were huge computational complexity, and nontrivial implementation. Successful training of neural net works require well chosen hyper-parameters, such as learning rat e and size of hidden layer. To help overcome these basic obstacles, we have decided to release our toolkit for training recurrent neural network b ased language models (RNNLM). We have shown that the recurrent architecture outperforms the feedforward one on several se tup in [7]. Moreover, the implemenation is simple and easy to understand. The most importantly, recurrent neural networ ks are very interesting from the research point of view, as they allow effective processing of sequences and patterns with arbitraty length these models can learn to store informati on in the hidden layer. Recurrent neural networks can have memory , and are thus important step forward to overcome the most painful and often criticized drawback of n-gram models dependence on previous two or three words only. In this paper we present an open source and freely available toolkit for training statistical language models base d or recurrent neural networks. It includes techniques for redu cing computational complexity (classes in the output layer and direct connections between input and output layer). Our too lkit has been designed to provide comparable results to the popul ar toolkit for training n-gram models, SRILM [8]. The main goals for the RNNLM toolkit are these: \u2022 promotion of research of advanced language modeling techniques \u2022 easy usage \u2022 simple portable code without any dependencies \u2022 computational efficiency In the paper, we describe how to easily make RNNLM part of almost any speech recognition or machine translation syste m that produces lattices. II. RECURRENTNEURAL NETWORK The recurrent neural network architecture used in the toolk it is shown at Figure 1 (usually called Elman network, or simple RNN). The input layer uses the 1-of-N representation of the previous wordw(t) concatenated with previous state of the hidden layers(t \u2212 1). The neurons in the hidden layer s(t) use sigmoid activation function. The output layer (t) has the same dimensionality as w(t), and after the network is trained, it represents probability distribution of the next word giv en the previous word and state of the hidden layer in the previous time step [9]. The class layer c(t) can be optionally used to reduce computational complexity of the model, at a small cost of accuracy [7]. Training is performed by the standard stochastic gradient descent algorithm, and the matrix W that", "title": "RNNLM - Recurrent Neural Network Language Modeling Toolkit"}, "3a91c3eb9a05ef5c8b875ab112448cc3f44a1268": {"paper_id": "3a91c3eb9a05ef5c8b875ab112448cc3f44a1268", "abstract": "We present several modifications of the original recurrent neural network language model (RNN LM).While this model has been shown to significantly outperform many competitive language modeling techniques in terms of accuracy, the remaining problem is the computational complexity. In this work, we show approaches that lead to more than 15 times speedup for both training and testing phases. Next, we show importance of using a backpropagation through time algorithm. An empirical comparison with feedforward networks is also provided. In the end, we discuss possibilities how to reduce the amount of parameters in the model. The resulting RNN model can thus be smaller, faster both during training and testing, and more accurate than the basic one.", "title": "Extensions of recurrent neural network language model"}, "4eb943bf999ce49e5ebb629d7d0ffee44becff94": {"paper_id": "4eb943bf999ce49e5ebb629d7d0ffee44becff94", "abstract": "Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves; the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands; indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction.", "title": "Finding Structure in Time"}, "56cbfcbfffd8c54bd8477d10b6e0e17e097b97c7": {"paper_id": "56cbfcbfffd8c54bd8477d10b6e0e17e097b97c7", "abstract": "This paper explores the difference between Connectionist proposals for cognitive a r c h i t e c t u r e a n d t h e s o r t s o f m o d e l s t hat have traditionally been assum e d i n c o g n i t i v e s c i e n c e . W e c l a i m t h a t t h e m a j o r d i s t i n c t i o n i s t h a t , w h i l e b o t h Connectionist and Classical architectures postulate representational mental states, the latter but not the former are committed to a symbol-level of representation, or to a \u2018language of thought\u2019: i.e., to representational states that have combinatorial syntactic and semantic structure. Several arguments for combinatorial structure in mental representations are then reviewed. These include arguments based on the \u2018systematicity\u2019 of mental representation: i.e., on the fact that cognitive capacities always exhibit certain symmetries, so that the ability to entertain a given thought implies the ability to entertain thoughts with semantically related contents. We claim that such arguments make a powerful case that mind/brain architecture is not Connectionist at the cognitive level. We then consider the possibility that Connectionism may provide an account of the neural (or \u2018abstract neurological\u2019) structures in which Classical cognitive architecture is implemented. We survey a n u m b e r o f t h e s t a n d a r d a r g u m e n t s t h a t h a v e b e e n o f f e r e d i n f a v o r o f Connectionism, and conclude that they are coherent only on this interpretation. Connectionist or PDP models are catching on. There are conferences and new books nearly every day, and the popular science press hails this new wave of theorizing as a breakthrough in understanding the mind (a typical example is the article in the May issue of Science 86, called \u201cHow we think: A new theory\u201d). There are also, inevitably, descriptions of the emergence of", "title": "Connectionism and cognitive architecture: A critical analysis"}, "06d51deada6e771a2571807a47dd991120c8dd1a": {"paper_id": "06d51deada6e771a2571807a47dd991120c8dd1a", "abstract": "Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder\u2013Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.", "title": "On the Properties of Neural Machine Translation: Encoder-Decoder Approaches"}, "652d159bf64a70194127722d19841daa99a69b64": {"paper_id": "652d159bf64a70194127722d19841daa99a69b64", "abstract": "This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.", "title": "Generating Sequences With Recurrent Neural Networks"}, "0b3cfbf79d50dae4a16584533227bb728e3522aa": {"paper_id": "0b3cfbf79d50dae4a16584533227bb728e3522aa", "abstract": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.", "title": "Long Short-Term Memory"}, "89eca547b1a2f6208ae529d45a65a51cf49adbff": {"paper_id": "89eca547b1a2f6208ae529d45a65a51cf49adbff", "abstract": "BLEU is the de facto standard automatic evaluation metric in machine translation. While BLEU is undeniably useful, it has a number of limitations. Although it works well for large documents and multiple references, it is unreliable at the sentence or sub-sentence levels, and with a single reference. In this paper, we propose new variants of BLEU which address these limitations, resulting in a more flexible metric which is not only more reliable, but also allows for more accurate discriminative training. Our best metric has better correlation with human judgements than standard BLEU, despite using a simpler formulation. Moreover, these improvements carry over to a system tuned for our new metric.", "title": "BLEU Deconstructed: Designing a Better MT Evaluation Metric"}, "7b4f3d0e4e2486a8d5d3f8e00549cf9a117bf88f": {"paper_id": "7b4f3d0e4e2486a8d5d3f8e00549cf9a117bf88f", "abstract": "Many machine learning tasks can be expressed as the transformation\u2014or transduction\u2014of input sequences into output sequences: speech recognition, machine translation, protein secondary structure prediction and text-to-speech to name but a few. One of the key challenges in sequence transduction is learning to represent both the input and output sequences in a way that is invariant to sequential distortions such as shrinking, stretching and translating. Recurrent neural networks (RNNs) are a powerful sequence learning architecture that has proven capable of learning such representations. However RNNs traditionally require a pre-defined alignment between the input and output sequences to perform transduction. This is a severe limitation since finding the alignment is the most difficult aspect of many sequence transduction problems. Indeed, even determining the length of the output sequence is often challenging. This paper introduces an end-to-end, probabilistic sequence transduction system, based entirely on RNNs, that returns a distribution over output sequences of all possible lengths and alignments for any input sequence. Experimental results are provided on the TIMIT speech corpus.", "title": "Sequence Transduction with Recurrent Neural Networks"}, "396aabd694da04cdb846cb724ca9f866f345cbd5": {"paper_id": "396aabd694da04cdb846cb724ca9f866f345cbd5", "abstract": "We explore efficient domain adaptation for the task of statistical machine translation based on extracting sentences from a large generaldomain parallel corpus that are most relevant to the target domain. These sentences may be selected with simple cross-entropy based methods, of which we present three. As these sentences are not themselves identical to the in-domain data, we call them pseudo in-domain subcorpora. These subcorpora \u2013 1% the size of the original \u2013 can then used to train small domain-adapted Statistical Machine Translation (SMT) systems which outperform systems trained on the entire corpus. Performance is further improved when we use these domain-adapted models in combination with a true in-domain model. The results show that more training data is not always better, and that best results are attained via proper domain-relevant data selection, as well as combining inand general-domain systems during decoding.", "title": "Domain Adaptation via Pseudo In-Domain Data Selection"}, "8729441d734782c3ed532a7d2d9611b438c0a09a": {"paper_id": "8729441d734782c3ed532a7d2d9611b438c0a09a", "abstract": "We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.", "title": "ADADELTA: An Adaptive Learning Rate Method"}, "20499f3c6fe9f84a12c9def941e2e12846a00c77": {"paper_id": "20499f3c6fe9f84a12c9def941e2e12846a00c77", "abstract": "The CoNLL-2013 shared task was devoted to grammatical error correction. In this paper, we give the task definition, present the data sets, and describe the evaluation metric and scorer used in the shared task. We also give an overview of the various approaches adopted by the participating teams, and present the evaluation results.", "title": "The CoNLL-2014 Shared Task on Grammatical Error Correction"}, "18b534c7207a1376fa92e87fe0d2cfb358d98c51": {"paper_id": "18b534c7207a1376fa92e87fe0d2cfb358d98c51", "abstract": "We demonstrate that an unlexicalized PCFG can parse much more accurately than previously shown, by making use of simple, linguistically motivated state splits, which break down false independence assumptions latent in a vanilla treebank grammar. Indeed, its performance of 86.36% (LP/LR F 1) is better than that of earlylexicalizedPCFG models, and surprisingly close to the current state-of-theart. This result has potential uses beyond establishing a strong lower bound on the maximum possible accuracy of unlexicalized models: an unlexicalized PCFG is much more compact, easier to replicate, and easier to interpret than more complex lexical models, and the parsing algorithms are simpler, more widely understood, of lower asymptotic complexity, and easier to optimize. In the early 1990s, as probabilistic methods swept NLP, parsing work revived the investigation of probabilistic context-free grammars ( PCFGs) (Booth and Thomson, 1973; Baker, 1979). However, early results on the utility ofPCFGs for parse disambiguation and language modeling were somewhat disappointing. A conviction arose that lexicalizedPCFGs (where head words annotate phrasal nodes) were the key tool for high performancePCFG parsing. This approach was congruent with the great success of word n-gram models in speech recognition, and drew strength from a broader interest in lexicalized grammars, as well as demonstrations that lexical dependencies were a key tool for resolving ambiguities such asPPattachments (Ford et al., 1982; Hindle and Rooth, 1993). In the following decade, great success in terms of parse disambiguation and even language modeling was achieved by various lexicalized PCFG models (Magerman, 1995; Charniak, 1997; Collins, 1999; Charniak, 2000; Charniak, 2001). However, several results have brought into question how large a role lexicalization plays in such parsers. Johnson (1998) showed that the performance of anunlexicalizedPCFGover the Penn treebank could be improved enormously simply by annotating each node by its parent category. The Penn treebank coveringPCFGis a poor tool for parsing because the context-freedom assumptions it embodies are far too strong, and weakening them in this way makes the model much better. More recently, Gildea (2001) discusses how taking the bilexical probabilities out of a good current lexicalized PCFG parser hurts performance hardly at all: by at most 0.5% for test text from the same domain as the training data, and not at all for test text from a different domain. 1 But it is precisely these bilexical dependencies that backed the intuition that lexicalized PCFGs should be very successful, for example in Hindle and Rooth\u2019s demonstration fromPPattachment. We take this as a reflection of the fundamental sparseness of the lexical dependency information available in the Penn Treebank. As a speech person would say, one million words of training data just isn\u2019t enough. Even for topics central to the treebank\u2019s Wall Street Journal text, such as stocks, many very plausible dependencies occur only once, for example stocks stabilized, while many others occur not at all, for example stocks skyrocketed .2 The best-performing lexicalized PCFGs have increasingly made use of subcategorization 3 of the 1There are minor differences, but all the current best-known lexicalized PCFGs employ bothmonolexicalstatistics, which describe the phrasal categories of arguments and adjuncts t hat appear around a head lexical item, and bilexicalstatistics, or dependencies, which describe the likelihood of a head word tak ing as a dependent a phrase headed by a certain other word. 2This observation motivates various classor similaritybased approaches to combating sparseness, and this remains a promising avenue of work, but success in this area has proven somewhat elusive, and, at any rate, current lexicalized PCFGs do simply use exact word matches if available, and interpola te with syntactic category-based estimates when they are not. 3In this paper we use the term subcategorizationin the original general sense of Chomsky (1965), for where a syntactic ca tcategories appearing in the Penn treebank. Charniak (2000) shows the value his parser gains from parentannotation of nodes, suggesting that this information is at least partly complementary to information derivable from lexicalization, and Collins (1999) uses a range of linguistically motivated and carefully hand-engineered subcategorizations to break down wrong context-freedom assumptions of the naive Penn treebank covering PCFG, such as differentiating \u201cbaseNPs\u201d from noun phrases with phrasal modifiers, and distinguishing sentences with empty subjects from those where there is an overt subject NP. While he gives incomplete experimental results as to their efficacy, we can assume that these features were incorporated because of beneficial effects on parsing that were complementary to lexicalization. In this paper, we show that the parsing performance that can be achieved by an unlexicalized PCFG is far higher than has previously been demonstrated, and is, indeed, much higher than community wisdom has thought possible. We describe several simple, linguistically motivated annotations which do much to close the gap between a vanilla PCFG and state-of-the-art lexicalized models. Specifically, we construct anunlexicalizedPCFG which outperforms the lexicalized PCFGs of Magerman (1995) and Collins (1996) (though not more recent models, such as Charniak (1997) or Collins (1999)). One benefit of this result is a much-strengthened lower bound on the capacity of an unlexicalized PCFG. To the extent that no such strong baseline has been provided, the community has tended to greatly overestimate the beneficial effect of lexicalization in probabilistic parsing, rather than looking critically at where lexicalized probabilities are both neededto make the right decision and availablein the training data. Secondly, this result affirms the value of linguistic analysis for feature discovery. The result has other uses and advantages: an unlexicalized PCFGis easier to interpret, reason about, and improve than the more complex lexicalized models. The grammar representation is much more compact, no longer requiring large structures that store lexicalized probabilities. The parsing algorithms have lower asymptotic complexity4 and have much smaller grammar egory is divided into several subcategories, for example di viding verb phrases into finite and non-finite verb phrases, rath e than in the modern restricted usage where the term refers onl y to the syntactic argument frames of predicators. 4O(n3) vs. O(n5) for a naive implementation, or vs. O(n4) if using the clever approach of Eisner and Satta (1999). constants. An unlexicalizedPCFG parser is much simpler to build and optimize, including both standard code optimization techniques and the investigation of methods for search space pruning (Caraballo and Charniak, 1998; Charniak et al., 1998). It is not our goal to argue against the use of lexicalized probabilities in high-performance probabilistic parsing. It has been comprehensively demonstrated that lexical dependencies are useful in resolving major classes of sentence ambiguities, and a parser should make use of such information where possible. We focus here on using unlexicalized, tructural context because we feel that this information has been underexploited and underappreciated. We see this investigation as only one part of the foundation for state-of-the-art parsing which employsboth lexical and structural conditioning. 1 Experimental Setup To facilitate comparison with previous work, we trained our models on sections 2\u201321 of the WSJsection of the Penn treebank. We used the first 20 files (393 sentences) of section 22 as a development set (devset ). This set is small enough that there is noticeable variance in individual results, but it allowed rapid search for good features via continually reparsing the devset in a partially manual hill-climb. All of section 23 was used as a test set for the final model. For each model, input trees were annotated or transformed in some way, as in Johnson (1998). Given a set of transformed trees, we viewed the local trees as grammar rewrite rules in the standard way, and used (unsmoothed) maximum-likelihood estimates for rule probabilities. 5 To parse the grammar, we used a simple array-based Java implementation of a generalizedCKY parser, which, for our final best model, was able to exhaustively parse all sentences in section 23 in 1GB of memory, taking approximately 3 sec for average length sentences. 6 5The tagging probabilities were smoothed to accommodate unknown words. The quantityP(tag|word) was estimated as follows: words were split into one of several categories wordclass, based on capitalization, suffix, digit, and other character features. For each of these categories, we took th e maximum-likelihood estimate of P(tag|wordclass). This distribution was used as a prior against which observed tagging s, if any, were taken, givingP(tag|word) = [c(tag, word) + \u03ba P(tag|wordclass)]/[c(word)+\u03ba]. This was then inverted to give P(word|tag). The quality of this tagging model impacts all numbers; for example the raw treebank grammar\u2019s devset F 1 is 72.62 with it and 72.09 without it. 6The parser is available for download as open source at: http://nlp.stanford.edu/downloads/lex-parser.shtml", "title": "Accurate Unlexicalized Parsing"}, "1a2ea7741b7fa1156d6785f13f3d6fa949b4f6bc": {"paper_id": "1a2ea7741b7fa1156d6785f13f3d6fa949b4f6bc", "abstract": "We present a novel method for evaluating grammatical error correction. The core of our method, which we call MaxMatch (M), is an algorithm for efficiently computing the sequence of phrase-level edits between a source sentence and a system hypothesis that achieves the highest overlap with the goldstandard annotation. This optimal edit sequence is subsequently scored using F1 measure. We test our M scorer on the Helping Our Own (HOO) shared task data and show that our method results in more accurate evaluation for grammatical error correction.", "title": "Better Evaluation for Grammatical Error Correction"}, "644602c65a5d8f30e62be027eb7b47f7c335191a": {"paper_id": "644602c65a5d8f30e62be027eb7b47f7c335191a", "abstract": "Sign Language Recognition (SLR) has been an active research field for the last two decades. However, most research to date has considered SLR as a naive gesture recognition problem. SLR seeks to recognize a sequence of continuous signs but neglects the underlying rich grammatical and linguistic structures of sign language that differ from spoken language. In contrast, we introduce the Sign Language Translation (SLT) problem. Here, the objective is to generate spoken language translations from sign language videos, taking into account the different word orders and grammar. We formalize SLT in the framework of Neural Machine Translation (NMT) for both end-to-end and pretrained settings (using expert knowledge). This allows us to jointly learn the spatial representations, the underlying language model, and the mapping between sign and spoken language. To evaluate the performance of Neural SLT, we collected the first publicly available Continuous SLT dataset, RWTH-PHOENIX-Weather 2014T1. It provides spoken language translations and gloss level annotations for German Sign Language videos of weather broadcasts. Our dataset contains over .95M frames with >67K signs from a sign vocabulary of >1K and >99K words from a German vocabulary of >2.8K. We report quantitative and qualitative results for various SLT setups to underpin future research in this newly established field. The upper bound for translation performance is calculated at 19.26 BLEU-4, while our end-to-end frame-level and gloss-level tokenization networks were able to achieve 9.58 and 18.13 respectively.", "title": "Neural Sign Language Translation"}, "261a056f8b21918e8616a429b2df6e1d5d33be41": {"paper_id": "261a056f8b21918e8616a429b2df6e1d5d33be41", "abstract": "Many real-world sequence learning tasks require the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their outputs into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label unsegmented sequences directly, thereby solving both problems. An experiment on the TIMIT speech corpus demonstrates its advantages over both a baseline HMM and a hybrid HMM-RNN.", "title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks"}, "4b8089bc9b49f84de43acc2eb8900035f7d492b2": {"paper_id": "4b8089bc9b49f84de43acc2eb8900035f7d492b2", "abstract": "In the first part of this paper, a regular recurrent neural network (RNN) is extended to a bidirectional recurrent neural network (BRNN). The BRNN can be trained without the limitation of using input information just up to a preset future frame. This is accomplished by training it simultaneously in positive and negative time direction. Structure and training procedure of the proposed network are explained. In regression and classification experiments on artificial data, the proposed structure gives better results than other approaches. For real data, classification experiments for phonemes from the TIMIT database show the same tendency. In the second part of this paper, it is shown how the proposed bidirectional structure can be easily modified to allow efficient estimation of the conditional posterior probability of complete symbol sequences without making any explicit assumption about the shape of the distribution. For this part, experiments on real data are reported.", "title": "Bidirectional recurrent neural networks"}, "30bc0ca0b965a7e01f1c4cf20684fd654f975e4a": {"paper_id": "30bc0ca0b965a7e01f1c4cf20684fd654f975e4a", "abstract": "In this paper, we carry out two experiments on the TIMIT speech corpus with bidirectional and unidirectional Long Short Term Memory (LSTM) networks. In the first experiment (framewise phoneme classification) we find that bidirectional LSTM outperforms both unidirectional LSTM and conventional Recurrent Neural Networks (RNNs). In the second (phoneme recognition) we find that a hybrid BLSTM-HMM system improves on an equivalent traditional HMM system, as well as unidirectional LSTM-HMM.", "title": "Bidirectional LSTM Networks for Improved Phoneme Classification and Recognition"}, "047655e733a9eed9a500afd916efa566915b9110": {"paper_id": "047655e733a9eed9a500afd916efa566915b9110", "abstract": "The temporal distance between events conveys information essential for numerous sequential tasks such as motor control and rhythm detection. While Hidden Markov Models tend to ignore this information, recurrent neural networks (RNNs) can in principle learn to make use of it. We focus on Long Short-Term Memory (LSTM) because it has been shown to outperform other RNNs on tasks involving long time lags. We find that LSTM augmented by \u201cpeephole connections\u201d from its internal cells to its multiplicative gates can learn the fine distinction between sequences of spikes spaced either 50 or 49 time steps apart without the help of any short training exemplars. Without external resets or teacher forcing, our LSTM variant also learns to generate stable streams of precisely timed spikes and other highly nonlinear periodic patterns. This makes LSTM a promising approach for tasks that require the accurate measurement or generation of time intervals.", "title": "Learning Precise Timing with LSTM Recurrent Networks"}, "42e21cd78f578fa6ce61b06b99848697da85ed76": {"paper_id": "42e21cd78f578fa6ce61b06b99848697da85ed76", "abstract": "We propose a generic method for iteratively approximating various second-order gradient steps-Newton, Gauss-Newton, Levenberg-Marquardt, and natural gradient-in linear time per iteration, using special curvature matrix-vector products that can be computed in O(n). Two recent acceleration techniques for on-line learning, matrix momentum and stochastic meta-descent (SMD), implement this approach. Since both were originally derived by very different routes, this offers fresh insight into their operation, resulting in further improvements to SMD.", "title": "Fast Curvature Matrix-Vector Products for Second-Order Gradient Descent"}, "eefcc7bcc05436dac9881acb4ff4e4a0b730e175": {"paper_id": "eefcc7bcc05436dac9881acb4ff4e4a0b730e175", "abstract": "We address image classification on a large-scale, i.e. when a large number of images and classes are involved. First, we study classification accuracy as a function of the image signature dimensionality and the training set size. We show experimentally that the larger the training set, the higher the impact of the dimensionality on the accuracy. In other words, high-dimensional signatures are important to obtain state-of-the-art results on large datasets. Second, we tackle the problem of data compression on very large signatures (on the order of 105 dimensions) using two lossy compression strategies: a dimensionality reduction technique known as the hash kernel and an encoding technique based on product quantizers. We explain how the gain in storage can be traded against a loss in accuracy and/or an increase in CPU cost. We report results on two large databases \u2014 ImageNet and a dataset of lM Flickr images \u2014 showing that we can reduce the storage of our signatures by a factor 64 to 128 with little loss in accuracy. Integrating the decompression in the classifier learning yields an efficient and scalable training algorithm. On ILSVRC2010 we report a 74.3% accuracy at top-5, which corresponds to a 2.5% absolute improvement with respect to the state-of-the-art. On a subset of 10K classes of ImageNet we report a top-1 accuracy of 16.7%, a relative improvement of 160% with respect to the state-of-the-art.", "title": "High-dimensional signature compression for large-scale image classification"}, "096e07ced8d32fc9a3617ff1f725efe45507ede8": {"paper_id": "096e07ced8d32fc9a3617ff1f725efe45507ede8", "abstract": "We assess the applicability of several popular learning methods for the problem of recognizing generic visual categories with invariance to pose, lighting, and surrounding clutter. A large dataset comprising stereo image pairs of 50 uniform-colored toys under 36 azimuths, 9 elevations, and 6 lighting conditions was collected (for a total of 194,400 individual images). The objects were 10 instances of 5 generic categories: four-legged animals, human figures, airplanes, trucks, and cars. Five instances of each category were used for training, and the other five for testing. Low-resolution grayscale images of the objects with various amounts of variability and surrounding clutter were used for training and testing. Nearest neighbor methods, support vector machines, and convolutional networks, operating on raw pixels or on PCA-derived features were tested. Test error rates for unseen object instances placed on uniform backgrounds were around 13% for SVM and 7% for convolutional nets. On a segmentation/recognition task with highly cluttered images, SVM proved impractical, while convolutional nets yielded 16/7% error. A real-time version of the system was implemented that can detect and classify objects in natural scenes at around 10 frames per second.", "title": "Learning methods for generic object recognition with invariance to pose and lighting"}, "19abef760f870ae57a83c9c89f02e6b2b5e64161": {"paper_id": "19abef760f870ae57a83c9c89f02e6b2b5e64161", "abstract": "Progress in understanding the brain mechanisms underlying vision requires the construction of computational models that not only emulate the brain's anatomy and physiology, but ultimately match its performance on visual tasks. In recent years, \"natural\" images have become popular in the study of vision and have been used to show apparently impressive progress in building such models. Here, we challenge the use of uncontrolled \"natural\" images in guiding that progress. In particular, we show that a simple V1-like model--a neuroscientist's \"null\" model, which should perform poorly at real-world visual object recognition tasks--outperforms state-of-the-art object recognition systems (biologically inspired and otherwise) on a standard, ostensibly natural image recognition test. As a counterpoint, we designed a \"simpler\" recognition test to better span the real-world variation in object pose, position, and scale, and we show that this test correctly exposes the inadequacy of the V1-like model. Taken together, these results demonstrate that tests based on uncontrolled natural images can be seriously misleading, potentially guiding progress in the wrong direction. Instead, we reexamine what it means for images to be natural and argue for a renewed focus on the core problem of object recognition--real-world image variation.", "title": "Why is Real-World Visual Object Recognition Hard?"}, "13d4c2f76a7c1a4d0a71204e1d5d263a3f5a7986": {"paper_id": "13d4c2f76a7c1a4d0a71204e1d5d263a3f5a7986", "abstract": "Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund & R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148\u2013156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.", "title": "Random Forests"}, "408f31b7f1a1b9f4a13ea797b6253585ff361430": {"paper_id": "408f31b7f1a1b9f4a13ea797b6253585ff361430", "abstract": "We explore a network architecture introduced by Elman (1988) for predicting successive elements of a sequence. The network uses the pattern of activation over a set of hidden units from time-step t1, together with element t, to predict element t 1. When the network is trained with strings from a particular finite-state grammar, it can learn to be a perfect finite-state recognizer for the grammar. When the network has a minimal number of hidden units, patterns on the hidden units come to correspond to the nodes of the grammar, although this correspondence is not necessary for the network to act as a perfect finite-state recognizer. We explore the conditions under which the network can carry information about distant sequential contingencies across intervening elements. Such information is maintained with relative ease if it is relevant at each intermediate step; it tends to be lost when intervening elements do not depend on it. At first glance this may suggest that such networks are not relevant to natural language, in which dependencies may span indefinite distances. However, embeddings in natural language are not completely independent of earlier information. The final simulation shows that long distance sequential contingencies can be encoded by the network even if only subtle statistical properties of embedded strings depend on the early information.", "title": "Finite State Automata and Simple Recurrent Networks"}, "2d9e3f53fcdb548b0b3c4d4efb197f164fe0c381": {"paper_id": "2d9e3f53fcdb548b0b3c4d4efb197f164fe0c381", "abstract": "In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.", "title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling"}, "3e090dac6019963715df50dc23d830d97a0e25ba": {"paper_id": "3e090dac6019963715df50dc23d830d97a0e25ba", "abstract": "Variational methods have been previously explored as a tractable approximation to Bayesian inference for neural networks. However the approaches proposed so far have only been applicable to a few simple network architectures. This paper introduces an easy-to-implement stochastic variational method (or equivalently, minimum description length loss function) that can be applied to most neural networks. Along the way it revisits several common regularisers from a variational perspective. It also provides a simple pruning heuristic that can both drastically reduce the number of network weights and lead to improved generalisation. Experimental results are provided for a hierarchical multidimensional recurrent neural network applied to the TIMIT speech corpus.", "title": "Practical Variational Inference for Neural Networks"}, "06bae254319f8d39e80c7254c841787b45baf820": {"paper_id": "06bae254319f8d39e80c7254c841787b45baf820", "abstract": "Recurrent neural networks are powerful sequence learners. They are able to incorporate context information in a flexible way, and are robust to localised distortions of the input data. These properties make them well suited to sequence labelling, where input sequences are transcribed with streams of labels. Long short-term memory is an especially promising recurrent architecture, able to bridge long time delays between relevant input and output events, and thereby access long range context. The aim of this thesis is to advance the state-of-the-art in supervised sequence labelling with recurrent networks in general, and long short-term memory in particular. Its two main contributions are (1) a new type of output layer that allows recurrent networks to be trained directly for sequence labelling tasks where the alignment between the inputs and the labels is unknown, and (2) an extension of long short-term memory to multidimensional data, such as images and video sequences. Experimental results are presented on speech recognition, online and offline handwriting recognition, keyword spotting, image segmentation and image classification, demonstrating the advantages of advanced recurrent networks over other sequential algorithms, such as hidden Markov Models.", "title": "Supervised sequence labelling with recurrent neural networks"}, "36c009379f804993de22e8b4bc1d35996b324f24": {"paper_id": "36c009379f804993de22e8b4bc1d35996b324f24", "abstract": "There are two widely known issues with properly training Recurrent Neural Networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.", "title": "On the difficulty of training recurrent neural networks"}, "d61c034c07c155c41b0cdaf991330ec6d72e82e9": {"paper_id": "d61c034c07c155c41b0cdaf991330ec6d72e82e9", "abstract": "Online two-sided matching markets such as Q&A forums (e.g. StackOverflow, Quora) and online labour platforms (e.g. Upwork) critically rely on the ability to propose adequate matches based on imperfect knowledge of the two parties to be matched. This prompts the following question: Which matching recommendation algorithms can, in the presence of such uncertainty, lead to efficient platform operation? To answer this question, we develop a model of a task / server matching system. For this model, we give a necessary and sufficient condition for an incoming stream of tasks to be manageable by the system. We further identify a so-called back-pressure policy under which the throughput that the system can handle is optimized. We show that this policy achieves strictly larger throughput than a natural greedy policy. Finally, we validate our model and confirm our theoretical findings with experiments based on logs of Math. StackExchange, a StackOverflow forum dedicated to mathematics.", "title": "Adaptive matching for expert systems with uncertain task types"}, "1b3c86ad6c149941750d97bd72b6b0122c1d8b5e": {"paper_id": "1b3c86ad6c149941750d97bd72b6b0122c1d8b5e", "abstract": "Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy's success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support.", "title": "Finite-time Analysis of the Multiarmed Bandit Problem"}, "20f7e6dec2b8dabe3a8ec39e9e246a6ecd71077a": {"paper_id": "20f7e6dec2b8dabe3a8ec39e9e246a6ecd71077a", "abstract": "Dynamic phenomena are key concerns of IS researchers. However, the methodological approaches usually selected to investigate IS phenomena often rely on variance theory. Underlying factor models represent a rather static approach to the phenomenon by focusing on independent and dependent variables and explaining the degree of the relationships between them. Process theory has been suggested to overcome this problem. Process theory provides a complementary, dynamic perspective on IS phenomena by explaining how independent and dependent variables are linked in terms of event sequences. Although applying both approaches provides more complete pictures of IS phenomena, a lack of research methods focusing on process theories may hinder this goal. This article seeks to help closing this gap by examining how case research can be applied to develop process theory. We analyzed IS case research as well as process research literature and consolidated inputs from both sources toward a single methodology. The results highlight that the development of process theory benefits from a consistent methodology and quality measures that have been suggested in general case research. However, we also found that each step requires specific consideration of process theory characteristics in order to develop rigorous process theory.", "title": "How To Rigorously Develop Process Theory Using Case Research"}, "ef8070a37fb6f0959acfcee9d40f0b3cb912ba9f": {"paper_id": "ef8070a37fb6f0959acfcee9d40f0b3cb912ba9f", "abstract": "Over the last three decades, a methodological pluralism has developed within information systems (IS) research. Various disciplines and many research communities as well, contribute to this discussion. However, working on the same research topic or studying the same phenomenon does not necessarily ensure mutual understanding. Especially within this multidisciplinary and international context, the epistemological assumptions made by different researchers may vary fundamentally. These assumptions exert a substantial impact on how concepts like validity, reliability, quality and rigour of research are understood. Thus, the extensive publication of epistemological assumptions is, in effect, almost mandatory. Hence, the aim of this paper is to develop an epistemological framework which can be used for systematically analysing the epistemological assumptions in IS research. Rather than attempting to identify and classify IS research paradigms, this research aims at a comprehensive discussion of epistemology within the context of IS. It seeks to contribute to building the basis for identifying similarities as well as differences between distinct IS approaches and methods. In order to demonstrate the epistemological framework, the consensus-oriented interpretivist approach to conceptual modelling is used as an example.", "title": "Epistemological perspectives on IS research: a framework for analysing and systematizing epistemological assumptions"}, "9a8d5fc07566b8b2ca38c0ed97a58e362be703da": {"paper_id": "9a8d5fc07566b8b2ca38c0ed97a58e362be703da", "abstract": "Developing computer-based information systems necessarily involves making a number of implicit and explicit assumptions. The authors examine four different approaches to information systems development.", "title": "Four Paradigms of Information Systems Development"}, "3296127a13d3cb71dec0d4c3d4ea588337d4f814": {"paper_id": "3296127a13d3cb71dec0d4c3d4ea588337d4f814", "abstract": "We examined 155 information systems research articles published from 1983 to 1988 and found that although this research is not rooted in a single overarching theoretical perspective, it does exhibit a single set of philosophical assumptions regarding the nature of the phenomena studied by information systems researchers, and what constitutes valid knowledge about those phenomena. We believe that a single research perspective for studying information systems phenomena is unnecessarily restrictive, and argue that there exist other philosophical assumptions that can inform studies of the relationships between information technology, people, and organizations. In this paper, we present two additional research philosophies for consideration-the interpretive and the critical-and for each we provide empirical examples to illustrate how they are used. We conclude by suggesting that much can be gained if a plurality of research perspectives is effectively employed to investigate information systems phenomena.", "title": "Studying Information Technology in Organizations: Research Approaches and Assumptions"}, "5b075c602cdbc4adfd789d99b15613e33d16d5f5": {"paper_id": "5b075c602cdbc4adfd789d99b15613e33d16d5f5", "abstract": "This article discusses the conduct and evaluation of interpretive research in information systems. Whiie the conventions for evaluating information systems case studies conducted according to the natural science model of social science are now widely accepted, this is not the case for interpretive field studies. A set of principles for the conduct and evaluation of interpretive field research in information systems is proposed, along with their philosophical rationale. The usefulness of the principles is illustrated by evaluating three pubiished interpretive field studies drawn from the IS research literature. The intention of the paper is to further reflection and debate on the important subject of grounding interpretive research methodology.", "title": "A Set of Principles for Conducting and Evaluating Interpretive Field Studies in Information Systems"}, "532a52efca3bdb576d993c0dc53f075f172c1b07": {"paper_id": "532a52efca3bdb576d993c0dc53f075f172c1b07", "abstract": "This thesis considers two alternative views of purposeful action and shared understanding. The first, adopted by researchers in Cognitive Science, views the organization and significance of action as derived from plans, which are prerequisite to and prescribe action at whatever level of detail one might imagine. Mutual intelligibility on this view is a matter of the recognizability of plans, due to common conventions for the expression of intent, and common knowledge about typical situations and appropriate actions. The second view, drawn from recent work in social science, treats plans as derivative from situated action. Situated action as such comprises necessarily ad hoc responses to the actions of others and to the contingencies of particular situations. Rather than depend upon the reliable recognition of intent, successful interaction consists in the collaborative production of intelligibility through mutual access to situation resources, and through the detection, repair or exploitation of differences in understanding. As common sense formulations designed to accomodate the unforseeable contingences of situated action, plans are inherently vague. Researchers interested in machine intelligence attempt to remedy the vagueness of plans, to make them the basis for artifacts intended to embody intelligent behavior, including the ability to interact with their human users. The idea that computational artifacts might interact with their users is supported by their reactive, linguistic, and internally opaque properties. Those properties suggest the possibility that computers might 'explain themselves: thereby providing a solution to the problem of conveying the designer's purposes to the user, and a means of establishing the intelligence of the artifact itself. I examine the problem of human-machine communication through a case study of people using a machine designed on the planning model, and intended to be intelligent and interactive~ A conversation analysis of \"interactions\" between users and the machine reveals that the machine's insensitivity to particular circumstances is a central design resource, and a fundamental limitation. I conclude that problems in Cognitive Science's theorizing about purposeful action as a basis for machine intelligence are due to the project of substituting plans for actions, and representations of the situation of action, for action's actual circumstances. XEROX PARe. ISL-6. FEBRLARY 1985", "title": "Plans and situated actions - the problem of human-machine communication"}, "ad37eaa119819f19a9864d86ec27c9d98ce817f5": {"paper_id": "ad37eaa119819f19a9864d86ec27c9d98ce817f5", "abstract": "Your use of the JSTOR archive indicates your acceptance of JSTOR's Terms and Conditions of Use, available at http://www.jstor.org/page/info/about/policies/terms.jsp. JSTOR's Terms and Conditions of Use provides, in part, that unless you have obtained prior permission, you may not download an entire issue of a journal or multiple copies of articles, and you may use content in the JSTOR archive only for your personal, non-commercial use.", "title": "Building Theories from Case Study Research"}, "7abeaf172af1129556ee8b3fcbb2139172e50bdf": {"paper_id": "7abeaf172af1129556ee8b3fcbb2139172e50bdf", "abstract": "Your use of the JSTOR archive indicates your acceptance of JSTOR's Terms and Conditions of Use, available at http://www.jstor.org/page/info/about/policies/terms.jsp. JSTOR's Terms and Conditions of Use provides, in part, that unless you have obtained prior permission, you may not download an entire issue of a journal or multiple copies of articles, and you may use content in the JSTOR archive only for your personal, non-commercial use.", "title": "STRATEGIC DECISION PROCESSES IN HIGH VELOCITY ENVIRONMENTS : FOUR CASES IN THE MICROCOMPUTER INDUSTRY *"}, "85978718f87a0299b6b3fbbc3e8c40210d21942b": {"paper_id": "85978718f87a0299b6b3fbbc3e8c40210d21942b", "abstract": "Intuitive predictions follow a judgmental heuristic\u2014representativeness. By this heuristic, people predict the outcome that appears most representative of the evidence. Consequently, intuitive predictions are insensitive to the reliability of the evidence or to the prior probability of the outcome, in violation of the logic of statistical prediction. The hypothesis that people predict by representativeness is supported in a series of studies with both naive and sophisticated subjects. It is shown that the ranking of outcomes by likelihood coincides with their ranking by representativeness and that people erroneously predict rare events and extreme values if these happen to be representative. The experience of unjustified confidence in predictions and the prevalence of fallacious intuitions concerning statistical regression are traced to the representativeness heuristic. In this paper, we explore the rules that determine intuitive predictions and judgments of confidence and contrast these rules to the normative principles of statistical prediction. Two classes of prediction are discussed: category prediction and numerical prediction. In a categorical case, the prediction is given in nominal form, for example, the winner in an election, the diagnosis of a patient, or a person's future occupation. In a numerical case, the prediction is given in numerical form, for example, the future value of a particular stock or of a student's grade point average. In making predictions and judgments under uncertainty, people do not appear to follow the calculus of chance or the statistical theory of prediction. Instead, they rely on a limited number of heuristics which sometimes yield reasonable judgments and sometimes lead to severe and The present paper is concerned with the role of one of these heuristics\u2014representa-tiveness\u2014in intuitive predictions. Given specific evidence (e.g., a personality sketch), the outcomes under consideration (e.g., occupations or levels of achievement) can be ordered by the degree to which they are representative of that evidence. The thesis of this paper is that people predict by representativeness, that is, they select or order outcomes by the 237", "title": "On the Psychology of Prediction"}, "86a74b76f237ff694856307f2c6034bc8754f2a6": {"paper_id": "86a74b76f237ff694856307f2c6034bc8754f2a6", "abstract": "ion (i.e., probes) that was more accurate than the individual instantiations (e.g., alliances, exploratory products). Multiple cases also enable broader exploration of research questions and theoretical elaboration. For example, Brown and Eisenhardt (1998) added successful and unsuccessful turnaround cases that enabled them to add further longitudinal elements to their theory. Because case numbers are typically small, a few additional cases can significantly affect the quality of the emergent theory. For example, adding three cases to a single-case study is modest in terms of numbers, but offers four times the analytic power. Thus, theory building from multiple cases typically yields more robust, generalizable, and testable theory than single-case research. But although multiple cases are likely to result in better theory, theoretical sampling is more complicated. The choice is based less on the uniqueness of a given case, and more on the contribution to theory development within the set of cases. That is, multiple cases are chosen for theoretical reasons such as replication, extension of theory, contrary replication, and elimination of alternative explanations (Yin, 1994). For example, Graebner and Eisenhardt (2004) studied acquisition from the seller perspective by examining three replicated cases in which the executives sold their companies, a contrary replication in which executives could have sold their companies but did not, and then further cases in different industries that explored industrylevel explanations. A particularly important theoretical sampling approach is \u201cpolar types,\u201d in which a researcher samples extreme (e.g., very high and very low performing) cases in order to more easily observe contrasting patterns in the data. Although such an approach can surprise reviewers because the resulting theory is so consistently supported by the empirical evidence, this sampling leads to very clear pattern recognition of the central constructs, relationships, and logic of the focal phenomenon. 2007 27 Eisenhardt and Graebner", "title": "THEORY BUILDING FROM CASES : OPPORTUNITIES AND CHALLENGES"}, "c7687f7a8683a76fa6c96ace79e5ce1c40ff868f": {"paper_id": "c7687f7a8683a76fa6c96ace79e5ce1c40ff868f", "abstract": "Using grounded theory as an example, this paper examines three methodological questions that are generally applicable to all qualitative methods. How should the usual scientific canons be reinterpreted for qualitative research? How should researchers report the procedures and canons used in their research? What evaluative criteria should be used in judging the research products? We propose that the criteria should be adapted to fit the procedures of the method. We demonstrate how this can be done for grounded theory and suggest criteria for evaluating studies following this approach. We argue that other qualitative researchers might be similarly specific about their procedures and evaluative criteria.", "title": "Grounded Theory Research : Procedures , Canons , and Evaluative Criteria"}, "93f0c02363d603f2e4d095160ede3031b80ec094": {"paper_id": "93f0c02363d603f2e4d095160ede3031b80ec094", "abstract": "Segmentation of blood vessels from magnetic resonance angiography (MRA) or computed tomography angiography (CTA) images is a complex process that usually takes a lot of computational resources. Also, most vascular segmentation and detection algorithms do not work properly due to the wide architectural variability of the blood vessels. Thus, the construction of convincing synthetic vascular trees makes it possible to validate new segmentation methodologies. In this work, an extension to the traditional Lindenmayer system (L-system) that generates synthetic 3D blood vessels by adding stochastic rules and parameters to the grammar is proposed. Towards this aim, we implement a parser and a generator of L-systems whose grammars simulate natural features of real vessels such as the bifurcation angle, average length and diameter, as well as vascular anomalies, such as aneurysms and stenoses. The resulting expressions are then used to create synthetic vessel images that mimic MRA and CTA images. In addition, this methodology allows for vessel growth to be limited by arbitrary 3D surfaces, and the vessel intensity profile can be tailored to match real angiographic intensities.", "title": "Three-dimensional synthetic blood vessel generation using stochastic L-systems"}, "75352a06f8f39701d59c3a3a78e5cce6dd469ea9": {"paper_id": "75352a06f8f39701d59c3a3a78e5cce6dd469ea9", "abstract": "This paper presents a comparison study between 10 automatic and six interactive methods for liver segmentation from contrast-enhanced CT images. It is based on results from the \"MICCAI 2007 Grand Challenge\" workshop, where 16 teams evaluated their algorithms on a common database. A collection of 20 clinical images with reference segmentations was provided to train and tune algorithms in advance. Participants were also allowed to use additional proprietary training data for that purpose. All teams then had to apply their methods to 10 test datasets and submit the obtained results. Employed algorithms include statistical shape models, atlas registration, level-sets, graph-cuts and rule-based systems. All results were compared to reference segmentations five error measures that highlight different aspects of segmentation accuracy. All measures were combined according to a specific scoring system relating the obtained values to human expert variability. In general, interactive methods reached higher average scores than automatic approaches and featured a better consistency of segmentation quality. However, the best automatic methods (mainly based on statistical shape models with some additional free deformation) could compete well on the majority of test images. The study provides an insight in performance of different segmentation approaches under real-world conditions and highlights achievements and limitations of current image analysis techniques.", "title": "Comparison and Evaluation of Methods for Liver Segmentation From CT Datasets"}, "2340b73d0a788755c7536e857c845905a3b614a9": {"paper_id": "2340b73d0a788755c7536e857c845905a3b614a9", "abstract": "We have implemented and validated an algorithm for three-dimensional positron emission tomography transmission-to-computed tomography registration in the chest, using mutual information as a similarity criterion. Inherent differences in the two imaging protocols produce significant nonrigid motion between the two acquisitions. A rigid body deformation combined with localized cubic B-splines is used to capture this motion. The deformation is defined on a regular grid and is parameterized by potentially several thousand coefficients. Together with a spline-based continuous representation of images and Parzen histogram estimates, our deformation model allows closed-form expressions for the criterion and its gradient. A limited-memory quasi-Newton optimization algorithm is used in a hierarchical multiresolution framework to automatically align the images. To characterize the performance of the method, 27 scans from patients involved in routine lung cancer staging were used in a validation study. The registrations were assessed visually by two expert observers in specific anatomic locations using a split window validation technique. The visually reported errors are in the 0- to 6-mm range and the average computation time is 100 min on a moderate-performance workstation.", "title": "PET-CT image registration in the chest using free-form deformations"}, "1bbd1d2ec91325157cb3d47ba4b2e02d41eee63d": {"paper_id": "1bbd1d2ec91325157cb3d47ba4b2e02d41eee63d", "abstract": "We propose a shape-based approach to curve evolution for the segmentation of medical images containing known object types. In particular, motivated by the work of Leventon, Grimson, and Faugeras (2000), we derive a parametric model for an implicit representation of the segmenting curve by applying principal component analysis to a collection of signed distance representations of the training data. The parameters of this representation are then manipulated to minimize an objective function for segmentation. The resulting algorithm is able to handle multidimensional data, can deal with topological changes of the curve, is robust to noise and initial contour placements, and is computationally efficient. At the same time, it avoids the need for point correspondences during the training phase of the algorithm. We demonstrate this technique by applying it to two medical applications; two-dimensional segmentation of cardiac magnetic resonance imaging (MRI) and three-dimensional segmentation of prostate MRI.", "title": "A shape-based approach to the segmentation of medical imagery using level sets"}, "c0ffbe57242579ee474c280a2008a016705faad1": {"paper_id": "c0ffbe57242579ee474c280a2008a016705faad1", "abstract": "The effects of iron substitution on the structural and magnetic properties of the GdCo(12-x)Fe(x)B6 (0 \u2264 x \u2264 3) series of compounds have been studied. All of the compounds form in the rhombohedral SrNi12B6-type structure and exhibit ferrimagnetic behaviour below room temperature: T(C) decreases from 158 K for x = 0 to 93 K for x = 3. (155)Gd M\u00f6ssbauer spectroscopy indicates that the easy magnetization axis changes from axial to basal-plane upon substitution of Fe for Co. This observation has been confirmed using neutron powder diffraction. The axial to basal-plane transition is remarkably sensitive to the Fe content and comparison with earlier (57)Fe-doping studies suggests that the boundary lies below x = 0.1.", "title": "Ferrimagnetism in GdCo(12-x)Fe(x)B6."}, "d69df4f3e395abc6ed75f515945eb00cff37fa4e": {"paper_id": "d69df4f3e395abc6ed75f515945eb00cff37fa4e", "abstract": "Graph sampling is a technique to pick a subset of vertices and/ or edges from original graph. It has a wide spectrum of applications, e.g. survey hidden population in sociology [54], visualize social graph [29], scale down Internet AS graph [27], graph sparsification [8], etc. In some scenarios, the whole graph is known and the purpose of sampling is to obtain a smaller graph. In other scenarios, the graph is unknown and sampling is regarded as a way to explore the graph. Commonly used techniques are Vertex Sampling, Edge Sampling and Traversal Based Sampling. We provide a taxonomy of different graph sampling objectives and graph sampling approaches. The relations between these approaches are formally argued and a general framework to bridge theoretical analysis and practical implementation is provided. Although being smaller in size, sampled graphs may be similar to original graphs in some way. We are particularly interested in what graph properties are preserved given a sampling procedure. If some properties are preserved, we can estimate them on the sampled graphs, which gives a way to construct efficient estimators. If one algorithm relies on the perserved properties, we can expect that it gives similar output on original and sampled graphs. This leads to a systematic way to accelerate a class of graph algorithms. In this survey, we discuss both classical text-book type properties and some advanced properties. The landscape is tabularized and we see a lot of missing works in this field. Some theoretical studies are collected in this survey and simple extensions are made. Most previous numerical evaluation works come in an ad hoc fashion, i.e. evaluate different type of graphs, different set of properties, and different sampling algorithms. A systematical and neutral evaluation is needed to shed light on further graph sampling studies.", "title": "A Survey and Taxonomy of Graph Sampling"}, "0bb7b5098d30b10422c2e52462f5152696390c3a": {"paper_id": "0bb7b5098d30b10422c2e52462f5152696390c3a", "abstract": "When the probability of measuring a particular value of some quantity varies inversely as a power of that value, the quantity is said to follow a power law, also known variously as Zipf\u2019s law or the Pareto distribution. Power laws appear widely in physics, biology, earth and planetary sciences, economics and finance, computer science, demography and the social sciences. For instance, the distributions of the sizes of cities, earthquakes, forest fires, solar flares, moon craters and people\u2019s personal fortunes all appear to follow power laws. The origin of power-law behaviour has been a topic of debate in the scientific community for more than a century. Here we review some of the empirical evidence for the existence of power-law forms and the theories proposed to explain them.", "title": "Power laws , Pareto distributions and Zipf \u2019 s law"}, "53bd1357a20550caf1317803e7bc88d3440a6984": {"paper_id": "53bd1357a20550caf1317803e7bc88d3440a6984", "abstract": "We propose a random graph model which is a special case of sparse random graphs with given degree sequences. This model involves only a small number of parameters, called logsize and log-log growth rate. These parameters capture some universal characteristics of massive graphs. Furthermore, from these parameters, various properties of the graph can be derived. For example, for certain ranges of the parameters, we will compute the expected distribution of the sizes of the connected components which almost surely occur with high probability. We will illustrate the consistency of our model with the behavior of some massive graphs derived from data in telecommunications. We will also discuss the threshold function, the giant component, and the evolution of random graphs in this model.", "title": "A random graph model for massive graphs"}, "a151fcaa3d003321d6e09602a927fc434d19b032": {"paper_id": "a151fcaa3d003321d6e09602a927fc434d19b032", "abstract": "A network is said to show assortative mixing if the nodes in the network that have many connections tend to be connected to other nodes with many connections. Here we measure mixing patterns in a variety of networks and find that social networks are mostly assortatively mixed, but that technological and biological networks tend to be disassortative. We propose a model of an assortatively mixed network, which we study both analytically and numerically. Within this model we find that networks percolate more easily if they are assortative and that they are also more robust to vertex removal.", "title": "Assortative mixing in networks."}, "081ed2d02a0602dfa8aa41d3cf284279bdb10e4b": {"paper_id": "081ed2d02a0602dfa8aa41d3cf284279bdb10e4b", "abstract": "Being able to keep the graph scale small while capturing the properties of the original social graph, graph sampling provides an efficient, yet inexpensive solution for social network analysis. The challenge is how to create a small, but representative sample out of the massive social graph with millions or even billions of nodes. Several sampling algorithms have been proposed in previous studies, but there lacks fair evaluation and comparison among them. In this paper, we analyze the state-of art graph sampling algorithms and evaluate their performance on some widely recognized graph properties on directed graphs using large-scale social network datasets. We evaluate not only the commonly used node degree distribution, but also clustering coefficient, which quantifies how well connected are the neighbors of a node in a graph. Through the comparison we have found that none of the algorithms is able to obtain satisfied sampling results in both of these properties, and the performance of each algorithm differs much in different kinds of datasets.", "title": "Understanding Graph Sampling Algorithms for Social Network Analysis"}, "0e0a79910ddb8dedc198c56a803ea774af47d3a3": {"paper_id": "0e0a79910ddb8dedc198c56a803ea774af47d3a3", "abstract": "Social networking services are a fast-growing business in the Internet. However, it is unknown if online relationships and their growth patterns are the same as in real-life social networks. In this paper, we compare the structures of three online social networking services: Cyworld, MySpace, and orkut, each with more than 10 million users, respectively. We have access to complete data of Cyworld's ilchon (friend) relationships and analyze its degree distribution, clustering property, degree correlation, and evolution over time. We also use Cyworld data to evaluate the validity of snowball sampling method, which we use to crawl and obtain partial network topologies of MySpace and orkut. Cyworld, the oldest of the three, demonstrates a changing scaling behavior over time in degree distribution. The latest Cyworld data's degree distribution exhibits a multi-scaling behavior, while those of MySpace and orkut have simple scaling behaviors with different exponents. Very interestingly, each of the two e ponents corresponds to the different segments in Cyworld's degree distribution. Certain online social networking services encourage online activities that cannot be easily copied in real life; we show that they deviate from close-knit online social networks which show a similar degree correlation pattern to real-life social networks.", "title": "Analysis of topological characteristics of huge online social networking services"}, "1a43ae25752f7b088f1bb92f0d5df1badab74d08": {"paper_id": "1a43ae25752f7b088f1bb92f0d5df1badab74d08", "abstract": "Social networks are popular platforms for interaction, communication and collaboration between friends. Researchers have recently proposed an emerging class of applications that leverage relationships from social networks to improve security and performance in applications such as email, web browsing and overlay routing. While these applications often cite social network connectivity statistics to support their designs, researchers in psychology and sociology have repeatedly cast doubt on the practice of inferring meaningful relationships from social network connections alone.\n This leads to the question: Are social links valid indicators of real user interaction? If not, then how can we quantify these factors to form a more accurate model for evaluating socially-enhanced applications? In this paper, we address this question through a detailed study of user interactions in the Facebook social network. We propose the use of interaction graphs to impart meaning to online social links by quantifying user interactions. We analyze interaction graphs derived from Facebook user traces and show that they exhibit significantly lower levels of the \"small-world\" properties shown in their social graph counterparts. This means that these graphs have fewer \"supernodes\" with extremely high degree, and overall network diameter increases significantly as a result. To quantify the impact of our observations, we use both types of graphs to validate two well-known social-based applications (RE and SybilGuard). The results reveal new insights into both systems, and confirm our hypothesis that studies of social applications should use real indicators of user interactions in lieu of social graphs.", "title": "User interactions in social networks and their implications"}, "389293e09c4f0c22e7e9f74be4a59b7c93bbd718": {"paper_id": "389293e09c4f0c22e7e9f74be4a59b7c93bbd718", "abstract": "With more than 250 million active users, Facebook (FB) is currently one of the most important online social networks. Our goal in this paper is to obtain a representative (unbiased) sample of Facebook users by crawling its social graph. In this quest, we consider and implement several candidate techniques. Two approaches that are found to perform well are the Metropolis-Hasting random walk (MHRW) and a re-weighted random walk (RWRW). Both have pros and cons, which we demonstrate through a comparison to each other as well as to the \"ground-truth\" (UNI - obtained through true uniform sampling of FB userIDs). In contrast, the traditional Breadth-First-Search (BFS) and Random Walk (RW) perform quite poorly, producing substantially biased results. In addition to offline performance assessment, we introduce online formal convergence diagnostics to assess sample quality during the data collection process. We show how these can be used to effectively determine when a random walk sample is of adequate size and quality for subsequent use (i.e., when it is safe to cease sampling). Using these methods, we collect the first, to the best of our knowledge, unbiased sample of Facebook. Finally, we use one of our representative datasets, collected through MHRW, to characterize several key properties of Facebook.", "title": "Walking in Facebook: A Case Study of Unbiased Sampling of OSNs"}, "9b90cb4aea40677494e4a3913878e355c4ae56e8": {"paper_id": "9b90cb4aea40677494e4a3913878e355c4ae56e8", "abstract": "Networks of coupled dynamical systems have been used to model biological oscillators, Josephson junction arrays,, excitable media, neural networks, spatial games, genetic control networks and many other self-organizing systems. Ordinarily, the connection topology is assumed to be either completely regular or completely random. But many biological, technological and social networks lie somewhere between these two extremes. Here we explore simple models of networks that can be tuned through this middle ground: regular networks \u2018rewired\u2019 to introduce increasing amounts of disorder. We find that these systems can be highly clustered, like regular lattices, yet have small characteristic path lengths, like random graphs. We call them \u2018small-world\u2019 networks, by analogy with the small-world phenomenon, (popularly known as six degrees of separation). The neural network of the worm Caenorhabditis elegans, the power grid of the western United States, and the collaboration graph of film actors are shown to be small-world networks. Models of dynamical systems with small-world coupling display enhanced signal-propagation speed, computational power, and synchronizability. In particular, infectious diseases spread more easily in small-world networks than in regular lattices.", "title": "Collective dynamics of \u2018small-world\u2019 networks"}, "c3fbc6de8b4ac661f1710c7f127171b4c6c6a14e": {"paper_id": "c3fbc6de8b4ac661f1710c7f127171b4c6c6a14e", "abstract": "In this paper, a \u201cvirtual slope method\u201d for walking trajectory planning on stairs for biped robots is proposed. In conventional methods for walking on stairs, there are two problems about the zero-moment point (ZMP). One is a ZMP equation problem, and the other is a ZMP definition problem in a double-support phase. First, a ZMP equation on stairs is different from that on flat ground. Therefore, the same trajectory generation as flat ground cannot be implemented. This problem is defined as a \u201cZMP equation problem.\u201d Second, the ZMP cannot be defined in the double-support phase on stairs because contact points of the feet do not constitute a plane. The ZMP can be defined only on the plane. This problem is defined as a \u201cZMP definition problem.\u201d These two problems are solved concurrently by the virtual slope method. It is the method that regards the stairs as a virtual slope. In walking trajectory planning on a slope of the constant gradient, the two problems about the ZMP do not exist. Additionally, a trajectory planning procedure based on the virtual slope method is explained. The validity of the proposed method is confirmed by some simulations and experiments.", "title": "Walking Trajectory Planning on Stairs Using Virtual Slope for Biped Robots"}, "6f96d433e91ddf46d8a7b174dfbdd8eed1087d40": {"paper_id": "6f96d433e91ddf46d8a7b174dfbdd8eed1087d40", "abstract": "Domain-invariant (view-invariant and modality-invariant) feature representation is essential for human action recognition. Moreover, given a discriminative visual representation, it is critical to discover the latent correlations among multiple actions in order to facilitate action modeling. To address these problems, we propose a multi-domain and multi-task learning (MDMTL) method to: 1) extract domain-invariant information for multi-view and multi-modal action representation and 2) explore the relatedness among multiple action categories. Specifically, we present a sparse transfer learning-based method to co-embed multi-domain (multi-view and multi-modality) data into a single common space for discriminative feature learning. Additionally, visual feature learning is incorporated into the multi-task learning framework, with the Frobenius-norm regularization term and the sparse constraint term, for joint task modeling and task relatedness-induced feature learning. To the best of our knowledge, MDMTL is the first supervised framework to jointly realize domain-invariant feature learning and task modeling for multi-domain action recognition. Experiments conducted on the INRIA Xmas Motion Acquisition Sequences data set, the MSR Daily Activity 3D (DailyActivity3D) data set, and the Multi-modal & Multi-view & Interactive data set, which is the most recent and largest multi-view and multi-model action recognition data set, demonstrate the superiority of MDMTL over the state-of-the-art approaches.", "title": "Multi-Domain and Multi-Task Learning for Human Action Recognition"}, "02a98118ce990942432c0147ff3c0de756b4b76a": {"paper_id": "02a98118ce990942432c0147ff3c0de756b4b76a", "abstract": "The aim of this paper is to address recognition of natural human actions in diverse and realistic video settings. This challenging but important subject has mostly been ignored in the past due to several problems one of which is the lack of realistic and annotated video datasets. Our first contribution is to address this limitation and to investigate the use of movie scripts for automatic annotation of human actions in videos. We evaluate alternative methods for action retrieval from scripts and show benefits of a text-based classifier. Using the retrieved action samples for visual learning, we next turn to the problem of action classification in video. We present a new method for video classification that builds upon and extends several recent ideas including local space-time features, space-time pyramids and multi-channel non-linear SVMs. The method is shown to improve state-of-the-art results on the standard KTH action dataset by achieving 91.8% accuracy. Given the inherent problem of noisy labels in automatic annotation, we particularly investigate and show high tolerance of our method to annotation errors in the training set. We finally apply the method to learning and classifying challenging action classes in movies and show promising results.", "title": "Learning realistic human actions from movies"}, "6090ebb9464986e458f8c37d2e08b042c88bf651": {"paper_id": "6090ebb9464986e458f8c37d2e08b042c88bf651", "abstract": "Recently, methods based on local image features have shown promise for texture and object recognition tasks. This paper presents a large-scale evaluation of an approach that represents images as distributions (signatures or histograms) of features extracted from a sparse set of keypoint locations and learns a Support Vector Machine classifier with kernels based on two effective measures for comparing distributions, the Earth Mover\u2019s Distance and the \u03c72 distance. We first evaluate the performance of our approach with different keypoint detectors and descriptors, as well as different kernels and classifiers. We then conduct a comparative evaluation with several state-of-the-art recognition methods on four texture and five object databases. On most of these databases, our implementation exceeds the best reported results and achieves comparable performance on the rest. Finally, we investigate the influence of background correlations on recognition performance via extensive tests on the PASCAL database, for which ground-truth object localization information is available. Our experiments demonstrate that image representations based on distributions of local features are surprisingly effective for classification of texture and object images under challenging real-world conditions, including significant intra-class variations and substantial background clutter.", "title": "Local Features and Kernels for Classification of Texture and Object Categories: A Comprehensive Study"}, "b98c68f01d84ac07dc7fc51af782018070da748f": {"paper_id": "b98c68f01d84ac07dc7fc51af782018070da748f", "abstract": "The objective of this paper is classifying images by the object categories they contain, for example motorbikes or dolphins. There are three areas of novelty. First, we introduce a descriptor that represents local image shape and its spatial layout, together with a spatial pyramid kernel. These are designed so that the shape correspondence between two images can be measured by the distance between their descriptors using the kernel. Second, we generalize the spatial pyramid kernel, and learn its level weighting parameters (on a validation set). This significantly improves classification performance. Third, we show that shape and appearance kernels may be combined (again by learning parameters on a validation set).\n Results are reported for classification on Caltech-101 and retrieval on the TRECVID 2006 data sets. For Caltech-101 it is shown that the class specific optimization that we introduce exceeds the state of the art performance by more than 10%.", "title": "Representing shape with a spatial pyramid kernel"}, "13082af1fd6bb9bfe63e73cf007de1655b7f9ae0": {"paper_id": "13082af1fd6bb9bfe63e73cf007de1655b7f9ae0", "abstract": "The automated categorization (or classification) of texts into predefined categories has witnessed a booming interest in the last 10 years, due to the increased availability of documents in digital form and the ensuing need to organize them. In the research community the dominant approach to this problem is based on machine learning techniques: a general inductive process automatically builds a classifier by learning, from a set of preclassified documents, the characteristics of the categories. The advantages of this approach over the knowledge engineering approach (consisting in the manual definition of a classifier by domain experts) are a very good effectiveness, considerable savings in terms of expert labor power, and straightforward portability to different domains. This survey discusses the main approaches to text categorization that fall within the machine learning paradigm. We will discuss in detail issues pertaining to three different problems, namely, document representation, classifier construction, and classifier evaluation.", "title": "Machine learning in automated text categorization"}, "124d967683544973581f951ee93b3f7c069e3ced": {"paper_id": "124d967683544973581f951ee93b3f7c069e3ced", "abstract": "We present a biologically-motivated system for the recognition of actions from video sequences. The approach builds on recent work on object recognition based on hierarchical feedforward architectures [25, 16, 20] and extends a neurobiological model of motion processing in the visual cortex [10]. The system consists of a hierarchy of spatio-temporal feature detectors of increasing complexity: an input sequence is first analyzed by an array of motion- direction sensitive units which, through a hierarchy of processing stages, lead to position-invariant spatio-temporal feature detectors. We experiment with different types of motion-direction sensitive units as well as different system architectures. As in [16], we find that sparse features in intermediate stages outperform dense ones and that using a simple feature selection approach leads to an efficient system that performs better with far fewer features. We test the approach on different publicly available action datasets, in all cases achieving the highest results reported to date.", "title": "A Biologically Inspired System for Action Recognition"}, "8f79f3adc02e7c58fb8d8bb297dd6bff06d91756": {"paper_id": "8f79f3adc02e7c58fb8d8bb297dd6bff06d91756", "abstract": "In this paper, we present a home-monitoring oriented human activity recognition benchmark database, based on the combination of a color video camera and a depth sensor. Our contributions are two-fold: 1) We have created a publicly releasable human activity video database (i.e., named as RGBD-HuDaAct), which contains synchronized color-depth video streams, for the task of human daily activity recognition. This database aims at encouraging more research efforts on human activity recognition based on multi-modality sensor combination (e.g., color plus depth). 2) Two multi-modality fusion schemes, which naturally combine color and depth information, have been developed from two state-of-the-art feature representation methods for action recognition, i.e., spatio-temporal interest points (STIPs) and motion history images (MHIs). These depth-extended feature representation methods are evaluated comprehensively and superior recognition performances over their uni-modality (e.g., color only) counterparts are demonstrated.", "title": "RGBD-HuDaAct: A color-depth video database for human daily activity recognition"}, "4a5a1afc6b59b7f8217f496000e8d5a82982f187": {"paper_id": "4a5a1afc6b59b7f8217f496000e8d5a82982f187", "abstract": "In this paper, we present a systematic framework for recognizing realistic actions from videos \u201cin the wild\u201d. Such unconstrained videos are abundant in personal collections as well as on the Web. Recognizing action from such videos has not been addressed extensively, primarily due to the tremendous variations that result from camera motion, background clutter, changes in object appearance, and scale, etc. The main challenge is how to extract reliable and informative features from the unconstrained videos. We extract both motion and static features from the videos. Since the raw features of both types are dense yet noisy, we propose strategies to prune these features. We use motion statistics to acquire stable motion features and clean static features. Furthermore, PageRank is used to mine the most informative static features. In order to further construct compact yet discriminative visual vocabularies, a divisive information-theoretic algorithm is employed to group semantically related features. Finally, AdaBoost is chosen to integrate all the heterogeneous yet complementary features for recognition. We have tested the framework on the KTH dataset and our own dataset consisting of 11 categories of actions collected from YouTube and personal videos, and have obtained impressive results for action recognition and action localization.", "title": "Recognizing realistic actions from videos \u201cin the wild\u201d"}, "79465f3bac4fb9f8cc66dcbe676022ddcd9c05c6": {"paper_id": "79465f3bac4fb9f8cc66dcbe676022ddcd9c05c6", "abstract": "This paper presents a method to recognize human actions from sequences of depth maps. Specifically, we employ an action graph to model explicitly the dynamics of the actions and a bag of 3D points to characterize a set of salient postures that correspond to the nodes in the action graph. In addition, we propose a simple, but effective projection based sampling scheme to sample the bag of 3D points from the depth maps. Experimental results have shown that over 90% recognition accuracy were achieved by sampling only about 1% 3D points from the depth maps. Compared to the 2D silhouette based recognition, the recognition errors were halved. In addition, we demonstrate the potential of the bag of points posture model to deal with occlusions through simulation.", "title": "Action recognition based on a bag of 3D points"}, "04c5268d7a4e3819344825e72167332240a69717": {"paper_id": "04c5268d7a4e3819344825e72167332240a69717", "abstract": "In this paper we introduce a template-based method for recognizing human actions called action MACH. Our approach is based on a maximum average correlation height (MACH) filter. A common limitation of template-based methods is their inability to generate a single template using a collection of examples. MACH is capable of capturing intra-class variability by synthesizing a single Action MACH filter for a given action class. We generalize the traditional MACH filter to video (3D spatiotemporal volume), and vector valued data. By analyzing the response of the filter in the frequency domain, we avoid the high computational cost commonly incurred in template-based approaches. Vector valued data is analyzed using the Clifford Fourier transform, a generalization of the Fourier transform intended for both scalar and vector-valued data. Finally, we perform an extensive set of experiments and compare our method with some of the most recent approaches in the field by using publicly available datasets, and two new annotated human action datasets which include actions performed in classic feature films and sports broadcast television.", "title": "Action MACH a spatio-temporal Maximum Average Correlation Height filter for action recognition"}, "0f16f6f478b5c788dce466eb50e36c612273c36e": {"paper_id": "0f16f6f478b5c788dce466eb50e36c612273c36e", "abstract": "LIBSVM is a library for Support Vector Machines (SVMs). We have been actively developing this package since the year 2000. The goal is to help users to easily apply SVM to their applications. LIBSVM has gained wide popularity in machine learning and many other areas. In this article, we present all implementation details of LIBSVM. Issues such as solving SVM optimization problems theoretical convergence multiclass classification probability estimates and parameter selection are discussed in detail.", "title": "LIBSVM: A library for support vector machines"}, "ac279613bb323633de6bb78755f5150f893226c0": {"paper_id": "ac279613bb323633de6bb78755f5150f893226c0", "abstract": "Today, the number of available videos on the Internet is significantly increased. Content-based video retrieval is used for finding the users\u2019 desired items among these big video data. Memorizing details of the videos and intricate relations between included objects in videos can be considered as the major challenges of this big data topic. A large portion of video data relates to the humans. Thus, human action retrieval has been introduced as a new big data topic that seeks to find video objects based on the included human action. Human action retrieval has been applicated in different domains such as video search, intelligent human\u2013computer interaction, robotics, video surveillance and human behavior analysis. There are some challenges such as variations in rotation, scale, style and above-mentioned challenges for the big video data that can impress the retrieval accuracy. In this paper, a survey on human action retrieval studies is presented that the methodologies have been analyzed from action representation and retrieving perspectives. Moreover, limitations and common datasets of human action retrieval are introduced before describing the state-of-the-arts\u2019 methodologies.", "title": "A review on human action analysis in videos for retrieval applications"}, "1b07253ea1b0b0b1279769b906234ff615f6cd8c": {"paper_id": "1b07253ea1b0b0b1279769b906234ff615f6cd8c", "abstract": "This paper presents a novel approach for detecting affine invariant interest points. Our method can deal with significant affine transformations including large scale changes. Such transformations introduce significant changes in the point location as well as in the scale and the shape of the neighbourhood of an interest point. Our approach allows to solve for these problems simultaneously. It is based on three key ideas : 1) The second moment matrix computed in a point can be used to normalize a region in an affine invariant way (skew and stretch). 2) The scale of the local structure is indicated by local extrema of normalized derivatives over scale. 3) An affine-adapted Harris detector determines the location of interest points. A multi-scale version of this detector is used for initialization. An iterative algorithm then modifies location, scale and neighbourhood of each point and converges to affine invariant points. For matching and recognition, the image is characterized by a set of affine invariant points ; the affine transformation associated with each point allows the computation of an affine invariant descriptor which is also invariant to affine illumination changes. A quantitative comparison of our detector with existing ones shows a significant improvement in the presence of large affine deformations. Experimental results for wide baseline matching show an excellent performance in the presence of large perspective transformations including significant scale changes. Results for recognition are very good for a database with more than 5000 images.", "title": "An Affine Invariant Interest Point Detector"}, "75d94afc1a9fe13e8e7ba05ce22fa3388511ac3e": {"paper_id": "75d94afc1a9fe13e8e7ba05ce22fa3388511ac3e", "abstract": "This paper identifies the possibility of using electronic compasses and accelerometers in mobile phones, as a simple and scalable method of localization without war-driving. The idea is not fundamentally different from ship or air navigation systems, known for centuries. Nonetheless, directly applying the idea to human-scale environments is non-trivial. Noisy phone sensors and complicated human movements present practical research challenges. We cope with these challenges by recording a person's walking patterns, and matching it against possible path signatures generated from a local electronic map. Electronic maps enable greater coverage, while eliminating the reliance on WiFi infrastructure and expensive war-driving. Measurements on Nokia phones and evaluation with real users confirm the anticipated benefits. Results show a location accuracy of less than 11m in regions where today's localization services are unsatisfactory or unavailable.", "title": "Towards Mobile Phone Localization without War-Driving"}, "0d5676d90f20215d08dfe7e71fb55303f23604f7": {"paper_id": "0d5676d90f20215d08dfe7e71fb55303f23604f7", "abstract": "This paper presents the design, implementation, and evaluation of Cricket, a location-support system for in-building, mobile, location-dependent applications. It allows applications running on mobile and static nodes to learn their physical location by using listeners that hear and analyze information from beacons spread throughout the building. Cricket is the result of several design goals, including user privacy, decentralized administration, network heterogeneity, and low cost. Rather than explicitly tracking user location, Cricket helps devices learn where they are and lets them decide whom to advertise this information to; it does not rely on any centralized management or control and there is no explicit coordination between beacons; it provides information to devices regardless of their type of network connectivity; and each Cricket device is made from off-the-shelf components and costs less than U.S. $10. We describe the randomized algorithm used by beacons to transmit information, the use of concurrent radio and ultrasonic signals to infer distance, the listener inference algorithms to overcome multipath and interference, and practical beacon configuration and positioning techniques that improve accuracy. Our experience with Cricket shows that several location-dependent applications such as in-building active maps and device control can be developed with little effort or manual configuration.", "title": "The Cricket location-support system"}, "15f705d41cfb39860c33d15435defaa4c6469d29": {"paper_id": "15f705d41cfb39860c33d15435defaa4c6469d29", "abstract": "Instrumenting the physical world through large networks of wireless sensor nodes, particularly for applications like marine biology, requires that these nodes be very small, light, un-tethered and unobtrusive, imposing substantial restrictions on the amount of additional hardware that can be placed at each node. Practical considerations such as the small size, form factor, cost and power constraints of nodes preclude the use of GPS(Global Positioning System) for all nodes in these networks. The problem of localization, i.e., determining where a given node is physically located in a network is a challenging one, and yet extremely crucial for many applications of very large device networks. It needs to be solved in the absence of GPS on all the nodes in outdoor environments. In this paper, we propose a simple connectivity-metric based method for localization in outdoor environments that makes use of the inherent radiofrequency(RF) communications capabilities of these devices. A fixed number of reference points in the network transmit periodic beacon signals. Nodes use a simple connectivity metric to infer proximity to a given subset of these reference points and then localize themselves to the centroid of the latter. The accuracy of localization is then dependent on the separation distance between two adjacent reference points and the transmission range of these reference points. Initial experimental results show that the accuracy for 90% of our data points is within one-third of the separation distance. Keywords\u2014localization, radio, wireless, GPS-less, connectivity, sensor networks.", "title": "GPS-less low-cost outdoor localization for very small devices"}, "6cf8ec34a008031b018c8a3a4640a87f476d0925": {"paper_id": "6cf8ec34a008031b018c8a3a4640a87f476d0925", "abstract": "We introduce a new interactive system: a game that is fun and can be used to create valuable output. When people play the game they help determine the contents of images by providing meaningful labels for them. If the game is played as much as popular online games, we estimate that most images on the Web can be labeled in a few months. Having proper labels associated with each image on the Web would allow for more accurate image search, improve the accessibility of sites (by providing descriptions of images to visually impaired individuals), and help users block inappropriate images. Our system makes a significant contribution because of its valuable output and because of the way it addresses the image-labeling problem. Rather than using computer vision techniques, which don't work well enough, we encourage people to do the work by taking advantage of their desire to be entertained.", "title": "Labeling images with a computer game"}, "0ff1dbd5a4a1c8c92ffc31f4e248de44ad98f7b4": {"paper_id": "0ff1dbd5a4a1c8c92ffc31f4e248de44ad98f7b4", "abstract": "We describe a model of object recognition as machine translation. In this model, recognition is a process of annotating image regions with words. Firstly, images are segmented into regions, which are classi ed into region types using a variety of features. A mapping between region types and keywords supplied with the images, is then learned, using a method based around EM. This process is analogous with learning a lexicon from an aligned bitext. For the implementation we describe, these words are nouns taken from a large vocabulary. On a large test set, the method can predict numerous words with high accuracy. Simple methods identify words that cannot be predicted well. We show how to cluster words that individually are di\u00c6cult to predict into clusters that can be predicted well | for example, we cannot predict the distinction between train and locomotive using the current set of features, but we can predict the underlying concept. The method is trained on a substantial collection of images. Extensive experimental results illustrate the strengths and weaknesses of the approach.", "title": "Object Recognition as Machine Translation: Learning a Lexicon for a Fixed Image Vocabulary"}, "16ccb8d307d3f33ebb395b32db23279b409f1228": {"paper_id": "16ccb8d307d3f33ebb395b32db23279b409f1228", "abstract": "The proliferation of mobile computing devices and local-area wireless networks has fostered a growing interest in location-aware systems and services. In this paper we present RADAR, a radio-frequency (RF) based system for locating and tracking users inside buildings. RADAR operates by recording and processing signal strength information at multiple base stations positioned to provide overlapping coverage in the area of interest. It combines empirical measurements with signal propagation modeling to determine user location and thereby enable locationaware services and applications. We present experimental results that demonstrate the ability of RADAR to estimate user location with a high degree of accuracy.", "title": "RADAR: An In-Building RF-Based User Location and Tracking System"}, "62edb6639dc857ad0f33e5d8ef97af89be7a3bc7": {"paper_id": "62edb6639dc857ad0f33e5d8ef97af89be7a3bc7", "abstract": "A novel system for the location of people in an office environment is described. Members of staff wear badges that transmit signals providing information about their location to a centralized location service, through a network of sensors. The paper also examines alternative location techniques, system design issues and applications, particularly relating to telephone call routing. Location systems raise concerns about the privacy of an individual and these issues are also addressed.", "title": "The Active Badge Location System"}, "774db16a3f25a73ceda9e6ab4d5a8b8f3c40605d": {"paper_id": "774db16a3f25a73ceda9e6ab4d5a8b8f3c40605d", "abstract": "In this paper, we propose a new method for indexing large amounts of point and spatial data in highdimensional space. An analysis shows that index structures such as the R*-tree are not adequate for indexing high-dimensional data sets. The major problem of R-tree-based index structures is the overlap of the bounding boxes in the directory, which increases with growing dimension. To avoid this problem, we introduce a new organization of the directory which uses a split algorithm minimizing overlap and additionally utilizes the concept of supernodes. The basic idea of overlap-minimizing split and supernodes is to keep the directory as hierarchical as possible, and at the same time to avoid splits in the directory that would result in high overlap. Our experiments show that for high-dimensional data, the X-tree outperforms the well-known R*-tree and the TV-tree by up to two orders of magnitude.", "title": "The X-tree : An Index Structure for High-Dimensional Data"}, "3ee16ee90baf28b83425f120a80c15451cb3ac97": {"paper_id": "3ee16ee90baf28b83425f120a80c15451cb3ac97", "abstract": "Principles of muscle coordination in gait have been based largely on analyses of body motion, ground reaction force and EMG measurements. However, data from dynamical simulations provide a cause-effect framework for analyzing these measurements; for example, Part I (Gait Posture, in press) of this two-part review described how force generation in a muscle affects the acceleration and energy flow among the segments. This Part II reviews the mechanical and coordination concepts arising from analyses of simulations of walking. Simple models have elucidated the basic multisegmented ballistic and passive mechanics of walking. Dynamical models driven by net joint moments have provided clues about coordination in healthy and pathological gait. Simulations driven by muscle excitations have highlighted the partial stability afforded by muscles with their viscoelastic-like properties and the predictability of walking performance when minimization of metabolic energy per unit distance is assumed. When combined with neural control models for exciting motoneuronal pools, simulations have shown how the integrative properties of the neuro-musculo-skeletal systems maintain a stable gait. Other analyses of walking simulations have revealed how individual muscles contribute to trunk support and progression. Finally, we discuss how biomechanical models and simulations may enhance our understanding of the mechanics and muscle function of walking in individuals with gait impairments.", "title": "Biomechanics and muscle coordination of human walking: part II: lessons from dynamical simulations and clinical implications."}, "0c04306eed8caeb17e873e01c76b99823bc386c3": {"paper_id": "0c04306eed8caeb17e873e01c76b99823bc386c3", "abstract": "In this paper, we introduce two novel metric learning algorithms, \u03c7-LMNN and GB-LMNN, which are explicitly designed to be non-linear and easy-to-use. The two approaches achieve this goal in fundamentally different ways: \u03c7-LMNN inherits the computational benefits of a linear mapping from linear metric learning, but uses a non-linear \u03c7-distance to explicitly capture similarities within histogram data sets; GB-LMNN applies gradient-boosting to learn non-linear mappings directly in function space and takes advantage of this approach\u2019s robustness, speed, parallelizability and insensitivity towards the single additional hyperparameter. On various benchmark data sets, we demonstrate these methods not only match the current state-of-the-art in terms of kNN classification error, but in the case of \u03c7-LMNN, obtain best results in 19 out of 20 learning settings.", "title": "Non-linear Metric Learning"}, "27d9d71afb06ea5d4295fbdcc7cd67e491d50cc4": {"paper_id": "27d9d71afb06ea5d4295fbdcc7cd67e491d50cc4", "abstract": "The case of n unity-variance random variables x1, XZ,. * *, x, governed by the joint probability density w(xl, xz, * * * x,) is considered, where the density depends on the (normalized) cross-covariances pii = E[(xi jzi)(xi li)]. It is shown that the condition holds for an \u201carbitrary\u201d function f(xl, x2, * * * , x,) of n variables if and only if the underlying density w(xl, XZ, * * * , x,) is the usual n-dimensional Gaussian density for correlated random variables. This result establishes a generalized form of Price\u2019s theorem in which: 1) the relevant condition (*) subsumes Price\u2019s original condition; 2) the proof is accomplished without appeal to Laplace integral expansions; and 3) conditions referring to derivatives with respect to diagonal terms pii are avoided, so that the unity variance assumption can be retained. Manuscript received February 10, 1966; revised May 2, 1966. The author is with the Ordnance Research Laboratory, Pennsylvania State University, State College, Pa. RICE\u2019S THEOREM and its various extensions ([l]-[4]) have had great utility in the determination of output correlations between zero-memory nonlinearities subjected to jointly Gaussian inputs. In its original form, the theorem considered n jointly normal random variables, x1, x2, . . . x,, with respective means 21, LE2, . . . ) Z, and nth-order joint probability density, P(z 1, .x2, . 1 . , :r,,) = (27p ply,, y . exp { -a F F ;;;, ~ (2,. 2,)(:r, 5,) I , (1) where IM,l is the determinant\u2019 of J1,, = [p,,], Pr-. = E[(sr 5$.)(x, ,2J] = xvx, &IL:, is the correlation coefficient of x, and x,, and Ail,, is the cofactor of p.? in ilf,. From [l], the theorem statement is as follows: \u201cLet there be n zero-memory nonlinear devices specified by the input-output relationship f<(x), i = 1, 2, . 1 . , n. Let each xi be the single input to a corresponding fi(x) Authorized licensed use limited to: West Virginia University. Downloaded on February 26, 2009 at 13:41 from IEEE Xplore. Restrictions apply.", "title": "Nearest neighbor pattern classification"}, "24c287d97982216c8f35c8d326dc2ec2d2475f3e": {"paper_id": "24c287d97982216c8f35c8d326dc2ec2d2475f3e", "abstract": "In this paper we propose a novel method for learning a Mahalan obis distance measure to be used in the KNN classification algorit hm. The algorithm directly maximizes a stochastic variant of the le ave-one-out KNN score on the training set. It can also learn a low-dimensi o al linear embedding of labeled data that can be used for data visual ization and fast classification. Unlike other methods, our classific at on model is non-parametric, making no assumptions about the shape of the class distributions or the boundaries between them. The performa nce of the method is demonstrated on several data sets, both for metric lea ning and linear dimensionality reduction.", "title": "Neighbourhood Components Analysis"}, "2704a9af1b368e2b68b0fe022b2fd48b8c7c25cc": {"paper_id": "2704a9af1b368e2b68b0fe022b2fd48b8c7c25cc", "abstract": "Many algorithms rely critically on being given a good metric over their inputs. For instance, data can often be clustered in many \u201cplausible\u201d ways, and if a clustering algorithm such as K-means initially fails to find one that is meaningful to a user, the only recourse may be for the user to manually tweak the metric until sufficiently good clusters are found. For these and other applications requiring good metrics, it is desirable that we provide a more systematic way for users to indicate what they consider \u201csimilar.\u201d For instance, we may ask them to provide examples. In this paper, we present an algorithm that, given examples of similar (and, if desired, dissimilar) pairs of points in , learns a distance metric over that respects these relationships. Our method is based on posing metric learning as a convex optimization problem, which allows us to give efficient, local-optima-free algorithms. We also demonstrate empirically that the learned metrics can be used to significantly improve clustering performance.", "title": "Distance Metric Learning with Application to Clustering with Side-Information"}, "0bacca0993a3f51649a6bb8dbb093fc8d8481ad4": {"paper_id": "0bacca0993a3f51649a6bb8dbb093fc8d8481ad4", "abstract": "Clustering is traditionally viewed as an unsupervised method for data analysis. However, in some cases information about the problem domain is available in addition to the data instances themselves. In this paper, we demonstrate how the popular k-means clustering algorithm can be profitably modified to make use of this information. In experiments with artificial constraints on six data sets, we observe improvements in clustering accuracy. We also apply this method to the real-world problem of automatically detecting road lanes from GPS data and observe dramatic increases in performance.", "title": "Constrained K-means Clustering with Background Knowledge"}, "15e0daa3d2e1438159e96f6c6fd6c4dd3756052c": {"paper_id": "15e0daa3d2e1438159e96f6c6fd6c4dd3756052c", "abstract": "We define the relevant information in a signal x \u2208 X as being the information that this signal provides about another signal y \u2208 Y . Examples include the information that face images provide about the names of the people portrayed, or the information that speech sounds provide about the words spoken. Understanding the signal x requires more than just predicting y, it also requires specifying which features of X play a role in the prediction. We formalize this problem as that of finding a short code for X that preserves the maximum information about Y . That is, we squeeze the information that X provides about Y through a \u2018bottleneck\u2019 formed by a limited set of codewords X\u0303. This constrained optimization problem can be seen as a generalization of rate distortion theory in which the distortion measure d(x, x\u0303) emerges from the joint statistics of X and Y . This approach yields an exact set of self consistent equations for the coding rules X \u2192 X\u0303 and X\u0303 \u2192 Y . Solutions to these equations can be found by a convergent re\u2013estimation method that generalizes the Blahut\u2013Arimoto algorithm. Our variational principle provides a surprisingly rich framework for discussing a variety of problems in signal processing and learning, as will be described in detail elsewhere.", "title": "The information bottleneck method"}, "3106e1d0b2f60b7e142bf103855a8f5aebd138b5": {"paper_id": "3106e1d0b2f60b7e142bf103855a8f5aebd138b5", "abstract": "This paper surveys locally weighted learning, a form of lazy learning and memory-based learning, and focuses on locally weighted linear regression. The survey discusses distance functions, smoothing parameters, weighting functions, local model structures, regularization of the estimates and bias, assessing predictions, handling noisy data and outliers, improving the quality of predictions by tuning fit parameters, interference between old and new data, implementing locally weighted learning efficiently, and applications of locally weighted learning. A companion paper surveys how locally weighted learning can be used in robot learning and control.", "title": "Locally Weighted Learning"}, "09370d132a1e238a778f5e39a7a096994dc25ec1": {"paper_id": "09370d132a1e238a778f5e39a7a096994dc25ec1", "abstract": "Nearest neighbor classification expects the class conditional probabilities to be locally constant, and suffers from bias in high dimensions We propose a locally adaptive form of nearest neighbor classification to try to finesse this curse of dimensionality. We use a local linear discriminant analysis to estimate an effective metric for computing neighborhoods. We determine the local decision boundaries from centroid information, and then shrink neighborhoods in directions orthogonal to these local decision boundaries, and elongate them parallel to the boundaries. Thereafter, any neighborhood-based classifier can be employed, using the modified neighborhoods. The posterior probabilities tend to be more homogeneous in the modified neighborhoods. We also propose a method for global dimension reduction, that combines local dimension information. In a number of examples, the methods demonstrate the potential for substantial improvements over nearest neighbour classification. Introduction We consider a discrimination problem with d classes and N training observations. The training observations consist of predictor measurements x = (zl,z2,...zp) on p predictors and the known class memberships. Our goal is to predict the class membership of an observation with predictor vector x0 Nearest neighbor classification is a simple and appealing approach to this problem. We find the set of K nearest neighbors in the training set to x0 and then classify x0 as the most frequent class among the K neighbors. Nearest neighbors is an extremely flexible classification scheme, and does not involve any pre-processing (fitting) of the training data. This can offer both space and speed advantages in very large problems: see Cover (1968), Duda & Hart (1973), McLachlan (1992) for background material on nearest neighborhood classification. Cover & Hart (1967) show that the one nearest neighbour rule has asymptotic error rate at most twice the Bayes rate. However in finite samples the curse of dimensionality can severely hurt the nearest neighbor rule. The relative radius of the nearest-neighbor sphere grows like r 1/p where p is the dimension and r the radius for p = 1, resulting in severe bias at the target point x. Figure 1 illustrates the situation for a simple example. Figure 1: The vertical strip denotes the NN region using only the X coordinate to find the nearest neighbor for the target point (solid dot). The sphere shows the NN region using both coordinates, and we see in this case it has extended into the class 1 region (and found the wrong class in this instance). Our illustration here is based on a 1-NN rule, but the same phenomenon ccurs for k-NN rules as well. Nearest neighbor techniques are based on the assumption that locally the class posterior probabilities are constant. While that is clearly true in the vertical strip using only coordinate X, using X and Y this is no longer true. The techniques outlined in the abstract are designed to overcome these problems. Figure 2 shows an example. There are two classes in two dimensions, one of which almost completely surrounds the other. The left panel shows a nearest neighborhood of size 25 at the target point (shown as origin), which is chosen to near the class boundary. The right panel shows the same size neighborhood using our discriminant adap142 KDD--95 From: KDD-95 Proceedings. Copyright \u00a9 1995, AAAI (www.aaai.org). All rights reserved.", "title": "Discriminant Adaptive Nearest Neighbor Classification"}, "b555b2d13b51bfcac8caf1fd7c407c110594858d": {"paper_id": "b555b2d13b51bfcac8caf1fd7c407c110594858d", "abstract": "We describe two new color indexing techniques. The rst one is a more robust version of the commonly used color histogram indexing. In the index we store the cumulative color histograms. The L 1-, L 2-, or L 1-distance between two cumulative color histograms can be used to deene a similarity measure of these two color distributions. We show that while this method produces only slightly better results than color histogram methods, it is more robust with respect to the quantization parameter of the histograms. The second technique is an example of a new approach to color indexing. Instead of storing the complete color distributions, the index contains only their dominant features. We implement this approach by storing the rst three moments of each color channel of an image in the index, i.e., for a HSV image we store only 9 oating point numbers per image. The similarity function which is used for the retrieval is a weighted sum of the absolute diierences between corresponding moments. Our tests clearly demonstrate that a retrieval based on this technique produces better results and runs faster than the histogram-based methods.", "title": "Similarity of Color Images"}, "06d05d42f7e6560e4474af105aca65bb704d0e90": {"paper_id": "06d05d42f7e6560e4474af105aca65bb704d0e90", "abstract": "Software-defined networking has emerged as a promising solution for supporting dynamic network functions and intelligent applications through decoupling control plane from forwarding plane. OpenFlow is the first standardized open management interface of SDN architecture. But it is unrealistic to simply swaping out conventional networks for new infrastructure. How to integrate OpenFlow with existing networks is still a serious challenge. We propose a tunnel splicing mechanism for heterogeneous network with MPLS and OpenFlow routers. Two key mechanisms were suggested: first, abstract the underlying network devices into uniformed nodes in order to shield the details of various equipments, second, strip the manipulation of flow table and lable switch table from controller and fulfill it in an independent module. This new paradigm has been developed on Linux system and tests have been carried out in experiment networks. The emulation results proved its feasibility and efficiency.", "title": "Splicing MPLS and OpenFlow Tunnels Based on SDN Paradigm"}, "1eb449b94ce9da33d60b231d2f9ec70a390e7b35": {"paper_id": "1eb449b94ce9da33d60b231d2f9ec70a390e7b35", "abstract": "An emerging Internet application, IPTV, has the potential to flood Internet access and backbone ISPs with massive amounts of new traffic. Although many architectures are possible for IPTV video distribution, several mesh-pull P2P architectures have been successfully deployed on the Internet. In order to gain insights into mesh-pull P2P IPTV systems and the traffic loads they place on ISPs, we have undertaken an in-depth measurement study of one of the most popular IPTV systems, namely, PPLive. We have developed a dedicated PPLive crawler, which enables us to study the global characteristics of the mesh-pull PPLive system. We have also collected extensive packet traces for various different measurement scenarios, including both campus access networks and residential access networks. The measurement results obtained through these platforms bring important insights into P2P IPTV systems. Specifically, our results show the following. 1) P2P IPTV users have the similar viewing behaviors as regular TV users. 2) During its session, a peer exchanges video data dynamically with a large number of peers. 3) A small set of super peers act as video proxy and contribute significantly to video data uploading. 4) Users in the measured P2P IPTV system still suffer from long start-up delays and playback lags, ranging from several seconds to a couple of minutes. Insights obtained in this study will be valuable for the development and deployment of future P2P IPTV systems.", "title": "A Measurement Study of a Large-Scale P2P IPTV System"}, "8296848850c729e70e4dac01198db7b8ae410f99": {"paper_id": "8296848850c729e70e4dac01198db7b8ae410f99", "abstract": "The diffusion of new digital technologies renders digital transformation relevant for nearly every industry. Therefore, the maturity of firms in mastering this fundamental organizational change is increasingly discussed in practice-oriented literature. These studies, however, suffer from some shortcomings. Most importantly, digital maturity is typically described along a linear scale, thus assuming that all firms do and need to proceed through the same path. We challenge this assumption and derive a more differentiated classification scheme based on a comprehensive literature review as well as an exploratory analysis of a survey on digital transformation amongst 327 managers. Based on these findings we propose two scales for describing a firm\u2019s digital maturity: first, the impact that digital transformation has on a specific firm; second, the readiness of the firm to master the upcoming changes. We demonstrate the usefulness of this two scale measure by empirically deriving five digital maturity clusters as well as further empirical evidence. Our framework illuminates the monolithic block of digital maturity by allowing for a more differentiated firm-specific assessment \u2013 thus, it may serve as a first foundation for future research on digital maturity.", "title": "Digital Maturity in Traditional industries - an Exploratory Analysis"}, "591c21d32fb98d52e887d50d0a8e204a284afdfa": {"paper_id": "591c21d32fb98d52e887d50d0a8e204a284afdfa", "abstract": "In the context of an ongoing digital transformation, companies across all industries are confronted with the challenge to exploit IT-induced business opportunities and to simultaneously avert IT-induced business risks. Due to this development, questions about a company\u2019s overall status with regard to its digital transformation become more and more relevant. In recent years, an unclear number of maturity models was established in order to address these kind of questions by assessing a company\u2019s digital maturity. Purpose of this Report is to show the large range of digital maturity models and to evaluate overall potential for approximating a company\u2019s digital transformation status.", "title": "How digital are we ? Maturity models for the assessment of a company \u2019 s status in the digital transformation"}, "beeab41a637b2161552b3f112fb56df11d96c507": {"paper_id": "beeab41a637b2161552b3f112fb56df11d96c507", "abstract": "Integrating and exploiting new digital technologies is one of the biggest challenges that companies currently face. No sector or organization is immune to the effects of digital transformation. The market-changing potential of digital technologies is often wider than products, business processes, sales channels or supply chains\u2014entire business models are being reshaped and frequently overturned.2 As a result, digital transformation has become a high priority on leadership agendas, with nearly 90% of business leaders in the U.S. and U.K. expecting IT and digital technologies to make an increasing strategic contribution to their overall business in the coming decade.3 The question is no longer when companies need to make digital transformation a strategic priority\u2014this tipping point has passed\u2014but how to embrace it and use it as a competitive advantage. Faced with the digital transformation challenge and the need to remain competitive in their industries, business leaders must formulate and execute strategies that embrace the implications of digital transformation and drive better operational performance. Unfortunately, there are many recent examples of organizations that have been unable to keep pace with the new digital reality. Prominent examples include the bankruptcy of the movie-rental company Blockbuster and the sale of the Washington Post to Jeff Bezos, founder of Amazon\u2014largely resulting from those firms\u2019", "title": "Options for Formulating a Digital Transformation Strategy"}, "42513a13a6954ce09ac1299fc478d73f79743d95": {"paper_id": "42513a13a6954ce09ac1299fc478d73f79743d95", "abstract": "An increasing number of firms are responding to new opportunities and risks originating from digital technologies by introducing company-wide digital transformation strategies as a means to systematically address their digital transformation. Yet, what processes and strategizing activities affect the formation of digital transformation strategies in organizations are not well understood. We adopt a phenomenon-based approach and investigate the formation of digital transformation strategies in organizations from a process perspective. Drawing on an activity-based process model that links Mintzberg\u2019s strategy typology with the concept of IS strategizing, we conduct a multiple-case study at three European car manufacturers. Our results indicate that digital transformation strategies are predominantly shaped by a diversity of emergent strategizing activities of separate organizational subcommunities through a bottom-up process and prior to the initiation of a holistic digital transformation strategy by top management. As a result, top management\u2019s deliberate strategies seek to accomplish the subsequent alignment of preexisting emergent strategy contents with their intentions and to simultaneously increase the share of deliberate contents. Besides providing practical implications for the formulation and implementation of a digital transformation strategy, we contribute to the literature on digital transformation and IS strategizing.", "title": "Understanding Digital Transformation Strategy formation: Insights from Europe's Automotive Industry"}, "00e62a7090ab72c5d1dfaebaa0c705708026c0e0": {"paper_id": "00e62a7090ab72c5d1dfaebaa0c705708026c0e0", "abstract": "Over the last two decades, IT strategy has been framed as a functional-level strategy that must be aligned with the firm\u2019s business strategy (Henderson and Venkatraman 1993). This thinking is also reflected in research studies in business process redesign, intraand interorganizational systems, business value of IT, and IT outsourcing. However, during the last decade, the digital infrastructure of business and society has undergone radical shifts and unleashed a new digital era. Today\u2019s digital technologies are fundamentally reshaping traditional business processes as modular, distributed, cross-functional, and global processes that enable work to be carried out across boundaries of time, distance, and function (e.g., Sambamurthy et al. 2003; Straub and Watson 2001; Wheeler 2002). They are also enabling different forms of dynamic capabilities suitable for turbulent environments (Pavlou and El Sawy 2006). Digital technologies are also transforming the structure of social relationships in both the consumer and the enterprise space. Furthermore, products and services increasingly have embedded digital technologies, and it is becoming more difficult to disentangle business processes from their underlying IT infrastructures (e.g., El Sawy 2003; Orlikowski 2009). Digital platforms are enabling cross-boundary industry disruptions, and thus inducing new forms of business strategies (e.g., Burgelman and Grove 2007). Furthermore, theoretical structures for strategy making in nonlinear dynamic environments are also emerging (e.g., Davis et al. 2009; Meyer et al. 2005; Pavlou and El Sawy 2010). Consequently, as exponential advancements in the price/performance capability of computing, storage, bandwidth, and software applications drives the next generation of digital technologies to be delivered through cloud computing, it is time to rethink the role of IT strategy, from that of a functional-level strategy aligned but essentially subordinate to business strategy, to a fusion between IT strategy and business strategy into an overarching digital business strategy.", "title": "Digital business strategy: toward a next generation of insights"}, "fd3f8a3e8828631afe02026ff72a2e2e28904d27": {"paper_id": "fd3f8a3e8828631afe02026ff72a2e2e28904d27", "abstract": null, "title": "Shaping Agility through Digital Options: Reconceptualizing the Role of Information Technology in Contemporary Firms"}, "e8402b65103442e2517982e5e3eb330f72886731": {"paper_id": "e8402b65103442e2517982e5e3eb330f72886731", "abstract": null, "title": "Strategic Alignment: Leveraging Information Technology for Transforming Organizations"}, "bb5588e5726e67c6368cf173d54d431a26632cc1": {"paper_id": "bb5588e5726e67c6368cf173d54d431a26632cc1", "abstract": "We study approximation algorithms for placing replicated data in arbitrary networks. Consider a network of nodes with individual storage capacities and a metric communication cost function, in which each node periodically issues a request for an object drawn from a collection of uniform-length objects. We consider the problem of placing copies of the objects among the nodes such that the average access cost is minimized. Our main result is a polynomial-time constant-factor approximation algorithm for this placement problem. Our algorithm is based on a careful rounding of a linear programming relaxation of the problem. We also show that the data placement problem is MAXSNP-hard.\nWe extend our approximation result to a generalization of the data placement problem that models additional costs such as the cost of realizing the placement. We also show that when object lengths are non-uniform, a constant-factor approximation is achievable if the capacity at each node in the approximate solution is allowed to exceed that in the optimal solution by the length of the largest object.", "title": "Approximation algorithms for data placement in arbitrary networks"}, "a4197b6d4db56c9cd3e71393295a1255bf99458b": {"paper_id": "a4197b6d4db56c9cd3e71393295a1255bf99458b", "abstract": "A fundamental facility location problem is to choose the location of facilities, such as industrial plants and warehouses, to minimize the cost of satisfying the demand for some commodity. There are associated costs for locating the facilities, as well as transportation costs for distributing the commodities. Thii problem is commonly referred to as the uncapacitated facility location problem. Applications to bank account location and clustering, as well as many related pieces of work, are discussed by Cornuejols, Nemhauser, and Wolsey [2]. Recently, the first constant factor approximation algorithm for this problem was obtained by Shmoys, Tardos and Aardal [14]. We show that a simple greedy heuristic combined with the Shmoys, Tardos and Aardal algorithm [14], can be used to derive a better approximation guarantee. We discuss a few variants of the problem, demonstrating better approximation factors for restricted versions of the problem. We also show that the problem is Max SNP-hard. However, the inapproximability constants derived from the Max SNP hardness are very close to one. By relating this problem to Set Cover, we can prove a better lower bound on the best possible approximation ratio.", "title": "Greedy Strikes Back: Improved Facility Location Algorithms"}, "42d882522dfc2e35459d0e35f0b284708a48a79c": {"paper_id": "42d882522dfc2e35459d0e35f0b284708a48a79c", "abstract": "Approximationalgorithmsfor facilitylocationproblems", "title": "Approximation Algorithms for Facility Location Problems (Extended Abstract)"}, "0b84f07af44f964817675ad961def8a51406dd2e": {"paper_id": "0b84f07af44f964817675ad961def8a51406dd2e", "abstract": "This paper presents a novel large-scale dataset and comprehensive baselines for end-to-end pedestrian detection and person recognition in raw video frames. Our baselines address three issues: the performance of various combinations of detectors and recognizers, mechanisms for pedestrian detection to help improve overall re-identification (re-ID) accuracy and assessing the effectiveness of different detectors for re-ID. We make three distinct contributions. First, a new dataset, PRW, is introduced to evaluate Person Re-identification in the Wild, using videos acquired through six synchronized cameras. It contains 932 identities and 11,816 frames in which pedestrians are annotated with their bounding box positions and identities. Extensive benchmarking results are presented on this dataset. Second, we show that pedestrian detection aids re-ID through two simple yet effective improvements: a cascaded fine-tuning strategy that trains a detection model first and then the classification model, and a Confidence Weighted Similarity (CWS) metric that incorporates detection scores into similarity measurement. Third, we derive insights in evaluating detector performance for the particular scenario of accurate person re-ID.", "title": "Person Re-identification in the Wild"}, "207e0ac5301a3c79af862951b70632ed650f74f7": {"paper_id": "207e0ac5301a3c79af862951b70632ed650f74f7", "abstract": "Most existing person re-identification (re-id) methods focus on learning the optimal distance metrics across camera views. Typically a person's appearance is represented using features of thousands of dimensions, whilst only hundreds of training samples are available due to the difficulties in collecting matched training images. With the number of training samples much smaller than the feature dimension, the existing methods thus face the classic small sample size (SSS) problem and have to resort to dimensionality reduction techniques and/or matrix regularisation, which lead to loss of discriminative power. In this work, we propose to overcome the SSS problem in re-id distance metric learning by matching people in a discriminative null space of the training data. In this null space, images of the same person are collapsed into a single point thus minimising the within-class scatter to the extreme and maximising the relative between-class separation simultaneously. Importantly, it has a fixed dimension, a closed-form solution and is very efficient to compute. Extensive experiments carried out on five person re-identification benchmarks including VIPeR, PRID2011, CUHK01, CUHK03 and Market1501 show that such a simple approach beats the state-of-the-art alternatives, often by a big margin.", "title": "Learning a Discriminative Null Space for Person Re-identification"}, "7c73b95790d8939146da94ff8c76bf8494e7e428": {"paper_id": "7c73b95790d8939146da94ff8c76bf8494e7e428", "abstract": "Detecting samples from previously unknown classes is a crucial task in object recognition, especially when dealing with real-world applications where the closed-world assumption does not hold. We present how to apply a null space method for novelty detection, which maps all training samples of one class to a single point. Beside the possibility of modeling a single class, we are able to treat multiple known classes jointly and to detect novelties for a set of classes with a single model. In contrast to modeling the support of each known class individually, our approach makes use of a projection in a joint subspace where training samples of all known classes have zero intra-class variance. This subspace is called the null space of the training data. To decide about novelty of a test sample, our null space approach allows for solely relying on a distance measure instead of performing density estimation directly. Therefore, we derive a simple yet powerful method for multi-class novelty detection, an important problem not studied sufficiently so far. Our novelty detection approach is assessed in comprehensive multi-class experiments using the publicly available datasets Caltech-256 and Image Net. The analysis reveals that our null space approach is perfectly suited for multi-class novelty detection since it outperforms all other methods.", "title": "Kernel Null Space Methods for Novelty Detection"}, "12fa3c73a7764cb65bb76fed0601fc5d79893bcd": {"paper_id": "12fa3c73a7764cb65bb76fed0601fc5d79893bcd", "abstract": "In this paper, we present an appearance-based method for person re-identification. It consists in the extraction of features that model three complementary aspects of the human appearance: the overall chromatic content, the spatial arrangement of colors into stable regions, and the presence of recurrent local motifs with high entropy. All this information is derived from different body parts, and weighted opportunely by exploiting symmetry and asymmetry perceptual principles. In this way, robustness against very low resolution, occlusions and pose, viewpoint and illumination changes is achieved. The approach applies to situations where the number of candidates varies continuously, considering single images or bunch of frames for each individual. It has been tested on several public benchmark datasets (ViPER, iLIDS, ETHZ), gaining new state-of-the-art performances.", "title": "Person re-identification by symmetry-driven accumulation of local features"}, "46638b810bf69023bca41db664b49bc935bcba3c": {"paper_id": "46638b810bf69023bca41db664b49bc935bcba3c", "abstract": "Human eyes can recognize person identities based on some small salient regions. However, such valuable salient information is often hidden when computing similarities of images with existing approaches. Moreover, many existing approaches learn discriminative features and handle drastic viewpoint change in a supervised way and require labeling new training data for a different pair of camera views. In this paper, we propose a novel perspective for person re-identification based on unsupervised salience learning. Distinctive features are extracted without requiring identity labels in the training procedure. First, we apply adjacency constrained patch matching to build dense correspondence between image pairs, which shows effectiveness in handling misalignment caused by large viewpoint and pose variations. Second, we learn human salience in an unsupervised manner. To improve the performance of person re-identification, human salience is incorporated in patch matching to find reliable and discriminative matched patches. The effectiveness of our approach is validated on the widely used VIPeR dataset and ETHZ dataset.", "title": "Unsupervised Salience Learning for Person Re-identification"}, "36358eff7c34de64c0ce8aa42cf7c4da24bf8e93": {"paper_id": "36358eff7c34de64c0ce8aa42cf7c4da24bf8e93", "abstract": "Various hand-crafted features and metric learning methods prevail in the field of person re-identification. Compared to these methods, this paper proposes a more general way that can learn a similarity metric from image pixels directly. By using a \"siamese\" deep neural network, the proposed method can jointly learn the color feature, texture feature and metric in a unified framework. The network has a symmetry structure with two sub-networks which are connected by a cosine layer. Each sub network includes two convolutional layers and a full connected layer. To deal with the big variations of person images, binomial deviance is used to evaluate the cost between similarities and labels, which is proved to be robust to outliers. Experiments on VIPeR illustrate the superior performance of our method and a cross database experiment also shows its good generalization.", "title": "Deep Metric Learning for Person Re-identification"}, "6273b3491e94ea4dd1ce42b791d77bdc96ee73a8": {"paper_id": "6273b3491e94ea4dd1ce42b791d77bdc96ee73a8", "abstract": "Traditionally, appearance models for recognition, reacquisition and tracking problems have been evaluated independently using metrics applied to a complete system. It is shown that appearance models for these three problems can be evaluated using a cumulative matching curve on a standardized dataset, and that this one curve can be converted to a synthetic reacquisition or disambiguation rate for tracking. A challenging new dataset for viewpoint invariant pedestrian recognition (VIPeR) is provided as an example. This dataset contains 632 pedestrian image pairs from arbitrary viewpoints. Several baseline methods are tested on this dataset and the results are presented as a benchmark for future appearance models and matchin methods.", "title": "Evaluating Appearance Models for Recognition, Reacquisition, and Tracking"}, "472ba8dd4ec72b34e85e733bccebb115811fd726": {"paper_id": "472ba8dd4ec72b34e85e733bccebb115811fd726", "abstract": "Face veri cation is the task of deciding by analyzing face images, whether a person is who he/she claims to be. This is very challenging due to image variations in lighting, pose, facial expression, and age. The task boils down to computing the distance between two face vectors. As such, appropriate distance metrics are essential for face veri cation accuracy. In this paper we propose a new method, named the Cosine Similarity Metric Learning (CSML) for learning a distance metric for facial veri cation. The use of cosine similarity in our method leads to an e ective learning algorithm which can improve the generalization ability of any given metric. Our method is tested on the state-of-the-art dataset, the Labeled Faces in the Wild (LFW), and has achieved the highest accuracy in the literature. Face veri cation has been extensively researched for decades. The reason for its popularity is the non-intrusiveness and wide range of practical applications, such as access control, video surveillance, and telecommunication. The biggest challenge in face veri cation comes from the numerous variations of a face image, due to changes in lighting, pose, facial expression, and age. It is a very di cult problem, especially using images captured in totally uncontrolled environment, for instance, images from surveillance cameras, or from the Web. Over the years, many public face datasets have been created for researchers to advance state of the art and make their methods comparable. This practice has proved to be extremely useful. FERET [1] is the rst popular face dataset freely available to researchers. It was created in 1993 and since then research in face recognition has advanced considerably. Researchers have come very close to fully recognizing all the frontal images in FERET [2,3,4,5,6]. However, these methods are not robust to deal with non-frontal face images. Recently a new face dataset named the Labeled Faces in the Wild (LFW) [7] was created. LFW is a full protocol for evaluating face veri cation algorithms. Unlike FERET, LFW is designed for unconstrained face veri cation. Faces in LFW can vary in all possible ways due to pose, lighting, expression, age, scale, and misalignment (Figure 1). Methods for frontal images cannot cope with these variations and as such many researchers have turned to machine learning to 2 Hieu V. Nguyen and Li Bai Fig. 1. From FERET to LFW develop learning based face veri cation methods [8,9]. One of these approaches is to learn a transformation matrix from the data so that the Euclidean distance can perform better in the new subspace. Learning such a transformation matrix is equivalent to learning a Mahalanobis metric in the original space [10]. Xing et al. [11] used semide nite programming to learn a Mahalanobis distance metric for clustering. Their algorithm aims to minimize the sum of squared distances between similarly labeled inputs, while maintaining a lower bound on the sum of distances between di erently labeled inputs. Goldberger et al. [10] proposed Neighbourhood Component Analysis (NCA), a distance metric learning algorithm especially designed to improve kNN classi cation. The algorithm is to learn a Mahalanobis distance by minimizing the leave-one-out cross validation error of the kNN classi er on a training set. Because it uses softmax activation function to convert distance to probability, the gradient computation step is expensive. Weinberger et al. [12] proposed a method that learns a matrix designed to improve the performance of kNN classi cation. The objective function is composed of two terms. The rst term minimizes the distance between target neighbours. The second term is a hinge-loss that encourages target neighbours to be at least one distance unit closer than points from other classes. It requires information about the class of each sample. As a result, their method is not applicable for the restricted setting in LFW (see section 2.1). Recently, Davis et al. [13] have taken an information theoretic approach to learn a Mahalanobis metric under a wide range of possible constraints and prior knowledge on the Mahalanobis distance. Their method regularizes the learned matrix to make it as close as possible to a known prior matrix. The closeness is measured as a Kullback-Leibler divergence between two Gaussian distributions corresponding to the two matrices. In this paper, we propose a new method named Cosine Similarity Metric Learning (CSML). There are two main contributions. The rst contribution is Cosine Similarity Metric Learning for Face Veri cation 3 that we have shown cosine similarity to be an e ective alternative to Euclidean distance in metric learning problem. The second contribution is that CSML can improve the generalization ability of an existing metric signi cantly in most cases. Our method is di erent from all the above methods in terms of distance measures. All of the other methods use Euclidean distance to measure the dissimilarities between samples in the transformed space whilst our method uses cosine similarity which leads to a simple and e ective metric learning method. The rest of this paper is structured as follows. Section 2 presents CSML method in detail. Section 3 present how CSML can be applied to face veri cation. Experimental results are presented in section 4. Finally, conclusion is given in section 5. 1 Cosine Similarity Metric Learning The general idea is to learn a transformation matrix from training data so that cosine similarity performs well in the transformed subspace. The performance is measured by cross validation error (cve). 1.1 Cosine similarity Cosine similarity (CS) between two vectors x and y is de ned as: CS(x, y) = x y \u2016x\u2016 \u2016y\u2016 Cosine similarity has a special property that makes it suitable for metric learning: the resulting similarity measure is always within the range of \u22121 and +1. As shown in section 1.3, this property allows the objective function to be simple and e ective. 1.2 Metric learning formulation Let {xi, yi, li}i=1 denote a training set of s labeled samples with pairs of input vectors xi, yi \u2208 R and binary class labels li \u2208 {1, 0} which indicates whether xi and yi match or not. The goal is to learn a linear transformation A : R \u2192 R(d \u2264 m), which we will use to compute cosine similarities in the transformed subspace as: CS(x, y,A) = (Ax) (Ay) \u2016Ax\u2016 \u2016Ay\u2016 = xAAy \u221a xTATAx \u221a yTATAy Speci cally, we want to learn the linear transformation that minimizes the cross validation error when similarities are measured in this way. We begin by de ning the objective function. 4 Hieu V. Nguyen and Li Bai 1.3 Objective function First, we de ne positive and negative sample index sets Pos and Neg as:", "title": "Cosine Similarity Metric Learning for Face Verification"}, "8aef5b3cfc80fafdcefc24c72a4796ca40f4bc8b": {"paper_id": "8aef5b3cfc80fafdcefc24c72a4796ca40f4bc8b", "abstract": "Problem: This paper addresses the problem of multi-camera person reidentification as a ranking problem using a scaleable combination model of Boosting and Support Vector Machines. Related Work: Most existing work has concentrated on compiling feature sets as a template to describe an individual, followed by template matching using a direct distance measure chosen independently from the data [2, 4, 6]. Gray and Tao [3] proposed to use Adaboost to search through a large feature set for those features that are more relevant (more discriminative). Regardless of the choice of features and distance mea sures, re-identification by these approaches are difficult because the re is often too much of an overlap between feature distributions of different objects. Our Approach: In this work, we present a novel reformulation of the person re-identification problem. While previous approaches have looked a t this problem as a classification of correct vs incorrect match, we propo se an approach based on the information retrieval concept of document ranking [1]. The main difference between this approach and previous pers on re-identification techniques is that we are not concerned with comparing direct distance scores between correct and incorrect matches. Ins tead, we are only interested in the relative ranking of these scores that reflects the relevance of each likely match to the query image. There are two main approaches for ranking: Boosting and Support Vector Machines (SVMs). Boosting involves picking weak rankers in each individual feature dimension, which is likely to lead to very weak rankers thus reducing matching effectiveness. In contrast, ranking SVMs seek to learn a ranking function in a higher dimensional feature space holistically (rather than individual feature dimension) where true matches and wrong matches can be made more separable than in the original feature space. However, they can also be computationally and spatially intensive when dealing with large numbers of negative samples. In order to overcome these problems, we explore an Ensemble of Primal SVMs (PRSVM) [1] that reduces the computation and memory cost by using several SVMs trained on subsets of the data while incorporating the SVM parameter C into the framework. Model Overview: Given a dataset X = {(xi,yi)}i=1 wherexi is a multidimensional feature vector representing the appearance of a person captured in one view,yi is its label andm is the number of training samples (images of people). Each query feature vector xi, has relevant feature vectors,x i, j, and related irrelevant feature vectors x \u2212 i, j\u2032 . The goal of ranking any paired image relevance is to learn a ranking function \u03b4 for all pairs of (xi,x + i, j) and(xi,x \u2212 i, j\u2032) such that the relevance ranking score \u03b4 (xi,x + i, j) is larger than\u03b4 (xi,x i, j\u2032). In our work\u03b4 is computed by a linear function w: \u03b4 (xi,xi, j) = w|xi \u2212 xi, j|, (1)", "title": "Person Re-Identification by Support Vector Ranking"}, "1c028833faf11dd565c749741eb97ce811b490de": {"paper_id": "1c028833faf11dd565c749741eb97ce811b490de", "abstract": "Matching people across non-overlapping camera views, known as person re-identification, is challenging due to the lack of spatial and temporal constraints and large visual appearance changes caused by variations in view angle, lighting, background clutter and occlusion. To address these challenges, most previous approaches aim to extract visual features that are both distinctive and stable under appearance changes. However, most visual features and their combinations under realistic conditions are neither stable nor distinctive thus should not be used indiscriminately. In this paper, we propose to formulate person re-identification as a distance learning problem, which aims to learn the optimal distance that can maximises matching accuracy regardless the choice of representation. To that end, we introduce a novel Probabilistic Relative Distance Comparison (PRDC) model, which differs from most existing distance learning methods in that, rather than minimising intra-class variation whilst maximising intra-class variation, it aims to maximise the probability of a pair of true match having a smaller distance than that of a wrong match pair. This makes our model more tolerant to appearance changes and less susceptible to model over-fitting. Extensive experiments are carried out to demonstrate that 1) by formulating the person re-identification problem as a distance learning problem, notable improvement on matching accuracy can be obtained against conventional person re-identification techniques, which is particularly significant when the training sample size is small; and 2) our PRDC outperforms not only existing distance learning methods but also alternative learning methods based on boosting and learning to rank.", "title": "Person re-identification by probabilistic relative distance comparison"}, "2dd2c7602d7f4a0b78494ac23ee1e28ff489be88": {"paper_id": "2dd2c7602d7f4a0b78494ac23ee1e28ff489be88", "abstract": "In this paper, we raise important issues on scalability and the required degree of supervision of existing Mahalanobis metric learning methods. Often rather tedious optimization procedures are applied that become computationally intractable on a large scale. Further, if one considers the constantly growing amount of data it is often infeasible to specify fully supervised labels for all data points. Instead, it is easier to specify labels in form of equivalence constraints. We introduce a simple though effective strategy to learn a distance metric from equivalence constraints, based on a statistical inference perspective. In contrast to existing methods we do not rely on complex optimization problems requiring computationally expensive iterations. Hence, our method is orders of magnitudes faster than comparable methods. Results on a variety of challenging benchmarks with rather diverse nature demonstrate the power of our method. These include faces in unconstrained environments, matching before unseen object instances and person re-identification across spatially disjoint cameras. In the latter two benchmarks we clearly outperform the state-of-the-art.", "title": "Large scale metric learning from equivalence constraints"}, "1da28f17e4df1f45056d6b8e76c08252ee909333": {"paper_id": "1da28f17e4df1f45056d6b8e76c08252ee909333", "abstract": "Matching people across nonoverlapping camera views at different locations and different times, known as person reidentification, is both a hard and important problem for associating behavior of people observed in a large distributed space over a prolonged period of time. Person reidentification is fundamentally challenging because of the large visual appearance changes caused by variations in view angle, lighting, background clutter, and occlusion. To address these challenges, most previous approaches aim to model and extract distinctive and reliable visual features. However, seeking an optimal and robust similarity measure that quantifies a wide range of features against realistic viewing conditions from a distance is still an open and unsolved problem for person reidentification. In this paper, we formulate person reidentification as a relative distance comparison (RDC) learning problem in order to learn the optimal similarity measure between a pair of person images. This approach avoids treating all features indiscriminately and does not assume the existence of some universally distinctive and reliable features. To that end, a novel relative distance comparison model is introduced. The model is formulated to maximize the likelihood of a pair of true matches having a relatively smaller distance than that of a wrong match pair in a soft discriminant manner. Moreover, in order to maintain the tractability of the model in large scale learning, we further develop an ensemble RDC model. Extensive experiments on three publicly available benchmarking datasets are carried out to demonstrate the clear superiority of the proposed RDC models over related popular person reidentification techniques. The results also show that the new RDC models are more robust against visual appearance changes and less susceptible to model overfitting compared to other related existing models.", "title": "Reidentification by Relative Distance Comparison"}, "611f9faa6f3aeff3ccd674d779d52c4f9245376c": {"paper_id": "611f9faa6f3aeff3ccd674d779d52c4f9245376c", "abstract": "Most current approaches to recognition aim to be scaleinvariant. However, the cues available for recognizing a 300 pixel tall object are qualitatively different from those for recognizing a 3 pixel tall object. We argue that for sensors with finite resolution, one should instead use scale-variant, or multiresolution representations that adapt in complexity to the size of a putative detection window. We describe a multiresolution model that acts as a deformable part-based model when scoring large instances and a rigid template with scoring small instances. We also examine the interplay of resolution and context, and demonstrate that context is most helpful for detecting low-resolution instances when local models are limited in discriminative power. We demonstrate impressive results on the Caltech Pedestrian benchmark, which contains object instances at a wide range of scales. Whereas recent state-of-theart methods demonstrate missed detection rates of 86%-37% at 1 falsepositive-per-image, our multiresolution model reduces the rate to 29%.", "title": "Multiresolution Models for Object Detection"}, "12169ff906633e486599660ebf77dd73060640b9": {"paper_id": "12169ff906633e486599660ebf77dd73060640b9", "abstract": "Cascaded classifiers have been widely used in pedestrian detection and achieved great success. These classifiers are trained sequentially without joint optimization. In this paper, we propose a new deep model that can jointly train multi-stage classifiers through several stages of back propagation. It keeps the score map output by a classifier within a local region and uses it as contextual information to support the decision at the next stage. Through a specific design of the training strategy, this deep architecture is able to simulate the cascaded classifiers by mining hard samples to train the network stage-by-stage. Each classifier handles samples at a different difficulty level. Unsupervised pre-training and specifically designed stage-wise supervised training are used to regularize the optimization problem. Both theoretical analysis and experimental results show that the training strategy helps to avoid over fitting. Experimental results on three datasets (Caltech, ETH and TUD-Brussels) show that our approach outperforms the state-of-the-art approaches.", "title": "Multi-stage Contextual Deep Learning for Pedestrian Detection"}, "e3f847d6b1441de15f439e89934ef549d9d7698b": {"paper_id": "e3f847d6b1441de15f439e89934ef549d9d7698b", "abstract": "Traffic sign recognition has been a recurring application domain for visual objects detection. The public datasets have only recently reached large enough size and variety to enable proper empirical studies. We revisit the topic by showing how modern methods perform on two large detection and classification datasets (thousand of images, tens of categories) captured in Belgium and Germany. We show that, without any application specific modification, existing methods for pedestrian detection, and for digit and face classification; can reach performances in the range of 95% ~ 99% of the perfect solution. We show detailed experiments and discuss the trade-off of different options. Our top performing methods use modern variants of HOG features for detection, and sparse representations for classification.", "title": "Traffic sign recognition \u2014 How far are we from the solution?"}, "2cfa006b33084abe8160b001f9a24944cda25d05": {"paper_id": "2cfa006b33084abe8160b001f9a24944cda25d05", "abstract": "A new method for r eal-time tracking of non-rigid objects seen from a moving camera is proposed. The central computational module is based on the mean shift iterations and nds the most probable tar get p osition in the current frame. The dissimilarity between the target model (its c olor distribution) and the target candidates is expr essed by a metric derive d from the Bhattacharyya coe cient. The theoretical analysis of the approach shows that it relates to the Bayesian framework while providing a practical, fast and e cient solution. The capability of the tracker to handle in real-time partial occlusions, signi cant clutter, and target scale variations, is demonstrated for several image sequences.", "title": "Real-Time Tracking of Non-Rigid Objects Using Mean Shift"}, "0ee9610d5771f4bd4b2ab9f077a1572a1007043a": {"paper_id": "0ee9610d5771f4bd4b2ab9f077a1572a1007043a", "abstract": "The efficiency and robustness of a vision system is often largely determined by the quality of the image features available to it. In data mining, one typically works with immense volumes of raw data, which demands effective algorithms to explore the data space. In analogy to data mining, the space of meaningful features for image analysis is also quite vast. Recently, the challenges associated with these problem areas have become more tractable through progress made in machine learning and concerted research effort in manual feature design by domain experts. In this paper, we propose a feature mining paradigm for image classification and examine several feature mining strategies. We also derive a principled approach for dealing with features with varying computational demands. Our goal is to alleviate the burden of manual feature design, which is a key problem in computer vision and machine learning. We include an in-depth empirical study on three typical data sets and offer theoretical explanations for the performance of various feature mining strategies. As a final confirmation of our ideas, we show results of a system, that utilizing feature mining strategies matches or outperforms the best reported results on pedestrian classification (where considerable effort has been devoted to expert feature design).", "title": "Feature Mining for Image Classification"}, "19bcd3bd41825a67f48db701a68030c5e6763152": {"paper_id": "19bcd3bd41825a67f48db701a68030c5e6763152", "abstract": "We address a new partial person re-identification (re-id) problem, where only a partial observation of a person is available for matching across different non-overlapping camera views. This differs significantly from the conventional person re-id setting where it is assumed that the full body of a person is detected and aligned. To solve this more challenging and realistic re-id problem without the implicit assumption of manual body-parts alignment, we propose a matching framework consisting of 1) a local patch-level matching model based on a novel sparse representation classification formulation with explicit patch ambiguity modelling, and 2) a global part-based matching model providing complementary spatial layout information. Our framework is evaluated on a new partial person re-id dataset as well as two existing datasets modified to include partial person images. The results show that the proposed method outperforms significantly existing re-id methods as well as other partial visual matching methods.", "title": "Partial Person Re-Identification"}, "46a01565e6afe7c074affb752e7069ee3bf2e4ef": {"paper_id": "46a01565e6afe7c074affb752e7069ee3bf2e4ef", "abstract": "This paper proposes a new descriptor for person reidentification building on the recent advances of Fisher Vectors. Specifically, a simple vector of attributes consisting in the pixel coordinates, its intensity as well as the first and second-order derivatives is computed for each pixel of the image. These local descriptors are turned into Fisher Vectors before being pooled to produce a global representation of the image. The so-obtained Local Descriptors encoded by Fisher Vector (LDFV) have been validated through experiments on two person re-identification benchmarks (VIPeR and ETHZ), achieving state-of-the-art performance on both datasets.", "title": "Local Descriptors Encoded by Fisher Vectors for Person Re-identification"}, "163f754773c24780b0d43b5058811664b6924d55": {"paper_id": "163f754773c24780b0d43b5058811664b6924d55", "abstract": "Atopic dermatitis (AD) is a chronic inflammatory skin disease with specific genetic and immunological mechanisms. The rapid development of new techniques in molecular biology had ushered in new discoveries on the role of cytokines, chemokines, and immune cells in the pathogenesis of AD. New polymorphisms of AD are continually being reported in different populations. The physical and immunological barrier of normal intact skin is an important part of the innate immune system that protects the host against microbials and allergens that are associated with AD. Defects in the filaggrin gene FLG may play a role in facilitating exposure to allergens and microbial pathogens, which may induce Th2 polarization. Meanwhile, Th22 cells also play roles in skin barrier impairment through IL-22, and AD is often considered to be a Th2/Th22-dominant allergic disease. Mast cells and eosinophils are also involved in the inflammation via Th2 cytokines. Release of pruritogenic substances by mast cells induces scratching that further disrupts the skin barrier. Th1 and Th17 cells are mainly involved in chronic phase of AD. Keratinocytes also produce proinflammatory cytokines such as thymic stromal lymphopoietin (TSLP), which can further affect Th cells balance. The immunological characteristics of AD may differ for various endotypes and phenotypes. Due to the heterogeneity of the disease, and the redundancies of these mechanisms, our knowledge of the pathophysiology of the disease is still incomplete, which is reflected by the absence of a cure for the disease.", "title": "Molecular biology of atopic dermatitis."}, "72bffcc0ce7f0e241ce5ef84cc78e512b4e0356d": {"paper_id": "72bffcc0ce7f0e241ce5ef84cc78e512b4e0356d", "abstract": "This article reports the findings of two experimental tests of self-efficacy theory of behavioral change. The first study investigated the hypothesis that systematic desensitization effects changes in avoidance behavior by creating and strengthening expectations of personal efficacy. Thorough extinction of anxiety arousal to visualized threats by desensitization treatment produced differential increases in self-efficacy. In accord with prediction, microanalysis of congruence between self-efficacy and performance showed self-efficacy to be a highly accurate predictor of degree of behavioral change following complete desensitization. The findings also lend support to the view that perceived self-efficacy mediates anxiety arousal. The second experiment investigated the process of efficacy and behavioral change during the course of treatment by participant modeling. Self-efficacy proved to be a superior predictor of amount of behavioral improvement phobics gained from partial mastery of threats at different phases of treatment.", "title": "Analysis of self-efficacy theory of behavioral change"}, "7b4536b84171dcd273458710f1e835772299f012": {"paper_id": "7b4536b84171dcd273458710f1e835772299f012", "abstract": "The authors investigated the extent to which touch, vision, and audition mediate the processing of statistical regularities within sequential input. Few researchers have conducted rigorous comparisons across sensory modalities; in particular, the sense of touch has been virtually ignored. The current data reveal not only commonalities but also modality constraints affecting statistical learning across the senses. To be specific, the authors found that the auditory modality displayed a quantitative learning advantage compared with vision and touch. In addition, they discovered qualitative learning biases among the senses: Primarily, audition afforded better learning for the final part of input sequences. These findings are discussed in terms of whether statistical learning is likely to consist of a single, unitary mechanism or multiple, modality-constrained ones.", "title": "Modality-constrained statistical learning of tactile, visual, and auditory sequences."}, "1a81da7f819466826ccb733156b91612d4fdbdf4": {"paper_id": "1a81da7f819466826ccb733156b91612d4fdbdf4", "abstract": "I examine the phenomenon of implicit learning, the process by which knowledge about the ralegoverned complexities of the stimulus environment is acquired independently of conscious attempts to do so. Our research with the two, seemingly disparate experimental paradigms of synthetic grammar learning and probability learning is reviewed and integrated with other approaches to the general problem of unconscious cognition. The conclusions reached are as follows: (a) Implicit learning produces a tacit knowledge base that is abstract and representative of the structure of the environment; (b) such knowledge is optimally acquired independently of conscious efforts to learn; and (c) it can be used implicitly to solve problems and make accurate decisions about novel stimulus circumstances. Various epistemological issues and related prob1 lems such as intuition, neuroclinical disorders of learning and memory, and the relationship of evolutionary processes to cognitive science are also discussed.", "title": "Implicit Learning and Tacit Knowledge"}, "53dd71dc5598d41c06d3eef1315e098dc4cbca28": {"paper_id": "53dd71dc5598d41c06d3eef1315e098dc4cbca28", "abstract": "One of the infant\u2019s first tasks in language acquisition is to discover the words embedded in a mostly continuous speech stream. This learning problem might be solved by using distributional cues to word boundaries\u2014for example, by computing the transitional probabilities between sounds in the language input and using the relative strengths of these probabilities to hypothesize word boundaries. The learner might be further aided by language-specific prosodic cues correlated with word boundaries. As a first step in testing these hypotheses, we briefly exposed adults to an artificial language in which the only cues available for word segmentation were the transitional probabilities between syllables. Subjects were able to learn the words of this language. Furthermore, the addition of certain prosodic cues served to enhance performance. These results suggest that distributional cues may play an important role in the initial word segmentation of language learners. q 1996 Academic Press, Inc.", "title": "Word Segmentation : The Role of Distributional Cues"}, "42004b6bdf5ea375dfaeb96c1fd6f8f77d908d65": {"paper_id": "42004b6bdf5ea375dfaeb96c1fd6f8f77d908d65", "abstract": "Internet search rankings have a significant impact on consumer choices, mainly because users trust and choose higher-ranked results more than lower-ranked results. Given the apparent power of search rankings, we asked whether they could be manipulated to alter the preferences of undecided voters in democratic elections. Here we report the results of five relevant double-blind, randomized controlled experiments, using a total of 4,556 undecided voters representing diverse demographic characteristics of the voting populations of the United States and India. The fifth experiment is especially notable in that it was conducted with eligible voters throughout India in the midst of India's 2014 Lok Sabha elections just before the final votes were cast. The results of these experiments demonstrate that (i) biased search rankings can shift the voting preferences of undecided voters by 20% or more, (ii) the shift can be much higher in some demographic groups, and (iii) search ranking bias can be masked so that people show no awareness of the manipulation. We call this type of influence, which might be applicable to a variety of attitudes and beliefs, the search engine manipulation effect. Given that many elections are won by small margins, our results suggest that a search engine company has the power to influence the results of a substantial number of elections with impunity. The impact of such manipulations would be especially large in countries dominated by a single search engine company.", "title": "The search engine manipulation effect (SEME) and its possible impact on the outcomes of elections."}, "65ea05d3570ef7756eafbe2c4b51ec7652882dee": {"paper_id": "65ea05d3570ef7756eafbe2c4b51ec7652882dee", "abstract": "Does media bias affect voting? We address this question by looking at the entry of Fox News in cable markets and its impact on voting. Between October 1996 and November 2000, the conservative Fox News Channel was introduced in the cable programming of 20 percent of US towns. Fox News availability in 2000 appears to be largely idiosyncratic. Using a data set of voting data for 9,256 towns, we investigate if Republicans gained vote share in towns where Fox News entered the cable market by the year 2000. We find a significant effect of the introduction of Fox News on the vote share in Presidential elections between 1996 and 2000. Republicans gain 0.4 to 0.6 percentage points in the towns which broadcast Fox News. The results are robust to town-level controls, district and county fixed effects, and alternative specifications. We also find a significant effect of Fox News on Senate vote share and, to a lesser extent, on voter turnout. Our estimates imply that Fox News convinced 3 to 8 percent of its viewers to vote Republican. The evidence is most consistent with voters suffering from persuasion bias. We also discuss a model of rational voter updating that fits most facts. \u2217George Akerlof, Stephen Ansolabehere, Larry M. Bartels, Matthew Gentzkow, Alan Gerber, Jay Hamilton, Alan Krueger, Marco Manacorda, Enrico Moretti, Torsten Persson, Sam Popkin, Riccardo Puglisi, Matthew Rabin, Jesse Shapiro, David Stromberg, and audiences at Bonn University, EUI (Florence), Fuqua, Harvard University, IIES (Stockholm), Princeton University, UC Berkeley, University of Chicago GSB, and the NBER 2005 Political Economy and Labor Studies Meetings provided useful comments. We would like to specially thank Jim Collins and Matthew Gentzkow for providing the Scarborough data. Shawn Bananzadeh, Jessica Chan, Marguerite Converse, Neil Dandavati, Tatyana Deryugina, Monica Deza, Dylan Fox, Melissa Galicia, Calvin Ho, Sudhamas Khanchanawong, Richard Kim, Martin Kohan, Vipul Kumar, Jonathan Leung, Clarice Li, Tze Yang Lim, Ming Mai, Sameer Parekh, Sharmini Radakrishnan, Rohan Relan, Chanda Singh, Matthew Stone, Nan Zhang, Sibo Zhao, and Liya Zhu helped collect the voting and the cable data. Dan Acland, Saurabh Bhargava, Avi Ebenstein, and Devin Pope provided excellent research assistance.", "title": "The Fox News Effect: Media Bias and Voting\u2217"}, "a735c3f3d759a0ed711d86358be195b12dfcd165": {"paper_id": "a735c3f3d759a0ed711d86358be195b12dfcd165", "abstract": "A Bayesian consumer who is uncertain about the quality of an information source will infer that the source is of higher quality when its reports conform to the consumer\u2019s prior expectations. We use this fact to build a model of media bias in which firms slant their reports toward the prior beliefs of their customers in order to build a reputation for quality. Bias emerges in our model even though it can make all market participants worse off. The model predicts that bias will be less severe when consumers receive independent evidence on the true state of the world and that competition between independently owned news outlets can reduce bias. We present a variety of empirical evidence consistent with these predictions.", "title": "Media Bias and Reputation"}, "ee71ee844053f5a1d0131f477bf73859b232f8b7": {"paper_id": "ee71ee844053f5a1d0131f477bf73859b232f8b7", "abstract": "We propose a boundedly-rational model of opinion formation in which individuals are subject to persuasion bias; that is, they fail to account for possible repetition in the information they receive. We show that persuasion bias implies the phenomenon of social influence, whereby one\u2019s influence on group opinions depends not only on accuracy, but also on how well-connected one is in the social network that determines communication. Persuasion bias also implies the phenomenon of unidimensional opinions; that is, individuals\u2019 opinions over a multidimensional set of issues converge to a single \u201cleft-right\u201d spectrum. We explore the implications of our model in several natural settings, including political science and marketing, and we obtain a number of novel empirical implications. DeMarzo and Zwiebel: Graduate School of Business, Stanford University, Stanford CA 94305, Vayanos: MIT Sloan School of Management, 50 Memorial Drive E52-437, Cambridge MA 02142. This paper is an extensive revision of our paper, \u201cA Model of Persuasion \u2013 With Implication for Financial Markets,\u201d (first draft, May 1997). We are grateful to Nick Barberis, Gary Becker, Jonathan Bendor, Larry Blume, Simon Board, Eddie Dekel, Stefano DellaVigna, Darrell Duffie, David Easley, Glenn Ellison, Simon Gervais, Ed Glaeser, Ken Judd, David Kreps, Edward Lazear, George Loewenstein, Lee Nelson, Anthony Neuberger, Matthew Rabin, Jos\u00e9 Scheinkman, Antoinette Schoar, Peter Sorenson, Pietro Veronesi, Richard Zeckhauser, three anonymous referees, and seminar participants at the American Finance Association Annual Meetings, Boston University, Cornell, Carnegie-Mellon, ESSEC, the European Summer Symposium in Financial Markets at Gerzensee, HEC, the Hoover Institution, Insead, MIT, the NBER Asset Pricing Conference, the Northwestern Theory Summer Workshop, NYU, the Stanford Institute for Theoretical Economics, Stanford, Texas A&M, UCLA, U.C. Berkeley, Universit\u00e9 Libre de Bruxelles, University of Michigan, University of Texas at Austin, University of Tilburg, and the Utah Winter Finance Conference for helpful comments and discussions. All errors are our own.", "title": "PERSUASION BIAS , SOCIAL INFLUENCE , AND UNIDIMENSIONAL"}, "854f3286fd379860bcf4eacc159290248fd44c16": {"paper_id": "854f3286fd379860bcf4eacc159290248fd44c16", "abstract": "We investigate the idea that stock-market participation is influenced by social interaction. We build a simple model in which any given \u201csocial\u201d investor finds it more attractive to invest in the market when the participation rate among his peers is higher. The model predicts higher participation rates among social investors than among \u201cnonsocials\u201d. It also admits the possibility of multiple social equilibria. We then test the theory using data from the Health and Retirement Study. Social households\u2014defined as those who interact with their neighbors, or who attend church\u2014are indeed substantially more likely to invest in the stock market than non-social households, controlling for other factors like wealth, race, education and risk tolerance. Moreover, consistent with a peereffects story, the impact of sociability is stronger in states where stock-market participation rates are higher. We are grateful to the National Science Foundation for research support, and to Michael Kremer, David Laibson, Michael Morris, Lara Tiedens, Tuomo Vuolteenaho and Ezra Zuckerman for their comments and suggestions. A special thanks to Jun Liu for some very helpful modeling tips.", "title": "Social Interaction and Stock-Market Participation"}, "cca66fa588bacd2793ed7ffb64a6920aa9c6129d": {"paper_id": "cca66fa588bacd2793ed7ffb64a6920aa9c6129d", "abstract": "Is there a difference between believing and merely understanding an idea?Descartes thought so. He considered the acceptance and rejection of an idea to be alternative outcomes of an effortful assessment process that occurs subsequent to the automatic comprehension of that idea. This article examined Spinoza's alternative suggestion that (a) the acceptance of an idea is part of the automatic comprehension of that idea and (b) the rejection of an idea occurs subsequent to, and more effortfully than, its acceptance. In this view, the mental representation of abstract ideas is quite similar to the mental representation of physical objects: People believe in the ideas they comprehend, as quickly and automatically as they believe in the objects they see. Research in social and cognitive psychology suggests that Spinoza's model may be a more accurate account of human belief than is that of Descartes.", "title": "How Mental Systems Believe"}, "f9837abc0f1278de0d4565e520276bb3dde2faf7": {"paper_id": "f9837abc0f1278de0d4565e520276bb3dde2faf7", "abstract": "We investigate how editorial slant\u2014defined as the quantity and tone of a newspaper\u2019s candidate coverage as influenced by its editorial position\u2014shapes candidate evaluations and vote choice. We avoid various methodological pitfalls by focusing on a single Senate campaign in a single market with two competing, editorially distinct newspapers. Combining comprehensive content analyses of the papers with an Election Day exit poll, we assess the slant of campaign coverage and its effects on voters. We find compelling evidence that editorial slant influences voters\u2019 decisions. Our results raise serious questions about the media\u2019s place in democratic processes.", "title": "The Impact of Media Bias : How Editorial Slant Affects Voters 1045"}, "8c040297f9cbc04901bbb4f1c01bf22d4aa3761e": {"paper_id": "8c040297f9cbc04901bbb4f1c01bf22d4aa3761e", "abstract": "Honeypot is a closely monitored computer resource that emulates behaviors of production host within a network in order to lure and attract the attackers. The workability and effectiveness of a deployed honeypot depends on its technical configuration. Since honeypot is a resource that is intentionally made attractive to the attackers, it is crucial to make it intelligent and self-manageable. This research reviews at artificial intelligence techniques such as expert system and case-based reasoning, in order to build an intelligent honeypot.", "title": "A review on artificial intelligence techniques for developing intelligent honeypot"}, "2cb93ece29f7c5a99d6a00424d349895380b6b3b": {"paper_id": "2cb93ece29f7c5a99d6a00424d349895380b6b3b", "abstract": "Honeypots are closely monitored decoys that are employed in a network to study the trail of hackers and to alert network administrators of a possible intrusion. Using honeypots provides a cost-effective solution to increase the security posture of an organization. Even though it is not a panacea for security breaches, it is useful as a tool for network forensics and intrusion detection. Nowadays, they are also being extensively used by the research community to study issues in network security, such as Internet worms, spam control, DoS attacks, etc. In this paper, we advocate the use of honeypots as an effective educational tool to study issues in network security. We support this claim by demonstrating a set of projects that we have carried out in a network, which we have deployed specifically for running distributed computer security projects. The design of our projects tackles the challenges in installing a honeypot in academic institution, by not intruding on the campus network while providing secure access to the Internet. In addition to a classification of honeypots, we present a framework for designing assignments/projects for network security courses. The three sample honeypot projects discussed in this paper are presented as examples of the framework.", "title": "Design of network security projects using honeypots"}, "b353e8b6a976422d1200ed29d8c6ab01f0c0cc3d": {"paper_id": "b353e8b6a976422d1200ed29d8c6ab01f0c0cc3d", "abstract": "Enterprise decision making is continuously transforming in the wake of ever increasing amounts of data. Organizations are collecting massive amounts of data in their quest for knowledge nuggets in form of novel, interesting, understandable patterns that underlie these data. The search for knowledge is a multi-step process comprising of various phases including development of domain (business) understanding, data understanding, data preparation, modeling, evaluation and ultimately, the deployment of the discovered knowledge. These phases are represented in form of Knowledge Discovery and Data Mining (KDDM) Process Models that are meant to provide explicit support towards execution of the complex and iterative knowledge discovery process. Review of existing KDDM process models reveals that they have certain limitations (fragmented design, only a checklist-type description of tasks, lack of support towards execution of tasks, especially those of the business understanding phase etc) which are likely to affect the efficiency and effectiveness with which KDDM projects are currently carried out. This dissertation addresses the various identified limitations of existing KDDM process models through an improved model (named the Integrated Knowledge Discovery and Data Mining Process Model) which presents an integrated view of the KDDM process and provides explicit support towards execution of each one of the tasks outlined in the model. We also evaluate the effectiveness and efficiency offered by the IKDDM model against CRISP-DM, a leading KDDM process model, in aiding data mining users to execute various tasks of the KDDM process. Results of statistical tests", "title": "Toward an integrated knowledge discovery and data mining process model"}, "1dd69bbaebe8a2f383e77efe429f2484ba30d4f5": {"paper_id": "1dd69bbaebe8a2f383e77efe429f2484ba30d4f5", "abstract": "We propose an interactive model, RuleViz, for visualizing the entire process of knowledge discovery and data mining. The model consists of ve components according to the main ingredients of the knowledge discovery process: original data visualization, visual data reduction, visual data preprocess, visual rule discovery, and rule visualization. The RuleViz model for visualizing the process of knowledge discovery is introduced and each component is discussed. Two aspects are emphasized, human-machine interaction and process visualization. The interaction helps the KDD system navigate through the enormous search spaces and recognize the intentions of the user, and the visualization of the KDD process helps users gain better insight into the multidimensional data, understand the intermediate results, and interpret the discovered patterns. According to the RuleViz model, we implement an interactive system, CViz, which exploits \\parallel coordinates\" technique to visualize the process of rule induction. The original data is visualized on the parallel coordinates, and can be interactively reduced both horizontally and vertically. Three approaches for discretizing numerical attributes are provided in the visual data preprocessing. CViz learns classi cation rules on the basis of a rule induction algorithm and presents the result as the algorithm proceeds. The discovered rules are nally visualized on the parallel coordinates with each rule being displayed as a directed \\polygon\", and the rule accuracy and quality are used to render the \\polygons\" and control the choice of rules to be displayed to avoid clutter. The CViz system has been experimented with the UCI data sets and synthesis data sets, and the results demonstrate that the RuleViz model and the implemented visualization system are useful and helpful for understanding the process of knowledge discovery and interpreting the nal results.", "title": "RuleViz: a model for visualizing knowledge discovery process"}, "1f000e2ca3552bd76f439dbb77c5f7c6c154a662": {"paper_id": "1f000e2ca3552bd76f439dbb77c5f7c6c154a662", "abstract": "Discusses how the VisDB system supports the query specification process by representing the result visually. The main idea behind the system stems from the view of relational database tables as sets of multidimensional data where the number of attributes corresponds to the number of dimensions. In such a view, it is often unclear. In this system, each display pixel represents one database item. Pixels are arranged and colored to indicate the item's relevance to a user query and to give a visual impression of the resulting data set.<<ETX>>", "title": "VisDB: database exploration using multidimensional visualization"}, "20d2438efcd5c21b13f9c226f023506ee98411f2": {"paper_id": "20d2438efcd5c21b13f9c226f023506ee98411f2", "abstract": "Visual data mining techniques ha ve proven to be of high v alue in exploratory data analysis and the y also have a high potential for mining lar ge databases. In this article, we describe and e valuate a ne w visualization-based approach to mining lar ge databases. The basic idea of our visual data mining techniques is to represent as man y d ta items as possible on the screen at the same time by mapping each data v alue o a pixel of the screen and arranging the pixels adequately . The major goal of this article is to e valuate our visual data mining techniques and to compare them to other well-kno wn visualization techniques for multidimensional data: the parallel coordinate and stick figure visualization techniques. F or the evaluation of visual data mining techniques, in the first place the perception of properties of the data counts, and only in the second place the CPU time and the number of secondary storage accesses are important. In addition to testing the visualization techniques using real data, we de velop d a testing environment for database visualizations similar to the benchmark approach used for comparing the performance of database systems. The testing en vironment allows the generation of test data sets with predefined data characteristics which are important for comparing the perceptual abilities of visual data mining techniques.", "title": "Visualization Techniques for Mining Large Databases: A Comparison"}, "863ac0deb1b600e58ed7b2f167479dd116124617": {"paper_id": "863ac0deb1b600e58ed7b2f167479dd116124617", "abstract": "Data mining (DM) techniques are being increasingly used in many modern organizations to retrieve valuable knowledge structures from organizational databases, including data warehouses. An important knowledge structure that can result from data mining activities is the decision tree (DT) that is used for the classi3cation of future events. The induction of the decision tree is done using a supervised knowledge discovery process in which prior knowledge regarding classes in the database is used to guide the discovery. The generation of a DT is a relatively easy task but in order to select the most appropriate DT it is necessary for the DM project team to generate and analyze a signi3cant number of DTs based on multiple performance measures. We propose a multi-criteria decision analysis based process that would empower DM project teams to do thorough experimentation and analysis without being overwhelmed by the task of analyzing a signi3cant number of DTs would o7er a positive contribution to the DM process. We also o7er some new approaches for measuring some of the performance criteria. ? 2003 Elsevier Ltd. All rights reserved.", "title": "Evaluation of decision trees: a multi-criteria approach"}, "508571db5d2f77c17d2829878bb1dc645010dda8": {"paper_id": "508571db5d2f77c17d2829878bb1dc645010dda8", "abstract": "In this paper, we address the problem of retrospectively pruning decision trees induced from data, according to a topdown approach. This problem has received considerable attention in the areas of pattern recognition and machine learning, and many distinct methods have been proposed in literature. We make a comparative study of six well-known pruning methods with the aim of understanding their theoretical foundations, their computational complexity, and the strengths and weaknesses of their formulation. Comments on the characteristics of each method are empirically supported. In particular, a wide experimentation performed on several data sets leads us to opposite conclusions on the predictive accuracy of simplified trees from some drawn in the literature. We attribute this divergence to differences in experimental designs. Finally, we prove and make use of a property of the reduced error pruning method to obtain an objective evaluation of the tendency to overprune/underprune observed in each method. Index Terms \u2014Decision trees, top-down induction of decision trees, simplification of decision trees, pruning and grafting operators, optimal pruning, comparative studies. \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 \u2726 \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014", "title": "A Comparative Analysis of Methods for Pruning Decision Trees"}, "235cf387b1a597078c9d9c1cbc794dd1e37b8dc3": {"paper_id": "235cf387b1a597078c9d9c1cbc794dd1e37b8dc3", "abstract": "Relevance feedback has a history in information retrieval that dates back well over thirty years (c.f. [SL96]). Relevance feedback is typically used for query expansion during short-term modeling of a user\u2019s immediate information need and for user profiling during long-term modeling of a user\u2019s persistent interests and preferences. Traditional relevance feedback methods require that users explicitly give feedback by, for example, specifying keywords, selecting and marking documents, or answering questions about their interests. Such relevance feedback methods force users to engage in additional activities beyond their normal searching behavior. Since the cost to the user is high and the benefits are not always apparent, it can be difficult to collect the necessary data and the effectiveness of explicit techniques can be limited.", "title": "Implicit feedback for inferring user preference: a bibliography"}, "eac936ca639a44da547d761b0f71ee7bac6ca8a0": {"paper_id": "eac936ca639a44da547d761b0f71ee7bac6ca8a0", "abstract": "The skewness of license plates affects negatively all processing steps of license plate recognition including character segmentation and character classification. The skewness of the license plate in the captured image may appear severely under PTZ camera environments since license plate appears to have various postures in the captured image due to panning and tilting positions of PTZ cameras. Thus far, most of previous works on this subject have mainly resolved skewness problem due to rotation in plane, but not rotation in depth. In this paper, by utilizing planar projective transformation (planar homography), we propose a reliable skew correction method of license plates which handles also the skewness due to rotation in depth. For more reliable deskewing process, the license plate needs to be detected more reliably so that this paper also proposes a reliable license plate detection method. The effectiveness of the proposed skew correction method as well as the proposed license plate detection method is verified though experiments.", "title": "Reliable detection and skew correction method of license plate for PTZ camera-based license plate recognition system"}, "babf6e639d134104e7629568c14048b9dc4aa0d2": {"paper_id": "babf6e639d134104e7629568c14048b9dc4aa0d2", "abstract": "Traffic control and vehicle owner identification has become major problem in every country. Sometimes it becomes difficult to identify vehicle owner who violates traffic rules and drives too fast. Therefore, it is not possible to catch and punish those kinds of people because the traffic personal might not be able to retrieve vehicle number from the moving vehicle because of the speed of the vehicle. Therefore, there is a need to develop Automatic Number Plate Recognition (ANPR) system as a one of the solutions to this problem. There are numerous ANPR systems available today. These systems are based on different methodologies but still it is really challenging task as some of the factors like high speed of vehicle, non-uniform vehicle number plate, language of vehicle number and different lighting conditions can affect a lot in the overall recognition rate. Most of the systems work under these limitations. In this paper, different approaches of ANPR are discussed by considering image size, success rate and processing time as parameters. Towards the end of this paper, an extension to ANPR is suggested.", "title": "Automatic Number Plate Recognition System ( ANPR ) : A Survey"}, "0fabb6851e2c2506620c539ce03dbe4095360b6f": {"paper_id": "0fabb6851e2c2506620c539ce03dbe4095360b6f", "abstract": "Automatic license plate recognition (ALPR) is one of the most important aspects of applying computer techniques towards intelligent transportation systems. In order to recognize a license plate efficiently, however, the location of the license plate, in most cases, must be detected in the first place. Due to this reason, detecting the accurate location of a license plate from a vehicle image is considered to be the most crucial step of an ALPR system, which greatly affects the recognition rate and speed of the whole system. In this paper, a region-based license plate detection method is proposed. In this method, firstly, mean shift is used to filter and segment a color vehicle image in order to get candidate regions. These candidate regions are then analyzed and classified in order to decide whether a candidate region contains a license plate. Unlike other existing license plate detection methods, the proposed method focuses on regions, which demonstrates to be more robust to interference characters and more accurate when compared with other methods.", "title": "Region-based license plate detection"}, "3d35929ce968b73f45adfb165314d94baa9872a1": {"paper_id": "3d35929ce968b73f45adfb165314d94baa9872a1", "abstract": "Automatic license plate recognition (ALPR) is the extraction of vehicle license plate information from an image or a sequence of images. The extracted information can be used with or without a database in many applications, such as electronic payment systems (toll payment, parking fee payment), and freeway and arterial monitoring systems for traffic surveillance. The ALPR uses either a color, black and white, or infrared camera to take images. The quality of the acquired images is a major factor in the success of the ALPR. ALPR as a real-life application has to quickly and successfully process license plates under different environmental conditions, such as indoors, outdoors, day or night time. It should also be generalized to process license plates from different nations, provinces, or states. These plates usually contain different colors, are written in different languages, and use different fonts; some plates may have a single color background and others have background images. The license plates can be partially occluded by dirt, lighting, and towing accessories on the car. In this paper, we present a comprehensive review of the state-of-the-art techniques for ALPR. We categorize different ALPR techniques according to the features they used for each stage, and compare them in terms of pros, cons, recognition accuracy, and processing speed. Future forecasts of ALPR are given at the end.", "title": "Automatic License Plate Recognition (ALPR): A State-of-the-Art Review"}, "d02b8570d159998a67c685f26a0336d92cd49330": {"paper_id": "d02b8570d159998a67c685f26a0336d92cd49330", "abstract": "Detecting the region of a license plate is the key component of the vehicle license plate recognition (VLPR) system. A new method is adopted in this paper to analyze road images which often contain vehicles and extract LP from natural properties by finding vertical and horizontal edges from vehicle region. The proposed vehicle license plate detection (VLPD) method consists of three main stages: (1) a novel adaptive image segmentation technique named as sliding concentric windows (SCWs) used for detecting candidate region; (2) color verification for candidate region by using HSI color model on the basis of using hue and intensity in HSI color model verifying green and yellow LP and white LP, respectively; and (3) finally, decomposing candidate region which contains predetermined LP alphanumeric character by using position histogram to verify and detect vehicle license plate (VLP) region. In the proposed method, input vehicle images are commuted into grey images. Then the candidate regions are found by sliding concentric windows. We detect VLP region which contains predetermined LP color by using HSI color model and LP alphanumeric character by using position histogram. Experimental results show that the proposed method is very effective in coping with different conditions such as poor illumination, varied distances from the vehicle and varied weather.", "title": "Vehicle License Plate Detection Method Based on Sliding Concentric Windows and Histogram"}, "98b03551b190ff0f03ed41a02dc4ce12ab58e97a": {"paper_id": "98b03551b190ff0f03ed41a02dc4ce12ab58e97a", "abstract": "Hough has proposed an interesting and computationally efficient procedure for detecting lines in pictures. This paper points out that the use of angle-radius rather than slope-intercept parameters simplifies the computation further. It also shows how the method can be used for more general curve fitting, and gives alternative interpretations that explain the source of its efficiency.", "title": "Use of the Hough Transformation to Detect Lines and Curves in Pictures"}, "73d934310be0470b4757be484bbb082bf2a101f4": {"paper_id": "73d934310be0470b4757be484bbb082bf2a101f4", "abstract": "License Plate Recognition (LPR) is a well known image processing technology. LPR system consists of four steps: capture the image from digital camera, pre-processing, character segmentation and character recognition. License plates are available in various styles and colors in various countries. Every country has their own license plate format. So each country develops the LPR system appropriate for the vehicle license plate format. Difficulties that the LPR systems face are the environmental and non-uniform outdoor illumination conditions. Therefore, most of the systems work under restricted environmental conditions like fixed illumination, limited vehicle speed, designated routes, and stationary backgrounds. Each LPR system use different combination of algorithms. From the papers being surveyed, it is realized that a good success rate of 93. 7% is obtained by the combination of fuzzy logic for license plate detection and Self Organizing (SO) neural network for character recognition. Comparisons of different LPR systems are discussed in this paper.", "title": "A Survey on License Plate Recognition Systems"}, "467024c6cc8fe73c64e501f8a12cdbafbf9561b0": {"paper_id": "467024c6cc8fe73c64e501f8a12cdbafbf9561b0", "abstract": "In this paper, a new algorithm for vehicle license plate identification is proposed, on the basis of a novel adaptive image segmentation technique (sliding concentric windows) and connected component analysis in conjunction with a character recognition neural network. The algorithm was tested with 1334 natural-scene gray-level vehicle images of different backgrounds and ambient illumination. The camera focused in the plate, while the angle of view and the distance from the vehicle varied according to the experimental setup. The license plates properly segmented were 1287 over 1334 input images (96.5%). The optical character recognition system is a two-layer probabilistic neural network (PNN) with topology 108-180-36, whose performance for entire plate recognition reached 89.1%. The PNN is trained to identify alphanumeric characters from car license plates based on data obtained from algorithmic image processing. Combining the above two rates, the overall rate of success for the license-plate-recognition algorithm is 86.0%. A review in the related literature presented in this paper reveals that better performance (90% up to 95%) has been reported, when limitations in distance, angle of view, illumination conditions are set, and background complexity is low", "title": "A License Plate-Recognition Algorithm for Intelligent Transportation System Applications"}, "2ae6014a451801671d41b6171f86e657d8b1fbaf": {"paper_id": "2ae6014a451801671d41b6171f86e657d8b1fbaf", "abstract": "WordNet::Similarity is a freely available software package that makes it possible to measure the semantic similarity or relatedness between a pair of concepts (or word senses). It provides six measures of similarity, and three measures of relatedness, all of which are based on the lexical database WordNet. These measures are implemented as Perl modules which take as input two concepts, and return a numeric value that represents the degree to which they are similar or related.", "title": "WordNet: : Similarity - Measuring the Relatedness of Concepts"}, "f9a25e0dc776857fc24ebc7115c980312f2719b1": {"paper_id": "f9a25e0dc776857fc24ebc7115c980312f2719b1", "abstract": "A semantic concordance is a textual corpus and a lexicon So combined that every substantive word in the text is linked to its appropriate ~nse in the lexicon. Thus it can be viewed either as a corpus in which words have been tagged syntactically and semantically, or as a lexicon in which example sentences can be found for many definitions. A semantic concordance is being constructed to u s e in studies of sense resolution in context (semantic disambiguation). The Brown Corpus is the text and WordNet is the lexicon. Semantic tags (pointers to WordNet synsets) are inserted in the text manually using an interface, ConText, that was designed to facilitate the task. Another interface supports searches of the tagged text. Some practical uses for semantic concordances are proposed. 1. I N T R O D U C T I O N We wish to propose a new version of an old idea. Lexicographers have traditionally based their work on a corpus of examples taken from approved usage, but considerations of cost usually limit published dictionaries to lexical entries having only a scattering of phrases to illustrate the usages from which definitions were derived. As a consequence of this economic pressure, most dictionaries are relatively weak in providing contextual information: someone learning English as a second language will find in an English dictionary many alternative meanings for a common word, but little or no help in determining the linguistic contexts in which the word can be used to express those different meanings. Today, however, large computer memories are affordable enough that this limitation can be removed; it would now be feasible to publish a dictionary electronically along with all of the citation sentences on which it was based. The resulting combination would be more than a lexicon and more than a corpus; we propose to call it a semantic concordance. If the corpus is some specific text, it is a specific semantic concordance; ff the corpus includes many different texts, it is a universal semantic concordance. We have begun constructing a universal semantic concordance in conjunction with our work on a lexical database. The result can be viewed either as a collection of passages in which words have been tagged syntactically and semantieally, or as a lexicon in which illustrative sentences can be found for many definitions. At the present time, the correlation of a lexical meaning with examples in which a word is used to express that meaning must be done by hand. Manual semantic tagging is tedious; it should be done automatically as soon as it is possible to resolve word senses in context automatically. It is hoped that the manual creation of a semantic concordance will provide an appropriate environment for developing and testing those automatic procedures. 2. W O R D N E T : A L E X I C A L D A T A B A S E The lexical component of the universal semantic concordance that we are constructing is WordNet, an on-line lexical resource inspired by current psycholinguistic theories of haman lexical memory [1, 2]. A standard, handheld dictionary is organized alphabetically; it puts together words that are spelled alike and scatters words with related meanings. Although on-line versions of such standard dictionaries can relieve a user of alphabetical searches, it is clearly inefficient to use a computer merely as a rapid page-turner. WordNet is an example of a more efficient combination of traditional lexicography and modern computer science. The most ambitious feature of WordNet is the attempt to organize lexical information in terms of word meanings, rather than word forms. WordNet is organized by semantic relations (rather than by semantic components) within the open-class categories of noun, verb, adjective, and adverb; closed-class categories of words (pronouns, prepositions, conjunctions, etc.) are not included in WordNet. The semantic relations among open-class words include: synonymy and antonymy (which are semantic relations between words and which are found in all four syntactic categories); hyponymy and hypernymy (which are semantic relations between concepts and which organize nouns into a categorical hierarchy); meronymy and holonymy (which represent part-whole relations among noun concepts); and troponymy (manner relations) and entailment relations between verb concepts. These semantic relations were chosen to be intuitively obvious to nonlinguists and to have broad applicability throughout the lexicon. The basic elements of WordNet are sets of synonyms (or synsets), which are taken to represent lexicalized concepts. A synset is a group of words that are synonymous, in the sense that there are contexts in which they can be interchanged without changing the meaning of the statement. For example, WordNet distinguishes between the synsets:", "title": "A Semantic Concordance"}, "00bbba51721dee6e0b1cd2a5b614ab46f33abab6": {"paper_id": "00bbba51721dee6e0b1cd2a5b614ab46f33abab6", "abstract": "1. Please explain how this manuscript advances this field of research and/or contributes something new to the literature. They tackle the problem of semantic similarity based on the information content the concepts share. They combine the taxonomic structure with the empirical problem estimates which provides better way of adapting knowledge to multiple context concepts. They also try to find a solution to the multiple inheritance problems.", "title": "Using Information Content to Evaluate Semantic Similarity in a Taxonomy"}, "259d0304adcb49e40436137684b78a80c9ef097b": {"paper_id": "259d0304adcb49e40436137684b78a80c9ef097b", "abstract": "In this paper, I will discuss four major topics relating to current research in lexical semantics: methodology, descriptive coverage, adequacy of the representation, and the computational usefulness of representations. In addressing these issues, I will discuss what I think are some of the central problems facing the lexical semantics community, and suggest ways of best approaching these issues. Then, I will provide a method for the decomposition of lexical categories and outline a theory of lexical semantics embodying a notion of cocompositionality and type coercion, as well as several levels of semantic description, where the semantic load is spread more evenly throughout the lexicon. I argue that lexical decomposition is possible if it is performed generatively. Rather than assuming a fixed set of primitives, I will assume a fixed number of generative devices that can be seen as constructing semantic expressions. I develop a theory of Qualia Structure, a representation language for lexical items, which renders much lexical ambiguity in the lexicon unnecessary, while still explaining the systematic polysemy that words carry. Finally, I discuss how individual lexical structures can be integrated into the larger lexical knowledge base through a theory of lexical inheritance. This provides us with the necessary principles of global organization for the lexicon, enabling us to fully integrate our natural language lexicon into a conceptual whole.", "title": "The Generative Lexicon"}, "a00bd22c2148fc0c2c32300742d9390431949f56": {"paper_id": "a00bd22c2148fc0c2c32300742d9390431949f56", "abstract": "Vegetarianism within the U.K. is growing in popularity, with the current estimate of 7% of the population eating a vegetarian diet. This study examined differences between the attitudes and beliefs of four dietary groups (meat eaters, meat avoiders, vegetarians and vegans) and the extent to which attitudes influenced intentions to follow each diet. In addition, the role of attitudinal ambivalence as a moderator variable was examined. Completed questionnaires were obtained from 111 respondents (25 meat eaters, 26 meat avoiders, 34 vegetarians, 26 vegans). In general, predictions were supported, in that respondents displayed most positive attitudes and beliefs towards their own diets, and most negative attitudes and beliefs towards the diet most different form their own. Regression analyses showed that, as predicted by the Theory of Planned Behaviour, attitudes, subjective norm and perceived behavioural control were significant predictors of intention to follow each diet (apart from the vegetarian diet, where subjective norm was non-significant). In each case, attitudinal ambivalence was found to moderate the attitude-intention relationship, such that attitudes were found to be stronger predictors at lower levels of ambivalence. The results not only highlight the extent to which such alternative diets are an interesting focus for psychological research, but also lend further support to the argument that ambivalence in an important influence on attitude strength.", "title": "Attitudes towards following meat, vegetarian and vegan diets: an examination of the role of ambivalence"}, "9c58c19b01b04ca7dbf122d59684bf05353cc77b": {"paper_id": "9c58c19b01b04ca7dbf122d59684bf05353cc77b", "abstract": "It has long been recognized that personality test scores are influenced by non-test-relevant response determinants. Wiggins and Rumrill (1959) distinguish three approaches to this problem. Briefly, interest in the problem of response distortion has been concerned with attempts at statistical correction for \"faking good\" or \"faking bad\" (Meehl & Hathaway, 1946), the analysis of response sets (Cronbach, 1946,1950), and ratings of the social desirability of personality test items (Edwards, 19 5 7). A further distinction can be made, however, which results in a somewhat different division of approaches to the question of response distortion. Common to both the Meehl and Hathaway corrections for faking good and faking bad and Cronbach's notion of response sets is an interest in the test behavior of the subject(S). By social desirability, on the other hand, Edwards primarily means the \"scale value for any personality statement such that the scale value indicates the position of the statement on the social desirability continuum . . .\" (1957, p. 3). Social desirability, thus, has been used to refer to a characteristic of test items, i.e., their scale position on a social desirability scale. Whether the test behavior of 5s or the social desirability properties of items are the focus of interest, however, it now seems clear that underlying both these approaches is the concept of statistical deviance. In the construction of the MMPI K scale, for example, items were selected which differentiated between clinically normal persons producing abnormal te\u00a5Tpfpfiles~snd^cTinically abnormal individuals with abnormal test profiles, and between clinically abnormal persons with normal test profiles and abnormal 5s whose test records were abnormal. Keyed responses to the K scale items tend to be statistically deviant in the parent populations. Similarly, the development of the Edwards Social Desirability Scale (SDS) illustrates this procedure. Items were drawn from various MMPI scales (F, L, K, and the Manifest Anxiety Scale [Taylor, 1953]) and submitted to judges who categorized them as either socially desirable or socially undesirable. Only items on which there was unanimous agreement among the 10 judges were included in the SDS. It seems clear that the items in Edwards SDS would, of necessity, have extreme social desirability scale positions or, in other words, be statistically deviant. Some unfortunate consequences follow from the strict use of the statistical deviance model in the development of-sOcialTtesirSbTBty scales. With items drawn from the MMPI, it is apparent that in addition to their scalability for social desirability the items may also be characterized by their content which,^n a general sense, has pathological implications. When a social desrrabtltty^scale constructed according to this procedure is then applied to a college student population, the meaning of high social desirability scores is not at all clear. When 5s given the Edwards SDS deny, for example, that their sleep is fitful and disturbed (Item 6) or that they worry quite a bit over possible misfortunes (Item 35), it cannot be determined whether these responses are attributable to social desirability or to a genuine absence of such symptoms. The probability of occurrence of the symptoms represented in MMPI items (and incorportated in the SDS)", "title": "A new scale of social desirability independent of psychopathology."}, "4781dc831cd92bebe0dcabc00bdcf718b514442c": {"paper_id": "4781dc831cd92bebe0dcabc00bdcf718b514442c", "abstract": "In this paper we have designed and implemented (15, k) a BCH Encoder and decoder using VHDL for reliable data transfer in AWGN channel with multiple error correction control. The digital logic implementation of binary encoding of multiple error correcting BCH code (15, k) of length n=15 over GF (2 4 ) with irreducible primitive polynomial x 4 +x+1 is organized into shift register circuits. Using the cyclic codes, the reminder b(x) can be obtained in a linear (15-k) stage shift register with feedback connections corresponding to the coefficients of the generated polynomial. Three encoders are designed using VHDL to encode the single, double and triple error correcting BCH code (15, k) corresponding to the coefficient of generated polynomial. Information bit is transmitted in unchanged form up to K clock cycles and during this period parity bits are calculated in the LFSR then the parity bits are transmitted from k+1 to 15 clock cycles. Total 15-k numbers of parity bits with k information bits are transmitted in 15 code word. In multiple error correction method, we have implemented (15, 5 ,3 ) ,(15,7, 2) and (15, 11, 1) BCH encoder and decoder using VHDL and the simulation is done using Xilinx ISE 14.2. KeywordsBCH, BER, SNR, BCH Encoder, Decoder VHDL, Error Correction, AWGN, LFSR", "title": "Implementation of BCH Code ( n , k ) Encoder and Decoder for Multiple Error Correction Control"}, "9a2c20b7b8602d1dd7332f8fb5017e5c846185c4": {"paper_id": "9a2c20b7b8602d1dd7332f8fb5017e5c846185c4", "abstract": "Various efforts have been made to quantify scientific impact and identify the mechanisms that influence its future evolution. The first step is the identification of what constitutes scholarly impact and how it is measured. In this direction, various approaches focus on future citation count or h-index prediction at author or publication level, on fitting the distribution of citation accumulation or accurately identifying award winners, upcoming hot research topics or academic rising stars. A plethora of features have been contemplated as possible influential factors and assorted machine-learning methodologies have been adopted to ensure timely and accurate estimations. Here, we provide an overview of the field challenges, as well as a taxonomy of the existing approaches to identify the open issues that are yet to be addressed.", "title": "Predicting the Evolution of Scientific Output"}, "8aeed4d5204e44695e13def6ce2850cfb01cf1aa": {"paper_id": "8aeed4d5204e44695e13def6ce2850cfb01cf1aa", "abstract": "Prediction of scholar popularity has become an important research topic for a number of reasons. In this paper, we tackle the problem of predicting the popularity {\\it trend} of scholars by concentrating on making predictions both as earlier and accurate as possible. In order to perform the prediction task, we first extract the popularity trends of scholars from a training set. To that end, we apply a time series clustering algorithm called K-Spectral Clustering (K-SC) to identify the popularity trends as cluster centroids. We then predict trends for scholars in a test set by solving a classification problem. Specifically, we first compute a set of measures for individual scholars based on the distance between earlier points in her particular popularity curve and the identified centroids. We then combine those distance measures with a set of academic features (e.g., number of publications, number of venues, etc) collected during the same monitoring period, and use them as input to a classification method. One aspect that distinguishes our method from other approaches is that the monitoring period, during which we gather information on each scholar popularity and academic features, is determined on a per scholar basis, as part of our approach. Using total citation count as measure of scientific popularity, we evaluate our solution on the popularity time series of more than 500,000 Computer Science scholars, gathered from Microsoft Azure Marketplace (https://datamarket.azure.com/dataset/mrc/microsoftacademic). The experimental results show that the our prediction method outperforms other alternative prediction methods. We also show how to apply our method jointly with regression models to improve the prediction of scholar popularity values (e.g., number of citations) at a given future time.", "title": "Early prediction of scholar popularity"}, "017372aec4b163ed6300499d40e316d2a0a7a9dd": {"paper_id": "017372aec4b163ed6300499d40e316d2a0a7a9dd", "abstract": "Online content exhibits rich temporal dynamics, and diverse realtime user generated content further intensifies this process. However, temporal patterns by which online content grows and fades over time, and by which different pieces of content compete for attention remain largely unexplored.\n We study temporal patterns associated with online content and how the content's popularity grows and fades over time. The attention that content receives on the Web varies depending on many factors and occurs on very different time scales and at different resolutions. In order to uncover the temporal dynamics of online content we formulate a time series clustering problem using a similarity metric that is invariant to scaling and shifting. We develop the K-Spectral Centroid (K-SC) clustering algorithm that effectively finds cluster centroids with our similarity measure. By applying an adaptive wavelet-based incremental approach to clustering, we scale K-SC to large data sets.\n We demonstrate our approach on two massive datasets: a set of 580 million Tweets, and a set of 170 million blog posts and news media articles. We find that K-SC outperforms the K-means clustering algorithm in finding distinct shapes of time series. Our analysis shows that there are six main temporal shapes of attention of online content. We also present a simple model that reliably predicts the shape of attention by using information about only a small number of participants. Our analyses offer insight into common temporal patterns of the content on theWeb and broaden the understanding of the dynamics of human attention.", "title": "Patterns of temporal variation in online media"}, "9434b0366efd8c5c691e3f726bee7240d8e2c3f9": {"paper_id": "9434b0366efd8c5c691e3f726bee7240d8e2c3f9", "abstract": "We study the problem of predicting the popularity of items in a dynamic environment in which authors post continuously new items and provide feedback on existing items. This problem can be applied to predict popularity of blog posts, rank photographs in a photo-sharing system, or predict the citations of a scientific article using author information and monitoring the items of interest for a short period of time after their creation. As a case study, we show how to estimate the number of citations for an academic paper using information about past articles written by the same author(s) of the paper. If we use only the citation information over a short period of time, we obtain a predicted value that has a correlation of r = 0.57 with the actual value. This is our baseline prediction. Our best-performing system can improve that prediction by adding features extracted from the past publishing history of its authors, increasing the correlation between the actual and the predicted values to r = 0.81.", "title": "Estimating Number of Citations Using Author Reputation"}, "e9239469aba4bccf3e36d1c27894721e8dbefc44": {"paper_id": "e9239469aba4bccf3e36d1c27894721e8dbefc44", "abstract": null, "title": "Foundations of Machine Learning"}, "5b324e2db551a7903cb4d867980c011e85cf8071": {"paper_id": "5b324e2db551a7903cb4d867980c011e85cf8071", "abstract": "A widely used measure of scientific impact is citations. However, due to their heavy-tailed distribution, citations are fundamentally difficult to predict. Instead, to characterize scientific impact, we address two analogous questions asked by many scientific researchers: \u201cHow will my h-index evolve over time, and which of my previously or newly published papers will contribute to it?\u201d To answer these questions, we perform two related tasks. First, we develop a model to predict authors' future h-indices based on their current scientific impact. Second, we examine the factors that drive papers-either previously or newly published-to increase their authors' predicted future h-indices. By leveraging relevant factors, we can predict an author's h-index in five years with an R2 value of 0.92 and whether a previously (newly) published paper will contribute to this future h-index with an F1 score of 0.99 (0.77). We find that topical authority and publication venue are crucial to these effective predictions, while topic popularity is surprisingly inconsequential. Further, we develop an online tool that allows users to generate informed h-index predictions. Our work demonstrates the predictability of scientific impact, and can help researchers to effectively leverage their scholarly position of \u201cstanding on the shoulders of giants\u201d.", "title": "Can Scientific Impact Be Predicted?"}, "58a63086b209374d5cf625d27617eba1e96288ef": {"paper_id": "58a63086b209374d5cf625d27617eba1e96288ef", "abstract": "This paper addresses several key issues in the ArnetMiner system, which aims at extracting and mining academic social networks. Specifically, the system focuses on: 1) Extracting researcher profiles automatically from the Web; 2) Integrating the publication data into the network from existing digital libraries; 3) Modeling the entire academic network; and 4) Providing search services for the academic network. So far, 448,470 researcher profiles have been extracted using a unified tagging approach. We integrate publications from online Web databases and propose a probabilistic framework to deal with the name ambiguity problem. Furthermore, we propose a unified modeling approach to simultaneously model topical aspects of papers, authors, and publication venues. Search services such as expertise search and people association search have been provided based on the modeling results. In this paper, we describe the architecture and main features of the system. We also present the empirical evaluation of the proposed methods.", "title": "ArnetMiner: extraction and mining of academic social networks"}, "1d89516427c0d91653b70171a0e8998af9d5960b": {"paper_id": "1d89516427c0d91653b70171a0e8998af9d5960b", "abstract": "MOTIVATION\nRecent advances in high-throughput genotyping and brain imaging techniques enable new approaches to study the influence of genetic variation on brain structures and functions. Traditional association studies typically employ independent and pairwise univariate analysis, which treats single nucleotide polymorphisms (SNPs) and quantitative traits (QTs) as isolated units and ignores important underlying interacting relationships between the units. New methods are proposed here to overcome this limitation.\n\n\nRESULTS\nTaking into account the interlinked structure within and between SNPs and imaging QTs, we propose a novel Group-Sparse Multi-task Regression and Feature Selection (G-SMuRFS) method to identify quantitative trait loci for multiple disease-relevant QTs and apply it to a study in mild cognitive impairment and Alzheimer's disease. Built upon regression analysis, our model uses a new form of regularization, group \u2113(2,1)-norm (G(2,1)-norm), to incorporate the biological group structures among SNPs induced from their genetic arrangement. The new G(2,1)-norm considers the regression coefficients of all the SNPs in each group with respect to all the QTs together and enforces sparsity at the group level. In addition, an \u2113(2,1)-norm regularization is utilized to couple feature selection across multiple tasks to make use of the shared underlying mechanism among different brain regions. The effectiveness of the proposed method is demonstrated by both clearly improved prediction performance in empirical evaluations and a compact set of selected SNP predictors relevant to the imaging QTs.\n\n\nAVAILABILITY\nSoftware is publicly available at: http://ranger.uta.edu/%7eheng/imaging-genetics/.", "title": "Identifying quantitative trait loci via group-sparse multitask regression and feature selection: an imaging genetics study of the ADNI cohort"}, "6662dbe13b725180d7f9ae114b709f8bc7d0ced9": {"paper_id": "6662dbe13b725180d7f9ae114b709f8bc7d0ced9", "abstract": "This paper demonstrates the computational advantages of a multiobjective framework that can overcome the generic and domain-related challenges in optical system design and optimisation. Non-dominated sorting genetic algorithms-II (Deb, 2003) is employed in this study. The optical systems studied in this paper are Cooke triplets, Petzval lens systems and achromatic doublets. We report the results of four studies. In the first study, we optimise the optical systems using computationally efficient image quality objective functions. Our approach uses only two paraxial rays to estimate the objective functions and thus improves the computational efficiency. This timesaving measure can partially compensate for the typically enormous number of fitness function evaluations required in evolutionary algorithms. The reduction in reliability due to the computations from a single ray pair is compensated by the availability of multiple objective functions that help us to navigate to the optima. In the second study, hybridisation of evolutionary and gradient-based approaches and scaling techniques are employed to speed up convergence and enforce the constraints. The third study shows how recent developments in optical system design research can be better integrated in a multi-objective framework. The fourth study optimises an achromatic doublet with suitable constraints applied to the thicknesses and image distance.", "title": "Lens design as multi-objective optimisation"}, "5d404211336ec535ac4f6e288cc5047cf433a327": {"paper_id": "5d404211336ec535ac4f6e288cc5047cf433a327", "abstract": "AJAX is a very promising approach for improving rich interactivity and responsiveness of web applications. At the same time, AJAX techniques shatter the metaphor of a web \"page\" upon which general search crawlers are based. This paper describes a novel technique for crawling AJAX applications through dynamic analysis and reconstruction of user interface state changes. Our method dynamically infers a state-flow graph modeling the various navigation paths and states within an AJAX application. This reconstructed model can be used to generate linked static pages. These pages could be used to expose AJAX sites to general search engines. Moreover, we believe that the crawling techniques that are part of our solution have other applications, such as within general search engines, accessibility improvements, or in automatically exercising all user interface elements and conducting state-based testing of AJAX applications. We present our open source tool called CRAWLJAX which implements the concepts discussed in this paper. Additionally, we report a case study in which we apply our approach to a number of representative AJAX applications and elaborate on the obtained results.", "title": "Crawling AJAX by Inferring User Interface State Changes"}, "33d726a349867b4d2c0a058e81ced78f29f053e8": {"paper_id": "33d726a349867b4d2c0a058e81ced78f29f053e8", "abstract": "Detecting and representing changes to data is important for active databases, data warehousing, view maintenance, and version and configuration management. Most previous work in change management has dealt with flat-file and relational data; we focus on hierarchically structured data. Since in many cases changes must be computed from old and new versions of the data, we define the hierarchical change detection problem as the problem of finding a \"minimum-cost edit script\" that transforms one data tree to another, and we present efficient algorithms for computing such an edit script. Our algorithms make use of some key domain characteristics to achieve substantially better performance than previous, general-purpose algorithms. We study the performance of our algorithms both analytically and empirically, and we describe the application of our techniques to hierarchically structured documents.", "title": "Change Detection in Hierarchically Structured Information"}, "184b5c5d0dcbe1d4685cccbfc729d759d5a67c29": {"paper_id": "184b5c5d0dcbe1d4685cccbfc729d759d5a67c29", "abstract": "AJAX applications are designed to have high user interactivity and low user-perceived latency. Real-time dynamic Web data such as news headlines, stock tickers, and auction updates need to be propagated to the users as soon as possible. However, AJAX still suffers from the limitations of the Web's request/response architecture which prevents servers from pushing real-time dynamic web data. Such applications usually use a pull style to obtain the latest updates, where the client actively requests the changes based on a predefined interval. It is possible to overcome this limitation by adopting a push style of interaction where the server broadcasts data when a change occurs on the server side. Both these options have their own trade-offs. This paper explores the fundamental limits of browser-based applications and analyzes push solutions for AJAX technology. It also shows the results of an empirical study comparing push and pull.", "title": "A Comparison of Push and Pull Techniques for AJAX"}, "65726870f66fe4e397767c9a3c5b30bf78a03971": {"paper_id": "65726870f66fe4e397767c9a3c5b30bf78a03971", "abstract": "This paper presents an open-source diarization toolkit which is mostly dedicated to speaker and developed by the LIUM. This toolkit includes hierarchical agglomerative clustering methods using well-known measures such as BIC and CLR. Two applications for which the toolkit has been used are presented: one is for broadcast news using the ESTER 2 data and the other is for telephone conversations using the MEDIA corpus.", "title": "LIUM SPKDIARIZATION: AN OPEN SOURCE TOOLKIT FOR DIARIZATION"}, "9bdca67c6b2e3b0a906b7e58195be584b2dec8ee": {"paper_id": "9bdca67c6b2e3b0a906b7e58195be584b2dec8ee", "abstract": "Many Peer-to-Peer (P2P) networks have emerged as scalable platforms for enabling file-sharing and content-distribution applications. Due to their reliance on decentralized models encompassing millions of users, P2P networks are subjected to being infested with contraband material, copyright infringing content, as well as malicious viruses and programs. Moreover, the dynamics involved as users join and leave these networks make investigating them quite complex and time-dependent. To that end, this paper presents PxP, a tool that creates an expanding reference library for content that appears on P2P networks. PxP continuously searches the P2P networks for new files, downloads and fingerprints them, deletes them, and add their meta data to a public reference library. We envision that this library will aid investigators in criminal investigations that involve contraband material, civil investigations with copyright infringement content. In addition, this library can be used to speed up forensics examination of media through skipping over some of the files shared on P2P networks and in validating carving tools. We assess the performance of our framework through analysis, simulations and real experiments for content collected from the Gnutella P2P network.", "title": "An expanding reference library for Peer-to-Peer content"}, "0d99a8787bd3abe24c7737775da4d842bb86e4ab": {"paper_id": "0d99a8787bd3abe24c7737775da4d842bb86e4ab", "abstract": "Digital content is for copying: quotation, revision, plagiarism, and file sharing all create copies. Document fingerprinting is concerned with accurately identifying copying, including small partial copies, within large sets of documents.We introduce the class of local document fingerprinting algorithms, which seems to capture an essential property of any finger-printing technique guaranteed to detect copies. We prove a novel lower bound on the performance of any local algorithm. We also develop winnowing, an efficient local fingerprinting algorithm, and show that winnowing's performance is within 33% of the lower bound. Finally, we also give experimental results on Web data, and report experience with MOSS, a widely-used plagiarism detection service.", "title": "Winnowing: Local Algorithms for Document Fingerprinting"}, "c47d151f09c567013761632c89e237431c6291a2": {"paper_id": "c47d151f09c567013761632c89e237431c6291a2", "abstract": "We present randomized algorithms to solve the following string-matching problem and some of its generalizations: Given a string X of length n (the pattern) and a string Y (the text), find the first occurrence of X as a consecutive block within Y. The algorithms represent strings of length n by much shorter strings called fingerprints, and achieve their efficiency by manipulating fingerprints instead of longer strings. The algorithms require a constant number of storage locations, and essentially run in real time. They are conceptually simple and easy to implement. The method readily generalizes to higher-dimensional patternmatching problems.", "title": "Efficient Randomized Pattern-Matching Algorithms"}, "46f766c11df69808453e14c900bcb3f4e081fcae": {"paper_id": "46f766c11df69808453e14c900bcb3f4e081fcae", "abstract": "In a digital library system, documents are available in digital form and therefore are more easily copied and their copyrights are more easily violated. This is a very serious problem, as it discourages owners of valuable information from sharing it with authorized users. There are two main philosophies for addressing this problem: prevention and detection. The former actually makes unauthorized use of documents difficult or impossible while the latter makes it easier to discover such activity.In this paper we propose a system for registering documents and then detecting copies, either complete copies or partial copies. We describe algorithms for such detection, and metrics required for evaluating detection mechanisms (covering accuracy, efficiency, and security). We also describe a working prototype, called COPS, describe implementation issues, and present experimental results that suggest the proper settings for copy detection parameters.", "title": "Copy Detection Mechanisms for Digital Documents"}, "547242ed248a57a726c212a307669aedadaa862e": {"paper_id": "547242ed248a57a726c212a307669aedadaa862e", "abstract": "The idea of Smart Grid has started to evolve more rapidly with the enhancement in Communication Technologies. Two way communication is a key aspect in realizing Smart Grids and is easily possible with the help of modern day advancements in both wired and wireless communication technologies. This paper discusses some of the major communication technologies which include IEEE specified ZigBee, WiMAX and Wireless LAN (Wi-Fi) technologies, GSM 3G/4G Cellular, DASH 7 and PLC (Power Line Communications), with special focus on their applications in Smart Grids. The Smart Grid environments and domains such as Home Area Automation, Substation Automation, Automated Metering Infrastructure, Vehicle-to-Grid Communications, etc. are considered as priority areas for developing smarter grids. The advancements, challenges and the opportunities present in these priority areas are discussed in this paper. & 2012 Elsevier Ltd. All rights reserved.", "title": "Evolution of Communication Technologies for Smart Grid applications"}, "a73389f1e3a1dc2ddf6922c433062db57970038c": {"paper_id": "a73389f1e3a1dc2ddf6922c433062db57970038c", "abstract": "A 6\u201318 GHz monolithic microwave integrated circuit (MMIC) power amplifier (PA) was successfully developed using a quarter wavelength short stub and a monolithic broadband coupler for a non-uniform distributed power amplifier (NDPA) topology. This topology improved the output power at 18 GHz and attained a flat output power profile over 6\u201318 GHz. It also achieved filtering characteristics for both lower and higher cut-off frequencies. A fabricated MMIC PA with 0.25\u00b5m GaN HEMTs delivered an output power of more than 10 W with average power added efficiency (PAE) of 18% over 6 to 18 GHz. To the best of our knowledge, this is the best combination of output power and bandwidth for any solid-state MMIC amplifier operating up to the full Ku-band.", "title": "Over 10W C-Ku band GaN MMIC non-uniform distributed power amplifier with broadband couplers"}, "23d6454604d33fc1be3db6ef2008a5910b1fbee0": {"paper_id": "23d6454604d33fc1be3db6ef2008a5910b1fbee0", "abstract": "We present some new results on the nonparametric estimation of entropy and mutual information. First, we use an exact local expansion of the entropy function to prove almost sure consistency and central limit theorems for three of the most commonly used discretized information estimators. The setup is related to Grenander's method of sieves and places no assumptions on the underlying probability measure generating the data. Second, we prove a converse to these consistency theorems, demonstrating that a misapplication of the most common estimation techniques leads to an arbitrarily poor estimate of the true information, even given unlimited data. This inconsistency theorem leads to an analytical approximation of the bias, valid in surprisingly small sample regimes and more accurate than the usual formula of Miller and Madow over a large region of parameter space. The two most practical implications of these results are negative: (1) information estimates in a certain data regime are likely contaminated by bias, even if bias-corrected estimators are used, and (2) confidence intervals calculated by standard techniques drastically underestimate the error of the most common estimation methods. Finally, we note a very useful connection between the bias of entropy estimators and a certain polynomial approximation problem. By casting bias calculation problems in this approximation theory framework, we obtain the best possible generalization of known asymptotic bias results. More interesting, this framework leads to an estimator with some nice properties: the estimator comes equipped with rigorous bounds on the maximum error over all possible underlying probability distributions, and this maximum error turns out to be surprisingly small. We demonstrate the application of this new estimator on both real and simulated data.", "title": "Estimation of Entropy and Mutual Information"}, "899b8bac810d3fc50e59425a3b6d7faf96470895": {"paper_id": "899b8bac810d3fc50e59425a3b6d7faf96470895", "abstract": "The numerical solution of nonlinear integral equations involves the iterative soIutioon of finite systems of nonlinear algebraic or transcendental equations. Certain corwent i o n a l techniqucs for treating such systems are reviewed in the context of a particular class of n o n l i n e a r equations. A procedure is synthesized to offset some of the disadvantages of these t e c h n i q u e s in this context; however, the procedure is not restricted to this pt~rticular class of s y s t e m s of nonlinear equations.", "title": "Iterative Procedures for Nonlinear Integral Equations"}, "61c77387a71f31a7c94f7e1ba07e2890929a750c": {"paper_id": "61c77387a71f31a7c94f7e1ba07e2890929a750c", "abstract": "Brain tumor is an uncontrolled growth of tissues in human brain. This tumor, when turns in to cancer become life-threatening. So medical imaging, it is necessary to detect the exact location of tumor and its type. For locating tumor in magnetic resonance image (MRI) segmentation of MRI plays an important role. This paper includes survey on different segmentation techniques applied to MR Images for locating tumor. It also includes a proposed method for the same using Fuzzy C-Means algorithm and an algorithm to find area of tumor which is usefull to decide type of brain tumor.", "title": "Detection and Identification of Brain Tumor in Brain MR Images Using Fuzzy C-Means Segmentation"}, "7b53f5aaa2dde43f27863d9e5eeda74a5a0b4d78": {"paper_id": "7b53f5aaa2dde43f27863d9e5eeda74a5a0b4d78", "abstract": "Brain segmentation is an important part of medical image processing. Most commonly, it aims at locating various lesions and pathologies inside the human brain. In this paper, a new brain segmentation algorithm is proposed. The method is seeded region growing approach which was developed to segment the area of the brain affected by a tumor. The proposed algorithm was described. Results of testing the developed method on real MRI data set are presented and discussed.", "title": "Brain tumor segmentation from MRI data sets using region growing approach"}, "64bda8b75fcb70e60efc6816888fa7a0368f2c96": {"paper_id": "64bda8b75fcb70e60efc6816888fa7a0368f2c96", "abstract": "This paper describes a framework for automatic brain tumor segmentation from MR images. The detection of edema is done simultaneously with tumor segmentation, as the knowledge of the extent of edema is important for diagnosis, planning, and treatment. Whereas many other tumor segmentation methods rely on the intensity enhancement produced by the gadolinium contrast agent in the T1-weighted image, the method proposed here does not require contrast enhanced image channels. The only required input for the segmentation procedure is the T2 MR image channel, but it can make use of any additional non-enhanced image channels for improved tissue segmentation. The segmentation framework is composed of three stages. First, we detect abnormal regions using a registered brain atlas as a model for healthy brains. We then make use of the robust estimates of the location and dispersion of the normal brain tissue intensity clusters to determine the intensity properties of the different tissue types. In the second stage, we determine from the T2 image intensities whether edema appears together with tumor in the abnormal regions. Finally, we apply geometric and spatial constraints to the detected tumor and edema regions. The segmentation procedure has been applied to three real datasets, representing different tumor shapes, locations, sizes, image intensities, and enhancement.", "title": "A brain tumor segmentation framework based on outlier detection"}, "48063bfcb7b3f62edcd135ab07b5cdc4ea8eb94a": {"paper_id": "48063bfcb7b3f62edcd135ab07b5cdc4ea8eb94a", "abstract": "Image segmentation is an indispensable process in the visualization of human tissues, particularly during clinical analysis of magnetic resonance (MR) images. Unfortunately, MR images always contain a significant amount of noise caused by operator performance, equipment, and the environment, which can lead to serious inaccuracies with segmentation. A robust segmentation technique based on an extension to the traditional fuzzy c-means (FCM) clustering algorithm is proposed in this paper. A neighborhood attraction, which is dependent on the relative location and features of neighboring pixels, is shown to improve the segmentation performance dramatically. The degree of attraction is optimized by a neural-network model. Simulated and real brain MR images with different noise levels are segmented to demonstrate the superiority of the proposed technique compared to other FCM-based methods. This segmentation method is a key component of an MR image-based classification system for brain tumors, currently being developed.", "title": "MRI fuzzy segmentation of brain tissue using neighborhood attraction with neural-network optimization"}, "68dd0001849efd875d5a865c2c0962f9cc2efc75": {"paper_id": "68dd0001849efd875d5a865c2c0962f9cc2efc75", "abstract": "An automated brain tumor segmentation method was developed and validated against manual segmentation with three-dimensional magnetic resonance images in 20 patients with meningiomas and low-grade gliomas. The automated method (operator time, 5-10 minutes) allowed rapid identification of brain and tumor tissue with an accuracy and reproducibility comparable to those of manual segmentation (operator time, 3-5 hours), making automated segmentation practical for low-grade gliomas and meningiomas.", "title": "Automated segmentation of MR images of brain tumors."}, "8472ac0c9f4f626765738ddeb102eb6cc598bc31": {"paper_id": "8472ac0c9f4f626765738ddeb102eb6cc598bc31", "abstract": "We present a novel depth image enhancement approach for RGB-D cameras such as the Kinect. Our approach employs optical flow of color images for refining the quality of corresponding depth images. We track every depth pixel over a sequence of frames in the temporal domain and use valid depth values of the same point for recovering missing and inaccurate information. We conduct experiments on different test datasets and present visually appealing results. Our method significantly reduces the temporal noise level and the flickering artifacts.", "title": "Temporal Filtering of Depth Images using Optical Flow"}, "a9230c7d4e800c1ee300819cc5ce2aaed3d82ffd": {"paper_id": "a9230c7d4e800c1ee300819cc5ce2aaed3d82ffd", "abstract": "We propose a method of filtering depth maps provided by Kinect depth camera. Filter uses output of the conventional Kinect camera along with the depth sensor to improve the temporal stability of the depth map and fill occlusion areas. To filter input depth map, the algorithm uses the information about motion and color of objects from the video. The proposed method can be applied as a preprocessing stage before using Kinect output data.", "title": "Temporal filtering for depth maps generated by Kinect depth camera"}, "6c316c08a7912f6427c27edbfbd2f53cbbad50a5": {"paper_id": "6c316c08a7912f6427c27edbfbd2f53cbbad50a5", "abstract": "Time-of-flight range sensors have error characteristics which are complementary to passive stereo. They provide real time depth estimates in conditions where passive stereo does not work well, such as on white walls. In contrast, these sensors are noisy and often perform poorly on the textured scenes for which stereo excels. We introduce a method for combining the results from both methods that performs better than either alone. A depth probability distribution function from each method is calculated and then merged. In addition, stereo methods have long used global methods such as belief propagation and graph cuts to improve results, and we apply these methods to this sensor. Since time-of-flight devices have primarily been used as individual sensors, they are typically poorly calibrated. We introduce a method that substantially improves upon the manufacturerpsilas calibration. We show that these techniques lead to improved accuracy and robustness.", "title": "Fusion of time-of-flight depth and stereo for high accuracy depth maps"}, "4b6afa2aaec1b7c193426d713eeeea8375020d18": {"paper_id": "4b6afa2aaec1b7c193426d713eeeea8375020d18", "abstract": "A depth image-based rendering (DIBR) technique is one of the rendering processes of virtual views with a color image and the corresponding depth map. The most important issue of DIBR is that the virtual view has no information at newly exposed areas, so called disocclusion. The general solution is to smooth the depth map using a Gaussian smoothing filter before 3D warping. However, the filtered depth map causes geometric distortion and the depth quality is seriously degraded. Therefore, we propose a new depth map filtering algorithm to solve the disocclusion problem while maintaining the depth quality. In order to preserve the visual quality of the virtual view, we smooth the depth map with further reduced deformation. After extracting object boundaries depending on the position of the virtual view, we apply a discontinuity-adaptive smoothing filter according to the distance of the object boundary and the amount of depth discontinuities. Finally, we obtain the depth map with higher quality compared to other methods. Experimental results showed that the disocclusion is efficiently removed and the visual quality of the virtual view is maintained.", "title": "Discontinuity-adaptive depth map filtering for 3D view generation"}, "c392b34431cc1cbf26aa972a3776d9dc7d786b0b": {"paper_id": "c392b34431cc1cbf26aa972a3776d9dc7d786b0b", "abstract": "Forty years ago, May proved that sufficiently large or complex ecological networks have a probability of persisting that is close to zero, contrary to previous expectations. May analysed large networks in which species interact at random. However, in natural systems pairs of species have well-defined interactions (for example predator\u2013prey, mutualistic or competitive). Here we extend May\u2019s results to these relationships and find remarkable differences between predator\u2013prey interactions, which are stabilizing, and mutualistic and competitive interactions, which are destabilizing. We provide analytic stability criteria for all cases. We use the criteria to prove that, counterintuitively, the probability of stability for predator\u2013prey networks decreases when a realistic food web structure is imposed or if there is a large preponderance of weak interactions. Similarly, stability is negatively affected by nestedness in bipartite mutualistic networks. These results are found by separating the contribution of network structure and interaction strengths to stability. Stable predator\u2013prey networks can be arbitrarily large and complex, provided that predator\u2013prey pairs are tightly coupled. The stability criteria are widely applicable, because they hold for any system of differential equations.", "title": "Stability criteria for complex ecosystems"}, "1cc0183d8fbef098d29b6b5f621745ff099f6c6c": {"paper_id": "1cc0183d8fbef098d29b6b5f621745ff099f6c6c", "abstract": "Many human activities involve object manipulations aiming to modify the object state. Examples of common state changes include full/empty bottle, open/closed door, and attached/detached car wheel. In this work, we seek to automatically discover the states of objects and the associated manipulation actions. Given a set of videos for a particular task, we propose a joint model that learns to identify object states and to localize state-modifying actions. Our model is formulated as a discriminative clustering cost with constraints. We assume a consistent temporal order for the changes in object states and manipulation actions, and introduce new optimization techniques to learn model parameters without additional supervision. We demonstrate successful discovery of seven manipulation actions and corresponding object states on a new dataset of videos depicting real-life object manipulations. We show that our joint formulation results in an improvement of object state discovery by action recognition and vice versa.", "title": "Joint Discovery of Object States and Manipulation Actions"}, "66e6f08873325d37e0ec20a4769ce881e04e964e": {"paper_id": "66e6f08873325d37e0ec20a4769ce881e04e964e", "abstract": "In this paper we present the first large-scale scene attribute database. First, we perform crowdsourced human studies to find a taxonomy of 102 discriminative attributes. We discover attributes related to materials, surface properties, lighting, affordances, and spatial layout. Next, we build the \u201cSUN attribute database\u201d on top of the diverse SUN categorical database. We use crowdsourcing to annotate attributes for 14,340 images from 707 scene categories. We perform numerous experiments to study the interplay between scene attributes and scene categories. We train and evaluate attribute classifiers and then study the feasibility of attributes as an intermediate scene representation for scene classification, zero shot learning, automatic image captioning, semantic image search, and parsing natural images. We show that when used as features for these tasks, low dimensional scene attributes can compete with or improve on the state of the art performance. The experiments suggest that scene attributes are an effective low-dimensional feature for capturing high-level context and semantics in scenes.", "title": "The SUN Attribute Database: Beyond Categories for Deeper Scene Understanding"}, "856c09ab10efbc8c61a84a951746654d947370f3": {"paper_id": "856c09ab10efbc8c61a84a951746654d947370f3", "abstract": "In this work, we propose to use attributes and parts for recognizing human actions in still images. We define action attributes as the verbs that describe the properties of human actions, while the parts of actions are objects and poselets that are closely related to the actions. We jointly model the attributes and parts by learning a set of sparse bases that are shown to carry much semantic meaning. Then, the attributes and parts of an action image can be reconstructed from sparse coefficients with respect to the learned bases. This dual sparsity provides theoretical guarantee of our bases learning and feature reconstruction approach. On the PASCAL action dataset and a new \u201cStanford 40 Actions\u201d dataset, we show that our method extracts meaningful high-order interactions between attributes and parts in human actions while achieving state-of-the-art classification performance.", "title": "Human action recognition by learning bases of action attributes and parts"}, "19d3b02185ad36fb0b792f2a15a027c58ac91e8e": {"paper_id": "19d3b02185ad36fb0b792f2a15a027c58ac91e8e", "abstract": "We develop and demonstrate automatic image description methods using a large captioned photo collection. One contribution is our technique for the automatic collection of this new dataset \u2013 performing a huge number of Flickr queries and then filtering the noisy results down to 1 million images with associated visually relevant captions. Such a collection allows us to approach the extremely challenging problem of description generation using relatively simple non-parametric methods and produces surprisingly effective results. We also develop methods incorporating many state of the art, but fairly noisy, estimates of image content to produce even more pleasing results. Finally we introduce a new objective performance measure for image captioning.", "title": "Im2Text: Describing Images Using 1 Million Captioned Photographs"}, "2bcd9b2b78eb353ea57cf50387083900eae5384a": {"paper_id": "2bcd9b2b78eb353ea57cf50387083900eae5384a", "abstract": "We propose a novel approach for ranking and retrieval of images based on multi-attribute queries. Existing image retrieval methods train separate classifiers for each word and heuristically combine their outputs for retrieving multiword queries. Moreover, these approaches also ignore the interdependencies among the query terms. In contrast, we propose a principled approach for multi-attribute retrieval which explicitly models the correlations that are present between the attributes. Given a multi-attribute query, we also utilize other attributes in the vocabulary which are not present in the query, for ranking/retrieval. Furthermore, we integrate ranking and retrieval within the same formulation, by posing them as structured prediction problems. Extensive experimental evaluation on the Labeled Faces in the Wild(LFW), FaceTracer and PASCAL VOC datasets show that our approach significantly outperforms several state-of-the-art ranking and retrieval methods.", "title": "Image ranking and retrieval based on multi-attribute queries"}, "13207dbc317041f43c4fc10b127cc27e26cfcf17": {"paper_id": "13207dbc317041f43c4fc10b127cc27e26cfcf17", "abstract": "We address the problem of automatically learning the main steps to complete a certain task, such as changing a car tire, from a set of narrated instruction videos. The contributions of this paper are three-fold. First, we develop a new unsupervised learning approach that takes advantage of the complementary nature of the input video and the associated narration. The method solves two clustering problems, one in text and one in video, applied one after each other and linked by joint constraints to obtain a single coherent sequence of steps in both modalities. Second, we collect and annotate a new challenging dataset of real-world instruction videos from the Internet. The dataset contains about 800,000 frames for five different tasks1 that include complex interactions between people and objects, and are captured in a variety of indoor and outdoor settings. Third, we experimentally demonstrate that the proposed method can automatically discover, in an unsupervised manner, the main steps to achieve the task and locate the steps in the input videos.", "title": "Unsupervised Learning from Narrated Instruction Videos"}, "21cbb7e58e8f3e5912b32df3e75b004e5a0c00cc": {"paper_id": "21cbb7e58e8f3e5912b32df3e75b004e5a0c00cc", "abstract": "This paper addresses the problem of automatic temporal annotation of realistic human actions in video using minimal manual supervision. To this end we consider two associated problems: (a) weakly-supervised learning of action models from readily available annotations, and (b) temporal localization of human actions in test videos. To avoid the prohibitive cost of manual annotation for training, we use movie scripts as a means of weak supervision. Scripts, however, provide only implicit, noisy, and imprecise information about the type and location of actions in video. We address this problem with a kernel-based discriminative clustering algorithm that locates actions in the weakly-labeled training data. Using the obtained action samples, we train temporal action detectors and apply them to locate actions in the raw video data. Our experiments demonstrate that the proposed method for weakly-supervised learning of action models leads to significant improvement in action detection. We present detection results for three action classes in four feature length movies with challenging and realistic video data.", "title": "Automatic annotation of human actions in video"}, "02431ed90700d5cfe4e3d3a20f1e97de3e131569": {"paper_id": "02431ed90700d5cfe4e3d3a20f1e97de3e131569", "abstract": "We address the problem of learning a joint model of actors and actions in movies using weak supervision provided by scripts. Specifically, we extract actor/action pairs from the script and use them as constraints in a discriminative clustering framework. The corresponding optimization problem is formulated as a quadratic program under linear constraints. People in video are represented by automatically extracted and tracked faces together with corresponding motion features. First, we apply the proposed framework to the task of learning names of characters in the movie and demonstrate significant improvements over previous methods used for this task. Second, we explore the joint actor/action constraint and show its advantage for weakly supervised action learning. We validate our method in the challenging setting of localizing and recognizing characters and their actions in feature length movies Casablanca and American Beauty.", "title": "Finding Actors and Actions in Movies"}, "20faa2ef4bb4e84b1d68750cda28d0a45fb16075": {"paper_id": "20faa2ef4bb4e84b1d68750cda28d0a45fb16075", "abstract": "Time series clustering has been shown effective in providing useful information in various domains. There seems to be an increased interest in time series clustering as part of the effort in temporal data mining research. To provide an overview, this paper surveys and summarizes previous works that investigated the clustering of time series data in various application domains. The basics of time series clustering are presented, including general-purpose clustering algorithms commonly used in time series clustering studies, the criteria for evaluating the performance of the clustering results, and the measures to determine the similarity/dissimilarity between two time series being compared, either in the forms of raw data, extracted features, or some model parameters. The past researchs are organized into three groups depending upon whether they work directly with the raw data either in the time or frequency domain, indirectly with features extracted from the raw data, or indirectly with models built from the raw data. The uniqueness and limitation of previous research are discussed and several possible topics for future research are identified. Moreover, the areas that time series clustering have been applied to are also summarized, including the sources of data used. It is hoped that this review will serve as the steppingstone for those interested in advancing this area of research. 2005 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.", "title": "Clustering of time series data - a survey"}, "74f61af390292fc197659ae698429df4a2de62df": {"paper_id": "74f61af390292fc197659ae698429df4a2de62df", "abstract": "Hand-coded scripts were used in the 1970-80s as knowledge backbones that enabled inference and other NLP tasks requiring deep semantic knowledge. We propose unsupervised induction of similar schemata called narrative event chains from raw newswire text. A narrative event chain is a partially ordered set of events related by a common protagonist. We describe a three step process to learning narrative event chains. The first uses unsupervised distributional methods to learn narrative relations between events sharing coreferring arguments. The second applies a temporal classifier to partially order the connected events. Finally, the third prunes and clusters self-contained chains from the space of events. We introduce two evaluations: the narrative cloze to evaluate event relatedness, and an order coherence task to evaluate narrative order. We show a 36% improvement over baseline for narrative prediction and 25% for temporal coherence.", "title": "Unsupervised Learning of Narrative Event Chains"}, "32a437ecdc20cc76c01e37c08920317ea18fde50": {"paper_id": "32a437ecdc20cc76c01e37c08920317ea18fde50", "abstract": "This paper describes a system for extracting typed dependency parses of English sentences from phrase structure parses. In order to capture inherent relations occurring in corpus texts that can be critical in real-world applications, many NP relations are included in the set of grammatical relations used. We provide a comparison of our system with Minipar and the Link parser. The typed dependency extraction facility described here is integrated in the Stanford Parser, available for download.", "title": "Generating Typed Dependency Parses from Phrase Structure Parses"}, "5443a1b18fed3173dc426735ff9f486194185172": {"paper_id": "5443a1b18fed3173dc426735ff9f486194185172", "abstract": "In this paper we present a method to capture video-wide temporal information for action recognition. We postulate that a function capable of ordering the frames of a video temporally (based on the appearance) captures well the evolution of the appearance within the video. We learn such ranking functions per video via a ranking machine and use the parameters of these as a new video representation. The proposed method is easy to interpret and implement, fast to compute and effective in recognizing a wide variety of actions. We perform a large number of evaluations on datasets for generic action recognition (Hollywood2 and HMDB51), fine-grained actions (MPII- cooking activities) and gestures (Chalearn). Results show that the proposed method brings an absolute improvement of 7-10%, while being compatible with and complementary to further improvements in appearance and local motion based methods.", "title": "Modeling video evolution for action recognition"}, "2c953b06c1c312e36f1fdb9919567b42c9322384": {"paper_id": "2c953b06c1c312e36f1fdb9919567b42c9322384", "abstract": "This paper proposes a conceptually simple but surprisingly powerful method which combines the effectiveness of a discriminative object detector with the explicit correspondence offered by a nearest-neighbor approach. The method is based on training a separate linear SVM classifier for every exemplar in the training set. Each of these Exemplar-SVMs is thus defined by a single positive instance and millions of negatives. While each detector is quite specific to its exemplar, we empirically observe that an ensemble of such Exemplar-SVMs offers surprisingly good generalization. Our performance on the PASCAL VOC detection task is on par with the much more complex latent part-based model of Felzenszwalb et al., at only a modest computational cost increase. But the central benefit of our approach is that it creates an explicit association between each detection and a single training exemplar. Because most detections show good alignment to their associated exemplar, it is possible to transfer any available exemplar meta-data (segmentation, geometric structure, 3D model, etc.) directly onto the detections, which can then be used as part of overall scene understanding.", "title": "Ensemble of exemplar-SVMs for object detection and beyond"}, "00cf965e32f89e475e076aab5db97c8b3b36fa63": {"paper_id": "00cf965e32f89e475e076aab5db97c8b3b36fa63", "abstract": "In this tutorial we give an overview of the basic ideas underlying Support Vector (SV) machines for function estimation. Furthermore, we include a summary of currently used algorithms for training SV machines, covering both the quadratic (or convex) programming part and advanced methods for dealing with large datasets. Finally, we mention somemodifications and extensions that have been applied to the standard SV algorithm, and discuss the aspect of regularization from a SV perspective.", "title": "A tutorial on support vector regression"}, "0799576a9c46b21f605fee978d86e044fe411b92": {"paper_id": "0799576a9c46b21f605fee978d86e044fe411b92", "abstract": "Representation of video is a vital problem in action recognition. This paper proposes Stacked Fisher Vectors (SFV), a new representation with multi-layer nested Fisher vector encoding, for action recognition. In the first layer, we densely sample large subvolumes from input videos, extract local features, and encode them using Fisher vectors (FVs). The second layer compresses the FVs of subvolumes obtained in previous layer, and then encodes them again with Fisher vectors. Compared with standard FV, SFV allows refining the representation and abstracting semantic information in a hierarchical way. Compared with recent mid-level based action representations, SFV need not to mine discriminative action parts but can preserve mid-level information through Fisher vector encoding in higher layer. We evaluate the proposed methods on three challenging datasets, namely Youtube, J-HMDB, and HMDB51. Experimental results demonstrate the effectiveness of SFV, and the combination of the traditional FV and SFV outperforms stateof-the-art methods on these datasets with a large margin.", "title": "Action Recognition with Stacked Fisher Vectors"}, "b91625e2ee9c041a2569b8dde6f938639b3ddf18": {"paper_id": "b91625e2ee9c041a2569b8dde6f938639b3ddf18", "abstract": "In fine-grained action (object manipulation) recognition, it is important to encode object semantic (contextual) information, i.e., which object is being manipulated and how it is being operated. However, previous methods for action recognition often represent the semantic information in a global and coarse way and therefore cannot cope with fine-grained actions. In this work, we propose a representation and classification pipeline which seamlessly incorporates localized semantic information into every processing step for fine-grained action recognition. In the feature extraction stage, we explore the geometric information between local motion features and the surrounding objects. In the feature encoding stage, we develop a semantic-grouped locality-constrained linear coding (SG-LLC) method that captures the joint distributions between motion and object-in-use information. Finally, we propose a semantic-aware multiple kernel learning framework (SA-MKL) by utilizing the empirical joint distribution between action and object type for more discriminative action classification. Extensive experiments are performed on the large-scale and difficult fine-grained MPII cooking action dataset. The results show that by effectively accumulating localized semantic information into the action representation and classification pipeline, we significantly improve the fine-grained action classification performance over the existing methods.", "title": "Pipelining Localized Semantic Features for Fine-Grained Action Recognition"}, "9f0663bd8f44d9c0811aa2447455fa334445479b": {"paper_id": "9f0663bd8f44d9c0811aa2447455fa334445479b", "abstract": "Fog computing based radio access network is a promising paradigm for the fifth generation wireless communication system to provide high spectral and energy efficiency. With the help of the new designed fog computing based access points (F-APs), the user-centric objectives can be achieved through the adaptive technique and will relieve the load of fronthaul and alleviate the burden of base band unit pool. In this paper, we derive the coverage probability and ergodic rate for both F-AP users and device-to-device users by taking into account the different nodes locations, cache sizes as well as user access modes. Particularly, the stochastic geometry tool is used to derive expressions for above performance metrics. Simulation results validate the accuracy of our analysis and we obtain interesting tradeoffs that depend on the effect of the cache size, user node density, and the quality of service constrains on the different performance metrics.", "title": "User access mode selection in fog computing based radio access networks"}, "0bfc3626485953e2d3f87854a00a50f88c62269d": {"paper_id": "0bfc3626485953e2d3f87854a00a50f88c62269d", "abstract": "Cellular networks are usually modeled by placing the base stations on a grid, with mobile users either randomly scattered or placed deterministically. These models have been used extensively but suffer from being both highly idealized and not very tractable, so complex system-level simulations are used to evaluate coverage/outage probability and rate. More tractable models have long been desirable. We develop new general models for the multi-cell signal-to-interference-plus-noise ratio (SINR) using stochastic geometry. Under very general assumptions, the resulting expressions for the downlink SINR CCDF (equivalent to the coverage probability) involve quickly computable integrals, and in some practical special cases can be simplified to common integrals (e.g., the Q-function) or even to simple closed-form expressions. We also derive the mean rate, and then the coverage gain (and mean rate loss) from static frequency reuse. We compare our coverage predictions to the grid model and an actual base station deployment, and observe that the proposed model is pessimistic (a lower bound on coverage) whereas the grid model is optimistic, and that both are about equally accurate. In addition to being more tractable, the proposed model may better capture the increasingly opportunistic and dense placement of base stations in future networks.", "title": "A Tractable Approach to Coverage and Rate in Cellular Networks"}, "e7b4ea66dff3966fc9da581f32cb69132a7bbd99": {"paper_id": "e7b4ea66dff3966fc9da581f32cb69132a7bbd99", "abstract": "The deployment of femtocells in a macrocell network is an economical and effective way to increase network capacity and coverage. Nevertheless, such deployment is challenging due to the presence of inter-tier and intra-tier interference, and the ad hoc operation of femtocells. Motivated by the flexible subchannel allocation capability of OFDMA, we investigate the effect of spectrum allocation in two-tier networks, where the macrocells employ closed access policy and the femtocells can operate in either open or closed access. By introducing a tractable model, we derive the success probability for each tier under different spectrum allocation and femtocell access policies. In particular, we consider joint subchannel allocation, in which the whole spectrum is shared by both tiers, as well as disjoint subchannel allocation, whereby disjoint sets of subchannels are assigned to both tiers. We formulate the throughput maximization problem subject to quality of service constraints in terms of success probabilities and per-tier minimum rates, and provide insights into the optimal spectrum allocation. Our results indicate that with closed access femtocells, the optimized joint and disjoint subchannel allocations provide the highest throughput among all schemes in sparse and dense femtocell networks, respectively. With open access femtocells, the optimized joint subchannel allocation provides the highest possible throughput for all femtocell densities.", "title": "Throughput Optimization, Spectrum Allocation, and Access Control in Two-Tier Femtocell Networks"}, "09168f7259e0df1484115bfd44ce4fdcafdc15f7": {"paper_id": "09168f7259e0df1484115bfd44ce4fdcafdc15f7", "abstract": "In a two tier cellular network - comprised of a central macrocell underlaid with shorter range femtocell hotspots - cross-tier interference limits overall capacity with universal frequency reuse. To quantify near-far effects with universal frequency reuse, this paper derives a fundamental relation providing the largest feasible cellular Signal-to-Interference-Plus-Noise Ratio (SINR), given any set of feasible femtocell SINRs. We provide a link budget analysis which enables simple and accurate performance insights in a two-tier network. A distributed utility- based SINR adaptation at femtocells is proposed in order to alleviate cross-tier interference at the macrocell from cochannel femtocells. The Foschini-Miljanic (FM) algorithm is a special case of the adaptation. Each femtocell maximizes their individual utility consisting of a SINR based reward less an incurred cost (interference to the macrocell). Numerical results show greater than 30% improvement in mean femtocell SINRs relative to FM. In the event that cross-tier interference prevents a cellular user from obtaining its SINR target, an algorithm is proposed that reduces transmission powers of the strongest femtocell interferers. The algorithm ensures that a cellular user achieves its SINR target even with 100 femtocells/cell-site (with typical cellular parameters) and requires a worst case SINR reduction of only 16% at femtocells. These results motivate design of power control schemes requiring minimal network overhead in two-tier networks with shared spectrum.", "title": "Power control in two-tier femtocell networks"}, "94eb2c993a5baae8bfba2db02ff8ecb0108dae5a": {"paper_id": "94eb2c993a5baae8bfba2db02ff8ecb0108dae5a", "abstract": "Due to its central location, the nose plays a prominent role in facial aesthetics. As tastes have shifted and techniques have advanced, the accepted \"ideal\" appearance and proportions of the nose have evolved over time. By assessing the aesthetics of the nasal dorsum through the use of lines and angles, one can more precisely elucidate a goal for the patient's postoperative nasal shape, which should, in turn, guide the surgeon to execute specific operative maneuvers needed to achieve that contour. In assessing the aesthetics of the nasal dorsum, practitioners calculate and observe aspects such as the paired dorsal aesthetic lines, the nasofrontal angle, and the nasofacial angle. There is also additional consideration given to nasal tip position as this must fit harmoniously with the shape of the dorsum. In contrast to the established aesthetic lines and angles, using nasal geometric polygons for the aesthetic evaluation and development of operative goals in rhinoplasty has recently been described in the literature. Constructed ideals, in the form of proportions, lines, and angles, should be used with caution, as there are many factors to consider in the aesthetic analysis of the nasal dorsum, including ethnic differences, and subjective and changing views of beauty.", "title": "Aesthetics of the Nasal Dorsum: Proportions, Light, and Shadow."}, "5eb552c3b282227b6abd7745c666526c7b7a0dd2": {"paper_id": "5eb552c3b282227b6abd7745c666526c7b7a0dd2", "abstract": "BACKGROUND\nThe author has used diced cartilage grafts in nasal surgery for more than 30 years. However, the number of cases and the variety of techniques have increased dramatically over the past 6 years.\n\n\nMETHODS\nThe author uses three methods of diced cartilage in rhinoplasty surgery: diced cartilage, diced cartilage wrapped in fascia (with fascia then sewn closed), and diced cartilage covered with fascia (with the recipient area covered with fascia after graft placement). The constructs are highly varied to fit the specific defect. Pieces of diced cartilage without any fascia can be placed in pockets in the peripyriform area, radix, or alongside structural rib grafts.\n\n\nRESULTS\nOver a 2-year period, the author treated 546 rhinoplasty cases in which 79 patients (14 percent) had a total of 91 diced cartilage grafts. There were 34 primary and 45 secondary operations involving the radix (n = 11), half-length grafts (n = 14), full-length dorsum (n = 43), peripyriform (n = 16), infralobule (n = 4), and lateral wall (n = 3). All charts were reviewed for the 256 rhinoplasties performed in 2006 of which 30 patients had 35 diced cartilage grafts. With a median follow-up of 19 months (range, 13 to 25 months), two patients had had revisions unrelated to their diced cartilage grafts. The three most common technical problems were overcorrection, visibility, and junctional step-offs.\n\n\nCONCLUSIONS\nDiced cartilage grafts are a valuable addition to rhinoplasty surgery. They are highly flexible and useful throughout the nose. Their use simplifies one of the greatest challenges in all of rhinoplasty--dorsal augmentation. Complications have been relatively minor and their correction relatively simple.", "title": "Diced cartilage grafts in rhinoplasty surgery: current techniques and applications."}, "1db3bd04093568bd2db822ad69946a9cde0bfacd": {"paper_id": "1db3bd04093568bd2db822ad69946a9cde0bfacd", "abstract": "Clustering is one of the most important techniques of data mining. Clustering technique in data mining is an unsupervised machine learning algorithm that finds the groups of object such that objects in one group will be similar to one another and are dissimilar to the objects belonging to other clusters. Clustering is called unsupervised machine learning algorithm as groups are not predefined but defined by the data. So the most similar data are grouped into the clusters. In this paper, we compare five clustering algorithm namely Farthest first, MakeDensityBasedClusterer, Simple K-means, EM, Hierarchical clustering algorithm for recommending the course to the student based on student course selection & present the result. According to our simulation, we find that Simple K-means works better than other algorithms.", "title": "A Comparative Study for Selecting the Best Unsupervised Learning Algorithm in E-Learning System"}, "0718da5ecdc5d34a42c930288b72072f6f380c46": {"paper_id": "0718da5ecdc5d34a42c930288b72072f6f380c46", "abstract": "The complex methodology of investigations was applied to study a movement structure on bench press. We have checked the usefulness of multimodular measuring system (SMART-E, BTS company, Italy) and a special device for tracking the position of barbell (pantograph). Software Smart Analyser was used to create a database allowing chosen parameters to be compared. The results from different measuring devices are very similar, therefore the replacement of many devices by one multimodular system is reasonable. In our study, the effect of increased barbell load on the values of muscles activity and bar kinematics during the flat bench press movement was clearly visible. The greater the weight of a barbell, the greater the myoactivity of shoulder muscles and vertical velocity of the bar. It was also confirmed the presence of the so-called sticking point (period) during the concentric phase of the bench press. In this study, the initial velocity of the barbell decreased (v(min)) not only under submaximal and maximal loads (90 and 100% of the one repetition maximum; 1-RM), but also under slightly lighter weights (70 and 80% of 1-RM).", "title": "Complex analysis of movement in evaluation of flat bench press performance."}, "14b9b036dd56e4798b3215d1936842a19bb8d541": {"paper_id": "14b9b036dd56e4798b3215d1936842a19bb8d541", "abstract": "Time series data are an ubiquitous and important data source in many domains. Most companies and organizations rely on this data for critical tasks like decision-making, planning, and analytics in general. Usually, all these tasks focus on actual data representing organization and business processes. In order to assess the robustness of current systems and methods, it is also desirable to focus on time-series scenarios which represent specific time-series features. This work presents a generally applicable and easy-to-use method for the feature-driven generation of time series data. Our approach extracts descriptive features of a data set and allows the construction of a specific version by means of the modification of these features.", "title": "Feature-driven Time Series Generation"}, "62405c029dd046e509ca577cc3d47fe6484381b3": {"paper_id": "62405c029dd046e509ca577cc3d47fe6484381b3", "abstract": "In this paper, we provide a complete study on the training based channel estimation issues for relay networks that employ the amplify-and-forward (AF) transmission scheme. We first point out that separately estimating the channel from source to relay and relay to destination suffers from many drawbacks. Then we provide a new estimation scheme that directly estimates the overall channels from the source to the destination. The proposed channel estimation well serves the AF based space time coding (STC) that was recently developed. There exists many differences between the proposed channel estimation and that in the traditional single input single out (SISO) and multiple input single output (MISO) systems. For example, a relay must linearly precode its received training sequence by a sophisticatedly designed matrix in order to minimize the channel estimation error. Besides, each relay node is individually constrained by a different power requirement because of the non-cooperation among all relay nodes. We study both the linear least-square (LS) estimator and the minimum mean-square-error (MMSE) estimator. The corresponding optimal training sequences, as well as the optimal preceding matrices are derived from an efficient convex optimization process.", "title": "On channel estimation and optimal training design for amplify and forward relay networks"}, "a11cef51b38c071a4f57d5454b79b7dc5d6c8e96": {"paper_id": "a11cef51b38c071a4f57d5454b79b7dc5d6c8e96", "abstract": "A relay channel consists of an input x,, a relay output yl, a cJmnnel output y, and a relay sender x2 (whose trasmission is allowed to depend on the past symbols y,). l%e dependence of the received symbols upm the inpnts is given by p(y,y,lx,,x,). \u2018l%e channel is assumed to be memoryless. In this paper the following capacity theorems are proved. 1) Ifyisadegnukdformofy,,the.n C-m=Phx2) min(l(X,,X,; Y),I(X,; Y,lX&). 2) Ify,isadegradedformofy,tben C==maxpcx,) m=JXI; Ylx2). 3) If p(y,yllx,,x2) is an arWnuy relay chaonel 4th feedback from Crud to both x1 and ~2, then C=maXpcx,.x*, min(W,,X*; Y)J(x,; y9 Y,lX3). 4) For a general relay channel, C< max+,,,d min(Z(Xl,X2; YMX,; K Y,lXJ. Superposition block Markov encoding is used to show achievabiiity of C, and converses are established. \u2018Ihe capacities of the Gaussian relay dumWI and certain discrete relay channels are evaluated. Finally, an acbievable lower bound to the capacity of the general relay channel is established. Manuscript received December 1, 1977; revised September 28, 1978. This work was partially supported by the National Science Foundation under Grant ENG76-03684, JSEP NooOl6-67-A-oOI2-oSdI, and Stanford Research Institute Contract DAHC-15-C-0187. This work was presented at the International Symposium on Information Theory, Grigano, Italy, June 25-27, 1979. T. M. Cover is with the Departments of Electrical Engineering and Statistics, Stanford University, Stanford, CA 94305. A. A. El Gamal is with the Department of Electrical Engineering Systems, University of Southern California, University Park, Los Angeles, CA 90007.", "title": "Capacity theorems for the relay channel"}, "007524794d49bebca5845722054e459a86d8b785": {"paper_id": "007524794d49bebca5845722054e459a86d8b785", "abstract": "Mobile users\u2019 data rate and quality of service are limited by the fact that, within the duration of any given call, they experience severe variations in signal attenuation, thereby necessitating the use of some type of diversity. In this two-part paper, we propose a new form of spatial diversity, in which diversity gains are achieved via the cooperation of mobile users. Part I describes the user cooperation strategy, while Part II focuses on implementation issues and performance analysis. Results show that, even though the interuser channel is noisy, cooperation leads not only to an increase in capacity for both users but also to a more robust system, where users\u2019 achievable rates are less susceptible to channel variations.", "title": "User cooperation diversity. Part I. System description"}, "5559e32a7f309c234f02adb9419c17bcd138cba8": {"paper_id": "5559e32a7f309c234f02adb9419c17bcd138cba8", "abstract": "In 1948 Shannon developed fundamental limits on the efficiency of communication over noisy channels. The coding theorem asserts that there are block codes with code rates arbitrarily close to channel capacity and probabilities of error arbitrarily close to zero. Fifty years later, codes for the Gaussian channel have been discovered that come close to these fundamental limits. There is now a substantial algebraic theory of error-correcting codes with as many connections to mathematics as to engineering practice, and the last 20 years have seen the construction of algebraic-geometry codes that can be encoded and decoded in polynomial time, and that beat the Gilbert\u2013Varshamov bound. Given the size of coding theory as a subject, this review is of necessity a personal perspective, and the focus is reliable communication, and not source coding or cryptography. The emphasis is on connecting coding theories for Hamming and Euclidean space and on future challenges, specifically in data networking, wireless communication, and quantum information theory.", "title": "The Art of Signaling: Fifty Years of Coding Theory"}, "8052c6fe35b5f5660fcfcf11edffedbeeabf0f60": {"paper_id": "8052c6fe35b5f5660fcfcf11edffedbeeabf0f60", "abstract": "Space-time codes (STC) are a class of signaling techniques, offering coding and diversity gains along with improved spectral efficiency. These codes exploit both the spatial and the temporal diversity of the wireless link by combining the design of the error correction code, modulation scheme and array processing. STC are well suited for improving the downlink performance, which is the bottleneck in asymmetric applications such as downstream Internet. Three original contributions to the area of STC are presented in this dissertation. First, the development of analytic tools that determine the fundamental limits on the performance of STC in a variety of channel conditions. For trellis-type STC, transfer function based techniques are applied to derive performance bounds over Rayleigh, Rician and correlated fading environments. For block-type STC, an analytic framework that supports various complex orthogonal designs with arbitrary signal cardinalities and array configurations is developed. In the second part of the dissertation, the Virginia Tech Space-Time Advanced Radio (VT-STAR) is designed, introducing a multi-antenna hardware laboratory test bed, which facilitates characterization of the multiple-input multiple-output (MIMO) channel and validation of various space-time approaches. In the third part of the dissertation, two novel space-time architectures paired with iterative processing principles are proposed. The first scheme extends the suitability of STC to outdoor wireless communications by employing iterative equalization/decoding for time dispersive channels and the second scheme employs iterative interference cancellation/decoding to solve the error propagation problem of Bell-Labs Layered Space-Time Architecture (BLAST). Results show that remarkable energy and spectral efficiencies are achievable by combining concepts drawn from space-time coding, multiuser detection, array processing and iterative decoding.", "title": "Space-Time Codes for High Data Rate Wireless Communications"}, "15ea14609d77c686df432032430efaddf8cd04b3": {"paper_id": "15ea14609d77c686df432032430efaddf8cd04b3", "abstract": "graphic designs The messages describing novel graphic designs and synthesized sounds obtained in the experiments by Krauss, et al. (in press) were coded into categories of description types, and the rate of gesturing associated with these description types was examined. For the novel designs, we used a category system developed by Fussell and Krauss (1989a) for descriptionsof these figures that partitions the descriptions into three categories: Literal descriptions, in which a design was characterized in terms of its geometric elements \u2014 as a collection of lines, arcs, angles, etc.; Figurative descriptions, in which a design was described in terms of objects or images it suggested; Symbol descriptions, in which a design was likened to a familiar symbol, typically one or more numbers or letters.21 When a message contained more than one type of description (as many did), it was coded for the type that predominated. Overall, about 60% of the descriptions were coded as figurative, about 24% as literal and the remaining 16% as symbols. For the descriptions of the graphic designs, a one-way ANOVA was performed with description type (literal, figurative or symbol) as the independent variable and gesture rate as the dependent variable to determine whether gesturing varied as a function of the kind of content. A significant effect was found F (2, 350) = 4.26, p = .015. Figurative descriptions were accompanied by slightly more gestures than literal descriptions; both were accompanied by more gestures than were the symbol descriptions (14.6 vs. 13.7 vs. 10.6 gestures per m, respectively). Both figurative and literal descriptions tended to be formulated in spatial terms. Symbol descriptions tended to be brief and static\u2014essentially a statement of the resemblance.", "title": "NONVERBAL BEHAVIOR AND NONVERBAL COMMUNICATION: WHAT DO CONVERSATIONAL HAND GESTURES TELL US?"}, "fc5b1c84af67a09e51e6755d042f1592c761ce0c": {"paper_id": "fc5b1c84af67a09e51e6755d042f1592c761ce0c", "abstract": "Two single-pole, double-throw transmit/receive switches were designed and fabricated with different substrate resistances using a 0.18-/spl mu/m p/sup $/substrate CMOS process. The switch with low substrate resistances exhibits 0.8-dB insertion loss and 17-dBm P/sub 1dB/ at 5.825 GHz, whereas the switch with high substrate resistances has 1-dB insertion loss and 18-dBm P/sub 1dB/. These results suggest that the optimal insertion loss can be achieved with low substrate resistances and 5.8-GHz T/R switches with excellent insertion loss and reasonable power handling capability can be implemented in a 0.18-/spl mu/m CMOS process.", "title": "5.8-GHz CMOS T/R switches with high and low substrate resistances in a 0.18-\u03bcm CMOS process"}, "268019f677df10e29297fba721d1557907f697b8": {"paper_id": "268019f677df10e29297fba721d1557907f697b8", "abstract": "In this thesis I will investigate the potential use of pre-existing Volumetric Variational Auto-Encoder architectures for object in-filling and de-noising. From the experiments presetned here it can be seen that even with relatively simple architectures, complex and varied noises can be repaired by learning generative latent spaces from training with data augmentation. For further improving the VAE's predictive abilities, I propose two novel redefinition of the Variational Bayes Auto-Encoder architecture for management of partial, semantically scaled input samples. The Located-VAE (LVAE) and Prior-VAE (PVAE) are extensions of variational reconstruction networks that attempt to connect real-world sliding window object segments to a latent space of known 3D objects for classification and prediction. Their predictive abilities are shown visually through use of the Classification and Prediction through Auto-Encoder Network (CaPtAEN) application for basic reconstruction tasks, as well as reconstruction with varying noise qualities at input. The classification abilities are demonstrated empirically through comparison of latent space representations of segments taken from the same object. Finally, we argue that although voxel models are visually interesting to work with, the computational complexity and massive sparsity are prohibitive for working with high-resolution models and prevent learning of structured high-level 3D filters. The lack of filter descriptiveness is visually explained using the application presented in this work.", "title": "Towards unsupervised object Classification and Prediction in 3 D through Semantic Sampling"}, "f0a4a3fb6997334511d7b8fc090f9ce894679faf": {"paper_id": "f0a4a3fb6997334511d7b8fc090f9ce894679faf", "abstract": "In this paper, we propose an effective face completion algorithm using a deep generative model. Different from well-studied background completion, the face completion task is more challenging as it often requires to generate semantically new pixels for the missing key components (e.g., eyes and mouths) that contain large appearance variations. Unlike existing nonparametric algorithms that search for patches to synthesize, our algorithm directly generates contents for missing regions based on a neural network. The model is trained with a combination of a reconstruction loss, two adversarial losses and a semantic parsing loss, which ensures pixel faithfulness and local-global contents consistency. With extensive experimental results, we demonstrate qualitatively and quantitatively that our model is able to deal with a large area of missing pixels in arbitrary shapes and generate realistic face completion results.", "title": "Generative Face Completion"}, "1779b6a17ee68afafb6801477b165f19901689b2": {"paper_id": "1779b6a17ee68afafb6801477b165f19901689b2", "abstract": "We develop a deep learning algorithm for contour detection with a fully convolutional encoder-decoder network. Different from previous low-level edge detection, our algorithm focuses on detecting higher-level object contours. Our network is trained end-to-end on PASCAL VOC with refined ground truth from inaccurate polygon annotations, yielding much higher precision in object contour detection than previous methods. We find that the learned model generalizes well to unseen object classes from the same supercategories on MS COCO and can match state-of-the-art edge detection on BSDS500 with fine-tuning. By combining with the multiscale combinatorial grouping algorithm, our method can generate high-quality segmented object proposals, which significantly advance the state-of-the-art on PASCAL VOC (improving average recall from 0.62 to 0.67) with a relatively small amount of candidates (~1660 per image).", "title": "Object Contour Detection with a Fully Convolutional Encoder-Decoder Network"}, "82e66c4832386cafcec16b92ac88088ffd1a1bc9": {"paper_id": "82e66c4832386cafcec16b92ac88088ffd1a1bc9", "abstract": "Cameras are becoming ubiquitous in the Internet of Things (IoT) and can use face recognition technology to improve context. There is a large accuracy gap between today\u2019s publicly available face recognition systems and the state-of-the-art private face recognition systems. This paper presents our OpenFace face recognition library that bridges this accuracy gap. We show that OpenFace provides near-human accuracy on the LFW benchmark and present a new classification benchmark for mobile scenarios. This paper is intended for non-experts interested in using OpenFace and provides a light introduction to the deep neural network techniques we use. We released OpenFace in October 2015 as an open source library under the Apache 2.0 license. It is available at: http://cmusatyalab.github.io/openface/ This research was supported by the National Science Foundation (NSF) under grant number CNS-1518865. Additional support was provided by Crown Castle, the Conklin Kistler family fund, Google, the Intel Corporation, and Vodafone. NVIDIA\u2019s academic hardware grant provided the Tesla K40 GPU used in all of our experiments. Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and should not be attributed to their employers or funding sources.", "title": "OpenFace: A general-purpose face recognition library with mobile applications"}, "385750bcf95036c808d63db0e0b14768463ff4c6": {"paper_id": "385750bcf95036c808d63db0e0b14768463ff4c6", "abstract": "We present an autoencoder that leverages learned representations to better measure similarities in data space. By combining a variational autoencoder with a generative adversarial network we can use learned feature representations in the GAN discriminator as basis for the VAE reconstruction objective. Thereby, we replace element-wise errors with feature-wise errors to better capture the data distribution while offering invariance towards e.g. translation. We apply our method to images of faces and show that it outperforms VAEs with element-wise similarity measures in terms of visual fidelity. Moreover, we show that the method learns an embedding in which high-level abstract visual features (e.g. wearing glasses) can be modified using simple arithmetic.", "title": "Autoencoding beyond pixels using a learned similarity metric"}, "6424b69f3ff4d35249c0bb7ef912fbc2c86f4ff4": {"paper_id": "6424b69f3ff4d35249c0bb7ef912fbc2c86f4ff4", "abstract": "Predicting face attributes in the wild is challenging due to complex face variations. We propose a novel deep learning framework for attribute prediction in the wild. It cascades two CNNs, LNet and ANet, which are fine-tuned jointly with attribute tags, but pre-trained differently. LNet is pre-trained by massive general object categories for face localization, while ANet is pre-trained by massive face identities for attribute prediction. This framework not only outperforms the state-of-the-art with a large margin, but also reveals valuable facts on learning face representation. (1) It shows how the performances of face localization (LNet) and attribute prediction (ANet) can be improved by different pre-training strategies. (2) It reveals that although the filters of LNet are fine-tuned only with image-level attribute tags, their response maps over entire images have strong indication of face locations. This fact enables training LNet for face localization with only image-level annotations, but without face bounding boxes or landmarks, which are required by all attribute recognition works. (3) It also demonstrates that the high-level hidden neurons of ANet automatically discover semantic concepts after pre-training with massive face identities, and such concepts are significantly enriched after fine-tuning with attribute tags. Each attribute can be well explained with a sparse linear combination of these concepts.", "title": "Deep Learning Face Attributes in the Wild"}, "3605b9befd5f1b53019b8edb3b3d227901e76c89": {"paper_id": "3605b9befd5f1b53019b8edb3b3d227901e76c89", "abstract": "We present a new supervised learning procedure for systems composed of many separate networks, each of which learns to handle a subset of the complete set of training cases. The new procedure can be viewed either as a modular version of a multilayer supervised network, or as an associative version of competitive learning. It therefore provides a new link between these two apparently different approaches. We demonstrate that the learning procedure divides up a vowel discrimination task into appropriate subtasks, each of which can be solved by a very simple expert network.", "title": "Adaptive Mixtures of Local Experts"}, "272216c1f097706721096669d85b2843c23fa77d": {"paper_id": "272216c1f097706721096669d85b2843c23fa77d", "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.", "title": "Adam: A Method for Stochastic Optimization"}, "735d4220d5579cc6afe956d9f6ea501a96ae99e2": {"paper_id": "735d4220d5579cc6afe956d9f6ea501a96ae99e2", "abstract": "A momentum term is usually included in the simulations of connectionist learning algorithms. Although it is well known that such a term greatly improves the speed of learning, there have been few rigorous studies of its mechanisms. In this paper, I show that in the limit of continuous time, the momentum parameter is analogous to the mass of Newtonian particles that move through a viscous medium in a conservative force field. The behavior of the system near a local minimum is equivalent to a set of coupled and damped harmonic oscillators. The momentum term improves the speed of convergence by bringing some eigen components of the system closer to critical damping. Similar results can be obtained for the discrete time case used in computer simulations. In particular, I derive the bounds for convergence on learning-rate and momentum parameters, and demonstrate that the momentum term can increase the range of learning rate over which the system converges. The optimal condition for convergence is also analyzed.", "title": "On the momentum term in gradient descent learning algorithms"}, "bcdce6325b61255c545b100ef51ec7efa4cced68": {"paper_id": "bcdce6325b61255c545b100ef51ec7efa4cced68", "abstract": "Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.", "title": "An overview of gradient descent optimization algorithms"}, "4856e7719e566f2466369ae2031afb07c934d4d3": {"paper_id": "4856e7719e566f2466369ae2031afb07c934d4d3", "abstract": "A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new approach to second-order optimization, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep or recurrent neural network training, and provide numerical evidence for its superior optimization performance.", "title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization"}, "4cad8f2a31b3c72742a761fe90a372d4a4717ebf": {"paper_id": "4cad8f2a31b3c72742a761fe90a372d4a4717ebf", "abstract": "Bowtie is an ultrafast, memory-efficient alignment program for aligning short DNA sequence reads to large genomes. For the human genome, Burrows-Wheeler indexing allows Bowtie to align more than 25 million reads per CPU hour with a memory footprint of approximately 1.3 gigabytes. Bowtie extends previous Burrows-Wheeler techniques with a novel quality-aware backtracking algorithm that permits mismatches. Multiple processor cores can be used simultaneously to achieve even greater alignment speeds. Bowtie is open source http://bowtie.cbcb.umd.edu .", "title": "Ultrafast and memory-efficient alignment of short DNA sequences to the human genome"}, "649870ea48b5ca108237a708d7c05faf6f9d1f14": {"paper_id": "649870ea48b5ca108237a708d7c05faf6f9d1f14", "abstract": "There is an upsurging interest in designing succinct data structures for basic searching problems (see [23] and references therein). The motivation has to be found in the exponential increase of electronic data nowadays available which is even surpassing the significant increase in memory and disk storage capacities of current computers. Space reduction is an attractive issue because it is also intimately related to performance improvements as noted by several authors (e.g. Knuth [15], Bentley [5]). In designing these implicit data structures the goal is to reduce as much as possible the auxiliary information kept together with the input data without introducing a significant slowdown in the final query performance. Yet input data are represented in their entirety thus taking no advantage of possible repetitiveness into them. The importance of those issues is well known to programmers who typically use various tricks to squeeze data as much as possible and still achieve good query performance. Their approaches, though, boil down to heuristics whose effectiveness is witnessed only by experimentation. In this paper, we address the issue of compressing and indexing data by studying it in a theoretical framework. We devise a novel data structure for indexing and searching whose space occupancy is a function of the entropy of the underlying data set. The novelty resides in the careful combination of a compression algorithm, proposed by Burrows and Wheeler [7], with the structural properties of a well known indexing tool, the Suffix Array [17]. We call the data structure opportunistic since its space occupancy is decreased when the input is compressible at no significant slowdown in the query performance. More precisely, its space occupancy is optimal in an information-content sense because a text T [1, u] is stored using O(Hk(T )) + o(1) bits per input symbol, where Hk(T ) is the kth order entropy of T (the bound holds for any fixed k). Given an arbitrary string P [1, p], the opportunistic data structure allows to search for the occ occurrences of P in T requiring O(p+occ log u) time complexity (for any fixed > 0). If data are uncompressible we achieve the best space bound currently known [11]; on compressible data our solution improves the succinct suffix array of [11] and the classical suffix tree and suffix array data structures either in space or in query time complexity or both. It is a belief [27] that some space overhead should be paid to use full-text indices (like suffix trees or suffix arrays) with respect to word-based indices (like inverted lists). The results in this paper show that a full-text index may achieve sublinear space overhead on compressible texts. As an application we devise a variant of the well-known Glimpse tool [18] which achieves sublinear space and sublinear query time complexity. Conversely, inverted lists achieve only the second goal [27], and classical Glimpse achieves both goals but under some restrictive conditions [4]. Finally, we investigate the modifiability of our opportunistic data structure by studying how to choreograph its basic ideas with a dynamic setting thus achieving effective searching and updating time bounds. \u2217Dipartimento di Informatica, Universit\u00e0 di Pisa, Italy. E-mail: ferragin@di.unipi.it. \u2020Dipartimento di Scienze e Tecnologie Avanzate, Universit\u00e0 del Piemonte Orientale, Alessandria, Italy and IMC-CNR, Pisa, Italy. E-mail: manzini@mfn.unipmn.it.", "title": "Opportunistic Data Structures with Applications"}, "2d7ea4f234120d33100e4a75ac351839c39bc80b": {"paper_id": "2d7ea4f234120d33100e4a75ac351839c39bc80b", "abstract": "In this paper we present a context-aware collaborative filtering system that predicts a user\u2019s preference in different context situations based on past experiences. We extend collaborative filtering techniques so that what other like-minded users have done in similar context can be used to predict a user\u2019s preference towards an activity in the current context. Such a system can help predict the user\u2019s behavior in different situations without the user actively defining it. For example, it could recommend activities customized for Bob for the given weather, location, and traveling companion(s), based on what other people like Bob have done in similar context.", "title": "Context-Aware Collaborative Filtering System: Predicting the User's Preference in the Ubiquitous Computing Environment"}, "98c524def61762d9c033a64a75562e274b687763": {"paper_id": "98c524def61762d9c033a64a75562e274b687763", "abstract": "Recommender systems are changing from novelties used by a few E-commerce sites, to serious business tools that are re-shaping the world of E-commerce. Many of the largest commerce Web sites are already using recommender systems to help their customers find products to purchase. A recommender system learns from a customer and recommends products that she will find most valuable from among the available products. In this paper we present an explanation of how recommender systems help E-commerce sites increase sales, and analyze six sites that use recommender systems including several sites that use more than one recommender system. Based on the examples, we create a taxonomy of recommender systems, including the interfaces they present to customers, the technologies used to create the recommendations, and the inputs they need from customers. We conclude with ideas for new applications of recommender systems to E-commerce.", "title": "Recommender systems in e-commerce"}, "c864175780adde19099108de66b4636f4c87d44c": {"paper_id": "c864175780adde19099108de66b4636f4c87d44c", "abstract": "When making a choice in the absence of decisive first-hand knowledge, choosing as other like-minded, similarly-situated people have successfully chosen in the past is a good strategy \u2014 in effect, using other people as filters and guides: filters to strain out potentially bad choices and guides to point out potentially good choices. Current human-computer interfaces largely ignore the power of the social strategy. For most choices within an interface, new users are left to fend for themselves and if necessary, to pursue help outside of the interface. We present a general history-of-use method that automates a social method for informing choice and report on how it fares in the context of a fielded test case: the selection of videos from a large set. The positive results show that communal history-of-use data can serve as a powerful resource for use in interfaces.", "title": "Recommending and Evaluating Choices in a Virtual Community of Use"}, "0fcc45600283abca12ea2f422e3fb2575f4c7fc0": {"paper_id": "0fcc45600283abca12ea2f422e3fb2575f4c7fc0", "abstract": "Collaborative ltering or recommender systems use a database about user preferences to predict additional topics or products a new user might like. In this paper we describe several algorithms designed for this task, including techniques based on correlation coefcients, vector-based similarity calculations, and statistical Bayesian methods. We compare the predictive accuracy of the various methods in a set of representative problem domains. We use two basic classes of evaluation metrics. The rst characterizes accuracy over a set of individual predictions in terms of average absolute deviation. The second estimates the utility of a ranked list of suggested items. This metric uses an estimate of the probability that a user will see a recommendation in an ordered list. Experiments were run for datasets associated with 3 application areas, 4 experimental protocols, and the 2 evaluation metrics for the various algorithms. Results indicate that for a wide range of conditions, Bayesian networks with decision trees at each node and correlation methods outperform Bayesian-clustering and vectorsimilarity methods. Between correlation and Bayesian networks, the preferred method depends on the nature of the dataset, nature of the application (ranked versus one-by-one presentation), and the availability of votes with which to make predictions. Other considerations include the size of database, speed of predictions, and learning time. Appears in Proceedings of the Fourteenth Conference on Uncertainty in Arti cial Intelligence, Madison, WI, July, 1998. Morgan Kaufmann Publisher.", "title": "Empirical Analysis of Predictive Algorithms for Collaborative Filtering"}, "92cc12f272ff55795c29cd97dc8ee17a5554308e": {"paper_id": "92cc12f272ff55795c29cd97dc8ee17a5554308e", "abstract": "The problem of recommending items from some fixed database has been studied extensively, and two main paradigms have emerged. In content-based recommendation one tries to recommend items similar to those a given user has liked in the past, whereas in collaborative recommendation one identifies users whose tastes are similar to those of the given user and recommends items they have liked. Our approach in Fab has been to combine these two methods. Here, we explain how a hybrid system can incorporate the advantages of both methods while inheriting the disadvantages of neither. In addition to what one might call the \u201cgeneric advantages\u201d inherent in any hybrid system, the particular design of the Fab architecture brings two additional benefits. First, two scaling problems common to all Web services are addressed\u2014an increasing number of users and an increasing number of documents. Second, the system automatically identifies emergent communities of interest in the user population, enabling enhanced group awareness and communications. Here we describe the two approaches for contentbased and collaborative recommendation, explain how a hybrid system can be created, and then describe Fab, an implementation of such a system. For more details on both the implemented architecture and the experimental design the reader is referred to [1]. The content-based approach to recommendation has its roots in the information retrieval (IR) community, and employs many of the same techniques. Text documents are recommended based on a comparison between their content and a user profile. Data", "title": "Content-Based, Collaborative Recommendation"}, "552d5338c6151cd0e4b61cc31ba5bc507d5db52f": {"paper_id": "552d5338c6151cd0e4b61cc31ba5bc507d5db52f", "abstract": "Here we propose an OWL encoded context ontology (CONON) for modeling context in pervasive computing environments, and for supporting logic-based context reasoning. CONON provides an upper context ontology that captures general concepts about basic context, and also provides extensibility for adding domain-specific ontology in a hierarchical manner. Based on this context ontology, we have studied the use of logic reasoning to check the consistency of context information, and to reason over low-level, explicit context to derive high-level, implicit context. By giving a performance study for our prototype, we quantitatively evaluate the feasibility of logic based context reasoning for nontime-critical applications in pervasive computing environments, where we always have to deal carefully with the limitation of computational resources.", "title": "Ontology based context modeling and reasoning using OWL"}, "3cd0b6a48b14f86ed261240f30113a41bacd2255": {"paper_id": "3cd0b6a48b14f86ed261240f30113a41bacd2255", "abstract": "Context is a key issue in interaction between human and computer, describing the surrounding facts that add meaning. In mobile computing research published the parameter location is most often used to approximate context and to implement context-aware applications. We propose that ultra-mobile computing, characterized by devices that are operational and operated while on the move (e.g. PDAs, mobile phones, wearable computers), can significantly benefit from a wider notion of context. To structure the field we introduce a working model for context, discuss mechanisms to acquire context beyond location, and application of context-awareness in ultra-mobile computing. We investigate the utility of sensors for context-awareness and present two prototypical implementations-a light sensitive display and an orientation aware PDA interface. The concept is then extended to a model for sensor fusion to enable more sophisticated context recognition. Based on an implementation of the model an experiment is described and the feasibility of the approach is demonstrated. Further we explore fusion of sensors for acquisition of information on more sophisticated contexts. 1 Introduction Context is \" that which surrounds, and gives meaning to something else \" =. Various areas of computer science have been investigating this concept over the last 40 years, to relate information processing and communication to aspects of the situations in which such processing occurs. Most notably, context is a key concept in Natural Language Processing and more generally in Human-Computer Interaction. For instance, state of the art graphical user interfaces use context to adapt menus to contexts such as user preference and dialogue status. A new domain, in which context currently receives growing attention, is mobile computing. While a first wave of mobile computing was based on portable general-purpose computers and primarily focussed on location transparency, a second wave is now based on ultra-mobile devices and an interest in relating these to their surrounding situation of usage. Ultra-mobile devices are a new class of small mobile computer, defined as computing devices that are operational and operated while on the move, and characterized by a shift from general-purpose computing to task-specific support. Ultra-mobile devices comprise for instance Personal Digital Assistants (PDAs), mobile phones, and wearable computers. A primary concern of context-awareness in mobile computing is awareness of the physical environment surrounding a user and their ultra-mobile device. In recent work, this concern has been addressed by implementation of location-awareness, for instance based on global positioning, or the use of beacons. Location \u2026", "title": "There is more to context than location"}, "11184876816647e4cd1501c5069671c867a18ef8": {"paper_id": "11184876816647e4cd1501c5069671c867a18ef8", "abstract": "This paper describes systems that examine and react to an individual's changing context. Such systems can promote and mediate people's interactions with devices, computers, and other people, and they can help navigate unfamiliar places. We believe that a limited amount of information covering a person's proximate environment is most important for this form of computing since the interesting part of the world around us is what we can see, hear, and touch. In this paper we define context-aware computing, and describe four catagories of context-aware applications: proximate selection, automatic contextual reconfiguration, contextual information and commands, and contex-triggered actions. Instances of these application types have been prototyped on the PARCTAB, a wireless, palm-sized computer.", "title": "Context-Aware Computing Applications"}, "196523c04f9e845cc6dde43f50e40d38909ffafd": {"paper_id": "196523c04f9e845cc6dde43f50e40d38909ffafd", "abstract": "Automated collaborative filtering (ACF) systems predict a person's affinity for items or information by connecting that person's recorded interests with the recorded interests of a community of people and sharing ratings between like-minded persons. However, current recommender systems are black boxes, providing no transparency into the working of the recommendation. Explanations provide that transparency, exposing the reasoning and data behind a recommendation. In this paper, we address explanation interfaces for ACF systems - how they should be implemented and why they should be implemented. To explore how, we present a model for explanations based on the user's conceptual model of the recommendation process. We then present experimental results demonstrating what components of an explanation are the most compelling. To address why, we present experimental evidence that shows that providing explanations can improve the acceptance of ACF systems. We also describe some initial explorations into measuring how explanations can improve the filtering performance of users.", "title": "Explaining collaborative filtering recommendations"}, "59807925235068fa6261da5cfec650f6429ed778": {"paper_id": "59807925235068fa6261da5cfec650f6429ed778", "abstract": "Modern web applications consist of a significant amount of client- side code, written in JavaScript, HTML, and CSS. In this paper, we present a study of common challenges and misconceptions among web developers, by mining related questions asked on Stack Over- flow. We use unsupervised learning to categorize the mined questions and define a ranking algorithm to rank all the Stack Overflow questions based on their importance. We analyze the top 50 questions qualitatively. The results indicate that (1) the overall share of web development related discussions is increasing among developers, (2) browser related discussions are prevalent; however, this share is decreasing with time, (3) form validation and other DOM related discussions have been discussed consistently over time, (4) web related discussions are becoming more prevalent in mobile development, and (5) developers face implementation issues with new HTML5 features such as Canvas. We examine the implications of the results on the development, research, and standardization communities.", "title": "Mining questions asked by web developers"}, "a840e49692787d7ec1466c8b950e3e918c928a8a": {"paper_id": "a840e49692787d7ec1466c8b950e3e918c928a8a", "abstract": "Traditionally, many types of software documentation, such as API documentation, require a process where a few people write for many potential users. The resulting documentation, when it exists, is often of poor quality and lacks sufficient examples and explanations. In this paper, we report on an empirical study to investigate how Question and Answer (Q&A) websites, such as Stack Overflow, facilitate crowd documentation \u2014 knowledge that is written by many and read by many. We examine the crowd documentation for three popular APIs: Android, GWT, and the Java programming language. We collect usage data using Google Code Search, and analyze the coverage, quality, and dynamics of the Stack Overflow documentation for these APIs. We find that the crowd is capable of generating a rich source of content with code examples and discussion that is actively viewed and used by many more developers. For example, over 35,000 developers contributed questions and answers about the Android API, covering 87% of the classes. This content has been viewed over 70 million times to date. However, there are shortcomings with crowd documentation, which we identify. In addition to our empirical study, we present future directions and tools that can be leveraged by other researchers and software designers for performing API analytics and mining of crowd documentation.", "title": "Crowd Documentation : Exploring the Coverage and the Dynamics of API Discussions on Stack Overflow"}, "2f40d0ee2ca5f6845f553b7b6e385a03ace36607": {"paper_id": "2f40d0ee2ca5f6845f553b7b6e385a03ace36607", "abstract": "According to its proponents, open source style software development has the capacity to compete successfully, and perhaps in many cases displace, traditional commercial development methods. In order to begin investigating such claims, we examine the development process of a major open source application, the Apache web server. By using email archives of source code change history and problem reports we quantify aspects of developer participation, core team size, code ownership, productivity, defect density, and problem resolution interval for this OSS project. This analysis reveals a unique process, which performs well on important measures. We conclude that hybrid forms of development that borrow the most effective techniques from both the OSS and commercial worlds may lead to high performance software processes.", "title": "A case study of open source software development: the Apache server"}, "d744c7cf1f5e9e77c7de55db8df5a918ee1f41d7": {"paper_id": "d744c7cf1f5e9e77c7de55db8df5a918ee1f41d7", "abstract": "Software engineering is a human task, and as such we must study what software engineers do and think. Understanding the normative practice of software engineering is the first step toward developing realistic solutions to better facilitate the engineering process. We conducted three studies using several data-gathering approaches to elucidate the patterns by which software engineers (SEs) use and update documentation. Our objective is to more accurately comprehend and model documentation use, usefulness, and maintenance, thus enabling better decision making and tool design by developers and project managers. Our results confirm the widely held belief that SEs typically do not update documentation as timely or completely as software process personnel and managers advocate. However, the results also reveal that out-of-date software documentation remains useful in many circumstances.", "title": "How software engineers use documentation: the state of the practice"}, "4a52090065c3da8231400dd2c3916951eb88e5b3": {"paper_id": "4a52090065c3da8231400dd2c3916951eb88e5b3", "abstract": "Security policies, which specify what applications are allowed to do, are notoriously difficult to specify correctly. Many applications were found to request over-liberal permissions. On mobile platforms, this might prevent a cautious user from installing an otherwise harmless application or, even worse, increase the attack surface in vulnerable applications. As a result of such difficulties, programmers frequently ask about them in on-line fora. Our goal is to gain some insight into both the misuse of permissions and the discussions of permissions in on-line fora. We analyze about 10,000 free apps from popular Android markets and found a significant sub-linear relationship between the popularity of a permission and the number of times when it is misused. We also study the relationship of permission use and the number of questions about the permission on StackOverflow. Finally, we study the effect of the influence of a permission (the functionality that it controls) and the interference of a permission (the number of other permissions that influence the same classes) on the occurrence of both permission misuse and permission discussions in StackOverflow.", "title": "Asking for (and about) permissions used by Android apps"}, "1931c24e9474c080f57fd6af9919b6885b75b0ac": {"paper_id": "1931c24e9474c080f57fd6af9919b6885b75b0ac", "abstract": "The Android platform has about 130 application level permissions that govern access to resources. The determination of which permissions to request is left solely to the application developer. Users are prompted to approve all application permissions at install time, and permissions are silently enforced at execution time. Although many applications make use of a wide range of permissions, we have observed that some applications request permissions that are not required for the application to execute, and that existing developer APIs make it difficult for developers to align their permission requests with application functionality. In this paper we describe a tool we developed to assist developers in utilizing least privilege.", "title": "Curbing Android Permission Creep Encouraging Least Privilege in development"}, "7ef62f6a7a86f76221f117d5ca7e2795307b7166": {"paper_id": "7ef62f6a7a86f76221f117d5ca7e2795307b7166", "abstract": "We present a static control-flow analysis for JavaScript programs running in a web browser. Our analysis tackles numerous challenges posed by modern web applications including asynchronous communication, frameworks, and dynamic code generation. We use our analysis to extract a model of expected client behavior as seen from the server, and build an intrusion-prevention proxy for the server: the proxy intercepts client requests and disables those that do not meet the expected behavior. We insert random asynchronous requests to foil mimicry attacks. Finally, we evaluate our technique against several real applications and show that it protects against an attack in a widely-used web application.", "title": "Using static analysis for Ajax intrusion detection"}, "84fe5a0e34c2683f683e9303a15a74ffac9c8815": {"paper_id": "84fe5a0e34c2683f683e9303a15a74ffac9c8815", "abstract": "Client-side JavaScript is being widely used in popular web applications to improve functionality, increase responsiveness, and decrease load times. However, it is challenging to build reliable applications using JavaScript. This paper presents an empirical characterization of the error messages printed by JavaScript code in web applications, and attempts to understand their root causes. We find that JavaScript errors occur in production web applications, and that the errors fall into a small number of categories. We further find that both non-deterministic and deterministic errors occur in the applications, and that the speed of testing plays an important role in exposing errors. Finally, we study the correlations among the static and dynamic properties of the application and the frequency of errors in it in order to understand the root causes of the errors.", "title": "JavaScript Errors in the Wild: An Empirical Study"}, "1e9e3d2f3c24bd71d74aea85c18ab0b8afb23ec2": {"paper_id": "1e9e3d2f3c24bd71d74aea85c18ab0b8afb23ec2", "abstract": "The JavaScript programming language is widely used for web programming and, increasingly, for general purpose computing. As such, improving the correctness, security and performance of JavaScript applications has been the driving force for research in type systems, static analysis and compiler techniques for this language. Many of these techniques aim to reign in some of the most dynamic features of the language, yet little seems to be known about how programmers actually utilize the language or these features. In this paper we perform an empirical study of the dynamic behavior of a corpus of widely-used JavaScript programs, and analyze how and why the dynamic features are used. We report on the degree of dynamism that is exhibited by these JavaScript programs and compare that with assumptions commonly made in the literature and accepted industry benchmark suites.", "title": "An analysis of the dynamic behavior of JavaScript programs"}, "6911b420bb4eb3004dbb6a89c94dfd524bcf0074": {"paper_id": "6911b420bb4eb3004dbb6a89c94dfd524bcf0074", "abstract": "Transforming text into executable code with a function such as JavaScript\u2019s eval endows programmers with the ability to extend applications, at any time, and in almost any way they choose. But, this expressive power comes at a price: reasoning about the dynamic behavior of programs that use this feature becomes challenging. Any ahead-of-time analysis, to remain sound, is forced to make pessimistic assumptions about the impact of dynamically created code. This pessimism affects the optimizations that can be applied to programs and significantly limits the kinds of errors that can be caught statically and the security guarantees that can be enforced. A better understanding of how eval is used could lead to increased performance and security. This paper presents a large-scale study of the use of eval in JavaScript-based web applications. We have recorded the behavior of 337 MB of strings given as arguments to 550,358 calls to the eval function exercised in over 10,000 web sites. We provide statistics on the nature and content of strings used in eval expressions, as well as their provenance and data obtained by observing their dynamic behavior. eval is evil. Avoid it. eval has aliases. Don\u2019t use them. \u2014Douglas Crockford", "title": "The Eval That Men Do - A Large-Scale Study of the Use of Eval in JavaScript Applications"}, "d26c665258fe6cd39373fdee85f269f0912cd62d": {"paper_id": "d26c665258fe6cd39373fdee85f269f0912cd62d", "abstract": "As new drugs are developed, it is essential to appropriately translate the drug dosage from one animal species to another. A misunderstanding appears to exist regarding the appropriate method for allometric dose translations, especially when starting new animal or clinical studies. The need for education regarding appropriate translation is evident from the media response regarding some recent studies where authors have shown that resveratrol, a compound found in grapes and red wine, improves the health and life span of mice. Immediately after the online publication of these papers, the scientific community and popular press voiced concerns regarding the relevance of the dose of resveratrol used by the authors. The animal dose should not be extrapolated to a human equivalent dose (HED) by a simple conversion based on body weight, as was reported. For the more appropriate conversion of drug doses from animal studies to human studies, we suggest using the body surface area (BSA) normalization method. BSA correlates well across several mammalian species with several parameters of biology, including oxygen utilization, caloric expenditure, basal metabolism, blood volume, circulating plasma proteins, and renal function. We advocate the use of BSA as a factor when converting a dose for translation from animals to humans, especially for phase I and phase II clinical trials.", "title": "Dose translation from animal to human studies revisited."}, "a54fcdcb02da0844d28b3191145bbc99675714df": {"paper_id": "a54fcdcb02da0844d28b3191145bbc99675714df", "abstract": "Facial expression recognition has been investigated for many years, and there are two popular models: Action Units (AUs) and the Valence-Arousal space (V-A space) that have been widely used. However, most of the databases for estimating V-A intensity are captured in laboratory settings, and the benchmarks \"in-the-wild\" do not exist. Thus, the First Affect-In-The-Wild Challenge released a database for V-A estimation while the videos were captured in wild condition. In this paper, we propose an integrated deep learning framework for facial attribute recognition, AU detection, and V-A estimation. The key idea is to apply AUs to estimate the V-A intensity since both AUs and V-A space could be utilized to recognize some emotion categories. Besides, the AU detector is trained based on the convolutional neural network (CNN) for facial attribute recognition. In experiments, we will show the results of the above three tasks to verify the performances of our proposed network framework.", "title": "FATAUVA-Net: An Integrated Deep Learning Framework for Facial Attribute Recognition, Action Unit Detection, and Valence-Arousal Estimation"}, "1d19c6857e798943cd0ecd110a7a0d514c671fec": {"paper_id": "1d19c6857e798943cd0ecd110a7a0d514c671fec", "abstract": "Despite being the appearance-based classifier of choice in recent years, relatively few works have examined how much convolutional neural networks (CNNs) can improve performance on accepted expression recognition benchmarks and, more importantly, examine what it is they actually learn. In this work, not only do we show that CNNs can achieve strong performance, but we also introduce an approach to decipher which portions of the face influence the CNN's predictions. First, we train a zero-bias CNN on facial expression data and achieve, to our knowledge, state-of-the-art performance on two expression recognition benchmarks: the extended Cohn-Kanade (CK+) dataset and the Toronto Face Dataset (TFD). We then qualitatively analyze the network by visualizing the spatial patterns that maximally excite different neurons in the convolutional layers and show how they resemble Facial Action Units (FAUs). Finally, we use the FAU labels provided in the CK+ dataset to verify that the FAUs observed in our filter visualizations indeed align with the subject's facial movements.", "title": "Do Deep Neural Networks Learn Facial Action Units When Doing Expression Recognition?"}, "f9c431f58565f874f76a024add2aa80717ec5cf5": {"paper_id": "f9c431f58565f874f76a024add2aa80717ec5cf5", "abstract": "We propose a semi-supervised approach to solve the task of emotion recognition in 2D face images using recent ideas in deep learning for handling the factors of variation present in data. An emotion classification algorithm should be both robust to (1) remaining variations due to the pose of the face in the image after centering and alignment, (2) the identity or morphology of the face. In order to achieve this invariance, we propose to learn a hierarchy of features in which we gradually filter the factors of variation arising from both (1) and (2). We address (1) by using a multi-scale contractive convolutional network (CCNET) in order to obtain invariance to translations of the facial traits in the image. Using the feature representation produced by the CCNET, we train a Contractive Discriminative Analysis (CDA) feature extractor, a novel variant of the Contractive Auto-Encoder (CAE), designed to learn a representation separating out the emotion-related factors from the others (which mostly capture the subject identity, and what is left of pose after the CCNET). This system beats the state-of-the-art on a recently proposed dataset for facial expression recognition, the Toronto Face Database, moving the state-of-art accuracy from 82.4% to 85.0%, while the CCNET and CDA improve accuracy of a standard CAE by 8%.", "title": "Disentangling Factors of Variation for Facial Expression Recognition"}, "1e799047e294267087ec1e2c385fac67074ee5c8": {"paper_id": "1e799047e294267087ec1e2c385fac67074ee5c8", "abstract": "\u00d0We propose a method for automatically classifying facial images based on labeled elastic graph matching, a 2D Gabor wavelet representation, and linear discriminant analysis. Results of tests with three image sets are presented for the classification of sex, arace,o and expression. A visual interpretation of the discriminant vectors is provided. Index Terms\u00d0Computer vision, face recognition, facial expression recognition, Gabor wavelets, principal component analysis, discriminant analysis.", "title": "Automatic Classification of Single Facial Images"}, "747c25bff37b96def96dc039cc13f8a7f42dbbc7": {"paper_id": "747c25bff37b96def96dc039cc13f8a7f42dbbc7", "abstract": "The task of the Emotion Recognition in the Wild (EmotiW) Challenge is to assign one of seven emotions to short video clips extracted from Hollywood style movies. The videos depict acted-out emotions under realistic conditions with a large degree of variation in attributes such as pose and illumination, making it worthwhile to explore approaches which consider combinations of features from multiple modalities for label assignment. In this paper we present our approach to learning several specialist models using deep learning techniques, each focusing on one modality. Among these are a convolutional neural network, focusing on capturing visual information in detected faces, a deep belief net focusing on the representation of the audio stream, a K-Means based \u201cbag-of-mouths\u201d model, which extracts visual features around the mouth region and a relational autoencoder, which addresses spatio-temporal aspects of videos. We explore multiple methods for the combination of cues from these modalities into one common classifier. This achieves a considerably greater accuracy than predictions from our strongest single-modality classifier. Our method was the winning submission in the 2013 EmotiW challenge and achieved a test set accuracy of 47.67\u00a0% on the 2014 dataset.", "title": "EmoNets: Multimodal deep learning approaches for emotion recognition in video"}, "878301453e3d5cb1a1f7828002ea00f59cbeab06": {"paper_id": "878301453e3d5cb1a1f7828002ea00f59cbeab06", "abstract": "We propose a deep convolutional neural network (CNN) for face detection leveraging on facial attributes based supervision. We observe a phenomenon that part detectors emerge within CNN trained to classify attributes from uncropped face images, without any explicit part supervision. The observation motivates a new method for finding faces through scoring facial parts responses by their spatial structure and arrangement. The scoring mechanism is data-driven, and carefully formulated considering challenging cases where faces are only partially visible. This consideration allows our network to detect faces under severe occlusion and unconstrained pose variations. Our method achieves promising performance on popular benchmarks including FDDB, PASCAL Faces, AFW, and WIDER FACE.", "title": "Faceness-Net: Face Detection through Deep Facial Part Responses"}, "04661729f0ff6afe4b4d6223f18d0da1d479accf": {"paper_id": "04661729f0ff6afe4b4d6223f18d0da1d479accf", "abstract": "In this paper, we propose a novel deep convolutional network (DCN) that achieves outstanding performance on FDDB, PASCAL Face, and AFW. Specifically, our method achieves a high recall rate of 90.99% on the challenging FDDB benchmark, outperforming the state-of-the-art method [23] by a large margin of 2.91%. Importantly, we consider finding faces from a new perspective through scoring facial parts responses by their spatial structure and arrangement. The scoring mechanism is carefully formulated considering challenging cases where faces are only partially visible. This consideration allows our network to detect faces under severe occlusion and unconstrained pose variation, which are the main difficulty and bottleneck of most existing face detection approaches. We show that despite the use of DCN, our network can achieve practical runtime speed.", "title": "From Facial Parts Responses to Face Detection: A Deep Learning Approach"}, "aa23d33983b1abd2d8a677040eb875e93c478a7f": {"paper_id": "aa23d33983b1abd2d8a677040eb875e93c478a7f", "abstract": "We present a generic objectness measure, quantifying how likely it is for an image window to contain an object of any class. We explicitly train it to distinguish objects with a well-defined boundary in space, such as cows and telephones, from amorphous background elements, such as grass and road. The measure combines in a Bayesian framework several image cues measuring characteristics of objects, such as appearing different from their surroundings and having a closed boundary. These include an innovative cue to measure the closed boundary characteristic. In experiments on the challenging PASCAL VOC 07 dataset, we show this new cue to outperform a state-of-the-art saliency measure, and the combined objectness measure to perform better than any cue alone. We also compare to interest point operators, a HOG detector, and three recent works aiming at automatic object segmentation. Finally, we present two applications of objectness. In the first, we sample a small numberof windows according to their objectness probability and give an algorithm to employ them as location priors for modern class-specific object detectors. As we show experimentally, this greatly reduces the number of windows evaluated by the expensive class-specific model. In the second application, we use objectness as a complementary score in addition to the class-specific model, which leads to fewer false positives. As shown in several recent papers, objectness can act as a valuable focus of attention mechanism in many other applications operating on image windows, including weakly supervised learning of object categories, unsupervised pixelwise segmentation, and object tracking in video. Computing objectness is very efficient and takes only about 4 sec. per image.", "title": "Measuring the Objectness of Image Windows"}, "21d4258394a9c8f0ea15f0792d67f7e645720ff6": {"paper_id": "21d4258394a9c8f0ea15f0792d67f7e645720ff6", "abstract": "We propose a unified approach for bottom-up hierarchical image segmentation and object candidate generation for recognition, called Multiscale Combinatorial Grouping (MCG). For this purpose, we first develop a fast normalized cuts algorithm. We then propose a high-performance hierarchical segmenter that makes effective use of multiscale information. Finally, we propose a grouping strategy that combines our multiscale regions into highly-accurate object candidates by exploring efficiently their combinatorial space. We conduct extensive experiments on both the BSDS500 and on the PASCAL 2012 segmentation datasets, showing that MCG produces state-of-the-art contours, hierarchical regions and object candidates.", "title": "Multiscale Combinatorial Grouping"}, "053931267af79a89791479b18d1b9cde3edcb415": {"paper_id": "053931267af79a89791479b18d1b9cde3edcb415", "abstract": "Attributes, or mid-level semantic features, have gained popularity in the past few years in domains ranging from activity recognition to face verification. Improving the accuracy of attribute classifiers is an important first step in any application which uses these attributes. In most works to date, attributes have been considered independent of each other. However, attributes can be strongly related, such as heavy makeup and wearing lipstick as well as male and goatee and many others. We propose a multi-task deep convolutional neural network (MCNN) with an auxiliary network at the top (AUX) which takes advantage of attribute relationships for improved classification. We call our final network MCNN-AUX. MCNNAUX uses attribute relationships in three ways: by sharing the lowest layers for all attributes, by sharing the higher layers for spatially-related attributes, and by feeding the attribute scores from MCNN into the AUX network to find score-level relationships. Using MCNN-AUX rather than individual attribute classifiers, we are able to reduce the number of parameters in the network from 64 million to fewer than 16 million and reduce the training time by a factor of 16. We demonstrate the effectiveness of our method by producing results on two challenging publicly available datasets achieving state-of-the-art performance on many attributes.", "title": "Attributes for Improved Attributes: A Multi-Task Network Utilizing Implicit and Explicit Relationships for Facial Attribute Classification"}, "39b22bcbd452d5fea02a9ee63a56c16400af2b83": {"paper_id": "39b22bcbd452d5fea02a9ee63a56c16400af2b83", "abstract": "Recently, deep neural networks have been shown to perform competitively on the task of predicting facial expression from images. Trained by gradient-based methods, these networks are amenable to \"multi-task\" learning via a multiple term objective. In this paper we demonstrate that learning representations to predict the position and shape of facial landmarks can improve expression recognition from images. We show competitive results on two large-scale datasets, the ICML 2013 Facial Expression Recognition challenge, and the Toronto Face Database.", "title": "Multi-task Learning of Facial Landmarks and Expression"}, "08a1fc55d03e4a73cad447e5c9ec79a6630f3e2d": {"paper_id": "08a1fc55d03e4a73cad447e5c9ec79a6630f3e2d", "abstract": "We propose a method of face verification that takes advantage of a reference set of faces, disjoint by identity from the test faces, labeled with identity and face part locations. The reference set is used in two ways. First, we use it to perform an \u201cidentity-preserving\u201d alignment, warping the faces in a way that reduces differences due to pose and expression but preserves differences that indicate identity. Second, using the aligned faces, we learn a large set of identity classifiers, each trained on images of just two people. We call these \u201cTom-vs-Pete\u201d classifiers to stress their binary nature. We assemble a collection of these classifiers able to discriminate among a wide variety of subjects and use their outputs as features in a same-or-different classifier on face pairs. We evaluate our method on the Labeled Faces in the Wild benchmark, achieving an accuracy of 93.10%, significantly improving on the published state of the art.", "title": "Tom-vs-Pete Classifiers and Identity-Preserving Alignment for Face Verification"}, "06ac6751e1a4f3dfc12bcc18132476fde2736001": {"paper_id": "06ac6751e1a4f3dfc12bcc18132476fde2736001", "abstract": "Computer Aided Design (CAD) typically involves tasks such as adjusting the camera perspective and assembling pieces in free space that require specifying 6 degrees of freedom (DOF). The standard approach is to factor these DOFs into 2D subspaces that are mapped to the x and y axes of a mouse. This metaphor is inherently modal because one needs to switch between subspaces, and disconnects the input space from the modeling space. In this paper, we propose a bimanual hand tracking system that provides physically-motivated 6-DOF control for 3D assembly. First, we discuss a set of principles that guide the design of our precise, easy-to-use, and comfortable-to-use system. Based on these guidelines, we describe a 3D input metaphor that supports constraint specification classically used in CAD software, is based on only a few simple gestures, lets users rest their elbows on their desk, and works alongside the keyboard and mouse. Our approach uses two consumer-grade webcams to observe the user's hands. We solve the pose estimation problem with efficient queries of a precomputed database that relates hand silhouettes to their 3D configuration. We demonstrate efficient 3D mechanical assembly of several CAD models using our hand-tracking system.", "title": "6D hands: markerless hand-tracking for computer aided design"}, "292e2b582e8232fcaf3b97d3f134ee057b8dbae0": {"paper_id": "292e2b582e8232fcaf3b97d3f134ee057b8dbae0", "abstract": "The majority of financial services companies in Germany and Switzerland have, with varying objectives and success, conducted customer relationship management (CRM) implementation projects. In this paper we present a framework for the analysis of CRM approaches in financial services companies. Building on previous research and using comprehensive literature research, we develop a CRM reference architecture that focuses on the process and system level for the description and classification of CRM approaches in companies. Moreover, we analyze three CRM case studies in Swiss and German financial services companies and derive different types of CRM approaches in the financial services industry: Customer Satisfaction Management, Customer Contact Management, and Customer Profitability Management. We describe each type in accordance with the CRM architecture and a case example.", "title": "Architecture for Customer Relationship Management Approaches in Financial Services"}, "09c59497e2bc759da75324336f602e2b03a5be22": {"paper_id": "09c59497e2bc759da75324336f602e2b03a5be22", "abstract": "Knowledge is a broad and abstract notion that has defined epistemological debate in western philosophy since the classical Greek era. In the past few years, however, there has been a raging interest in treating knowledge as a significant organizational resource. The heightened interest in organizational knowledge and knowledge management stems from the transition into the knowledge economy, where knowledge is viewed as the principle source of value creation and sustainable competitive advantage. Consistent with the growing interest in organizational knowledge and knowledge management (KM), recently IS researchers have been promoting a class of information systems, referred to as knowledge management systems (KMS). The objective of KMS is to support construction, sharing and application of knowledge in organizations. Knowledge and knowledge management are complex and multi-faceted concepts. Thus, effective development and implementation of KMS requires a foundation in several rich literatures. We believe that to be credible, KMS research and development should preserve and built upon the significant literature that exists in different but related fields. We have promoted this view in this paper by providing a review and interpretation of knowledge management literatures in different fields with an eye towards identifying the important areas for future research. Next, we have presented a detailed process-view of organizational knowledge management with a focus on the potential role of IT in this process. The paper concludes with a discussion of major research questions that emerge from the review of literature as well as the process-view of KM. It is our contention that in large and global firms information technologies (in form of KMS) will be interlaced with organizational knowledge management strategies and processes. We therefore believe that the KMS should and will receive considerable scholarly attention and will become a focal point of inquiry. It is our hope that the ideas, discussion, and the broad research issues set forth in this paper contributes to future work in the knowledge management area by IS researchers. Knowledge Management and Knowledge Management Systems: Conceptual Foundations and Research Issues \"In post-capitalism, power comes from transmitting information to make it productive, not from hiding it.\" (Drucker, 1995)", "title": "Knowledge Management and Knowledge Management Systems: Conceptual Foundations and Research Issues Knowledge Management and Knowledge Management Systems: Conceptual Foundations and Research Issues Knowledge Management and Knowledge Management Systems: Conceptual Foundations and Research Issues Knowle"}, "7e317387594c88c1f583323d3e8c5f2fae11aa89": {"paper_id": "7e317387594c88c1f583323d3e8c5f2fae11aa89", "abstract": "This paper aims to present a framework for describing Customer Knowledge Management in online purchase process using two models from literature including consumer online purchase process and ECKM. Since CKM is a recent concept and little empirical research is available, we will first present the theories from which CKM derives. In the first stage we discuss about e-commerce trend and increasing importance of customer loyalty in today\u2019s business environment. Then some related concepts about Knowledge Management, Customer Relationship Management and CKM are presented, in order to provide the reader with a better understanding and clear picture regarding CKM. Finally, providing models representing e-CKM and online purchasing process, we propose a comprehensive procedure to manage customer data and knowledge in e-commerce.", "title": "Customer Knowledge Management Framework in E-commerce"}, "66ceda6b42cb9614bf67ac0b6aa3cec51d29f183": {"paper_id": "66ceda6b42cb9614bf67ac0b6aa3cec51d29f183", "abstract": "In this paper, we approach electronic commerce Customer Relationship Management (e-CRM) from the perspective of five research areas. Our purpose is to define a conceptual framework to examine the relationships among and between these five research areas within e-CRM and to propose how they might be integrated to further research this area. We begin with a discussion of each of the research areas through brief reviews of relevant literature for each and a discussion of the theoretical and strategic implications associated with some CRM technologies and research areas. Next we present our framework, which focuses on e-CRM from the five research perspectives. We then present a theoretical framework for e-CRM in terms of the five research areas and how they affect one another, as well as e-CRM processes and both performance and non-performance outcomes.", "title": "Electronic Commerce Customer Relationship Management: A Research Agenda"}, "d4903a683c7b60a27a0c19c28d0a7774eb9dd373": {"paper_id": "d4903a683c7b60a27a0c19c28d0a7774eb9dd373", "abstract": "This paper aims to predict consumer acceptance of e-commerce by proposing a set of key drivers for engaging consumers in on-line transactions. The primary constructs for capturing consumer acceptance of e-commerce are intention to transact and on-line transaction behavior. Following the theory of reasoned action (TRA) as applied to a technology-driven environment, technology acceptance model (TAM) variables (perceived usefulness and ease of use) are posited as key drivers of e-commerce acceptance. The practical utility of TAM stems from the fact that e-commerce is technology-driven. The proposed model integrates trust and perceived risk, which are incorporated given the implicit uncertainty of the e-commerce environment. The proposed integration of the hypothesized independent variables is justified by placing all the variables under the nomological TRA structure and proposing their interrelationships. The resulting research model is tested using data from two empirical studies. The first, exploratory study comprises three experiential scenarios with 103 students. The second, confirmatory study uses a sample of 155 on-line consumers. Both studies strongly support the e-commerce acceptance model by validating the proposed hypotheses. The paper discusses the implications for e-commerce theory, research, and practice, and makes several suggestions for", "title": "Consumer Acceptance of Electronic Commerce: Integrating Trust and Risk with the Technology Acceptance Model"}, "9e03eb52f39e226656d2ae58e0e5c8e4d7564a18": {"paper_id": "9e03eb52f39e226656d2ae58e0e5c8e4d7564a18", "abstract": "Corporations are beginning to realize that the proverbial \u2018if we only knew what we know\u2019 also includes \u2018if we only knew what our customers know.\u2019 The authors discuss the concept of Customer Knowledge Management (CKM), which refers to the management of knowledge from customers, i.e. knowledge resident in customers. CKM is contrasted with knowledge about customers, e.g. customer characteristics and preferences prevalent in previous work on knowledge management and customer relationship management. Five styles of CKM are proposed and practically illustrated by way of corporate examples. Implications are discussed for knowledge management, the resource based view, and strategy process research. \uf8e9 2002 Elsevier Science Ltd. All rights reserved", "title": "Five Styles of Customer Knowledge Management , and How Smart Companies Use Them To Create Value"}, "f67449e965d9e23b2c2fb777d1c3d05133fcc3e8": {"paper_id": "f67449e965d9e23b2c2fb777d1c3d05133fcc3e8", "abstract": "Food production requires application of fertilizers containing phosphorus, nitrogen and potassium on agricultural fields in order to sustain crop yields. However modern agriculture is dependent on phosphorus derived from phosphate rock, which is a non-renewable resource and current global reserves may be depleted in 50\u2013100 years. While phosphorus demand is projected to increase, the expected global peak in phosphorus production is predicted to occur around 2030. The exact timing of peak phosphorus production might be disputed, however it is widely acknowledged within the fertilizer industry that the quality of remaining phosphate rock is decreasing and production costs are increasing. Yet future access to phosphorus receives little or no international attention. This paper puts forward the case for including long-term phosphorus scarcity on the priority agenda for global food security. Opportunities for recovering phosphorus and reducing demand are also addressed together with institutional challenges. 2009 Published by Elsevier Ltd.", "title": "The story of phosphorus : Global food security and food for thought"}, "fa95ecd69426d5a7f3ba5831d72f03f21c8e2cda": {"paper_id": "fa95ecd69426d5a7f3ba5831d72f03f21c8e2cda", "abstract": "The amount of text that is generated every day is increasing dramatically. This tremendous volume of mostly unstructured text cannot be simply processed and perceived by computers. Therefore, efficient and effective techniques and algorithms are required to discover useful patterns. Text mining is the task of extracting meaningful information from text, which has gained significant attentions in recent years. In this paper, we describe several of the most fundamental text mining tasks and techniques including text pre-processing, classification and clustering. Additionally, we briefly explain text mining in biomedical and health care domains.", "title": "A Brief Survey of Text Mining: Classification, Clustering and Extraction Techniques"}, "5e46fc68ede1108529f4db78bc7e1def69d70ba3": {"paper_id": "5e46fc68ede1108529f4db78bc7e1def69d70ba3", "abstract": "In this paper, we observe that there exists a second dimension to the relation extraction (RE) problem that is orthogonal to the relation type dimension. We show that most of these second dimensional structures are relatively constrained and not difficult to identify. We propose a novel algorithmic approach to RE that starts by first identifying these structures and then, within these, identifying the semantic type of the relation. In the real RE problem where relation arguments need to be identified, exploiting these structures also allows reducing pipelined propagated errors. We show that this RE framework provides significant improvement in RE performance.", "title": "Exploiting Syntactico-Semantic Structures for Relation Extraction"}, "27496a2ee337db705e7c611dea1fd8e6f41437c2": {"paper_id": "27496a2ee337db705e7c611dea1fd8e6f41437c2", "abstract": "We analyze some of the fundamental design challenges and misconceptions that underlie the development of an efficient and robust NER system. In particular, we address issues such as the representation of text chunks, the inference approach needed to combine local NER decisions, the sources of prior knowledge and how to use them within an NER system. In the process of comparing several solutions to these challenges we reach some surprising conclusions, as well as develop an NER system that achieves 90.8 F1 score on the CoNLL-2003 NER shared task, the best reported result for this dataset.", "title": "Design Challenges and Misconceptions in Named Entity Recognition"}, "50e983fd06143cad9d4ac75bffc2ef67024584f2": {"paper_id": "50e983fd06143cad9d4ac75bffc2ef67024584f2", "abstract": "LIBLINEAR is an open source library for large-scale linear classification. It supports logistic regression and linear support vector machines. We provide easy-to-use command-line tools and library calls for users and developers. Comprehensive documents are available for both beginners and advanced users. Experiments demonstrate that LIBLINEAR is very efficient on large sparse data sets.", "title": "LIBLINEAR: A Library for Large Linear Classification"}, "16ee10c390e4e4963083072e57ecce7999d00a65": {"paper_id": "16ee10c390e4e4963083072e57ecce7999d00a65", "abstract": "MOTIVATION\nThe discovery of regulatory pathways, signal cascades, metabolic processes or disease models requires knowledge on individual relations like e.g. physical or regulatory interactions between genes and proteins. Most interactions mentioned in the free text of biomedical publications are not yet contained in structured databases.\n\n\nRESULTS\nWe developed RelEx, an approach for relation extraction from free text. It is based on natural language preprocessing producing dependency parse trees and applying a small number of simple rules to these trees. We applied RelEx on a comprehensive set of one million MEDLINE abstracts dealing with gene and protein relations and extracted approximately 150,000 relations with an estimated performance of both 80% precision and 80% recall.\n\n\nAVAILABILITY\nThe used natural language preprocessing tools are free for use for academic research. Test sets and relation term lists are available from our website (http://www.bio.ifi.lmu.de/publications/RelEx/).", "title": "RelEx - Relation extraction using dependency parse trees"}, "06d0a9697a0f0242dbdeeff08ec5266b74bfe457": {"paper_id": "06d0a9697a0f0242dbdeeff08ec5266b74bfe457", "abstract": "We presenta novel generati ve model for natural languagetree structuresin whichsemantic(lexical dependenc y) andsyntacticstructuresare scoredwith separatemodels.Thisfactorizationprovidesconceptual simplicity, straightforwardopportunitiesfor separatelyimproving the componentmodels,anda level of performancealreadycloseto thatof similar, non-factoredmodels.Most importantly, unlikeothermodernparsing models,thefactoredmodeladmitsanextremelyeffectiveA parsingalgorithm,which makesefficient,exactinferencefeasible.", "title": "Fast Exact Inference with a Factored Model for Natural Language Parsing"}, "0150ebf22ed14e09e1191d88fd56c1c30f108268": {"paper_id": "0150ebf22ed14e09e1191d88fd56c1c30f108268", "abstract": "While today many online platforms employ complex algorithms to curate content, these algorithms are rarely highlighted in interfaces, preventing users from understanding these algorithms' operation or even existence. Here, we study how knowledgeable users are about these algorithms, showing that providing insight to users about an algorithm's existence or functionality through design facilitates rapid processing of the underlying algorithm models and increases users' engagement with the system. We also study algorithmic systems that might introduce bias to users' online experience to gain insight into users' behavior around biased algorithms. We will leverage these insights to build an algorithm-aware design that shapes a more informed interaction between users and algorithmic systems.", "title": "Understanding and Designing around Users' Interaction with Hidden Algorithms in Sociotechnical Systems"}, "4c1b546d0bc95209cbaa900a7aaeeddc159154c3": {"paper_id": "4c1b546d0bc95209cbaa900a7aaeeddc159154c3", "abstract": "In this paper, we present the first formal study of how mothers of young children (aged three and under) use social networking sites, particularly Facebook and Twitter, including mothers' perceptions of which SNSes are appropriate for sharing information about their children, changes in post style and frequency after birth, and the volume and nature of child-related content shared in these venues. Our findings have implications for improving the utility and usability of SNS tools for mothers of young children, as well as for creating and improving sociotechnical systems related to maternal and child health.", "title": "Social networking site use by mothers of young children"}, "91c09bed0c0caa8e3df87bf33d50edd242a1b997": {"paper_id": "91c09bed0c0caa8e3df87bf33d50edd242a1b997", "abstract": "We propose an approach to determine the ethnic breakdown of a population based solely on people\u2019s names and data provided by the U.S. Census Bureau. We demonstrate that our approach is able to predict the ethnicities of individuals as well as the ethnicity of an entire population better than natural alternatives. We apply our technique to the population of U.S. Facebook users and uncover the demographic characteristics of ethnicities and how they relate. We also discover that while Facebook has always been diverse, diversity has increased over time leading to a population that today looks very similar to the overall U.S. population. We also find that different ethnic groups relate to one another in an assortative manner, and that these groups have different profiles across demographics, beliefs, and usage of site features.", "title": "ePluribus: Ethnicity on Social Networks"}, "38da47d4fb6281690dc263e46840bb762b1ab5a7": {"paper_id": "38da47d4fb6281690dc263e46840bb762b1ab5a7", "abstract": "This study examines self-presentation in online dating profiles using a novel cross-validation technique for establishing accuracy. Eighty online daters rated the accuracy of their online self-presentation. Information about participants' physical attributes was then collected (height, weight, and age) and compared with their online profile, revealing that deviations tended to be ubiquitous but small in magnitude. Men lied more about their height, and women lied more about their weight, with participants farther from the mean lying more. Participants' self-ratings of accuracy were significantly correlated with observed accuracy, suggesting that inaccuracies were intentional rather than self-deceptive. Overall, participants reported being the least accurate about their photographs and the most accurate about their relationship information. Deception patterns suggest that participants strategically balanced the deceptive opportunities presented by online self-presentation (e.g., the editability of profiles) with the social constraints of establishing romantic relationships (e.g., the anticipation of future interaction).", "title": "Separating fact from fiction: an examination of deceptive self-presentation in online dating profiles."}, "a4d12c87adb67a38cbfe51d40785c08b030aa54b": {"paper_id": "a4d12c87adb67a38cbfe51d40785c08b030aa54b", "abstract": "We consider social media as a promising tool for public health, focusing on the use of Twitter posts to build predictive models about the forthcoming influence of childbirth on the behavior and mood of new mothers. Using Twitter posts, we quantify postpartum changes in 376 mothers along dimensions of social engagement, emotion, social network, and linguistic style. We then construct statistical models from a training set of observations of these measures before and after the reported childbirth, to forecast significant postpartum changes in mothers. The predictive models can classify mothers who will change significantly following childbirth with an accuracy of 71%, using observations about their prenatal behavior, and as accurately as 80-83% when additionally leveraging the initial 2-3 weeks of postnatal data. The study is motivated by the opportunity to use social media to identify mothers at risk of postpartum depression, an underreported health concern among large populations, and to inform the design of low-cost, privacy-sensitive early-warning systems and intervention programs aimed at promoting wellness postpartum.", "title": "Predicting postpartum changes in emotion and behavior via social media"}, "3411f97b3ed4761f3d20fffaa0cbcdd62e76e650": {"paper_id": "3411f97b3ed4761f3d20fffaa0cbcdd62e76e650", "abstract": "Many online platforms use curation algorithms that are opaque to the user. Recent work suggests that discovering a filtering algorithm's existence in a curated feed influences user experience, but it remains unclear how users reason about the operation of these algorithms. In this qualitative laboratory study, researchers interviewed a diverse, non-probability sample of 40 Facebook users before, during, and after being presented alternative displays of Facebook's News Feed curation algorithm's output. Interviews revealed 10 \"folk theories' of automated curation, some quite unexpected. Users who were given a probe into the algorithm's operation via an interface that incorporated \"seams,' visible hints disclosing aspects of automation operations, could quickly develop theories. Users made plans that depended on their theories. We conclude that foregrounding these automated processes may increase interface design complexity, but it may also add usability benefits.", "title": "First I \"like\" it, then I hide it: Folk Theories of Social Feeds"}, "a97db60641142fcbc50b380f514268c23664b76e": {"paper_id": "a97db60641142fcbc50b380f514268c23664b76e", "abstract": "Language in social media is a dynamic system, constantly evolving and adapting, with words and concepts rapidly emerging, disappearing, and changing their meaning. These changes can be estimated using word representations in context, over time and across locations. A number of methods have been proposed to track these spatiotemporal changes but no general method exists to evaluate the quality of these representations. Previous work largely focused on qualitative evaluation, which we improve by proposing a set of visualizations that highlight changes in text representation over both space and time. We demonstrate usefulness of novel spatiotemporal representations to explore and characterize specific aspects of the corpus of tweets collected from European countries over a two-week period centered around the terrorist attacks in Brussels in March 2016. In addition, we quantitatively evaluate spatiotemporal representations by feeding them into a downstream classification task \u2013 event type prediction. Thus, our work is the first to provide both intrinsic (qualitative) and extrinsic (quantitative) evaluation of text representations for spatiotemporal trends.", "title": "Intrinsic and Extrinsic Evaluation of Spatiotemporal Text Representations in Twitter Streams"}, "0183b3e9d84c15c7048e6c2149ed86257ccdc6cb": {"paper_id": "0183b3e9d84c15c7048e6c2149ed86257ccdc6cb", "abstract": "While continuous word embeddings are gaining popularity, current models are based solely on linear contexts. In this work, we generalize the skip-gram model with negative sampling introduced by Mikolov et al. to include arbitrary contexts. In particular, we perform experiments with dependency-based contexts, and show that they produce markedly different embeddings. The dependencybased embeddings are less topical and exhibit more functional similarity than the original skip-gram embeddings.", "title": "Dependency-Based Word Embeddings"}, "4561c13c2907f15398cdd34a272eb099be0c0587": {"paper_id": "4561c13c2907f15398cdd34a272eb099be0c0587", "abstract": "The idea that at least some aspects of word meaning can be induced from patterns of word co-occurrence is becoming increasingly popular. However, there is less agreement about the precise computations involved, and the appropriate tests to distinguish between the various possibilities. It is important that the effect of the relevant design choices and parameter values are understood if psychological models using these methods are to be reliably evaluated and compared. In this article, we present a systematic exploration of the principal computational possibilities for formulating and validating representations of word meanings from word co-occurrence statistics. We find that, once we have identified the best procedures, a very simple approach is surprisingly successful and robust over a range of psychologically relevant evaluation measures.", "title": "Extracting semantic representations from word co-occurrence statistics: a computational study."}, "1a07186bc10592f0330655519ad91652125cd907": {"paper_id": "1a07186bc10592f0330655519ad91652125cd907", "abstract": "We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in state-of-the-art-performance.", "title": "A unified architecture for natural language processing: deep neural networks with multitask learning"}, "1005645c05585c2042e3410daeed638b55e2474d": {"paper_id": "1005645c05585c2042e3410daeed638b55e2474d", "abstract": "Neural probabilistic language models (NPLMs) have been shown to be competitive with and occasionally superior to the widely-used n-gram language models. The main drawback of NPLMs is their extremely long training and testing times. Morin and Bengio have proposed a hierarchical language model built around a binary tree of words, which was two orders of magnitude faster than the nonhierarchical model it was based on. However, it performed considerably worse than its non-hierarchical counterpart in spite of using a word tree created using expert knowledge. We introduce a fast hierarchical language model along with a simple feature-based algorithm for automatic construction of word trees from the data. We then show that the resulting models can outperform non-hierarchical neural models as well as the best n-gram models.", "title": "A Scalable Hierarchical Distributed Language Model"}, "0826c98d1b1513aa2f45e6654bb5075a58b64649": {"paper_id": "0826c98d1b1513aa2f45e6654bb5075a58b64649", "abstract": "\u2022 Neural network language model and distributed representation for words (Vector representation) \u2022 Capture syntactic and remantic regularities in language \u2022 Outperform state-of-the-art", "title": "Linguistic Regularities in Continuous Space Word Representations"}, "3bff03b7b0b0c4e8f6384dbb2a95e4338d156524": {"paper_id": "3bff03b7b0b0c4e8f6384dbb2a95e4338d156524", "abstract": "We introduce a novel machine learning framework based on recursive autoencoders for sentence-level prediction of sentiment label distributions. Our method learns vector space representations for multi-word phrases. In sentiment prediction tasks these representations outperform other state-of-the-art approaches on commonly used datasets, such as movie reviews, without using any pre-defined sentiment lexica or polarity shifting rules. We also evaluate the model\u2019s ability to predict sentiment distributions on a new dataset based on confessions from the experience project. The dataset consists of personal user stories annotated with multiple labels which, when aggregated, form a multinomial distribution that captures emotional reactions. Our algorithm can more accurately predict distributions over such labels compared to several competitive baselines.", "title": "Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions"}, "10dd5320f568a41ac72cdb4a148cf6809eadd0dd": {"paper_id": "10dd5320f568a41ac72cdb4a148cf6809eadd0dd", "abstract": "This paper presents and compares WordNetbased and distributional similarity approaches. The strengths and weaknesses of each approach regarding similarity and relatedness tasks are discussed, and a combination is presented. Each of our methods independently provide the best results in their class on the RG and WordSim353 datasets, and a supervised combination of them yields the best published results on all datasets. Finally, we pioneer cross-lingual similarity, showing that our methods are easily adapted for a cross-lingual task with minor losses.", "title": "A Study on Similarity and Relatedness Using Distributional and WordNet-based Approaches"}, "3a0e788268fafb23ab20da0e98bb578b06830f7d": {"paper_id": "3a0e788268fafb23ab20da0e98bb578b06830f7d", "abstract": "Computers understand very little of the meaning of human language. This profoundly limits our ability to give instructions to computers, the ability of computers to explain their actions to us, and the ability of computers to analyse and process text. Vector space models (VSMs) of semantics are beginning to address these limits. This paper surveys the use of VSMs for semantic processing of text. We organize the literature on VSMs according to the structure of the matrix in a VSM. There are currently three broad classes of VSMs, based on term\u2013document, word\u2013context, and pair\u2013pattern matrices, yielding three classes of applications. We survey a broad range of applications in these three categories and we take a detailed look at a specific open source project in each category. Our goal in this survey is to show the breadth of applications of VSMs for semantics, to provide a new perspective on VSMs for those who are already familiar with the area, and to provide pointers into the literature for those who are less familiar with the field.", "title": "From Frequency to Meaning: Vector Space Models of Semantics"}, "1f8b5734ab57a81875ed52f708c9bb9d5f4cad2c": {"paper_id": "1f8b5734ab57a81875ed52f708c9bb9d5f4cad2c", "abstract": "We introduce a model for incorporating contextual information (such as geography) in learning vector-space representations of situated language. In contrast to approaches to multimodal representation learning that have used properties of the object being described (such as its color), our model includes information about the subject (i.e., the speaker), allowing us to learn the contours of a word\u2019s meaning that are shaped by the context in which it is uttered. In a quantitative evaluation on the task of judging geographically informed semantic similarity between representations learned from 1.1 billion words of geo-located tweets, our joint model outperforms comparable independent models that learn meaning in isolation.", "title": "Distributed Representations of Geographically Situated Language"}, "03efbbf1c8fa661f31fe94efcc894ad95eeff3e2": {"paper_id": "03efbbf1c8fa661f31fe94efcc894ad95eeff3e2", "abstract": "Social media outlets such as Twitter have become an important forum for peer interaction. Thus the ability to classify latent user attributes, including gender, age, regional origin, and political orientation solely from Twitter user language or similar highly informal content has important applications in advertising, personalization, and recommendation. This paper includes a novel investigation of stacked-SVM-based classification algorithms over a rich set of original features, applied to classifying these four user attributes. It also includes extensive analysis of features and approaches that are effective and not effective in classifying user attributes in Twitter-style informal written genres as distinct from the other primarily spoken genres previously studied in the user-property classification literature. Our models, singly and in ensemble, significantly outperform baseline models in all cases. A detailed analysis of model components and features provides an often entertaining insight into distinctive language-usage variation across gender, age, regional origin and political orientation in modern informal communication.", "title": "Classifying latent user attributes in twitter"}, "2011feb353fed560b0643dc9db6528317c643957": {"paper_id": "2011feb353fed560b0643dc9db6528317c643957", "abstract": "More and more technologies are taking advantage of the explosion of social media (Web search, content recommendation services, marketing, ad targeting, etc.). This paper focuses on the problem of automatically constructing user profiles, which can significantly benefit such technologies. We describe a general and robust machine learning framework for large-scale classification of social media users according to dimensions of interest. We report encouraging experimental results on 3 tasks with different characteristics: political affiliation detection, ethnicity identification and detecting affinity for a particular business.", "title": "Democrats, republicans and starbucks afficionados: user classification in twitter"}, "0ff9ea8409c932baf3c0302c89ede79add1431aa": {"paper_id": "0ff9ea8409c932baf3c0302c89ede79add1431aa", "abstract": "We introduce a globally normalized transition-based neural network model that achieves state-of-the-art part-ofspeech tagging, dependency parsing and sentence compression results. Our model is a simple feed-forward neural network that operates on a task-specific transition system, yet achieves comparable or better accuracies than recurrent models. We discuss the importance of global as opposed to local normalization: a key insight is that the label bias problem implies that globally normalized models can be strictly more expressive than locally normalized models.", "title": "Globally Normalized Transition-Based Neural Networks"}, "396b7932beac62a72288eaea047981cc9a21379a": {"paper_id": "396b7932beac62a72288eaea047981cc9a21379a", "abstract": "We propose a technique for learning representations of parser states in transitionbased dependency parsers. Our primary innovation is a new control structure for sequence-to-sequence neural networks\u2014 the stack LSTM. Like the conventional stack data structures used in transitionbased parsing, elements can be pushed to or popped from the top of the stack in constant time, but, in addition, an LSTM maintains a continuous space embedding of the stack contents. This lets us formulate an efficient parsing model that captures three facets of a parser\u2019s state: (i) unbounded look-ahead into the buffer of incoming words, (ii) the complete history of actions taken by the parser, and (iii) the complete contents of the stack of partially built tree fragments, including their internal structures. Standard backpropagation techniques are used for training and yield state-of-the-art parsing performance.", "title": "Transition-Based Dependency Parsing with Stack Long Short-Term Memory"}, "c3227702dd212965157a615332f3dd78b0f11b5e": {"paper_id": "c3227702dd212965157a615332f3dd78b0f11b5e", "abstract": "We propose a non-linear graphical model for structured prediction. It combines the power of deep neural networks to extract high level features with the graphical framework of Markov networks, yielding a powerful and scalable probabilistic model that we apply to signal labeling tasks.", "title": "Neural conditional random fields"}, "1841e687a6a9f092574477d62784c13ce6361ae3": {"paper_id": "1841e687a6a9f092574477d62784c13ce6361ae3", "abstract": "This paper describes a parsing model that combines the exact dynamic programming of CRF parsing with the rich nonlinear featurization of neural net approaches. Our model is structurally a CRF that factors over anchored rule productions, but instead of linear potential functions based on sparse features, we use nonlinear potentials computed via a feedforward neural network. Because potentials are still local to anchored rules, structured inference (CKY) is unchanged from the sparse case. Computing gradients during learning involves backpropagating an error signal formed from standard CRF sufficient statistics (expected rule counts). Using only dense features, our neural CRF already exceeds a strong baseline CRF model (Hall et al., 2014). In combination with sparse features, our system1 achieves 91.1 F1 on section 23 of the Penn Treebank, and more generally outperforms the best prior single parser results on a range of languages.", "title": "Neural CRF Parsing"}, "75b0c67e9c81a880c29cb8d8a2dbcb7c96f4d644": {"paper_id": "75b0c67e9c81a880c29cb8d8a2dbcb7c96f4d644", "abstract": "We present a comprehensive study of evaluation methods for unsupervised embedding techniques that obtain meaningful representations of words from text. Different evaluations result in different orderings of embedding methods, calling into question the common assumption that there is one single optimal vector representation. We present new evaluation techniques that directly compare embeddings with respect to specific queries. These methods reduce bias, provide greater insight, and allow us to solicit data-driven relevance judgments rapidly and accurately through crowdsourcing.", "title": "Evaluation methods for unsupervised word embeddings"}, "51c49cc4654dbce3c3de2919800da1e7477d88b3": {"paper_id": "51c49cc4654dbce3c3de2919800da1e7477d88b3", "abstract": "Distributional representations of words have been recently used in supervised settings for recognizing lexical inference relations between word pairs, such as hypernymy and entailment. We investigate a collection of these state-of-the-art methods, and show that they do not actually learn a relation between two words. Instead, they learn an independent property of a single word in the pair: whether that word is a \u201cprototypical hypernym\u201d.", "title": "Do Supervised Distributional Methods Really Learn Lexical Inference Relations?"}, "9be7d5bcee2128021fcf31d638e10d96fd6684a4": {"paper_id": "9be7d5bcee2128021fcf31d638e10d96fd6684a4", "abstract": "Due to the short and noisy nature of Twitter microposts, detecting named entities is often a cumbersome task. As part of the ACL2015 Named Entity Recognition (NER) shared task, we present a semisupervised system that detects 10 types of named entities. To that end, we leverage 400 million Twitter microposts to generate powerful word embeddings as input features and use a neural network to execute the classification. To further boost the performance, we employ dropout to train the network and leaky Rectified Linear Units (ReLUs). Our system achieved the fourth position in the final ranking, without using any kind of hand-crafted features such as lexical features or gazetteers.", "title": "Multimedia Lab $@$ ACL WNUT NER Shared Task: Named Entity Recognition for Twitter Microposts using Distributed Word Representations"}, "666b639aadcd2a8a11d24b36bae6a4f07e802b34": {"paper_id": "666b639aadcd2a8a11d24b36bae6a4f07e802b34", "abstract": "Word representations have proven useful for many NLP tasks, e.g., Brown clusters as features in dependency parsing (Koo et al., 2008). In this paper, we investigate the use of continuous word representations as features for dependency parsing. We compare several popular embeddings to Brown clusters, via multiple types of features, in both news and web domains. We find that all embeddings yield significant parsing gains, including some recent ones that can be trained in a fraction of the time of others. Explicitly tailoring the representations for the task leads to further improvements. Moreover, an ensemble of all representations achieves the best results, suggesting their complementarity.", "title": "Tailoring Continuous Word Representations for Dependency Parsing"}, "1b2f2bb90fb08d0e02eabb152120dbf1d6e5837e": {"paper_id": "1b2f2bb90fb08d0e02eabb152120dbf1d6e5837e", "abstract": "We present a family of neural-network\u2013 inspired models for computing continuous word representations, specifically designed to exploit both monolingual and multilingual text. This framework allows us to perform unsupervised training of embeddings that exhibit higher accuracy on syntactic and semantic compositionality, as well as multilingual semantic similarity, compared to previous models trained in an unsupervised fashion. We also show that such multilingual embeddings, optimized for semantic similarity, can improve the performance of statistical machine translation with respect to how it handles words not present in the parallel data.", "title": "Multilingual Word Embeddings using Multigraphs"}, "61777bb512c5c1327778d9d7698e1994bdf5ca9a": {"paper_id": "61777bb512c5c1327778d9d7698e1994bdf5ca9a", "abstract": "Recent work in learning bilingual representations tend to tailor towards achieving good performance on bilingual tasks, most often the crosslingual document classification (CLDC) evaluation, but to the detriment of preserving clustering structures of word representations monolingually. In this work, we propose a joint model to learn word representations from scratch that utilizes both the context coocurrence information through the monolingual component and the meaning equivalent signals from the bilingual constraint. Specifically, we extend the recently popular skipgram model to learn high quality bilingual representations efficiently. Our learned embeddings achieve a new state-of-the-art accuracy of 80.3 for the German to English CLDC task and a highly competitive performance of 90.7 for the other classification direction. At the same time, our models outperform best embeddings from past bilingual representation work by a large margin in the monolingual word similarity evaluation.1", "title": "Bilingual Word Representations with Monolingual Quality in Mind"}, "184b1c7403e727d6f4355fa6509eff1cfe89a6d3": {"paper_id": "184b1c7403e727d6f4355fa6509eff1cfe89a6d3", "abstract": "Topic models are a useful tool for analyzing large text collections, but have previously been applied in only monolingual, or at most bilingual, contexts. Meanwhile, massive collections of interlinked documents in dozens of languages, such as Wikipedia, are now widely available, calling for tools that can characterize content in many languages. We introduce a polylingual topic model that discovers topics aligned across multiple languages. We explore the model\u2019s characteristics using two large corpora, each with over ten different languages, and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.", "title": "Polylingual Topic Models"}, "1df176123ada3c3aebba3d9fcec55386091b0d13": {"paper_id": "1df176123ada3c3aebba3d9fcec55386091b0d13", "abstract": "We consider the task of estimating, from observed data, a pro babilistic model that is parameterized by a finite number of parameters. In particular, we are consid ering the situation where the model probability density function is unnormalized. That is, the model is only specified up to the partition function. The partition function normalizes a model so that it integrates to one for any choice of the parameters. However, it is often impossible to obtain it closed form. Gibbs distributions, Markov and multi-layer networks are examples of models wher e analytical normalization is often impossible. Maximum likelihood estimation can then not be u s d without resorting to numerical approximations which are often computationally expensive . W propose here a new objective function for the estimation of both normalized and unnormalized models. The basic idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially generated noise. With this approach, the normalizing partition f u ction can be estimated like any other parameter. We prove that the new estimation method leads to a consistent (convergent) estimator of the parameters. For large noise sample sizes, the new esti mator is furthermore shown to behave like the maximum likelihood estimator. In the estimati on of unnormalized models, there is a trade-off between statistical and computational performa nce. We show that the new method strikes a competitive trade-off in comparison to other estimation m ethods for unnormalized models. As an application to real data, we estimate novel two-layer model s of natural image statistics with spline nonlinearities.", "title": "Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics"}, "9703efad5e36e1ef3ab2292144c1a796515e5f6a": {"paper_id": "9703efad5e36e1ef3ab2292144c1a796515e5f6a", "abstract": "We describe a series o,f five statistical models o,f the translation process and give algorithms,for estimating the parameters o,f these models given a set o,f pairs o,f sentences that are translations o,f one another. We define a concept o,f word-by-word alignment between such pairs o,f sentences. For any given pair of such sentences each o,f our models assigns a probability to each of the possible word-by-word alignments. We give an algorithm for seeking the most probable o,f these alignments. Although the algorithm is suboptimal, the alignment thus obtained accounts well for the word-by-word relationships in the pair o,f sentences. We have a great deal o,f data in French and English from the proceedings o,f the Canadian Parliament. Accordingly, we have restricted our work to these two languages; but we,feel that because our algorithms have minimal linguistic content they would work well on other pairs o,f languages. We also ,feel, again because of the minimal linguistic content o,f our algorithms, that it is reasonable to argue that word-by-word alignments are inherent in any sufficiently large bilingual corpus.", "title": "The Mathematics of Statistical Machine Translation: Parameter Estimation"}, "04e0fefb859f4b02b017818915a2645427bfbdb2": {"paper_id": "04e0fefb859f4b02b017818915a2645427bfbdb2", "abstract": "Recurrent neural network language models (RNNLMs) have recently demonstrated state-of-the-art performance across a variety of tasks. In this paper, we improve their performance by providing a contextual real-valued input vector in association with each word. This vector is used to convey contextual information about the sentence being modeled. By performing Latent Dirichlet Allocation using a block of preceding text, we achieve a topic-conditioned RNNLM. This approach has the key advantage of avoiding the data fragmentation associated with building multiple topic models on different data subsets. We report perplexity results on the Penn Treebank data, where we achieve a new state-of-the-art. We further apply the model to the Wall Street Journal speech recognition task, where we observe improvements in word-error-rate.", "title": "Context dependent recurrent neural network language model"}, "28d9c89f946a39d3c4146bf32ddff546845e1801": {"paper_id": "28d9c89f946a39d3c4146bf32ddff546845e1801", "abstract": "We present a simple and effective semisupervised method for training dependency parsers. We focus on the problem of lexical representation, introducing features that incorporate word clusters derived from a large unannotated corpus. We demonstrate the effectiveness of the approach in a series of dependency parsing experiments on the Penn Treebank and Prague Dependency Treebank, and we show that the cluster-based features yield substantial gains in performance across a wide range of conditions. For example, in the case of English unlabeled second-order parsing, we improve from a baseline accuracy of 92.02% to 93.16%, and in the case of Czech unlabeled second-order parsing, we improve from a baseline accuracy of 86.13% to 87.13%. In addition, we demonstrate that our method also improves performance when small amounts of training data are available, and can roughly halve the amount of supervised data required to reach a desired level of performance.", "title": "Simple Semi-supervised Dependency Parsing"}, "b969803114b8406f253c6ec93d636ba35280fc8f": {"paper_id": "b969803114b8406f253c6ec93d636ba35280fc8f", "abstract": "Dependency parsers are critical components within many NLP systems. However, currently available dependency parsers each exhibit at least one of several weaknesses, including high running time, limited accuracy, vague dependency labels, and lack of nonprojectivity support. Furthermore, no commonly used parser provides additional shallow semantic interpretation, such as preposition sense disambiguation and noun compound interpretation. In this paper, we present a new dependency-tree conversion of the Penn Treebank along with its associated fine-grain dependency labels and a fast, accurate parser trained on it. We explain how a non-projective extension to shift-reduce parsing can be incorporated into non-directional easy-first parsing. The parser performs well when evaluated on the standard test section of the Penn Treebank, outperforming several popular open source dependency parsers; it is, to the best of our knowledge, the first dependency parser capable of parsing more than 75 sentences per second at over 93% accuracy.", "title": "A Fast, Accurate, Non-Projective, Semantically-Enriched Parser"}, "c829b63a3ae72a47e1953e1295826c7b2f93bf50": {"paper_id": "c829b63a3ae72a47e1953e1295826c7b2f93bf50", "abstract": "The recently introduced continuous Skip-gram model is an ef fici nt method for learning high-quality distributed vector representation s that capture a large number of precise syntactic and semantic word relationships. I n this paper we present several extensions that improve both the quality of the vect ors and the training speed. By subsampling of the frequent words we obtain signifi ca t speedup and also learn more regular word representations. We also descr ib a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their ind ifference to word order and their inability to represent idiomatic phrases. For exa mple, the meanings of \u201cCanada\u201d and \u201cAir\u201d cannot be easily combined to obtain \u201cAir C anada\u201d. Motivated by this example, we present a simple method for finding phrase s in t xt, and show that learning good vector representations for millions of p hrases is possible.", "title": "Distributed Representations of Words and Phrases and their Compositionality"}, "5d4247ebdca64fc7d9f5a22654d4414a5f3b4ce5": {"paper_id": "5d4247ebdca64fc7d9f5a22654d4414a5f3b4ce5", "abstract": "Untyped dependency parsing can be viewed as the problem of finding maximum spanning trees (MSTs) in directed graphs. Using this representation, the Eisner (1996) parsing algorithm is sufficient for searching the space of projective trees. More importantly, the representation is extended naturally to non-projective parsing using Chu-Liu-Edmonds (Chu and Liu, 1965; Edmonds, 1967) MST algorithm. These efficient parse search methods support large-margin discriminative training methods for learning dependency parsers. We evaluate these methods experimentally on the English and Czech treebanks. Comments University of Pennsylvania Department of Computer and Information Science Technical Report No. MSCIS-06-11. This technical report is available at ScholarlyCommons: http://repository.upenn.edu/cis_reports/55 Spanning Tree Methods for Discriminative Training of Dependency Parsers Ryan McDonald Koby Crammer Fernando Pereira Department of Computer and Information Science University of Pennsylvania Philadelphia, PA, 19107 {ryantm,crammer,pereira}@cis.upenn.edu", "title": "Spanning Tree Methods for Discriminative Training of Dependency Parsers"}, "3d8c9e6af31a0f4cd3cd47706a8735167ca95b0b": {"paper_id": "3d8c9e6af31a0f4cd3cd47706a8735167ca95b0b", "abstract": "Recently, there has been substantial interest in using large amounts of unlabeled data to learn word representations which can then be used as features in supervised classifiers for NLP tasks. However, most current approaches are slow to train, do not model the context of the word, and lack theoretical grounding. In this paper, we present a new learning method, Low Rank Multi-View Learning (LR-MVL) which uses a fast spectral method to estimate low dimensional context-specific word representations from unlabeled data. These representation features can then be used with any supervised learner. LR-MVL is extremely fast, gives guaranteed convergence to a global optimum, is theoretically elegant, and achieves state-ofthe-art performance on named entity recognition (NER) and chunking problems.", "title": "Multi-View Learning of Word Embeddings via CCA"}, "2990cf242558ede739d6a26a2f8b098f94390323": {"paper_id": "2990cf242558ede739d6a26a2f8b098f94390323", "abstract": "Languages with rich inflectional morphology exhibit lexical data sparsity, since the word used to express a given concept will vary with the syntactic context. For instance, each count noun in Czech has 12 forms (where English uses only singular and plural). Even in large corpora, we are unlikely to observe all inflections of a given lemma. This reduces the vocabulary coverage of methods that induce continuous representations for words from distributional corpus information. We solve this problem by exploiting existing morphological resources that can enumerate a word\u2019s component morphemes. We present a latentvariable Gaussian graphical model that allows us to extrapolate continuous representations for words not observed in the training corpus, as well as smoothing the representations provided for the observed words. The latent variables represent embeddings of morphemes, which combine to create embeddings of words. Over several languages and training sizes, our model improves the embeddings for words, when evaluated on an analogy task, skip-gram predictive accuracy, and word similarity.", "title": "Morphological Smoothing and Extrapolation of Word Embeddings"}, "00a28138c74869cfb8236a18a4dbe3a896f7a812": {"paper_id": "00a28138c74869cfb8236a18a4dbe3a896f7a812", "abstract": "Vector-space word representations have been very successful in recent years at improving performance across a variety of NLP tasks. However, common to most existing work, words are regarded as independent entities without any explicit relationship among morphologically related words being modeled. As a result, rare and complex words are often poorly estimated, and all unknown words are represented in a rather crude way using only one or a few vectors. This paper addresses this shortcoming by proposing a novel model that is capable of building representations for morphologically complex words from their morphemes. We combine recursive neural networks (RNNs), where each morpheme is a basic unit, with neural language models (NLMs) to consider contextual information in learning morphologicallyaware word representations. Our learned models outperform existing word representations by a good margin on word similarity tasks across many datasets, including a new dataset we introduce focused on rare words to complement existing ones in an interesting way.", "title": "Better Word Representations with Recursive Neural Networks for Morphology"}, "62579cf2192c1513048addb54971b440b6a6631b": {"paper_id": "62579cf2192c1513048addb54971b440b6a6631b", "abstract": "The goal of this note is to point out that any distributed representation can be turned into a classifier through inversion via Bayes rule. The approach is simple and modular, in that it will work with any language representation whose training can be formulated as optimizing a probability model. In our application to 2 million sentences from Yelp reviews, we also find that it performs as well as or better than complex purpose-built algorithms.", "title": "Document Classification by Inversion of Distributed Language Representations"}, "f70391f9a6aeec75bc52b9fe38588b9d0b4d40c6": {"paper_id": "f70391f9a6aeec75bc52b9fe38588b9d0b4d40c6", "abstract": "Distributed word representations have recently been proven to be an invaluable resource for NLP. These representations are normally learned using neural networks and capture syntactic and semantic information about words. Information about word morphology and shape is normally ignored when learning word representations. However, for tasks like part-of-speech tagging, intra-word information is extremely useful, specially when dealing with morphologically rich languages. In this paper, we propose a deep neural network that learns character-level representation of words and associate them with usual word representations to perform POS tagging. Using the proposed approach, while avoiding the use of any handcrafted feature, we produce stateof-the-art POS taggers for two languages: English, with 97.32% accuracy on the Penn Treebank WSJ corpus; and Portuguese, with 97.47% accuracy on the Mac-Morpho corpus, where the latter represents an error reduction of 12.2% on the best previous known result.", "title": "Learning Character-level Representations for Part-of-Speech Tagging"}, "8744030818d5e890fcd03c186714755e6c3cf2b1": {"paper_id": "8744030818d5e890fcd03c186714755e6c3cf2b1", "abstract": "This paper presents a universal morphological feature schema that represents the finest distinctions in meaning that are expressed by overt, affixal inflectional morphology across languages. This schema is used to universalize data extracted from Wiktionary via a robust multidimensional table parsing algorithm and feature mapping algorithms, yielding 883,965 instantiated paradigms in 352 languages. These data are shown to be effective for training morphological analyzers, yielding significant accuracy gains when applied to Durrett and DeNero\u2019s (2013) paradigm learning framework.", "title": "A Language-Independent Feature Schema for Inflectional Morphology"}, "29c34a034f6f35915a141dac98cabf625bea2b3c": {"paper_id": "29c34a034f6f35915a141dac98cabf625bea2b3c", "abstract": "Conditional random fields (Lafferty et al., 2001) are quite effective at sequence labeling tasks like shallow parsing (Sha and Pereira, 2003) and namedentity extraction (McCallum and Li, 2003). CRFs are log-linear, allowing the incorporation of arbitrary features into the model. To train on u labeled data, we requireunsupervisedestimation methods for log-linear models; few exist. We describe a novel approach,contrastive estimation . We show that the new technique can be intuitively understood as exploiting implicit negative evidence and is computationally efficient. Applied to a sequence labeling problem\u2014POS tagging given a tagging dictionary and unlabeled text\u2014contrastive estimation outperforms EM (with the same feature set), is more robust to degradations of the dictionary, and can largely recover by modeling additional features.", "title": "Contrastive Estimation: Training Log-Linear Models on Unlabeled Data"}, "27e38351e48fe4b7da2775bf94341738bc4da07e": {"paper_id": "27e38351e48fe4b7da2775bf94341738bc4da07e", "abstract": "Single-word vector space models have been very successful at learning lexical information. However, they cannot capture the compositional meaning of longer phrases, preventing them from a deeper understanding of language. We introduce a recursive neural network (RNN) model that learns compositional vector representations for phrases and sentences of arbitrary syntactic type and length. Our model assigns a vector and a matrix to every node in a parse tree: the vector captures the inherent meaning of the constituent, while the matrix captures how it changes the meaning of neighboring words or phrases. This matrix-vector RNN can learn the meaning of operators in propositional logic and natural language. The model obtains state of the art performance on three different experiments: predicting fine-grained sentiment distributions of adverb-adjective pairs; classifying sentiment labels of movie reviews and classifying semantic relationships such as cause-effect or topic-message between nouns using the syntactic path between them.", "title": "Semantic Compositionality through Recursive Matrix-Vector Spaces"}, "4716cca7c5c936d4db5d0faa62673997e3bdff3c": {"paper_id": "4716cca7c5c936d4db5d0faa62673997e3bdff3c", "abstract": "The state of advice given to people today on how to stay safe online has plenty of room for improvement. Too many things are asked of them, which may be unrealistic, time consuming, or not really worth the effort. To improve the security advice, our community must find out what practices people use and what recommendations, if messaged well, are likely to bring the highest benefit while being realistic to ask of people. In this paper, we present the results of a study which aims to identify which practices people do that they consider most important at protecting their security online. We compare self-reported security practices of non-experts to those of security experts (i.e., participants who reported having five or more years of experience working in computer security). We report on the results of two online surveys\u2014one with 231 security experts and one with 294 MTurk participants\u2014on what the practices and attitudes of each group are. Our findings show a discrepancy between the security practices that experts and non-experts report taking. For instance, while experts most frequently report installing software updates, using two-factor authentication and using a password manager to stay safe online, non-experts report using antivirus software, visiting only known websites, and changing passwords frequently.", "title": "\"...No one Can Hack My Mind\": Comparing Expert and Non-Expert Security Practices"}, "3f770cc7662340485f8fb328b3f2c95403a08e8d": {"paper_id": "3f770cc7662340485f8fb328b3f2c95403a08e8d", "abstract": "We empirically assess whether browser security warnings are as ineffective as suggested by popular opinion and previous literature. We used Mozilla Firefox and Google Chrome\u2019s in-browser telemetry to observe over 25 million warning impressions in situ. During our field study, users continued through a tenth of Mozilla Firefox\u2019s malware and phishing warnings, a quarter of Google Chrome\u2019s malware and phishing warnings, and a third of Mozilla Firefox\u2019s SSL warnings. This demonstrates that security warnings can be effective in practice; security experts and system architects should not dismiss the goal of communicating security information to end users. We also find that user behavior varies across warnings. In contrast to the other warnings, users continued through 70.2% of Google Chrome\u2019s SSL warnings. This indicates that the user experience of a warning can have a significant impact on user behavior. Based on our findings, we make recommendations for warning designers and researchers.", "title": "Alice in Warningland: A Large-Scale Field Study of Browser Security Warning Effectiveness"}, "12d6cf6346f6d693b6dc3b88d176a8a7b192355c": {"paper_id": "12d6cf6346f6d693b6dc3b88d176a8a7b192355c", "abstract": "To build systems shielding users from fraudulent (or phishing) websites, designers need to know which attack strategies work and why. This paper provides the first empirical evidence about which malicious strategies are successful at deceiving general users. We first analyzed a large set of captured phishing attacks and developed a set of hypotheses about why these strategies might work. We then assessed these hypotheses with a usability study in which 22 participants were shown 20 web sites and asked to determine which ones were fraudulent. We found that 23% of the participants did not look at browser-based cues such as the address bar, status bar and the security indicators, leading to incorrect choices 40% of the time. We also found that some visual deception attacks can fool even the most sophisticated users. These results illustrate that standard security indicators are not effective for a substantial fraction of users, and suggest that alternative approaches are needed.", "title": "Why phishing works"}, "ff8e5b85877cf7364ce959cb965e6656ae8b9cb4": {"paper_id": "ff8e5b85877cf7364ce959cb965e6656ae8b9cb4", "abstract": "BACKGROUND\nAbnormal scar development following burn injury can cause substantial physical and psychological distress to children and their families. Common burn scar prevention and management techniques include silicone therapy, pressure garment therapy, or a combination of both. Currently, no definitive, high-quality evidence is available for the effectiveness of topical silicone gel or pressure garment therapy for the prevention and management of burn scars in the paediatric population. Thus, this study aims to determine the effectiveness of these treatments in children.\n\n\nMETHODS\nA randomised controlled trial will be conducted at a large tertiary metropolitan children's hospital in Australia. Participants will be randomised to one of three groups: Strataderm\u00ae topical silicone gel only, pressure garment therapy only, or combined Strataderm\u00ae topical silicone gel and pressure garment therapy. Participants will include 135 children (45 per group) up to 16\u00a0years of age who are referred for scar management for a new burn. Children up to 18\u00a0years of age will also be recruited following surgery for burn scar reconstruction. Primary outcomes are scar itch intensity and scar thickness. Secondary outcomes include scar characteristics (e.g. colour, pigmentation, pliability, pain), the patient's, caregiver's and therapist's overall opinion of the scar, health service costs, adherence, health-related quality of life, treatment satisfaction and adverse effects. Measures will be completed on up to two sites per person at baseline and 1\u00a0week post scar management commencement, 3\u00a0months and 6\u00a0months post burn, or post burn scar reconstruction. Data will be analysed using descriptive statistics and univariate and multivariate regression analyses.\n\n\nDISCUSSION\nResults of this study will determine the effectiveness of three noninvasive scar interventions in children at risk of, and with, scarring post burn or post reconstruction.\n\n\nTRIAL REGISTRATION\nAustralian New Zealand Clinical Trials Registry, ACTRN12616001100482 . Registered on 5 August 2016.", "title": "Effectiveness of topical silicone gel and pressure garment therapy for burn scar prevention and management in children: study protocol for a randomised controlled trial"}, "e18e148c747697f79045842c10bd54f3cf15b038": {"paper_id": "e18e148c747697f79045842c10bd54f3cf15b038", "abstract": "This paper reviews the past and current research progress in microwave antenna applications using 2D planar metamaterials or metasurfaces. Firstly, we revisit the definitions of metamaterial and metasurface, and the applications of the ultra-thin metasurfaces for low-profile printed antennas in microwave frequency regime, for the purposes of size reduction and performance enhancement. Then we summarize the different types of metasurfaces and their applications. Finally, we provide the future lookout into microwave antenna and EM related applications using metasurfaces.", "title": "Microwave antenna applications of metasurfaces"}, "915581e6cdd2961d09594577951b82309a31f90e": {"paper_id": "915581e6cdd2961d09594577951b82309a31f90e", "abstract": "Bandwidth and gain enhancement of microstrip patch antennas (MPAs) is proposed using reflective metasurface (RMS) as a superstrate. Two different types of the RMS, namelythe double split-ring resonator (DSR) and double closed-ring resonator (DCR) are separately investigated. The two antenna prototypes were manufactured, measured and compared. The experimental results confirm that the RMS loaded MPAs achieve high-gain as well as bandwidth improvement. The desinged antenna using the RMS as a superstrate has a high-gain of over 9.0 dBi and a wide impedance bandwidth of over 13%. The RMS is also utilized to achieve a thin antenna with a cavity height of 6 mm, which is equivalent to \u03bb/21 at the center frequency of 2.45 GHz. At the same time, the cross polarization level and front-to-back ratio of these antennas are also examined. key words: wideband, high-gain, metamaterial, Fabry-Perot cavity (FPC), frequency selective surface (FSS)", "title": "Bandwidth and Gain Enhancement of Microstrip Patch Antennas Using Reflective Metasurface"}, "95d7fb618d343b03dd766660b374b3079aa8dfb2": {"paper_id": "95d7fb618d343b03dd766660b374b3079aa8dfb2", "abstract": "We propose an improved method to retrieve the effective constitutive parameters (permittivity and permeability) of a slab of metamaterial from the measurement of S parameters. Improvements over existing methods include the determination of the first boundary and the thickness of the effective slab, the selection of the correct sign of effective impedance, and a mathematical method to choose the correct branch of the real part of the refractive index. The sensitivity of the effective constitutive parameters to the accuracy of the S parameters is also discussed. The method has been applied to various metamaterials and the successful retrieval results prove its effectiveness and robustness.", "title": "Robust method to retrieve the constitutive effective parameters of metamaterials."}, "0b715c8f615731f5b6d45832b6f0991f7a17182b": {"paper_id": "0b715c8f615731f5b6d45832b6f0991f7a17182b", "abstract": "This letter presents a polarization-reconfigurable compact slot antenna with reduced radar cross section (RCS) using an asymmetric cross-shaped metasurface (MS). The proposed MS can reconfigure the polarization of the slot antenna between right-hand circular polarization (RHCP), left-hand circular polarization (LHCP), and linear polarization (LP) by rotating it with respect to the center of the slot antenna. In addition, the MS reduces the RCS of the slot antenna significantly in all polarization states. The cross-slot MS is placed just over the planar slot antenna without any air gap. The simulated monostatic RCS of -19.5 dBsm is observed at 4.4 GHz for LHCP and RHCP cases and -17.0 dBsm for LP mode of operation. Antenna performance in terms of its input matching, far-field parameters, monostatic RCS, and axial ratio are measured at its three polarization states, which are in agreement with simulated results.", "title": "Low-RCS and Polarization-Reconfigurable Antenna Using Cross-Slot-Based Metasurface"}, "3289b159b5e01d7fd024cac502edd94f7b49322d": {"paper_id": "3289b159b5e01d7fd024cac502edd94f7b49322d", "abstract": "In this letter, electromagnetic band-gap (EBG) structure is used to reduce the radar cross section (RCS) of the patch array antenna. The proposition of this method is based on the high impedance characteristic of the mushroom-like EBG structure. The basic patch array antenna is designed with a central frequency of 5.0 GHz while replacing the substrate of the array with the mushroom-like EBG structure. The frequency band in which RCS of the patch array antenna reduced significantly can be adjusted by parameters of the EBG. The backward RCS of the patch array antenna with EBG can be reduced as much as 10 dB compared to that of the conventional array antenna, and the degradation of the antenna performance is not significant.", "title": "RCS Reduction of Patch Array Antenna by Electromagnetic Band-Gap Structure"}, "b4818b0e19eb562dd8918bd2196013d0e024b635": {"paper_id": "b4818b0e19eb562dd8918bd2196013d0e024b635", "abstract": "A planar polarization-reconfigurable metasurfaced antenna (PRMS) designed using metasurface (MS) is proposed. The PRMS antenna consists of a planar MS placed atop of and in direct contact with a planar slot antenna, both having a circular shape with a diameter of 78 mm (0.9 \u03bb0), making it compact and low profile. By rotating the MS around the center with respect to the slot antenna, the PRMS antenna can be reconfigured to linear polarization, left-hand and right-hand circular polarizations. An equivalent circuit is used to explain the reconfigurability of the antenna. The PRMS antenna is studied and designed to operate at around 3.5 GHz using computer simulation. For verification of simulation results, the PRMS antenna is fabricated and measured. The antenna performance, in terms of polarization reconfigurability, axial-ratio bandwidth, impedance bandwidth, realized boresight gain and radiation pattern, is presented. Results show that the PRMS antenna in circular polarizations achieves an operating bandwidth of 3.3-3.7 GHz (i.e., fractional bandwidth 11.4%), a boresight gain of above 5 dBi and high-polarization isolation of larger than 15 dB. While the PRMS antenna in linear polarization achieves a gain of above 7.5 dBi with cross-polarization isolation larger than 50 dB.", "title": "Design of Polarization Reconfigurable Antenna Using Metasurface"}, "5cd1c9838c85d0360fc1086df2bae132d3f31929": {"paper_id": "5cd1c9838c85d0360fc1086df2bae132d3f31929", "abstract": "Reconfigurable computing is becoming increasingly attractive for many applications. This survey covers two aspects of reconfigurable computing: architectures and design methods. Our chapter includes recent advances in reconfigurable architectures, such as the Altera Stratix II and Xilinx Virtex 4 FPGA devices. We identify major trends in general-purpose and special-purpose design methods.", "title": "Reconfigurable Computing : Architectures and Design Methods"}, "1c734a14c2325cb76783ca0431862c7f04a69268": {"paper_id": "1c734a14c2325cb76783ca0431862c7f04a69268", "abstract": "Recent reports suggest that a generic supervised deep CNN model trained on a large-scale dataset reduces, but does not remove, dataset bias on a standard benchmark. Fine-tuning deep models in a new domain can require a significant amount of data, which for many applications is simply not available. We propose a new CNN architecture which introduces an adaptation layer and an additional domain confusion loss, to learn a representation that is both semantically meaningful and domain invariant. We additionally show that a domain confusion metric can be used for model selection to determine the dimension of an adaptation layer and the best position for the layer in the CNN architecture. Our proposed adaptation method offers empirical performance which exceeds previously published results on a standard benchmark visual domain adaptation task.", "title": "Deep Domain Confusion: Maximizing for Domain Invariance"}, "51a4d658c93c5169eef7568d3d1cf53e8e495087": {"paper_id": "51a4d658c93c5169eef7568d3d1cf53e8e495087", "abstract": "In this paper, we introduce a new domain adaptation (DA) algorithm where the source and target domains are represented by subspaces described by eigenvectors. In this context, our method seeks a domain adaptation solution by learning a mapping function which aligns the source subspace with the target one. We show that the solution of the corresponding optimization problem can be obtained in a simple closed form, leading to an extremely fast algorithm. We use a theoretical result to tune the unique hyper parameter corresponding to the size of the subspaces. We run our method on various datasets and show that, despite its intrinsic simplicity, it outperforms state of the art DA methods.", "title": "Unsupervised Visual Domain Adaptation Using Subspace Alignment"}, "2671bf82168234a25fce7950e0527eb03b201e0c": {"paper_id": "2671bf82168234a25fce7950e0527eb03b201e0c", "abstract": "Statistical parsers trained and tested on the Penn Wall Street Journal (WSJ) treebank have shown vast improvements over the last 10 years. Much of this improvement, however, is based upon an ever-increasing number of features to be trained on (typically) the WSJ treebank data. This has led to concern that such parsers may be too finely tuned to this corpus at the expense of portability to other genres. Such worries have merit. The standard \u201cCharniak parser\u201d checks in at a labeled precisionrecall f -measure of 89.7% on the Penn WSJ test set, but only 82.9% on the test set from the Brown treebank corpus. This paper should allay these fears. In particular, we show that the reranking parser described in Charniak and Johnson (2005) improves performance of the parser on Brown to 85.2%. Furthermore, use of the self-training techniques described in (McClosky et al., 2006) raise this to 87.8% (an error reduction of 28%) again without any use of labeled Brown data. This is remarkable since training the parser and reranker on labeled Brown data achieves only 88.4%.", "title": "Reranking and Self-Training for Parser Adaptation"}, "355e60c4d39a60c56f46f7dba3c89806ae6990f1": {"paper_id": "355e60c4d39a60c56f46f7dba3c89806ae6990f1", "abstract": "We propose the hierarchical Dirichlet process (HDP), a nonp arametric Bayesian model for clustering problems involving multiple groups of data. Each group of data is modeled with a mixture, with the nu mber of components being open-ended and inferred automatically by the model. Further, components can be shared across groups, allowing d epen encies across groups to be modeled effectively as well as conferrin g generalization to new groups. Such grouped clustering problems occur o ften in practice, e.g. in the problem of topic discovery in document corpora. We report experimental results on three text corpora showing t he effective and superior performance of the HDP over previous models.", "title": "Sharing Clusters among Related Groups: Hierarchical Dirichlet Processes"}, "0b15b4fec6e98aa94bebe37d001cd006c4138c47": {"paper_id": "0b15b4fec6e98aa94bebe37d001cd006c4138c47", "abstract": "Many multimedia applications can benefit from techniques for adapting existing classifiers to data with different distributions. One example is cross-domain video concept detection which aims to adapt concept classifiers across various video domains. In this paper, we explore two key problems for classifier adaptation: (1) how to transform existing classifier(s) into an effective classifier for a new dataset that only has a limited number of labeled examples, and (2) how to select the best existing classifier(s) for adaptation. For the first problem, we propose Adaptive Support Vector Machines (A-SVMs) as a general method to adapt one or more existing classifiers of any type to the new dataset. It aims to learn the \"delta function\" between the original and adapted classifier using an objective function similar to SVMs. For the second problem, we estimate the performance of each existing classifier on the sparsely-labeled new dataset by analyzing its score distribution and other meta features, and select the classifiers with the best estimated performance. The proposed method outperforms several baseline and competing methods in terms of classification accuracy and efficiency in cross-domain concept detection in the TRECVID corpus.", "title": "Cross-domain video concept detection using adaptive svms"}, "ae4614f758dfa344a04b33377c96abc10d5eeda7": {"paper_id": "ae4614f758dfa344a04b33377c96abc10d5eeda7", "abstract": "A method of speaker adaptation for continuous density hidden Markov models (HMMs) is presented. An initial speaker-independent system is adapted to improve the modelling of a new speaker by updating the HMM parameters. Statistics are gathered from the available adaptation data and used to calculate a linear regressionbased transformation for the mean vectors. The transformation matrices are calculated to maximize the likelihood of the adaptation data and can be implemented using the forward\u2013backward algorithm. By tying the transformations among a number of distributions, adaptation can be performed for distributions which are not represented in the training data. An important feature of the method is that arbitrary adaptation data can be used\u2014no special enrolment sentences are needed. Experiments have been performed on the ARPA RM1 database using an HMM system with cross-word triphones and mixture Gaussian output distributions. Results show that adaptation can be performed using as little as 11 s of adaptation data, and that as more data is used the adaptation performance improves. For example, using 40 adaptation utterances, a 37% reduction in error from the speakerindependent system was achieved with supervised adaptation and a 32% reduction in unsupervised mode.", "title": "Maximum likelihood linear regression for speaker adaptation of continuous density hidden Markov models"}, "0747faf8daeda0a4b65b4daa23759ee5fc915be4": {"paper_id": "0747faf8daeda0a4b65b4daa23759ee5fc915be4", "abstract": "Concept, design, and measurement results of a frequency-modulated continuous-wave radar sensor in low-temperature co-fired ceramics (LTCC) technology is presented in this paper. The sensor operates in the frequency band between 77\u201381 GHz. As a key component of the system, wideband microstrip grid array antennas with a broadside beam are presented and discussed. The combination with a highly integrated feeding network and a four-channel transceiver chip based on SiGe technology results in a very compact LTCC RF frontend (23 mm <formula formulatype=\"inline\"><tex Notation=\"TeX\">$\\times$</tex></formula> 23 mm). To verify the feasibility of the concept, first radar measurement results are presented.", "title": "A 79-GHz Radar Sensor in LTCC Technology Using Grid Array Antennas"}, "cce47bd73e1a8e8c678b9029701bc8692333f28f": {"paper_id": "cce47bd73e1a8e8c678b9029701bc8692333f28f", "abstract": "A grid array antenna is presented in this paper with sub grid arrays and multiple feed points, showing enhanced radiation characteristics and sufficient design flexibility. For instance, the grid array antenna can be easily designed as a linearly- or circularly-polarized, unbalanced or balanced antenna. A design example is given for a linearly-polarized unbalanced grid array antenna in Ferro A6M low temperature co-fired ceramic technology for 60-GHz radios to operate from 57 to 66 GHz (\u2248 14.6% at 61.5 GHz ). It consists of 4 sub grid arrays and 4 feed points that are connected to a single-ended 50-\u03a9 source by a quarter-wave matched T-junction network. The simulated results indicate that the grid array antenna has the maximum gain of 17.7 dBi at 59 GHz , an impedance bandwidth (|S11| \u2264 -10&nbsp;dB) nearly from 56 to 67.5 GHz (or 18.7%), a 3-dB gain bandwidth from 55.4 to 66 GHz (or 17.2%), and a vertical beam bandwidth in the broadside direction from 57 to 66 GHz (14.6%). The measured results are compared with the simulated ones. Discrepancies and their causes are identified with a tolerance analysis on the fabrication process.", "title": "Grid Array Antennas With Subarrays and Multiple Feeds for 60-GHz Radios"}, "684ca51bba2110395fc76c45152bd719375bbb8d": {"paper_id": "684ca51bba2110395fc76c45152bd719375bbb8d", "abstract": "In this paper, we present a cost-effective chip-scale packaging solution for a 60-GHz industrial-scientific-medical band receiver (Rx) and transmitter (Tx) chipset capable of gigabit-per-second wireless communications. Envisioned applications of the packaged chipset include 1-3-Gb/s directional links using amplitude shift-keying or phase shift-keying modulation and 500-Mb/s-1-Gb/s omni-directional links using orthogonal frequency-division multiplexing modulation. This paper demonstrates the first fully package-integrated 60-GHz chipset including receive and transmit antennas in a cost-effective plastic package. A direct-chip-attach (DCA) and surface mountable land-grid-array (LGA) package technology is presented. The size of the DCA package is 7times11 mm2 and the LGA package size is 6times13 mm2. Optionally, the Tx and Rx chip can be packaged together with Tx and Rx antennas in a combined 13times13 mm2 LGA transceiver package", "title": "A chip-scale packaging technology for 60-GHz wireless chipsets"}, "f7952959262958ef3c9868d7a717cbc8a6a24eaa": {"paper_id": "f7952959262958ef3c9868d7a717cbc8a6a24eaa", "abstract": "First cars equipped with 24 GHz short range radar (SRR) systems in combination with 77 GHz long range radar (LRR) system enter the market in autumn 2005 enabling new safety and comfort functions. In Europe the 24 GHz ultra wideband (UWB) frequency band is temporally allowed only till end of June 2013 with a limitation of the car pare penetration of 7%. From middle of 2013 new cars have to be equipped with SRR sensors which operate in the frequency band of 79 GHz (77 GHz to 81 GHz). The development of the 79 GHz SRR technology within the German government (BMBF) funded project KOKON is described", "title": "Development of future short range radar technology"}, "9ca0bcb74141aa9f96c067af8e0c515af42321f2": {"paper_id": "9ca0bcb74141aa9f96c067af8e0c515af42321f2", "abstract": "The market for driver assistance systems based on millimeter-wave radar sensor technology is gaining momentum. In the near future, the full range of newly introduced car models will be equipped with radar based systems which leads to high volume production with low cost potential. This paper provides background and an overview of the state of the art of millimeter-wave technology for automotive radar applications, including two actual silicon based fully integrated radar chips. Several advanced packaging concepts and antenna systems are presented and discussed in detail. Finally measurement results of the fully integrated radar front ends are shown.", "title": "Millimeter-Wave Technology for Automotive Radar Sensors in the 77 GHz Frequency Band"}, "81e64d4139572d08a063adf8f555634729730e19": {"paper_id": "81e64d4139572d08a063adf8f555634729730e19", "abstract": "Design and results of a 77 GHz FM/CW radar sensor based on a simple waveguide circuitry and a novel type of printed, low-profile, and low-loss antenna are presented. A Gunn VCO and a finline mixer act as transmitter and receiver, connected by two E-plane couplers. The folded reflector type antenna consists of a printed slot array and another planar substrate which, at the same time, provide twisting of the polarization and focussing of the incident wave. In this way, a folded low-profile, low-loss antenna can be realized. The performance of the radar is described, together with first results on a scanning of the antenna beam.", "title": "A 77 GHz FM/CW radar frontend with a low-profile, low-loss printed antenna"}, "ada331f645409b9205218b6ef8375896c2ebf978": {"paper_id": "ada331f645409b9205218b6ef8375896c2ebf978", "abstract": "Driver fatigue is a significant factor in many traffic accidents. We propose a novel approach for driver fatigue detection from facial image sequences, which is based on multiscale dynamic features. First, Gabor filters are used to get a multiscale representation for image sequences. Then Local Binary Patterns are extracted from each multiscale image. To account for the temporal aspect of human fatigue, the LBP image sequence is divided into dynamic units, and a histogram of each dynamic unit is computed and concatenated as dynamic features. Finally a statistical learning algorithm is applied to extract the most discriminative features from the multiscale dynamic features and construct a strong classifier for fatigue detection. The proposed approach is validated under real-life fatigue conditions. The test data includes 600 image sequences with illumination and pose variations from 30 people\u2019s videos. Experimental results show the validity of the proposed approach, and a correct rate of 98.33% is achieved which is much better than the baselines.", "title": "Multiscale Dynamic Features Based Driver Fatigue Detection"}, "a1f279725f02a697e066246149dbf737932d131c": {"paper_id": "a1f279725f02a697e066246149dbf737932d131c", "abstract": "It is well known that how to extract dynamical features is a key issue for video based face analysis. In this paper, we present a novel approach of facial action units (AU) and expression recognition based on coded dynamical features. In order to capture the dynamical characteristics of facial events, we design the dynamical haar-like features to represent the temporal variations of facial events. Inspired by the binary pattern coding, we further encode the dynamic haar-like features into binary pattern features, which are useful to construct weak classifiers for boosting learning. Finally the Adaboost is performed to learn a set of discriminating coded dynamic features for facial active units and expression recognition. Experiments on the CMU expression database and our own facial AU database show its encouraging performance.", "title": "Boosting Coded Dynamic Features for Facial Action Units and Facial Expression Recognition"}, "044fdb693a8d96a61a9b2622dd1737ce8e5ff4fa": {"paper_id": "044fdb693a8d96a61a9b2622dd1737ce8e5ff4fa", "abstract": "Dynamic texture (DT) is an extension of texture to the temporal domain. Description and recognition of DTs have attracted growing attention. In this paper, a novel approach for recognizing DTs is proposed and its simplifications and extensions to facial image analysis are also considered. First, the textures are modeled with volume local binary patterns (VLBP), which are an extension of the LBP operator widely used in ordinary texture analysis, combining motion and appearance. To make the approach computationally simple and easy to extend, only the co-occurrences of the local binary patterns on three orthogonal planes (LBP-TOP) are then considered. A block-based method is also proposed to deal with specific dynamic events such as facial expressions in which local information and its spatial locations should also be taken into account. In experiments with two DT databases, DynTex and Massachusetts Institute of Technology (MIT), both the VLBP and LBP-TOP clearly outperformed the earlier approaches. The proposed block-based method was evaluated with the Cohn-Kanade facial expression database with excellent results. The advantages of our approach include local processing, robustness to monotonic gray-scale changes, and simple computation", "title": "Dynamic Texture Recognition Using Local Binary Patterns with an Application to Facial Expressions"}, "1f1970df3155799ae35fc47e71c3047198f532d9": {"paper_id": "1f1970df3155799ae35fc47e71c3047198f532d9", "abstract": "The most expressive way humans display emotions is through facial expressions. In this work we report on several advances we have made in building a system for classification of facial expressions from continuous video input. We introduce and test different Bayesian network classifiers for classifying expressions from video, focusing on changes in distribution assumptions, and feature dependency structures. In particular we use Naive\u2013Bayes classifiers and change the distribution from Gaussian to Cauchy, and use Gaussian Tree-Augmented Naive Bayes (TAN) classifiers to learn the dependencies among different facial motion features. We also introduce a facial expression recognition from live video input using temporal cues. We exploit the existing methods and propose a new architecture of hidden Markov models (HMMs) for automatically segmenting and recognizing human facial expression from video sequences. The architecture performs both segmentation and recognition of the facial expressions automatically using a multi-level architecture composed of an HMM layer and a Markov model layer. We explore both person-dependent and person-independent recognition of expressions and compare the different methods. 2003 Elsevier Inc. All rights reserved. * Corresponding author. E-mail addresses: iracohen@ifp.uiuc.edu (I. Cohen), nicu@science.uva.nl (N. Sebe), ashutosh@ us.ibm.com (A. Garg), lawrence.chen@kodak.com (L. Chen), huang@ifp.uiuc.edu (T.S. Huang). 1077-3142/$ see front matter 2003 Elsevier Inc. All rights reserved. doi:10.1016/S1077-3142(03)00081-X I. Cohen et al. / Computer Vision and Image Understanding 91 (2003) 160\u2013187 161", "title": "Facial expression recognition from video sequences: temporal and static modeling"}, "66b9d954dd8204c3a970d86d91dd4ea0eb12db47": {"paper_id": "66b9d954dd8204c3a970d86d91dd4ea0eb12db47", "abstract": "Previous work suggests that Gabor-wavelet-based methods can achieve high sensitivity and specificity for emotionspecified expressions (e.g., happy, sad) and single action units (AUs) of the Facial Action Coding System (FACS). This paper evaluates a Gabor-wavelet-based method to recognize AUs in image sequences of increasing complexity. A recognition rate of 83% is obtained for three single AUs when image sequences contain homogeneous subjects and are without observable head motion. The accuracy of AU recognition decreases to 32% when the number of AUs increases to nine and the image sequences consist of AU combinations, head motion, and non-homogeneous subjects. For comparison, an average recognition rate of 87.6% is achieved for the geometry-feature-based method. The best recognition is a rate of 92.7% obtained by combining Gabor wavelets and geometry features.", "title": "Evaluation of Gabor-Wavelet-Based Facial Action Unit Recognition in Image Sequences of Increasing Complexity"}, "33fad977a6b317cfd6ecd43d978687e0df8a7338": {"paper_id": "33fad977a6b317cfd6ecd43d978687e0df8a7338", "abstract": "\u00d0This paper presents a theoretically very simple, yet efficient, multiresolution approach to gray-scale and rotation invariant texture classification based on local binary patterns and nonparametric discrimination of sample and prototype distributions. The method is based on recognizing that certain local binary patterns, termed auniform,o are fundamental properties of local image texture and their occurrence histogram is proven to be a very powerful texture feature. We derive a generalized gray-scale and rotation invariant operator presentation that allows for detecting the auniformo patterns for any quantization of the angular space and for any spatial resolution and presents a method for combining multiple operators for multiresolution analysis. The proposed approach is very robust in terms of gray-scale variations since the operator is, by definition, invariant against any monotonic transformation of the gray scale. Another advantage is computational simplicity as the operator can be realized with a few operations in a small neighborhood and a lookup table. Excellent experimental results obtained in true problems of rotation invariance, where the classifier is trained at one particular rotation angle and tested with samples from other rotation angles, demonstrate that good discrimination can be achieved with the occurrence statistics of simple rotation invariant local binary patterns. These operators characterize the spatial configuration of local image texture and the performance can be further improved by combining them with rotation invariant variance measures that characterize the contrast of local image texture. The joint distributions of these orthogonal measures are shown to be very powerful tools for rotation invariant texture analysis. Index Terms\u00d0Nonparametric, texture analysis, Outex, Brodatz, distribution, histogram, contrast.", "title": "Multiresolution Gray-Scale and Rotation Invariant Texture Classification with Local Binary Patterns"}, "6be461dd5869d00fc09975a8f8e31eb5f86be402": {"paper_id": "6be461dd5869d00fc09975a8f8e31eb5f86be402", "abstract": "Computer animated agents and robots bring a social dimension to human computer interaction and force us to think in new ways about how computers could be used in daily life. Face to face communication is a real-time process operating at a a time scale in the order of 40 milliseconds. The level of uncertainty at this time scale is considerable, making it necessary for humans and machines to rely on sensory rich perceptual primitives rather than slow symbolic inference processes. In this paper we present progress on one such perceptual primitive. The system automatically detects frontal faces in the video stream and codes them with respect to 7 dimensions in real time: neutral, anger, disgust, fear, joy, sadness, surprise. The face finder employs a cascade of feature detectors trained with boosting techniques [15, 2]. The expression recognizer receives image patches located by the face detector. A Gabor representation of the patch is formed and then processed by a bank of SVM classifiers. A novel combination of Adaboost and SVM's enhances performance. The system was tested on the Cohn-Kanade dataset of posed facial expressions [6]. The generalization performance to new subjects for a 7- way forced choice correct. Most interestingly the outputs of the classifier change smoothly as a function of time, providing a potentially valuable representation to code facial expression dynamics in a fully automatic and unobtrusive manner. The system has been deployed on a wide variety of platforms including Sony's Aibo pet robot, ATR's RoboVie, and CU animator, and is currently being evaluated for applications including automatic reading tutors, assessment of human-robot interaction.", "title": "Real Time Face Detection and Facial Expression Recognition: Development and Applications to Human Computer Interaction."}, "63f9f3f0e1daede934d6dde1a84fb7994f8929f0": {"paper_id": "63f9f3f0e1daede934d6dde1a84fb7994f8929f0", "abstract": "For years, researchers in face recognition area have been representing and recognizing faces based on subspace discriminant analysis or statistical learning. Nevertheless, these approaches are always suffering from the generalizability problem. This paper proposes a novel non-statistics based face representation approach, local Gabor binary pattern histogram sequence (LGBPHS), in which training procedure is unnecessary to construct the face model, so that the generalizability problem is naturally avoided. In this approach, a face image is modeled as a \"histogram sequence\" by concatenating the histograms of all the local regions of all the local Gabor magnitude binary pattern maps. For recognition, histogram intersection is used to measure the similarity of different LGBPHSs and the nearest neighborhood is exploited for final classification. Additionally, we have further proposed to assign different weights for each histogram piece when measuring two LGBPHSes. Our experimental results on AR and FERET face database show the validity of the proposed approach especially for partially occluded face images, and more impressively, we have achieved the best result on FERET face database.", "title": "Local Gabor binary pattern histogram sequence (LGBPHS): a novel non-statistical model for face representation and recognition"}, "7c27c259a5a0ee9ed0624f1d4a89d5755b70cbe5": {"paper_id": "7c27c259a5a0ee9ed0624f1d4a89d5755b70cbe5", "abstract": "Over the last 20 years, several different techniques have been proposed for computer recognition of human faces. The purpose of this paper is to compare two simple but general strategies on a common database (frontal images of faces of 47 people: 26 males and 21 females, four images per person). We have developed and implemented two new algorithms; the first one is based on the computation of a set of geometrical features, such as nose width and length, mouth position, and chin shape, and the second one is based on almost-grey-level template matching. The results obtained on the testing sets (about 90% correct recognition using geometrical features and perfect recognition using template matching) favor our implementation of the template-matching approach.", "title": "Face Recognition: Features Versus Templates"}, "4e6238c8613b5b81f81552939bce33296aedfbfe": {"paper_id": "4e6238c8613b5b81f81552939bce33296aedfbfe", "abstract": "David Baehrens\u2217 BAEHRENS@CS.TU-BERLIN.DE Timon Schroeter\u2217 TIMON@CS.TU-BERLIN.DE Technische Universit \u00e4 Berlin Franklinstr. 28/29, FR 6-9 10587 Berlin, Germany Stefan Harmeling\u2217 STEFAN.HARMELING@TUEBINGEN.MPG.DE MPI for Biological Cybernetics Spemannstr. 38 72076 T\u0308ubingen, Germany Motoaki Kawanabe\u2020 MOTOAKI .KAWANABE @FIRST.FRAUNHOFER.DE Fraunhofer Institute FIRST.IDA Kekulestr.7 12489 Berlin, Germany", "title": "How to Explain Individual Classification Decisions"}, "168d7350770a4051b106399f2bb2c37090802bca": {"paper_id": "168d7350770a4051b106399f2bb2c37090802bca", "abstract": "A situation where training and test samples follow different input distributions is calledcovariate shift . Under covariate shift, standard learning methods such as maximum likelihood estimation are no longer consistent\u2014weighted variants according to the ratio of test and training input densities are consistent. Therefore, accurately estimating the density ratio, called the importance, is one of the key issues in covariate shift adaptation. A naive approach to this task is to first estimate training and test input densities separately and then estimate the importance by taking the ratio of the estimated densities. However, this naive approach tends to perform poorly since density estimation is a hard task particularly in high dimensional cases. In this paper, we propose a direct importance estimation method that does not involve density estimation. Our method is equipped with a natural cross validation procedure and hence tuning parameters such as the kernel width can be objectively optimized. Simulations illustrate the usefulness of our approach.", "title": "Direct Importance Estimation with Model Selection and Its Application to Covariate Shift Adaptation"}, "2b3818c141da414cf9e783c8b2d4928019cb70fd": {"paper_id": "2b3818c141da414cf9e783c8b2d4928019cb70fd", "abstract": "We address classification problems for which the training instances are governed by a distribution that is allowed to differ arbitrarily from the test distribution---problems also referred to as classification under covariate shift. We derive a solution that is purely discriminative: neither training nor test distribution are modeled explicitly. We formulate the general problem of learning under covariate shift as an integrated optimization problem. We derive a kernel logistic regression classifier for differing training and test distributions.", "title": "Discriminative learning for differing training and test distributions"}, "70183fe2bab484cd26d2a9146345f871bdd29115": {"paper_id": "70183fe2bab484cd26d2a9146345f871bdd29115", "abstract": "Your use of the JSTOR archive indicates your acceptance of JSTOR's Terms and Conditions of Use, available at http://www.jstor.org/page/info/about/policies/terms.jsp. JSTOR's Terms and Conditions of Use provides, in part, that unless you have obtained prior permission, you may not download an entire issue of a journal or multiple copies of articles, and you may use content in the JSTOR archive only for your personal, non-commercial use.", "title": "Sample Selection Bias as a Specification Error"}, "5264ae4ea4411426ddd91dc780c2892c3ff933d3": {"paper_id": "5264ae4ea4411426ddd91dc780c2892c3ff933d3", "abstract": "Variable and feature selection have become the focus of much research in areas of application for which datasets with tens or hundreds of thousands of variabl es are available. These areas include text processing of internet documents, gene expression arr ay nalysis, and combinatorial chemistry. The objective of variable selection is three-fold: improvi ng the prediction performance of the predictors, providing faster and more cost-effective predict ors, and providing a better understanding of the underlying process that generated the data. The contrib utions of this special issue cover a wide range of aspects of such problems: providing a better definit ion of the objective function, feature construction, feature ranking, multivariate feature sele ction, efficient search methods, and feature validity assessment methods.", "title": "An Introduction to Variable and Feature Selection"}, "0b47b6ffe714303973f40851d975c042ff4fcde1": {"paper_id": "0b47b6ffe714303973f40851d975c042ff4fcde1", "abstract": "We describe and experimentally evaluate a method for automatically clustering words according to their distribution in particular syntactic contexts. Deterministic annealing is used to find lowest distortion sets of clusters. As the annealing parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical \u201csoft\u201d clustering of the data. Clusters are used as the basis for class models of word coocurrence, and the models evaluated with respect to held-out test data.", "title": "Distributional Clustering of English Words"}, "125d7bd51c44907e166d82469aa4a7ba1fb9b77f": {"paper_id": "125d7bd51c44907e166d82469aa4a7ba1fb9b77f", "abstract": "Although cancer classification has improved over the past 30 years, there has been no general approach for identifying new cancer classes (class discovery) or for assigning tumors to known classes (class prediction). Here, a generic approach to cancer classification based on gene expression monitoring by DNA microarrays is described and applied to human acute leukemias as a test case. A class discovery procedure automatically discovered the distinction between acute myeloid leukemia (AML) and acute lymphoblastic leukemia (ALL) without previous knowledge of these classes. An automatically derived class predictor was able to determine the class of new leukemia cases. The results demonstrate the feasibility of cancer classification based solely on gene expression monitoring and suggest a general strategy for discovering and predicting cancer classes for other types of cancer, independent of previous biological knowledge.", "title": "Molecular classification of cancer: class discovery and class prediction by gene expression monitoring."}, "9e5bfcae060393fb4822504cdb04897c1de214ab": {"paper_id": "9e5bfcae060393fb4822504cdb04897c1de214ab", "abstract": "Digital processing of electroencephalography (EEG) signals has now been popularly used in a wide variety of applications such as seizure detection/prediction, motor imagery classification, mental task classification, emotion classification, sleep state classification, and drug effects diagnosis. With the large number of EEG channels acquired, it has become apparent that efficient channel selection algorithms are needed with varying importance from one application to another. The main purpose of the channel selection process is threefold: (i) to reduce the computational complexity of any processing task performed on EEG signals by selecting the relevant channels and hence extracting the features of major importance, (ii) to reduce the amount of overfitting that may arise due to the utilization of unnecessary channels, for the purpose of improving the performance, and (iii) to reduce the setup time in some applications. Signal processing tools such as time-domain analysis, power spectral estimation, and wavelet transform have been used for feature extraction and hence for channel selection in most of channel selection algorithms. In addition, different evaluation approaches such as filtering, wrapper, embedded, hybrid, and human-based techniques have been widely used for the evaluation of the selected subset of channels. In this paper, we survey the recent developments in the field of EEG channel selection methods along with their applications and classify these methods according to the evaluation approach.", "title": "A review of channel selection algorithms for EEG signal processing"}, "080c47cb63f3418953115faca46e3a5531dc6487": {"paper_id": "080c47cb63f3418953115faca46e3a5531dc6487", "abstract": "Receiver Operator Characteristic (ROC) curves are commonly used to present results for binary decision problems in machine learning. However, when dealing with highly skewed datasets, Precision-Recall (PR) curves give a more informative picture of an algorithm's performance. We show that a deep connection exists between ROC space and PR space, such that a curve dominates in ROC space if and only if it dominates in PR space. A corollary is the notion of an achievable PR curve, which has properties much like the convex hull in ROC space; we show an efficient algorithm for computing this curve. Finally, we also note differences in the two types of curves are significant for algorithm design. For example, in PR space it is incorrect to linearly interpolate between points. Furthermore, algorithms that optimize the area under the ROC curve are not guaranteed to optimize the area under the PR curve.", "title": "The relationship between Precision-Recall and ROC curves"}, "05393361e6d9e56ee7dbabb1e5ef6c1c212fc34d": {"paper_id": "05393361e6d9e56ee7dbabb1e5ef6c1c212fc34d", "abstract": "The support vector machine (SVM) is a popular classification technique. However, beginners who are not familiar with SVM often get unsatisfactory results since they miss some easy but significant steps. In this guide, we propose a simple procedure which usually gives reasonable results.", "title": "A Practical Guide to Support Vector Classification"}, "f1161cd952f7c52266d0761ab4973c1c134b4962": {"paper_id": "f1161cd952f7c52266d0761ab4973c1c134b4962", "abstract": "Algorithms which construct classifiers from sample data -such as neural networks, radial basis functions, and decision trees -have attracted growing attention for their wide applicability. Researchers in the fields of Statistics, Artificial Intelligence, Machine Learning, Data Mining, and Pattern Recognition are continually introducing (or rediscovering) induction methods, and often publishing implementing code. It is natural for practitioners and potential users to wonder, \"Which classification technique is best?\", or more realistically, \"What subset of methods tend to work well for a given type of dataset?\". This book provides perhaps the best current answer to that question.", "title": "Machine Learning, Neural and Statistical Classification"}, "13e013398a425c4ace819f6c26c1432030dbfd1a": {"paper_id": "13e013398a425c4ace819f6c26c1432030dbfd1a", "abstract": "Support vector machines (SVMs) with the gaussian (RBF) kernel have been popular for practical use. Model selection in this class of SVMs involves two hyper parameters: the penalty parameter C and the kernel width . This letter analyzes the behavior of the SVM classifier when these hyper parameters take very small or very large values. Our results help in understanding the hyperparameter space that leads to an efficient heuristic method of searching for hyperparameter values with small generalization errors. The analysis also indicates that if complete model selection using the gaussian kernel has been conducted, there is no need to consider linear SVM.", "title": "Asymptotic Behaviors of Support Vector Machines with Gaussian Kernel"}, "dd9b0b349c55779bdc060a160299ff3f1f4111b2": {"paper_id": "dd9b0b349c55779bdc060a160299ff3f1f4111b2", "abstract": "With the rapid development of the satellite sensor technology, high spatial resolution remote sensing (HSR) data have attracted extensive attention in military and civilian applications. In order to make full use of these data, remote sensing scene classification becomes an important and necessary precedent task. In this paper, an unsupervised representation learning method is proposed to investigate deconvolution networks for remote sensing scene classification. First, a shallow weighted deconvolution network is utilized to learn a set of feature maps and filters for each image by minimizing the reconstruction error between the input image and the convolution result. The learned feature maps can capture the abundant edge and texture information of high spatial resolution images, which is definitely important for remote sensing images. After that, the spatial pyramid model (SPM) is used to aggregate features at different scales to maintain the spatial layout of HSR image scene. A discriminative representation for HSR image is obtained by combining the proposed weighted deconvolution model and SPM. Finally, the representation vector is input into a support vector machine to finish classification. We apply our method on two challenging HSR image data sets: the UCMerced data set with 21 scene categories and the Sydney data set with seven land-use categories. All the experimental results achieved by the proposed method outperform most state of the arts, which demonstrates the effectiveness of the proposed method.", "title": "Remote Sensing Scene Classification by Unsupervised Representation Learning"}, "842ca93d770edef147e9ca117e1c0294a596cb82": {"paper_id": "842ca93d770edef147e9ca117e1c0294a596cb82", "abstract": "Learning efficient image representations is at the core of the scene classification task of remote sensing imagery. The existing methods for solving the scene classification task, based on either feature coding approaches with low-level hand-engineered features or unsupervised feature learning, can only generate mid-level image features with limited representative ability, which essentially prevents them from achieving better performance. Recently, the deep convolutional neural networks (CNNs), which are hierarchical architectures trained on large-scale datasets, have shown astounding performance in object recognition and detection. However, it is still not clear how to use these deep convolutional neural networks for high-resolution remote sensing (HRRS) scene classification. In this paper, we investigate how to transfer features from these successfully pre-trained CNNs for HRRS scene classification. We propose two scenarios for generating image features via extracting CNN features from different layers. In the first scenario, the activation vectors extracted from fully-connected layers are regarded as the final image features; in the second scenario, we extract dense features from the last convolutional layer at multiple scales and then encode the dense features into global image features through commonly used feature coding approaches. Extensive experiments on two public scene classification datasets demonstrate that the image features obtained by the two proposed scenarios, even with a simple linear classifier, can result in remarkable performance and improve the state-of-the-art by a significant margin. The results reveal that the features Remote Sens. 2015, 7 14681 from pre-trained CNNs generalize well to HRRS datasets and are more expressive than the lowand mid-level features. Moreover, we tentatively combine features extracted from different CNN models for better performance.", "title": "Transferring Deep Convolutional Neural Networks for the Scene Classification of High-Resolution Remote Sensing Imagery"}, "013cd20c0eaffb9cab80875a43086e0c3224fe20": {"paper_id": "013cd20c0eaffb9cab80875a43086e0c3224fe20", "abstract": "The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.", "title": "Representation Learning: A Review and New Perspectives"}, "a55392918b3e7db1c45481253425a0a52e703282": {"paper_id": "a55392918b3e7db1c45481253425a0a52e703282", "abstract": "Detection of salient objects from images is gaining increasing research interest in recent years as it can substantially facilitate a wide range of content-based multimedia applications. Based on the assumption that foreground salient regions are distinctive within a certain context, most conventional approaches rely on a number of hand-designed features and their distinctiveness is measured using local or global contrast. Although these approaches have been shown to be effective in dealing with simple images, their limited capability may cause difficulties when dealing with more complicated images. This paper proposes a novel framework for saliency detection by first modeling the background and then separating salient objects from the background. We develop stacked denoising autoencoders with deep learning architectures to model the background where latent patterns are explored and more powerful representations of data are learned in an unsupervised and bottom-up manner. Afterward, we formulate the separation of salient objects from the background as a problem of measuring reconstruction residuals of deep autoencoders. Comprehensive evaluations of three benchmark datasets and comparisons with nine state-of-the-art algorithms demonstrate the superiority of this paper.", "title": "Background Prior-Based Salient Object Detection via Deep Reconstruction Residual"}, "22d6d9c1b7ac2738b51d93be45ac8f753f81867c": {"paper_id": "22d6d9c1b7ac2738b51d93be45ac8f753f81867c", "abstract": "Medical image analysis remains a challenging application area for artificial intelligence. When applying machine learning, obtaining ground-truth labels for supervised learning is more difficult than in many more common applications of machine learning. This is especially so for datasets with abnormalities, as tissue types and the shapes of the organs in these datasets differ widely. However, organ detection in such an abnormal dataset may have many promising potential real-world applications, such as automatic diagnosis, automated radiotherapy planning, and medical image retrieval, where new multimodal medical images provide more information about the imaged tissues for diagnosis. Here, we test the application of deep learning methods to organ identification in magnetic resonance medical images, with visual and temporal hierarchical features learned to categorize object classes from an unlabeled multimodal DCE-MRI dataset so that only a weakly supervised training is required for a classifier. A probabilistic patch-based method was employed for multiple organ detection, with the features learned from the deep learning model. This shows the potential of the deep learning model for application to medical images, despite the difficulty of obtaining libraries of correctly labeled training datasets and despite the intrinsic abnormalities present in patient datasets.", "title": "Stacked Autoencoders for Unsupervised Feature Learning and Multiple Organ Detection in a Pilot Study Using 4D Patient Data"}, "55b81991fbb025038d98e8c71acf7dc2b78ee5e9": {"paper_id": "55b81991fbb025038d98e8c71acf7dc2b78ee5e9", "abstract": "Learning algorithms related to artificial neural networks and in particular for Deep Learning may seem to involve many bells and whistles, called hyperparameters. This chapter is meant as a practical guide with recommendations for some of the most commonly used hyper-parameters, in particular in the context of learning algorithms based on backpropagated gradient and gradient-based optimization. It also discusses how to deal with the fact that more interesting results can be obtained when allowing one to adjust many hyper-parameters. Overall, it describes elements of the practice used to successfully and efficiently train and debug large-scale and often deep multi-layer neural networks. It closes with open questions about the training difficulties observed with deeper architectures.", "title": "Practical recommendations for gradient-based training of deep architectures"}, "8ec543a9e6b4ae5b3c9f6f938ae5a9bdf77d82ac": {"paper_id": "8ec543a9e6b4ae5b3c9f6f938ae5a9bdf77d82ac", "abstract": "We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout\u2019s fast approximate model averaging technique. We empirically verify that the model successfully accomplishes both of these tasks. We use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR100, and SVHN.", "title": "Maxout Networks"}, "283743048be918a1a4144d78ea65f45398c37e2d": {"paper_id": "283743048be918a1a4144d78ea65f45398c37e2d", "abstract": "Empirical evaluation of salient object segmentation methods requires i) a dataset of ground truth object segmentations and ii) a performance measure to compare the output of the algorithm with the ground truth. In this paper, we provide such a dataset, and evaluate 5 distinct performance measures that have been used in the literature practically and psychophysically. Our results suggest that a measure based upon minimal contour mappings is most sensitive to shape irregularities and most consistent with human judgements. In fact, the contour mapping measure is as predictive of human judgements as human subjects are of each other. Region-based methods, and contour methods such as Hausdorff distances that do not respect the ordering of points on shape boundaries are significantly less consistent with human judgements. We also show that minimal contour mappings can be used as the correspondence paradigm for Precision-Recall analysis. Our findings can provide guidance in evaluating the results of segmentation algorithms in the future.", "title": "Design and perceptual validation of performance measures for salient object segmentation"}, "ab9d87a03ce3538ff42b6c26d87e676107cc392c": {"paper_id": "ab9d87a03ce3538ff42b6c26d87e676107cc392c", "abstract": "We propose a principled probabilistic formulation of object saliency as a sampling problem. This novel formulation allows us to learn, from a large corpus of unlabelled images, which patches of an image are of the greatest interest and most likely to correspond to an object. We then sample the object saliency map to propose object locations. We show that using only a single object location proposal per image, we are able to correctly select an object in over 42% of the images in the Pascal VOC 2007 dataset, substantially outperforming existing approaches. Furthermore, we show that our object proposal can be used as a simple unsupervised approach to the weakly supervised annotation problem. Our simple unsupervised approach to annotating objects of interest in images achieves a higher annotation accuracy than most weakly supervised approaches.", "title": "Looking Beyond the Image: Unsupervised Learning for Object Saliency and Detection"}, "08d6aecf1ee531f8c62c22a256b2c2e58081df9d": {"paper_id": "08d6aecf1ee531f8c62c22a256b2c2e58081df9d", "abstract": "The automatic discovery of distinctive parts for an object or scene class is challenging since it requires simultaneously to learn the part appearance and also to identify the part occurrences in images. In this paper, we propose a simple, efficient, and effective method to do so. We address this problem by learning parts incrementally, starting from a single part occurrence with an Exemplar SVM. In this manner, additional part instances are discovered and aligned reliably before being considered as training examples. We also propose entropy-rank curves as a means of evaluating the distinctiveness of parts shareable between categories and use them to select useful parts out of a set of candidates. We apply the new representation to the task of scene categorisation on the MIT Scene 67 benchmark. We show that our method can learn parts which are significantly more informative and for a fraction of the cost, compared to previous part-learning methods such as Singh et al. [28]. We also show that a well constructed bag of words or Fisher vector model can substantially outperform the previous state-of-the-art classification performance on this data.", "title": "Blocks That Shout: Distinctive Parts for Scene Classification"}, "3e9219fdcbc17772041456cf8dbfd361a82cbdd4": {"paper_id": "3e9219fdcbc17772041456cf8dbfd361a82cbdd4", "abstract": "A standard approach to describe an image for classification and retrieval purposes is to extract a set of local patch descriptors, encode them into a high dimensional vector and pool them into an image-level signature. The most common patch encoding strategy consists in quantizing the local descriptors into a finite set of prototypical elements. This leads to the popular Bag-of-Visual words representation. In this work, we propose to use the Fisher Kernel framework as an alternative patch encoding strategy: we describe patches by their deviation from an \u201cuniversal\u201d generative Gaussian mixture model. This representation, which we call Fisher vector has many advantages: it is efficient to compute, it leads to excellent results even with efficient linear classifiers, and it can be compressed with a minimal loss of accuracy using product quantization. We report experimental results on five standard datasets\u2014PASCAL VOC 2007, Caltech 256, SUN 397, ILSVRC 2010 and ImageNet10K\u2014with up to 9M images and 10K classes, showing that the FV framework is a state-of-the-art patch encoding technique.", "title": "Image Classification with the Fisher Vector: Theory and Practice"}, "48257a889a9aa61998ae20fa52b25d90c441f63a": {"paper_id": "48257a889a9aa61998ae20fa52b25d90c441f63a", "abstract": "The problem of large-scale image search has been traditionally addressed with the bag-of-visual-words (BOV). In this article, we propose to use as an alternative the Fisher kernel framework. We first show why the Fisher representation is well-suited to the retrieval problem: it describes an image by what makes it different from other images. One drawback of the Fisher vector is that it is high-dimensional and, as opposed to the BOV, it is dense. The resulting memory and computational costs do not make Fisher vectors directly amenable to large-scale retrieval. Therefore, we compress Fisher vectors to reduce their memory footprint and speed-up the retrieval. We compare three binarization approaches: a simple approach devised for this representation and two standard compression techniques. We show on two publicly available datasets that compressed Fisher vectors perform very well using as little as a few hundreds of bits per image, and significantly better than a very recent compressed BOV approach.", "title": "Large-scale image retrieval with compressed Fisher vectors"}, "86ab4cae682fbd49c5a5bedb630e5a40fa7529f6": {"paper_id": "86ab4cae682fbd49c5a5bedb630e5a40fa7529f6", "abstract": "We present an application of back-propagation networks to handwritten digit recognition. Minimal preprocessing of the data was required, but architecture of the network was highly constrained and specifically designed for the task. The input of the network consists of normalized images of isolated digits. The method has 1 % error rate and about a 9% reject rate on zipcode digits provided by the U.S. Postal Service.", "title": "Handwritten Digit Recognition with a Back-Propagation Network"}, "9ae252d3b0821303f8d63ba9daf10030c9c97d37": {"paper_id": "9ae252d3b0821303f8d63ba9daf10030c9c97d37", "abstract": "We propose a novel approach to learn and recognize natural scene categories. Unlike previous work, it does not require experts to annotate the training set. We represent the image of a scene by a collection of local regions, denoted as codewords obtained by unsupervised learning. Each region is represented as part of a \"theme\". In previous work, such themes were learnt from hand-annotations of experts, while our method learns the theme distributions as well as the codewords distribution over the themes without supervision. We report satisfactory categorization performances on a large set of 13 categories of complex scenes.", "title": "A Bayesian hierarchical model for learning natural scene categories"}, "dc8f89865ad9c9b6e643abc296ec5000ccdb16ee": {"paper_id": "dc8f89865ad9c9b6e643abc296ec5000ccdb16ee", "abstract": "This paper presents a novel statistical method for factor analysis of binary and count data which is closely related to a technique known as Latent Semantic Analysis. In contrast to the latter method which stems from linear algebra and performs a Singular Value Decomposition of co-occurrence tables, the proposed technique uses a generative latent class model to perform a probabilistic mixture decomposition. This results in a more principled approach with a solid foundation in statistical inference. More precisely, we propose to make use of a temperature controlled version of the Expectation Maximization algorithm for model fitting, which has shown excellent performance in practice. Probabilistic Latent Semantic Analysis has many applications, most prominently in information retrieval, natural language processing, machine learning from text, and in related areas. The paper presents perplexity results for different types of text and linguistic data collections and discusses an application in automated document indexing. The experiments indicate substantial and consistent improvements of the probabilistic method over standard Latent Semantic Analysis.", "title": "Unsupervised Learning by Probabilistic Latent Semantic Analysis"}, "442f82563dbc983001f859feb1572253cb18eaf8": {"paper_id": "442f82563dbc983001f859feb1572253cb18eaf8", "abstract": "The appearance of an object is composed of local structure. This local structure can be described and characterized by a vector of local features measured by local operators such as Gaussian derivatives or Gabor filters. This article presents a technique where appearances of objects are represented by the joint statistics of such local neighborhood operators. As such, this represents a new class of appearance based techniques for computer vision. Based on joint statistics, the paper develops techniques for the identification of multiple objects at arbitrary positions and orientations in a cluttered scene. Experiments show that these techniques can identify over 100 objects in the presence of major occlusions. Most remarkably, the techniques have low complexity and therefore run in real-time.", "title": "Recognition without Correspondence using Multidimensional Receptive Field Histograms"}, "bcf48b5e76c7e22335c6820f0de0abe8c5f708b5": {"paper_id": "bcf48b5e76c7e22335c6820f0de0abe8c5f708b5", "abstract": "We consider the class of iterative shrinkage-thresholding algorithms (ISTA) for solving linear inverse problems arising in signal/image processing. This class of methods, which can be viewed as an extension of the classical gradient algorithm, is attractive due to its simplicity and thus is adequate for solving large-scale problems even with dense matrix data. However, such methods are also known to converge quite slowly. In this paper we present a new fast iterative shrinkage-thresholding algorithm (FISTA) which preserves the computational simplicity of ISTA but with a global rate of convergence which is proven to be significantly better, both theoretically and practically. Initial promising numerical results for wavelet-based image deblurring demonstrate the capabilities of FISTA which is shown to be faster than ISTA by several orders of magnitude.", "title": "A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems"}, "5c8c82ea1a458b821993865bc9d6a575a93576fb": {"paper_id": "5c8c82ea1a458b821993865bc9d6a575a93576fb", "abstract": "Recent research on universal and minimax wavelet shrinkage and thresholding methods has demonstrated near{ideal estimation performance in various asymptotic frameworks. However, image processing practice has shown that universal thresholding methods are outperformed by simple Bayesian estimators assuming independent wavelet coeecients and heavy{tailed priors such as Generalized Gaussian distributions (GGDs). In this paper, we investigate various connections between shrinkage methods and MAP estimation using such priors. In particular, we state a simple condition under which MAP estimates are sparse. We also introduce a new family of complexity priors based upon Rissanen's universal prior on integers. One particular estimator in this class outperforms conventional wavelet{based MDL estimators. We develop analytical expressions for the shrinkage rules implied by GGD and complexity priors. This allows us to show the equivalence between universal hard thresholding, MAP estimation using a very heavy{ tailed GGD, and MDL estimation using one of the new complexity priors. Theoretical analysis supported by numerous practical experiments shows the robustness of some of these estimates against misspeciications of the prior { a basic concern in image processing applications.", "title": "Analysis of Multiresolution Image Denoising Schemes Using Generalized Gaussian and Complexity Priors"}, "56974187b4d9a8757f4d8a6fd6facc8b4ad08240": {"paper_id": "56974187b4d9a8757f4d8a6fd6facc8b4ad08240", "abstract": "We show that various inverse problems in signal recovery can be formulated as the generic problem of minimizing the sum of two convex functions with certain regularity properties. This formulation makes it possible to derive existence, uniqueness, characterization, and stability results in a unified and standardized fashion for a large class of apparently disparate problems. Recent results on monotone operator splitting methods are applied to establish the convergence of a forward-backward algorithm to solve the generic problem. In turn, we recover, extend, and provide a simplified analysis for a variety of existing iterative methods. Applications to geometry/texture image decomposition schemes are also discussed. A novelty of our framework is to use extensively the notion of a proximity operator, which was introduced by Moreau in the 1960s.", "title": "Signal Recovery by Proximal Forward-Backward Splitting"}, "0df60433201af2e407850bdcc947f6d37d845827": {"paper_id": "0df60433201af2e407850bdcc947f6d37d845827", "abstract": "We consider linear inverse problems where the solution is assumed to have a sparse expansion on an arbitrary preassigned orthonormal basis. We prove that replacing the usual quadratic regularizing penalties by weighted ppenalties on the coefficients of such expansions, with 1 \u2264 p \u2264 2, still regularizes the problem. Use of such p-penalized problems with p < 2 is often advocated when one expects the underlying ideal noiseless solution to have a sparse expansion with respect to the basis under consideration. To compute the corresponding regularized solutions, we analyze an iterative algorithm that amounts to a Landweber iteration with thresholding (or nonlinear shrinkage) applied at each iteration step. We prove that this algorithm converges in norm. c \u00a9 2004 Wiley Periodicals, Inc.", "title": "FA ] 2 N ov 2 00 3 An iterative thresholding algorithm for linear inverse problems with a sparsity constraint"}, "48bc95b0e2abee493fc3e9a6cbf087cf870072e9": {"paper_id": "48bc95b0e2abee493fc3e9a6cbf087cf870072e9", "abstract": "This paper considers the design of a real time parking space locating system using a network of wireless sensor nodes that are equipped with magnetic sensors. Considerations for designing a reliable detection scheme for cars using magnetic signatures obtained by the wireless sensors are presented. All design considerations are derived from experimental data obtained from a campus parking garage. Results indicate that the proposed detection algorithm can effectively detect a wide range of passing vehicles with small error probability.", "title": "Design considerations for a wireless sensor network for locating parking spaces"}, "4961c844b1e25f5c07a6da7e703e4ebc55aa69d6": {"paper_id": "4961c844b1e25f5c07a6da7e703e4ebc55aa69d6", "abstract": "Instagram is a popular social networking application that allows users to express themselves through the uploaded content and the different filters they can apply. In this study we look at personality prediction from Instagram picture features. We explore two different features that can be extracted from pictures: 1) visual features (e.g., hue, valence, saturation), and 2) content features (i.e., the content of the pictures). To collect data, we conducted an online survey where we asked participants to fill in a personality questionnaire and grant us access to their Instagram account through the Instagram API. We gathered 54,962 pictures of 193 Instagram users. With our results we show that visual and content features can be used to predict personality from and perform in general equally well. Combining the two however does not result in an increased predictive power. Seemingly, they are not adding more value than they already consist of independently.", "title": "Predicting Users' Personality from Instagram Pictures: Using Visual and/or Content Features?"}, "f4d133d9933879f550d09955aa66e49a98110609": {"paper_id": "f4d133d9933879f550d09955aa66e49a98110609", "abstract": "Personality-based personalized systems are increasingly gaining interest as personality traits has been shown to be a stable construct within humans. In order to provide a personality-based experience to the user, users' behavior, preferences, and needs in relation to their personality need to be investigated. Although for a technological mediated environment the search for these relationships is often new territory, there are findings from personality research of the real world that can be used in personalized systems. However, for these findings to be implementable, we need to investigate whether they hold in a technologically mediated environment. In this study we assess prior work on personality-based music genre preferences from traditional personality research. We analyzed a dataset consisting of music listening histories and personality scores of 1415 Last.fm users. Our results show agreements with prior work, but also important differences that can help to inform personalized systems.", "title": "Personality Traits and Music Genres: What Do People Prefer to Listen To?"}, "e769d68752ee6c489900392b2a5ed5b1c051edc3": {"paper_id": "e769d68752ee6c489900392b2a5ed5b1c051edc3", "abstract": "Music plays an important part in people\u2019s lives to regulate their emotions throughout the day. We conducted an online user study to investigate how the emotional state relates to the use of emotionally laden music. We found among 359 participants that they in general prefer emotionally laden music that correspond with their emotional state. However, when looking at personality traits, different patterns emerged. We found that when in a negative emotional state, those who scored high on openness, extraversion, and agreeableness tend to cheer themselves up with happy music, while those who scored high on neuroticism tend to increase their worry with sad music. With our results we show general patterns of music usage, but also individual differences. Our results contribute to the improvement of applications such as recommender systems in order to provide tailored recommendations based on users\u2019 personality and emotional state.", "title": "Personality & Emotional States: Understanding Users' Music Listening Needs"}, "44cb308b1f609befb937aaf83e417470e8aa21fb": {"paper_id": "44cb308b1f609befb937aaf83e417470e8aa21fb", "abstract": "Applications are getting increasingly interconnected. Although the interconnectedness provide new ways to gather information about the user, not all user information is ready to be directly implemented in order to provide a personalized experience to the user. Therefore, a general model is needed to which users\u2019 behavior, preferences, and needs can be connected to. In this paper we present our works on a personality-based music recommender system in which we use users\u2019 personality traits as a general model. We identified relationships between users\u2019 personality and their behavior, preferences, and needs, and also investigated different ways to infer users\u2019 personality traits from user-generated data of social networking sites (i.e., Facebook, Twitter, and Instagram). Our work contributes to new ways to mine and infer personality-based user models, and show how these models can be implemented in a music recommender system to positively contribute to the user experience.", "title": "Personality-Based User Modeling for Music Recommender Systems"}, "136453addebb04b046e06a524c19fa4e891ea7ae": {"paper_id": "136453addebb04b046e06a524c19fa4e891ea7ae", "abstract": "The present research examined individual differences in music preferences. A series of 6 studies investigated lay beliefs about music, the structure underlying music preferences, and the links between music preferences and personality. The data indicated that people consider music an important aspect of their lives and listening to music an activity they engaged in frequently. Using multiple samples, methods, and geographic regions, analyses of the music preferences of over 3,500 individuals converged to reveal 4 music-preference dimensions: Reflective and Complex, Intense and Rebellious, Upbeat and Conventional, and Energetic and Rhythmic. Preferences for these music dimensions were related to a wide array of personality dimensions (e.g., Openness), self-views (e.g., political orientation), and cognitive abilities (e.g., verbal IQ).", "title": "The do re mi's of everyday life: the structure and personality correlates of music preferences."}, "fd0c8c98ff6361b5feaae8d3db05b85c021decc4": {"paper_id": "fd0c8c98ff6361b5feaae8d3db05b85c021decc4", "abstract": "Though a variety of persuasive health applications have been designed with a preventive standpoint toward diseases in mind, many have been designed largely for a general audience. Designers of these technologies may achieve more success if applications consider an individual\u2019s personality type. Our goal for this research was to explore the relationship between personality and persuasive technologies in the context of health-promoting mobile applications. We conducted an online survey with 240 participants using storyboards depicting eight different persuasive strategies, the Big Five Inventory for personality domains, and questions on perceptions of the persuasive technologies. Our results and analysis revealed a number of significant relationships between personality and the persuasive technologies we evaluated. The findings from this study can guide the development of persuasive technologies that can cater to individual personalities to improve the likelihood of their success.", "title": "Personality and Persuasive Technology: An Exploratory Study on Health-Promoting Mobile Applications"}, "8b74344ab088d61b1dce3680e0f630df1d05a009": {"paper_id": "8b74344ab088d61b1dce3680e0f630df1d05a009", "abstract": "This position paper describes the initial research assumptions to improve music recommendations by including personality and emotional states. By including these psychological factors, we believe that the accuracy of the recommendation can be enhanced. We will give attention to how people use music to regulate their emotional states, and how this regulation is related to their personality. Furthermore, we will focus on how to acquire data from social media (i.e., microblogging sites such as Twitter) to predict the current emotional state of users. Finally, we will discuss how we plan to connect the correct emotionally laden music pieces to support the emotion regulation style of users.", "title": "Enhancing Music Recommender Systems with Personality Information and Emotional States: A Proposal"}, "0298a1adf550d9019f9e3e29b71ecc33af03c3bf": {"paper_id": "0298a1adf550d9019f9e3e29b71ecc33af03c3bf", "abstract": "We present a preliminary study on the relations between personality types and user preferences in multiple entertainment domains, namely movies, TV shows, music, and books. We analyze a total of 53,226 Facebook user profiles composed of both personality scores (openness, conscientiousness, extraversion, agreeableness, neuroticism) from the Five Factor model, and explicit interests about 16 genres in each of the above domains. As a result of our analysis, we extract personality-based user stereotypes and association rules for some of the considered domain genres, and infer similarities of personality types related to genres in different domains.", "title": "Relating Personality Types with User Preferences in Multiple Entertainment Domains"}, "c23b07f2f71646843d71447b1e2207046fb81ca1": {"paper_id": "c23b07f2f71646843d71447b1e2207046fb81ca1", "abstract": "Personality is a critical factor which influences people's behavior and interests. There is a high potential that incorporating users' characteristics into recommender systems could enhance recommendation quality and user experience. The goal of this tutorial is to give an overview of personality-based recommender systems and discuss challenges and possible research directions in this topic.", "title": "Personality-based recommender systems: an overview"}, "2ab9b5fb21b3e4f1de78494380135df535dd75e0": {"paper_id": "2ab9b5fb21b3e4f1de78494380135df535dd75e0", "abstract": "Incorporating users\u2019 personality traits has shown to be instrumental in many personalized retrieval and recommender systems. Analysis of users\u2019 digital traces has become an important resource for inferring personality traits. To date, the analysis of users\u2019 explicit and latent characteristics is typically restricted to a single social networking site (SNS). In this work, we propose a novel method that integrates text, image, and users\u2019 meta features from two different SNSs: Twitter and Instagram. Our preliminary results indicate that the joint analysis of users\u2019 simultaneous activities in two popular SNSs seems to lead to a consistent decrease of the prediction errors for each personality trait.", "title": "Fusing Social Media Cues: Personality Prediction from Twitter and Instagram"}, "83b427459fc5769b869a51f6f530388770ea83c1": {"paper_id": "83b427459fc5769b869a51f6f530388770ea83c1", "abstract": "Personality is a psychological construct aimed at explaining the wide variety of human behaviors in terms of a few, stable and measurable individual characteristics. In this respect, any technology involving understanding, prediction and synthesis of human behavior is likely to benefit from Personality Computing approaches, i.e. from technologies capable of dealing with human personality. This paper is a survey of such technologies and it aims at providing not only a solid knowledge base about the state-of-the-art, but also a conceptual model underlying the three main problems addressed in the literature, namely Automatic Personality Recognition (inference of the true personality of an individual from behavioral evidence), Automatic Personality Perception (inference of personality others attribute to an individual based on her observable behavior) and Automatic Personality Synthesis (generation of artificial personalities via embodied agents). Furthermore, the article highlights the issues still open in the field and identifies potential application areas.", "title": "A Survey of Personality Computing"}, "491e54f8f916fa544616bca3d34b6f8e37af0bcd": {"paper_id": "491e54f8f916fa544616bca3d34b6f8e37af0bcd", "abstract": "Social media is a place where users present themselves to the world, revealing personal details and insights into their lives. We are beginning to understand how some of this information can be utilized to improve the users' experiences with interfaces and with one another. In this paper, we are interested in the personality of users. Personality has been shown to be relevant to many types of interactions, it has been shown to be useful in predicting job satisfaction, professional and romantic relationship success, and even preference for different interfaces. Until now, to accurately gauge users' personalities, they needed to take a personality test. This made it impractical to use personality analysis in many social media domains. In this paper, we present a method by which a user's personality can be accurately predicted through the publicly available information on their Twitter profile. We will describe the type of data collected, our methods of analysis, and the machine learning techniques that allow us to successfully predict personality. We then discuss the implications this has for social media design, interface design, and broader domains.", "title": "Predicting Personality from Twitter"}, "988b9068ad08b1f0e05b6720ea646335b34c911f": {"paper_id": "988b9068ad08b1f0e05b6720ea646335b34c911f", "abstract": "The five-factor model of personality is a hierarchical organization of personality traits in terms of five basic dimensions: Extraversion, Agreeableness, Conscientiousness, Neuroticism, and Openness to Experience. Research using both natural language adjectives and theoretically based personality questionnaires supports the comprehensiveness of the model and its applicability across observers and cultures. This article summarizes the history of the model and its supporting evidence; discusses conceptions of the nature of the factors; and outlines an agenda for theorizing about the origins and operation of the factors. We argue that the model should prove useful both for individual assessment and for the elucidation of a number of topics of interest to personality psychologists.", "title": "An introduction to the five-factor model and its applications."}, "04a355c76146ea28e87de16fb0df3761b264c9eb": {"paper_id": "04a355c76146ea28e87de16fb0df3761b264c9eb", "abstract": "Psychological personality has been shown to affect a variety of aspects: preferences for interaction styles in the digital world and for music genres, for example. Consequently, the design of personalized user interfaces and music recommender systems might benefit from understanding the relationship between personality and use of social media. Since there has not been a study between personality and use of Twitter at large, we set out to analyze the relationship between personality and different types of Twitter users, including popular users and influentials. For 335 users, we gather personality data, analyze it, and find that both popular users and influentials are extroverts and emotionally stable (low in the trait of Neuroticism). Interestingly, we also find that popular users are `imaginative' (high in Openness), while influentials tend to be `organized' (high in Conscientiousness). We then show a way of accurately predicting a user's personality simply based on three counts publicly available on profiles: following, followers, and listed counts. Knowing these three quantities about an active user, one can predict the user's five personality traits with a root-mean-squared error below 0.88 on a $[1,5]$ scale. Based on these promising results, we argue that being able to predict user personality goes well beyond our initial goal of informing the design of new personalized applications as it, for example, expands current studies on privacy in social media.", "title": "Our Twitter Profiles, Our Selves: Predicting Personality with Twitter"}, "2c411a12f33f15451e1659a3435391962c0cc144": {"paper_id": "2c411a12f33f15451e1659a3435391962c0cc144", "abstract": "Images can affect people on an emotional level. Since the emotions that arise in the viewer of an image are highly subjective, they are rarely indexed. However there are situations when it would be helpful if images could be retrieved based on their emotional content. We investigate and develop methods to extract and combine low-level features that represent the emotional content of an image, and use these for image emotion classification. Specifically, we exploit theoretical and empirical concepts from psychology and art theory to extract image features that are specific to the domain of artworks with emotional expression. For testing and training, we use three data sets: the International Affective Picture System (IAPS); a set of artistic photography from a photo sharing site (to investigate whether the conscious use of colors and textures displayed by the artists improves the classification); and a set of peer rated abstract paintings to investigate the influence of the features and ratings on pictures without contextual content. Improved classification results are obtained on the International Affective Picture System (IAPS), compared to state of the art work.", "title": "Affective image classification using features inspired by psychology and art theory"}, "04979919f0e3419750df13c47a9605ac2c9a4721": {"paper_id": "04979919f0e3419750df13c47a9605ac2c9a4721", "abstract": "In this paper, we address the issue of personality and interaction style recognition from profile pictures in Facebook. We recruited volunteers among Facebook users and collected a dataset of profile pictures, labeled with gold standard self-assessed personality and interaction style labels. Then, we exploited a bag-of-visual-words technique to extract features from pictures. Finally, different machine learning approaches were used to test the effectiveness of these features in predicting personality and interaction style traits. Our good results show that this task is very promising, because profile pictures convey a lot of information about a user and are directly connected to impression formation and identity management.", "title": "Automatic Personality and Interaction Style Recognition from Facebook Profile Pictures"}, "a66fe50037d1a4c3798fb0aadb6e9b7c5c8b6319": {"paper_id": "a66fe50037d1a4c3798fb0aadb6e9b7c5c8b6319", "abstract": "Despite an increasing interest in understanding human perception in social media through the automatic analysis of users' personality, existing attempts have explored user profiles and text blog data only. We approach the study of personality impressions in social media from the novel perspective of crowdsourced impressions, social attention, and audiovisual behavioral analysis on slices of conversational vlogs extracted from YouTube. Conversational vlogs are a unique case study to understand users in social media, as vloggers implicitly or explicitly share information about themselves that words, either written or spoken cannot convey. In addition, research in vlogs may become a fertile ground for the study of video interactions, as conversational video expands to innovative applications. In this work, we first investigate the feasibility of crowdsourcing personality impressions from vlogging as a way to obtain judgements from a variate audience that consumes social media video. Then, we explore how these personality impressions mediate the online video watching experience and relate to measures of attention in YouTube. Finally, we investigate on the use of automatic nonverbal cues as a suitable lens through which impressions are made, and we address the task of automatic prediction of vloggers' personality impressions using nonverbal cues and machine learning techniques. Our study, conducted on a dataset of 442 YouTube vlogs and 2210 annotations collected in Amazon's Mechanical Turk, provides new findings regarding the suitability of collecting personality impressions from crowdsourcing, the types of personality impressions that emerge through vlogging, their association with social attention, and the level of utilization of nonverbal cues in this particular setting. In addition, it constitutes a first attempt to address the task of automatic vlogger personality impression prediction using nonverbal cues, with promising results.", "title": "The YouTube Lens: Crowdsourced Personality Impressions and Audiovisual Analysis of Vlogs"}, "00a403a0fe1e44510cc5096631ca9c585e69fc37": {"paper_id": "00a403a0fe1e44510cc5096631ca9c585e69fc37", "abstract": "Social media is a place where users present themselves to the world, revealing personal details and insights into their lives. We are beginning to understand how some of this information can be utilized to improve the users' experiences with interfaces and with one another. In this paper, we are interested in the personality of users. Personality has been shown to be relevant to many types of interactions; it has been shown to be useful in predicting job satisfaction, professional and romantic relationship success, and even preference for different interfaces. Until now, to accurately gauge users' personalities, they needed to take a personality test. This made it impractical to use personality analysis in many social media domains. In this paper, we present a method by which a user's personality can be accurately predicted through the publicly available information on their Facebook profile. We will describe the type of data collected, our methods of analysis, and the results of predicting personality traits through machine learning. We then discuss the implications this has for social media design, interface design, and broader domains.", "title": "Predicting personality with social media"}, "2d54534ac78698e5db495f376b3469a9e4f8ecc0": {"paper_id": "2d54534ac78698e5db495f376b3469a9e4f8ecc0", "abstract": "Individual differences in personality affect users\u2019 online activities as much as they do in the offline world. This work, based on a sample of over a third of a million users, examines how users\u2019 behaviour in the online environment, captured by their website choices and Facebook profile features, relates to their personality, as measured by the standard Five Factor Model personality questionnaire. Results show that there are psychologically meaningful links between users\u2019 personalities, their website preferences and Facebook profile features. We show how website audiences differ in terms of their personality, present the relationships between personality and Facebook profile features, and show how an individual\u2019s personality can be predicted from Facebook profile features. We conclude that predicting a user\u2019s personality profile can be applied to personalize content, optimize search results, and improve online advertising.", "title": "Manifestations of user personality in website choice and behaviour on online social networks"}, "27d675bc4570f0867079f044e678545322f5f4fc": {"paper_id": "27d675bc4570f0867079f044e678545322f5f4fc", "abstract": "It is well known that utterances convey a great deal of information about the speaker in addition to their semantic content. One such type of information consists of cues to the speaker\u2019s personality traits, the most fundamental dimension of variation between humans. Recent work explores the automatic detection of other types of pragmatic variation in text and conversation, such as emotion, deception, speaker charisma, dominance, point of view, subjectivity, opinion and sentiment. Personality affects these other aspects of linguistic production, and thus personality recognition may be useful for these tasks, in addition to many other potential applications. However, to date, there is little work on the automatic recognition of personality traits. This article reports experimental results for recognition of all Big Five personality traits, in both conversation and text, utilising both self and observer ratings of personality. While other work reports classification results, we experiment with classification, regression and ranking models. For each model, we analyse the effect of different feature sets on accuracy. Results show that for some traits, any type of statistical model performs significantly better than the baseline, but ranking models perform best overall. We also present an experiment suggesting that ranking models are more accurate than multi-class classifiers for modelling personality. In addition, recognition models trained on observed personality perform better than models trained using selfreports, and the optimal feature set depends on the personality trait. A qualitative analysis of the learned models confirms previous findings linking language and personality, while revealing many new linguistic markers.", "title": "Using Linguistic Cues for the Automatic Recognition of Personality in Conversation and Text"}, "5ead970c6a7b75e2483325a92da622fbc5f027ee": {"paper_id": "5ead970c6a7b75e2483325a92da622fbc5f027ee", "abstract": "We study the relationship between Facebook popularity (number of contacts) and personality traits on a large number of subjects. We test to which extent two prevalent viewpoints hold. That is, popular users (those with many social contacts) are the ones whose personality traits either predict many offline (real world) friends or predict propensity to maintain superficial relationships. We find that the predictor for number of friends in the real world (Extraversion) is also a predictor for number of Facebook contacts. We then test whether people who have many social contacts on Facebook are the ones who are able to adapt themselves to new forms of communication, present themselves in likable ways, and have propensity to maintain superficial relationships. We show that there is no statistical evidence to support such a conjecture.", "title": "The personality of popular facebook users"}, "46465a97ab883cc72d9f601263a208afae6fb31a": {"paper_id": "46465a97ab883cc72d9f601263a208afae6fb31a", "abstract": "To provide a measure of the Big Five for contexts in which participant time is severely limited, we abbreviated the Big Five Inventory (BFI-44) to a 10-item version, the BFI-10. To permit its use in cross-cultural research, the BFI-10 was developed simultaneously in several samples in both English and German. Results focus on the psychometric characteristics of the 2-item scales on the BFI-10, including their part-whole correlations with the BFI-44 scales, retest reliability, structural validity, convergent validity with the NEO-PI-R and its facets, and external validity using peer ratings. Overall, results indicate that the BFI-10 scales retain signiWcant levels of reliability and validity. Thus, reducing the items of the BFI-44 to less than a fourth yielded eVect sizes that were lower than those for the full BFI-44 but still suYcient for research settings with truly limited time constraints. \u00a9 2006 Elsevier Inc. All rights reserved.", "title": "Measuring personality in one minute or less : A 10-item short version of the Big Five Inventory in English and German"}, "6d8c9fcce8177d6f8d122d653c7d32d7624d6714": {"paper_id": "6d8c9fcce8177d6f8d122d653c7d32d7624d6714", "abstract": "More than twelve years have elapsed since the first public release of WEKA. In that time, the software has been rewritten entirely from scratch, evolved substantially and now accompanies a text on data mining [35]. These days, WEKA enjoys widespread acceptance in both academia and business, has an active community, and has been downloaded more than 1.4 million times since being placed on Source-Forge in April 2000. This paper provides an introduction to the WEKA workbench, reviews the history of the project, and, in light of the recent 3.6 stable release, briefly discusses what has been added since the last stable version (Weka 3.4) released in 2003.", "title": "The WEKA data mining software: an update"}, "bbcc25ff101b263aab2e46dc2175b2ddae36cb76": {"paper_id": "bbcc25ff101b263aab2e46dc2175b2ddae36cb76", "abstract": "More than 700 million people worldwide now have profiles on on-line social networking sites (OSNs), such as MySpace and Facebook (ComScore, 2008); OSNs have become integrated into the milieu of modern-day social interactions and are widely used as a primary medium for communication and networking (boyd & Ellison, 2007; Valkenburg & Peter, 2009). Despite the increasing integration of OSN activity into everyday life, however, there has been no research on the most fundamental question about OSN profiles: Do they convey accurate impressions of profile owners? A widely held assumption, supported by content analyses, suggests that OSN profiles are used to create and communicate idealized selves (Manago, Graham, Greenfield, & Salim-khan, 2008). According to this idealized virtual-identity hypothesis, profile owners display idealized characteristics that do not reflect their actual personalities. Thus, personality impressions based on OSN profiles should reflect profile own-ers' ideal-self views rather than what the owners are actually like. A contrasting view holds that OSNs may constitute an extended social context in which to express one's actual personality characteristics, thus fostering accurate interpersonal perceptions. OSNs integrate various sources of personal information that mirror those found in personal environments, private thoughts, facial images, and social behavior, all of which are known to contain valid information about personality creating idealized identities should be hard to accomplish because (a) OSN profiles include information about one's reputation that is difficult to control (e.g., wall posts) and (b) friends provide accountability and subtle feedback on one's profile. Accordingly, the extended real-life hypothesis predicts that people use OSNs to communicate their real personality. If this supposition is true, lay observers should be able to accurately infer the personality characteristics of OSN profile owners. In the present study, we tested the two competing hypotheses. were recruited from the University of Texas campus, where flyers and candy were used to find volunteers for a laboratory-based study of personality judgment. Participants were compensated with a combination of money and course credit. In Germany, participants were recruited through advertisements for an on-line study on personality measurement. As compensation , they received individual feedback on their personality scores. To ensure that participants did not alter their OSN profiles, we saved their profiles before the subject of OSNs was raised. Scores on all measures were normally distributed. Measures Accuracy criteria. Accuracy criteria (i.e., indices of what profile owners were actually like) were created by aggregating across multiple personality reports, each of \u2026", "title": "Facebook profiles reflect actual personality, not self-idealization."}, "2c15f86a318c103eb3422423dec1b3eb494a6f8f": {"paper_id": "2c15f86a318c103eb3422423dec1b3eb494a6f8f", "abstract": "Secondary analyses of Revised NEO Personality Inventory data from 26 cultures (N = 23,031) suggest that gender differences are small relative to individual variation within genders; differences are replicated across cultures for both college-age and adult samples, and differences are broadly consistent with gender stereotypes: Women reported themselves to be higher in Neuroticism, Agreeableness, Warmth, and Openness to Feelings, whereas men were higher in Assertiveness and Openness to Ideas. Contrary to predictions from evolutionary theory, the magnitude of gender differences varied across cultures. Contrary to predictions from the social role model, gender differences were most pronounced in European and American cultures in which traditional sex roles are minimized. Possible explanations for this surprising finding are discussed, including the attribution of masculine and feminine behaviors to roles rather than traits in traditional cultures.", "title": "Gender differences in personality traits across cultures: robust and surprising findings."}, "e6d84c3b17ade98da650ffe25b1822966734809a": {"paper_id": "e6d84c3b17ade98da650ffe25b1822966734809a", "abstract": "The current study focuses on the emergence of friendship networks among just-acquainted individuals, investigating the effects of Big Five personality traits on friendship selection processes. Sociometric nominations and self-ratings on personality traits were gathered from 205 late adolescents (mean age=19 years) at 5 time points during the first year of university. SIENA, a novel multilevel statistical procedure for social network analysis, was used to examine effects of Big Five traits on friendship selection. Results indicated that friendship networks between just-acquainted individuals became increasingly more cohesive within the first 3 months and then stabilized. Whereas individuals high on Extraversion tended to select more friends than those low on this trait, individuals high on Agreeableness tended to be selected more as friends. In addition, individuals tended to select friends with similar levels of Agreeableness, Extraversion, and Openness.", "title": "Emerging late adolescent friendship networks and Big Five personality traits: a social network approach."}, "fadc0059a2ef31e19a8f98e2516d3e28ed9c5975": {"paper_id": "fadc0059a2ef31e19a8f98e2516d3e28ed9c5975", "abstract": "Title of dissertation: COMPUTING AND APPLYING TRUST IN WEB-BASED SOCIAL NETWORKS Jennifer Ann Golbeck, Doctor of Philosophy, 2005 Dissertation directed by: Professor James Hendler Department of Computer Science The proliferation of web-based social networks has lead to new innovations in social networking, particularly by allowing users to describe their relationships beyond a basic connection. In this dissertation, I look specifically at trust in web-based social networks, how it can be computed, and how it can be used in applications. I begin with a definition of trust and a description of several properties that affect how it is used in algorithms. This is complemented by a survey of web-based social networks to gain an understanding of their scope, the types of relationship information available, and the current state of trust. The computational problem of trust is to determine how much one person in the network should trust another person to whom they are not connected. I present two sets of algorithms for calculating these trust inferences: one for networks with binary trust ratings, and one for continuous ratings. For each rating scheme, the algorithms are built upon the defined notions of trust. Each is then analyzed theoretically and with respect to simulated and actual trust networks to determine how accurately they calculate the opinions of people in the system. I show that in both rating schemes the algorithms presented can be expected to be quite accurate. These calculations are then put to use in two applications. FilmTrust is a website that combines trust, social networks, and movie ratings and reviews. Trust is used to personalize the website for each user, displaying recommended movie ratings, and ordering reviews by relevance. I show that, in the case where the user's opinion is divergent from the average, the trust-based recommended ratings are more accurate than several other common collaborative filtering techniques. The second application is TrustMail, an email client that uses the trust rating of each sender as a score for the message. Users can then sort messages according to their trust value. I conclude with a description of other applications where trust inferences can be used, and how the lessons from this dissertation can be applied to infer information about relationships in other complex systems. COMPUTING AND APPLYING TRUST IN WEB-BASED SOCIAL NETWORKS by Jennifer Ann Golbeck Dissertation Submitted to the Faculty of the Graduate School of the University of Maryland, College Park in partial fulfillment of the requirements for the degree of Doctor of Philosophy 2005 Advisory Committee: Professor James Hendler, Chair/Advisor Professor Ashok Agrawala Professor Mark Austin Professor Benjamin Bederson Professor Lise Getoor Professor Ben Shneiderman \u00a9Copyright by Jennifer Ann Golbeck 2005", "title": "Computing and Applying Trust in Web-based Social Networks"}, "434720ab8ea28b1f9c808df651c07dc6ed627ac9": {"paper_id": "434720ab8ea28b1f9c808df651c07dc6ed627ac9", "abstract": "Two key elements in the area of cognitive ergonomics are user-system performance and the user similarity. However, with the introduction of skinnable user interfaces, a technology that gives the user interface a chameleon-like ability, elements such as aesthetic, fun, and especially user individuality and identity become more important. This paper presents two explorative studies on user personality in relation to skin preferences. In the studies participants were asked to rate their preference of a set of Windows Media Player skins and to complete the BIS/BAS and the IPIP-NEO personality inventories. The results of the first study suggest colour and similarity-attraction as two possible underlying factors for the correlations found between personality traits and skin preferences. The results of the second study partly confirm these findings, however not for similar personality traits and skin types correlations.", "title": "Towards Customized Emotional Design : An Explorative Study of User Personality and User Interface Skin Preferences"}, "26a2a2f683b6e6d93c510a2f8065870c54b05f05": {"paper_id": "26a2a2f683b6e6d93c510a2f8065870c54b05f05", "abstract": "User studies are important for many aspects of the design process and involve techniques ranging from informal surveys to rigorous laboratory studies. However, the costs involved in engaging users often requires practitioners to trade off between sample size, time requirements, and monetary costs. Micro-task markets, such as Amazon's Mechanical Turk, offer a potential paradigm for engaging a large number of users for low time and monetary costs. Here we investigate the utility of a micro-task market for collecting user measurements, and discuss design considerations for developing remote micro user evaluation tasks. Although micro-task markets have great potential for rapidly collecting user measurements at low costs, we found that special care is needed in formulating tasks in order to harness the capabilities of the approach.", "title": "Crowdsourcing user studies with Mechanical Turk"}, "9238534e7836ce094b2498b3587b10c89b733afe": {"paper_id": "9238534e7836ce094b2498b3587b10c89b733afe", "abstract": "For decades our common understanding of the organization of economic production has been that individuals order their productive activities in one of two ways: either as employees in firms, following the directions of managers, or as individuals in markets, following price signals. This dichotomy was first identified in the early work of Ronald Coase and was developed most explicitly in the work of institutional economist Oliver Williamson. In this paper I explain why we are beginning to see the emergence of a new, third mode of production, in the digitally networked environment, a mode I call commons-based peer production. In the past three or four years, public attention has focused on a fifteen-yearold social-economic phenomenon in the software development world. This phenomenon, called free software or open source software, involves thousands or even tens of thousands of programmers contributing to large and small scale projects, where the central organizing principle is that the software remains free of most constraints on copying and use common to proprietary materials. No one \u201cowns\u201d the software in the traditional sense of being able to command how it is used or developed, or to control its disposition. The result has been the emergence of a vibrant, innovative and productive collaboration, whose participants are not organized in firms and do not choose their projects in response to price signals. This paper explains that while free software is highly visible, it is in fact only one example of a much broader social-economic phenomenon. I suggest that we are seeing the broad and deep emergence of a new, third mode of production in the \u2217 Professor of Law, New York University School of Law. Research for this paper was partly supported by a grant from the Filomen D\u2019Agostino and Max Greenberg Fund at NYU School of Law. I owe thanks to many for comments on this and earlier drafts, including: Bruce Ackerman, Ed Baker, Elazar Barkan, Dan Burk, Jamie Boyle, Niva Elkin Koren, Terry Fisher, Natalie Jeremijenko, Dan Kahan, Doug Lichtman, Tara Lemmy, Mark Nadel, Carol Rose, Bob Ellickson, Peggy Radin, Clay Shirky, Helen Nissenbaum, Jerry Mashaw, Eben Moglen, Larry Lessig, Chuck Sabel, Alan Schwartz, Richard Stallman, and Kenji Yoshino. I owe special thanks to Steve Snyder for his invaluable research assistance on the peer production enterprises described here. I have gotten many question about the \u201cCoase\u2019s Penguin\u201d portion of the title. It turns out that the geek culture that easily recognizes \u201cCoase\u201d doesn\u2019t\u2019 recognize the \u201cpenguin,\u201d and vice versa. So, \u201cCoase\u201d refers to Ronald Coase, who originated the transactions costs theory of the firm that provides the methodological template for the positive analysis of peer production that I offer here. The penguin refers to the fact that the Linux kernel development community has adopted the image of a paunchy penguin as its mascot/trademark. One result of this cross-cultural conversation is that I will occasionally explain in some detail concepts that are well known in one community but not in the other. 2 COASE\u2019S PENGUIN V.04.3 AUGUST. 2002 2 digitally networked environment. I call this mode \u201ccommons-based peer production,\u201d to distinguish it from the propertyand contract-based modes of firms and markets. Its central characteristic is that groups of individuals successfully collaborate on largescale projects following a diverse cluster of motivational drives and social signals, rather than either market prices or managerial commands. I explain why this mode has systematic advantages over markets and managerial hierarchies when the object of production is information or culture, and where the physical capital necessary for that production\u2014computers and communications capabilities\u2014is widely distributed instead of concentrated. In particular, this mode of production is better than firms and markets for two reasons. First, it is better at identifying and assigning human capital to information and cultural production processes. In this regard, peer production has an advantage in what I call \u201cinformation opportunity cost.\u201d That is, it loses less information about who the best person for a given job might be than either of the other two organizational modes. Second, there are substantial increasing returns, in terms of allocation efficiency, to allowing larger clusters of potential contributors to interact with large clusters of information resources in search of new projects and opportunities for collaboration. Removing property and contract as the organizing principles of collaboration substantially reduces transaction costs involved in allowing these large clusters of potential contributors to review and select which resources to work on, for which projects, and with which collaborators. This results in the potential for substantial allocation gains. The article concludes with an overview of how these models use a variety of technological and social strategies to overcome the collective action problems usually solved in managerial and market-based systems by property, contract, and managerial commands.", "title": "Coase's Penguin, or Linux and the Nature of the Firm"}, "74b52491132407ffcb0486f544bfd2b892429442": {"paper_id": "74b52491132407ffcb0486f544bfd2b892429442", "abstract": "Personality traits are increasingly being incorporated in systems to provide a personalized experience to the user. Current work focusing on identifying the relationship between personality and behavior, preferences, and needs o\u0089en do not take into account di\u0082erences between age groups. With music playing an important role in our lives, di\u0082erences between age groups may be especially prevalent. In this work we investigate whether di\u0082erences exist in music listening behavior between age groups. We analyzed a dataset with the music listening histories and personality information of 1415 users. Our results show agreements with prior work that identi\u0080ed personality-based music listening preferences. However, our results show that the agreements we found are in some cases divided over di\u0082erent age groups, whereas in other cases additional correlations were found within age groups. With our results personality-based systems can provide be\u008aer music recommendations that is in line with the user\u2019s age.", "title": "Personality Traits and Music Genre Preferences: How Music Taste Varies Over Age Groups"}, "cd86884760b6e4c908f2481cd8de5b4e72f47393": {"paper_id": "cd86884760b6e4c908f2481cd8de5b4e72f47393", "abstract": "Music streaming services increasingly incorporate additional music taxonomies (i.e., mood, activity, and genre) to provide users different ways to browse through music collections. However, these additional taxonomies can distract the user from reaching their music goal, and influence choice satisfaction. We conducted an online user study with an application called \"Tune-A-Find,\" where we measured participants' music taxonomy choice (mood, activity, and genre). Among 297 participants, we found that the chosen taxonomy is related to personality traits. We found that openness to experience increased the choice for browsing music by mood, while conscientiousness increased the choice for browsing music by activity. In addition, those high in neuroticism were most likely to browse for music by activity or genre. Our findings can support music streaming services to further personalize user interfaces. By knowing the user's personality, the user interface can adapt to the user's preferred way of music browsing.", "title": "Personality Traits Predict Music Taxonomy Preferences"}, "9d9eddf405b1aa4eaf6c35b0d16e0b96422f0666": {"paper_id": "9d9eddf405b1aa4eaf6c35b0d16e0b96422f0666", "abstract": "This study aimed to investigate the surgical techniques and the clinical efficacy of a modified Frosch approach in the treatment of posterolateral tibial plateau fractures. The standard Frosch approach was performed on 5 fresh-frozen cadavers. The mean bony surface area was measured upon adequate exposure of the proximal tibial cortex. Lateral proximal tibial plate and posterolateral T-plates were placed and the ease of the procedure was noted. In the study, 12 clinical cases of posterolateral tibial plateau fractures were treated via modified or standard Frosch approaches. The outcome was assessed over short to medium follow-up period. Cadaver studies allowed the inspection of the posterolateral joint surface of all specimens from the lateral side. The mean bony surface areas of the exposed lateral and posterolateral tibial plateau were (6.78 \u00b1 1.13) cm2 and (3.59 \u00b1 0.65) cm2, respectively. Lateral and posterolateral plates were implanted successfully. Lateral proximal tibial plates were fixed in 10 patients via a modified Frosch approach while posterolateral plates were fixed in 2 patients via a standard Frosch approach. Patients were followed up for 10 to 24 months (average: 15.7 months) and no complications were observed during this period. Based on the Rasmussen knee function score system, the results were recorded as excellent, good, and fair in 6, 4, and 2 patients, respectively. In conclusion, the modified Frosch approach has offers advantages of clear clarity in exposure, convenient for reduction and internal fixation of a fracture, and good positive clinical results over the normal approach.", "title": "Treatment of posterolateral tibial plateau fractures with modified Frosch approach : a cadaveric study and case series"}, "2071101b239b56b226c3941b872ee2cadb58543c": {"paper_id": "2071101b239b56b226c3941b872ee2cadb58543c", "abstract": "We describe a posterolateral transfibular neck approach to the proximal tibia. This approach was developed as an alternative to the anterolateral approach to the tibial plateau for the treatment of two fracture subtypes: depressed and split depressed fractures in which the comminution and depression are located in the posterior half of the lateral tibial condyle. These fractures have proved particularly difficult to reduce and adequately internally fix through an anterior or anterolateral approach. The approach described in this article exposes the posterolateral aspect of the tibial plateau between the posterior margin of the iliotibial band and the posterior cruciate ligament. The approach allows lateral buttressing of the lateral tibial plateau and may be combined with a simultaneous posteromedial and/or anteromedial approach to the tibial plateau. Critically, the proximal tibial soft tissue envelope and its blood supply are preserved. To date, we have used this approach either alone or in combination with a posteromedial approach for the successful reduction of tibial plateau fractures in eight patients. No complications related to this approach were documented, including no symptoms related to the common peroneal nerve, and all fractures and fibular neck osteotomies healed uneventfully.", "title": "Posterolateral transfibular approach to tibial plateau fractures: technique, results, and rationale."}, "cca7984d0339a3345b3253736bde3fed61b4ab15": {"paper_id": "cca7984d0339a3345b3253736bde3fed61b4ab15", "abstract": "The selection of a surgical approach for the treatment of tibia plateau fractures is an important decision. Approximately 7% of all tibia plateau fractures affect the posterolateral corner. Displaced posterolateral tibia plateau fractures require anatomic articular reduction and buttress plate fixation on the posterior aspect. These aims are difficult to reach through a lateral or anterolateral approach. The standard posterolateral approach with fibula osteotomy and release of the posterolateral corner is a traumatic procedure, which includes the risk of fragment denudation. Isolated posterior approaches do not allow sufficient visual control of fracture reduction, especially if the fracture is complex. Therefore, the aim of this work was to present a surgical approach for posterolateral tibial plateau fractures that both protects the soft tissue and allows for good visual control of fracture reduction. The approach involves a lateral arthrotomy for visualizing the joint surface and a posterolateral approach for the fracture reduction and plate fixation, which are both achieved through one posterolateral skin incision. Using this approach, we achieved reduction of the articular surface and stable fixation in six of seven patients at the final follow-up visit. No complications and no loss of reduction were observed. Additionally, the new posterolateral approach permits direct visual exposure and facilitates the application of a buttress plate. Our approach does not require fibular osteotomy, and fragments of the posterolateral corner do not have to be detached from the soft tissue network.", "title": "A new posterolateral approach without fibula osteotomy for the treatment of tibial plateau fractures."}, "ff7336c1c90110243c8bf0e92cf2b217694d13c9": {"paper_id": "ff7336c1c90110243c8bf0e92cf2b217694d13c9", "abstract": "Moore type II Entire Condyle fractures of the tibia plateau represent a rare and highly unstable fracture pattern that usually results from high impact traumas. Specific recommendations regarding the surgical treatment of these fractures are sparse. We present a series of Moore type II fractures treated by open reduction and internal fixation through a direct dorsal approach. Five patients (3 females, 2 males) with Entire Condyle fractures were retrospectively analyzed after a mean follow-up period of 39\u00a0months (range 12\u201361\u00a0months). Patient mean age at the time of operation was 36\u00a0years (range 26\u201343\u00a0years). Follow-up included clinical and radiological examination. Furthermore, all patient finished a SF36 and Lysholm knee score questionnaire. Average range of motion was 127/0/1\u00b0 with all patients reaching full extension at the time of last follow up. Patients reached a mean Lysholm score of 81.2 points (range 61\u2013100 points) and an average SF36 of 82.36 points (range 53.75\u201398.88 points). One patient sustained deep wound infection after elective implant removal 1\u00a0year after the initial surgery. Overall all patients were highly satisfied with the postoperative result. The direct dorsal approach to the tibial plateau represents an adequate method to enable direct fracture exposure, open reduction, and internal fixation in posterior shearing medial Entire Condyle fractures and is especially valuable when also the dorso-lateral plateau is depressed.", "title": "Open reduction and fixation of medial Moore type II fractures of the tibial plateau by a direct dorsal approach"}, "8ae14aedfa58b9f1f1161b56e38f1a9e5190ec25": {"paper_id": "8ae14aedfa58b9f1f1161b56e38f1a9e5190ec25", "abstract": "The impact of air quality on health and on life comfort is well established. In many societies, vulnerable elderly and young populations spend most of their time indoors. Therefore, indoor air quality monitoring (IAQM) is of great importance to human health. Engineers and researchers are increasingly focusing their efforts on the design of real-time IAQM systems using wireless sensor networks. This paper presents an end-to-end IAQM system enabling measurement of CO\u2082, CO, SO\u2082, NO\u2082, O\u2083, Cl\u2082, ambient temperature, and relative humidity. In IAQM systems, remote users usually use a local gateway to connect wireless sensor nodes in a given monitoring site to the external world for ubiquitous access of data. In this work, the role of the gateway in processing collected air quality data and its reliable dissemination to end-users through a web-server is emphasized. A mechanism for the backup and the restoration of the collected data in the case of Internet outage is presented. The system is adapted to an open-source Internet-of-Things (IoT) web-server platform, called Emoncms, for live monitoring and long-term storage of the collected IAQM data. A modular IAQM architecture is adopted, which results in a smart scalable system that allows seamless integration of various sensing technologies, wireless sensor networks (WSNs) and smart mobile standards. The paper gives full hardware and software details of the proposed solution. Sample IAQM results collected in various locations are also presented to demonstrate the abilities of the system.", "title": "A Modular IoT Platform for Real-Time Indoor Air Quality Monitoring"}, "e7336fd63f5bc41ba7920fc8d17eea69acccba2e": {"paper_id": "e7336fd63f5bc41ba7920fc8d17eea69acccba2e", "abstract": "With growing transportation and population density, increasing global warming and sudden climate change, air quality is one of the critical measures that is needed to be monitored closely on a real-time basis in today's urban ecosystems. This paper examines the issues, infrastructure, information processing, and challenges of designing and implementing an integrated sensing system for real-time indoor air quality monitoring. The system aims to detect the level of seven gases, ozone (O3), particulate matter, carbon monoxide (CO), nitrogen oxides (NO2), sulfur dioxide (SO2), volatile organic compound, and carbon dioxide (CO2), on a real-time basis and provides overall air quality alert timely. Experiments are conducted to validate and support the development of the system for real-time monitoring and alerting.", "title": "ISSAQ: An Integrated Sensing Systems for Real-Time Indoor Air Quality Monitoring"}, "8e81ff7f3ba86b130aaae88550e6f5fcc9afe165": {"paper_id": "8e81ff7f3ba86b130aaae88550e6f5fcc9afe165", "abstract": "Indoor air quality is important. It influences human productivity and health. Personal pollution exposure can be measured using stationary or mobile sensor networks, but each of these approaches has drawbacks. Stationary sensor network accuracy suffers because it is difficult to place a sensor in every location people might visit. In mobile sensor networks, accuracy and drift resistance are generally sacrificed for the sake of mobility and economy. We propose a hybrid sensor network architecture, which contains both stationary sensors (for accurate readings and calibration) and mobile sensors (for coverage). Our technique uses indoor pollutant concentration prediction models to determine the structure of the hybrid sensor network. In this work, we have (1) developed a predictive model for pollutant concentration that minimizes prediction error; (2) developed algorithms for hybrid sensor network construction; and (3) deployed a sensor network to gather data on the airflow in a building, which are later used to evaluate the prediction model and hybrid sensor network synthesis algorithm. Our modeling technique reduces sensor network error by 40.4% on average relative to a technique that does not explicitly consider the inaccuracies of individual sensors. Our hybrid sensor network synthesis technique improves personal exposure measurement accuracy by 35.8% on average compared with a stationary sensor network architecture.", "title": "A Hybrid Sensor System for Indoor Air Quality Monitoring"}, "461b64ed2fd8c921bf543518a10357f464785de9": {"paper_id": "461b64ed2fd8c921bf543518a10357f464785de9", "abstract": "Wireless Sensor Networks (WSNs) are becoming increasingly popular since they can gather information from different locations without wires. This advantage is exploited in applications such as robotic systems, telecare, domotic or smart cities, among others. To gain independence from the electricity grid, WSNs devices are equipped with batteries, therefore their operational time is determined by the time that the batteries can power on the device. As a consequence, engineers must consider low energy consumption as a critical objective to design WSNs. Several approaches can be taken to make efficient use of energy in WSNs, for instance low-duty-cycling sensor networks (LDC-WSN). Based on the LDC-WSNs, we present LOKA, a LOw power Konsumption Algorithm to minimize WSNs energy consumption using different power modes in a sensor mote. The contribution of the work is a novel algorithm called LOKA that implements two duty-cycling mechanisms using the end-device of the ZigBee protocol (of the Application Support Sublayer) and an external microcontroller (Cortex M0+) in order to minimize the energy consumption of a delay tolerant networking. Experiments show that using LOKA, the energy required by the sensor device is reduced to half with respect to the same sensor device without using LOKA.", "title": "A Low Power Consumption Algorithm for Efficient Energy Consumption in ZigBee Motes"}, "f3a386d0989a3d6e05e32bad01b1190f9bdd8f83": {"paper_id": "f3a386d0989a3d6e05e32bad01b1190f9bdd8f83", "abstract": "In recent years, Wireless Sensor Networks (WSNs) have emerged as a new powerful technology used in many applications such as military operations, surveillance system, Intelligent Transport Systems (ITS) etc. These networks consist of many Sensor Nodes (SNs), which are not only used for monitoring but also capturing the required data from the environment. Most of the research proposals on WSNs have been developed keeping in view of minimization of energy during the process of extracting the essential data from the environment where SNs are deployed. The primary reason for this is the fact that the SNs are operated on battery which discharges quickly after each operation. It has been found in literature that clustering is the most common technique used for energy aware routing in WSNs. The most popular protocol for clustering in WSNs is Low Energy Adaptive Clustering Hierarchy (LEACH) which is based on adaptive clustering technique. This paper provides the taxonomy of various clustering and routing techniques in WSNs based upon metrics such as power management, energy management, network lifetime, optimal cluster head selection, multihop data transmission etc. A comprehensive discussion is provided in the text highlighting the relative advantages and disadvantages of many of the prominent proposals in this category which helps the designers to select a particular proposal based upon its merits over the others. & 2012 Elsevier Ltd. All rights reserved.", "title": "A systematic review on clustering and routing techniques based upon LEACH protocol for wireless sensor networks"}, "7d37384e9650fb62785e331cfcc54afa9da339bf": {"paper_id": "7d37384e9650fb62785e331cfcc54afa9da339bf", "abstract": "One of the emerging networking standards that gap between the physical world and the cyber one is the Internet of Things. In the Internet of Things, smart objects communicate with each other, data are gathered and certain requests of users are satisfied by different queried data. The development of energy efficient schemes for the IoT is a challenging issue as the IoT becomes more complex due to its large scale the current techniques of wireless sensor networks cannot be applied directly to the IoT. To achieve the green networked IoT, this paper addresses energy efficiency issues by proposing a novel deployment scheme. This scheme, introduces: (1) a hierarchical network design; (2) a model for the energy efficient IoT; (3) a minimum energy consumption transmission algorithm to implement the optimal model. The simulation results show that the new scheme is more energy efficient and flexible than traditional WSN schemes and consequently it can be implemented for efficient communication in the IoT.", "title": "A Novel Scheme for an Energy Efficient Internet of Things Based on Wireless Sensor Networks"}, "10198412922b6a21e379aff3d8b2c8bc0130c56b": {"paper_id": "10198412922b6a21e379aff3d8b2c8bc0130c56b", "abstract": "In this paper, we propose a wireless sensor network paradigm for real-time forest fire detection. The wireless sensor network can detect and forecast forest fire more promptly than the traditional satellite-based detection approach. This paper mainly describes the data collecting and processing in wireless sensor networks for real-time forest fire detection. A neural network method is applied to in-network data processing. We evaluate the performance of our approach by simulations.", "title": "Real-time forest fire detection with wireless sensor networks"}, "304c025473e876625dc828a78f40aae3f93ac761": {"paper_id": "304c025473e876625dc828a78f40aae3f93ac761", "abstract": "We present TinyOS, a flexible, application-specific operating system for sensor networks. Sensor networks consist of (potentially) thousands of tiny, low-power nodes, each of which execute concurrent, reactive programs that must operate with severe memory and power constraints. The sensor network challenges of limited resources, event-centric concurrent applications, and low-power operation drive the design of TinyOS. Our solution combines flexible, fine-grain components with an execution model that supports complex yet safe concurrent operations. TinyOS meets these challenges well and has become the platform of choice for sensor network research; it is in use by over a hundred groups worldwide, and supports a broad range of applications and research topics. We provide a qualitative and quantitative evaluation of the system, showing that it supports complex, concurrent programs with very low memory requirements (many applications fit within 16KB of memory, and the core OS is 400 bytes) and efficient, low-power operation. We present our experiences with TinyOS as a platform for sensor network innovation and applications.", "title": "TinyOS: An Operating System for Sensor Networks"}, "ef67c96eed314c0b0c79f54d436bf05c7bab4cc3": {"paper_id": "ef67c96eed314c0b0c79f54d436bf05c7bab4cc3", "abstract": "Penile dysmorphic disorder (PDD) is shorthand for men diagnosed with body dysmorphic disorder, in whom the size or shape of the penis is their main, if not their exclusive, preoccupation causing significant shame or handicap. There are no specific measures for identifying men with PDD compared to men who are anxious about the size of their penis but do not have PDD. Such a measure might be helpful for treatment planning, reducing unrealistic expectations, and measuring outcome after any psychological or physical intervention. Our aim was, therefore, to validate a specific measure, termed the Cosmetic Procedure Screening Scale for PDD (COPS-P). Eighty-one male participants were divided into three groups: a PDD group (n = 21), a small penis anxiety group (n = 37), and a control group (n = 23). All participants completed the COPS-P as well as standardized measures of depression, anxiety, social phobia, body image, quality of life, and erectile function. Penis size was also measured. The final COPS-P was based on nine items. The scale had good internal reliability and significant convergent validity with measures of related constructs. It discriminated between the PDD group, the small penis anxiety group, and the control group. This is the first study to develop a scale able to discriminate between those with PDD and men anxious about their size who did not have PDD. Clinicians and researchers may use the scale as part of an assessment for men presenting with anxiety about penis size and as an audit or outcome measure after any intervention for this population.", "title": "Penile Dysmorphic Disorder: Development of a Screening Scale."}, "229de26bca8acf9937d5f2a8560d76d158219e2a": {"paper_id": "229de26bca8acf9937d5f2a8560d76d158219e2a", "abstract": "Equilibria in mechanics or in transportation models are not always expressed through a system of equations, but sometimes they are characterized by means of complementarity conditions involving a convex cone. This work deals with the analysis of cone-constrained eigenvalue problems. We discuss some theoretical issues like, for instance, the estimation of the maximal number of eigenvalues in a cone-constrained problem. Special attention is paid to the Paretian case. As a short addition to the theoretical part, we introduce and study two algorithms for solving numerically such type of eigenvalue problems.", "title": "Cone-constrained eigenvalue problems: theory and algorithms"}, "be78f08193641b0202754f318fb3862e8a47bb01": {"paper_id": "be78f08193641b0202754f318fb3862e8a47bb01", "abstract": "Each year, millions of dollars are invested on road maintenance and reparation all over the world. In order to minimize costs, one of the main aspects is the early detection of those flaws. Different types of cracks require different types of repairs; therefore, not only a crack detection is required but a crack type classification. Also, the earlier the crack is detected, the cheaper the reparation is. Once the images are captured, several processes are applied in order to extract the main characteristics for emphasizing the cracks (logarithmic transformation, bilateral filter, Canny algorithm, and a morphological filter). After image preprocessing, a decision tree heuristic algorithm is applied to finally classify the image. This work obtained an average of 88% of success detecting cracks and an 80% of success detecting the type of the crack. It could be implemented in a vehicle traveling as fast as 130 kmh or 81 mph.", "title": "Efficient pavement crack detection and classification"}, "2d8e268ae3d99e519c01a5338dd8974606e2c142": {"paper_id": "2d8e268ae3d99e519c01a5338dd8974606e2c142", "abstract": "Proceedings of the 1998 IEEE International Conference on Computer Vision, Bombay, India Bilateral filtering smooths images while preserving edges, by means of a nonlinear combination of nearby image values. The method is noniterative, local, and simple. It combines gray levels or colors based on both their geometric closeness and their photometric similarity, and prefers near values to distant values in both domain and range. In contrast with filters that operate on the three bands of a color image separately, a bilateral filter can enforce the perceptual metric underlying the CIE-Lab color space, and smooth colors and preserve edges in a way that is tuned to human perception. Also, in contrast with standard filtering, bilateral filtering produces no phantom colors along edges in color images, and reduces phantom colors where they appear in the original image.", "title": "Bilateral Filtering for Gray and Color Images"}, "182fdcb0d6cd872a5a35c58cc2230486d2750201": {"paper_id": "182fdcb0d6cd872a5a35c58cc2230486d2750201", "abstract": "Abstracf-The scale-space technique introduced by Witkin involves generating coarser resolution images by convolving the original image with a Gaussian kernel. This approach has a major drawback: it is difficult to obtain accurately the locations of the \u201csemantically meaningful\u201d edges at coarse scales. In this paper we suggest a new definition of scale-space, and introduce a class of algorithms that realize it using a diffusion process. The diffusion coefficient is chosen to vary spatially in such a way as to encourage intraregion smoothing in preference to interregion smoothing. It is shown that the \u201cno new maxima should be generated at coarse scales\u201d property of conventional scale space is preserved. As the region boundaries in our approach remain sharp, we obtain a high quality edge detector which successfully exploits global information. Experimental results are shown on a number of images. The algorithm involves elementary, local operations replicated over the image making parallel hardware implementations feasible.", "title": "Scale-Space and Edge Detection Using Anisotropic Diffusion"}, "82a46a91665af46f6251005e11c3d64589e5edd1": {"paper_id": "82a46a91665af46f6251005e11c3d64589e5edd1", "abstract": "Computational techniques involving contrast enhancement and noise filtering on two-dimensional image arrays are developed based on their local mean and variance. These algorithms are nonrecursive and do not require the use of any kind of transform. They share the same characteristics in that each pixel is processed independently. Consequently, this approach has an obvious advantage when used in real-time digital image processing applications and where a parallel processor can be used. For both the additive and multiplicative cases, the a priori mean and variance of each pixel is derived from its local mean and variance. Then, the minimum mean-square error estimator in its simplest form is applied to obtain the noise filtering algorithms. For multiplicative noise a statistical optimal linear approximation is made. Experimental results show that such an assumption yields a very effective filtering algorithm. Examples on images containing 256 \u00d7 256 pixels are given. Results show that in most cases the techniques developed in this paper are readily adaptable to real-time image processing.", "title": "Digital Image Enhancement and Noise Filtering by Use of Local Statistics"}, "87a15e68bd40c76cff5c3c21fd5414475b616977": {"paper_id": "87a15e68bd40c76cff5c3c21fd5414475b616977", "abstract": "While face-to-face interaction is fundamental in agile software development, distributed environments must rely extensively on mediated interactions. Practicing agile principles in distributed environments therefore poses particular control challenges related to balancing fixed vs. evolving quality requirements and people vs. process-based collaboration. To investigate these challenges, we conducted an in-depth case study of a successful agile distributed software project with participants from a Russian firm and a Danish firm. Applying Kirsch\u2019s elements of control framework, we offer an analysis of how control was enacted through the project context and in the participants\u2019 mediated communication. The analysis reveals that formal measurement and evaluation control were persistently enacted through mediated communication. These formal control practices were, however, predominantly carried out in conjunction with informal roles and relationships such as clan-like control inherent in agile development. Overall, the study demonstrates that, if appropriately applied, communication technologies can significantly support distributed, agile practices by allowing concurrent enactment of both formal and informal controls. The paper discusses these findings as they relate to previous research and concludes with their implications for future research.", "title": "Agile distributed software development: enacting control through media and context"}, "125d55bc7b78c6028912be78a157ae8466e11017": {"paper_id": "125d55bc7b78c6028912be78a157ae8466e11017", "abstract": "I this paper, we draw on control theory to understand the conditions under which the use of agile practices is most effective in improving software project quality. Although agile development methodologies offer the potential of improving software development outcomes, limited research has examined how project managers can structure the software development environment to maximize the benefits of agile methodology use during a project. As a result, project managers have little guidance on how to manage teams who are using agile methodologies. Arguing that the most effective control modes are those that provide teams with autonomy in determining the methods for achieving project objectives, we propose hypotheses related to the interaction between control modes, agile methodology use, and requirements change. We test the model in a field study of 862 software developers in 110 teams. The model explains substantial variance in four objective measures of project quality\u2014bug severity, component complexity, coordinative complexity, and dynamic complexity. Results largely support our hypotheses, highlighting the interplay between project control, agile methodology use, and requirements change. The findings contribute to extant literature by integrating control theory into the growing literature on agile methodology use and by identifying specific contingencies affecting the efficacy of different control modes. We discuss the theoretical and practical implications of our results.", "title": "A Control Theory Perspective on Agile Methodology Use and Changing User Requirements"}, "6bfdeb68af6a80ee12cb2ab47dddb855743e2bd7": {"paper_id": "6bfdeb68af6a80ee12cb2ab47dddb855743e2bd7", "abstract": "The problems of designing large software systems were studied through interviewing personnel from 17 large projects. A layered behavioral model is used to analyze how three of these problems\u2014the thin spread of application domain knowledge, fluctuating and conflicting requirements, and communication bottlenecks and breakdowns\u2014affected software productivity and quality through their impact on cognitive, social, and organizational processes.", "title": "A Field Study of the Software Design Process for Large Systems"}, "6275d239b104a258a85e37decf74b28fe9b1e300": {"paper_id": "6275d239b104a258a85e37decf74b28fe9b1e300", "abstract": "Researchers from a wide range of management areas agree that conflicts are an important part of organizational life and that their study is important. Yet, interpersonal conflict is a neglected topic in information system development (ISD). Based on definitional properties of interpersonal conflict identified in the management and organizational behavior literatures, this paper presents a model of how individuals participating in ISD projects perceive conflict and its influence on ISD outcomes. Questionnaire data was obtained from 265 IS staff (main sample) and 272 users (confirmatory sample) working on 162 ISD projects. Results indicated that the construct of interpersonal conflict was reflected by three key dimensions: disagreement, interference, and negative emotion. While conflict management was found to have positive effects on ISD outcomes, it did not substantially mitigate the negative effects of interpersonal conflict on these outcomes. In other words, the impact of interpersonal conflict was perceived to be negative, regardless of how it was managed or resolved.", "title": "Interpersonal Conflict and Its Management in Information System Development"}, "42a8dae9dfa3addd231bb0eb044edfa3e261be32": {"paper_id": "42a8dae9dfa3addd231bb0eb044edfa3e261be32", "abstract": "As the Internet of Things edges closer to mainstream adoption, with it comes an exponential rise in data transmission across the current Internet architecture. Capturing and analyzing this data will lead to a wealth of opportunities. However, this ungoverned, unstructured data has the potential to exhaust the resources of an already strained infrastructure. Analyzing data as close to the sources as possible would greatly enhance the success of the IoT. This paper proposes a distributed data processing architecture for edge devices in an IoT environment. Our approach focuses on a vehicular trucking use case. The goal is to recreate the traditionally centralized Storm processes on the edge devices using a combination of Apache MiNiFi and the user\u2019s custombuilt programs. Our approach is shown to preserve computational accuracy while reducing by upwards of 90 percent the volume of data transferred from edge devices for centralized processing.", "title": "An Architecture for Intelligent Data Processing on IoT Edge Devices"}, "3fab16e80d7eeedc0b266e5ae7c5261e84cf97c4": {"paper_id": "3fab16e80d7eeedc0b266e5ae7c5261e84cf97c4", "abstract": "With the abundance of raw data generated from various sources, Big Data has become a preeminent approach in acquiring, processing, and analyzing large amounts of heterogeneous data to derive valuable evidences. The size, speed, and formats in which data is generated and processed affect the overall quality of information. Therefore, Quality of Big Data (QBD) has become an important factor to ensure that the quality of data is maintained at all Big data processing phases. This paper addresses the QBD at the pre-processing phase, which includes sub-processes like cleansing, integration, filtering, and normalization. We propose a QBD model incorporating processes to support Data quality profile selection and adaptation. In addition, it tracks and registers on a data provenance repository the effect of every data transformation happened in the pre-processing phase. We evaluate the data quality selection module using large EEG dataset. The obtained results illustrate the importance of addressing QBD at an early phase of Big Data processing lifecycle since it significantly save on costs and perform accurate data analysis.", "title": "Big Data Pre-processing: A Quality Framework"}, "0c0f9ab5b5dd633d861c45103a4a77e26ce918ae": {"paper_id": "0c0f9ab5b5dd633d861c45103a4a77e26ce918ae", "abstract": "Big Data has become the new ubiquitous term used to describe massive collection of datasets that are difficult to process using traditional database and software techniques. Most of this data is inaccessible to users, as we need technology and tools to find, transform, analyze, and visualize data in order to make it consumable for decision-making. One aspect of Big Data research is dealing with the Variety of data that includes various formats such as structured, numeric, unstructured text data, email, video, audio, stock ticker, etc. Managing, merging, and governing a variety of data is the focus of this paper. This paper proposes a semantic Extract-Transform-Load (ETL) framework that uses semantic technologies to integrate and publish data from multiple sources as open linked data. This includes - creation of a semantic data model to provide a basis for integration and understanding of knowledge from multiple sources, creation of a distributed Web of data using Resource Description Framework (RDF) as the graph data model, extraction of useful knowledge and information from the combined data using SPARQL as the semantic query language.", "title": "Towards a Semantic Extract-Transform-Load (ETL) Framework for Big Data Integration"}, "d5459b42d0c304fc5dacc1d2f266f89415418140": {"paper_id": "d5459b42d0c304fc5dacc1d2f266f89415418140", "abstract": "Cloud computing is a powerful technology to perform massive-scale and complex computing. It eliminates the need to maintain expensive computing hardware, dedicated space, and software. Massive growth in the scale of data or big data generated through cloud computing has been observed. Addressing big data is a challenging and timedata processing and analysis. The rise of big data in cloud computing is reviewed in this study. The definition, characteristics, and classification of big data along with some discussions on cloud computing are introduced. The relationship between big data and cloud computing, big data storage systems, and Hadoop technology are also discussed. Furthermore, research challenges are investigated, with focus on scalability, availability, data integrity, data transformation, data quality, data heterogeneity, privacy, legal and regulatory issues, and governance. Lastly, open research issues that require substantial research efforts are summarized. & 2014 Elsevier Ltd. All rights reserved.", "title": "The rise of \"big data\" on cloud computing: Review and open research issues"}, "7e799292e855ea1d1c5c7502f284702363a327ab": {"paper_id": "7e799292e855ea1d1c5c7502f284702363a327ab", "abstract": "OpenCL and OpenMP are the most commonly used programming models for multi-core processors. They are also fundamentally different in their approach to parallelization. In this paper, we focus on comparing the performance of OpenCL and OpenMP. We select three applications from the Rodinia benchmark suite (which provides equivalent OpenMP and OpenCL implementations), and carry out experiments with different datasets on three multi-core platforms. We see that the incorrect usage of the multi-core CPUs, the inherent OpenCL fine-grained parallelism, and the immature OpenCL compilers are the main reasons that lead to the OpenCL poorer performance. After tuning the OpenCL versions to be more CPU-friendly, we show that OpenCL either outperforms or achieves similar performance in more than 80% of the cases. Therefore, we believe that OpenCL is a good alternative for multi-core CPU programming.", "title": "Performance Gaps between OpenMP and OpenCL for Multi-core CPUs"}, "bb4cf037d8a5adbb3f08a3405d926d022b8c27c5": {"paper_id": "bb4cf037d8a5adbb3f08a3405d926d022b8c27c5", "abstract": "The OpenCL standard offers a common API for program execution on systems composed of different types of computational devices such as multicore CPUs, GPUs, or other accelerators.", "title": "OpenCL: A Parallel Programming Standard for Heterogeneous Computing Systems"}, "6ec8e9fe1b6150aaa0995134c001234a71304730": {"paper_id": "6ec8e9fe1b6150aaa0995134c001234a71304730", "abstract": "CUDA and OpenCL offer two different interfaces for programming GPUs. OpenCL is an open standard that can be used to program CPUs, GPU s, and other devices from different vendors, while CUDA is specific to NVIDIA GPUs. Alt hough OpenCL promises a portable language for GPU programming, its generali ty may entail a performance penalty. In this paper, we compare the performance of CUDA and OpenCL using complex, near-identical kernels. We show that when using NVIDIA compiler tools, converting a CUDA kernel to an OpenCL kernel involv es minimal modifications. Making such a kernel compile with ATI\u2019s build tools involves more modifications. Our performance tests measure and compare data transfer times to and from the GPU, kernel execution times, and end-to-end application executi on times for both CUDA and OpenCL.", "title": "A Performance Comparison of CUDA and OpenCL"}, "423be52973dab29c31a845ea54c9050aba0d650a": {"paper_id": "423be52973dab29c31a845ea54c9050aba0d650a", "abstract": "Link-based dissimilarity measures, such as shortest path or Euclidean commute time distance, base their distance on paths between nodes of a weighted graph. These measures are known to be better suited to data manifold with nonconvex-shaped clusters, compared to Euclidean distance, so that k-nearest neighbor (NN) search is improved in such metric spaces. In this paper we present a new link-based dissimilarity measure based on minimax paths between nodes. Two main benefits of minimax path-based dissimilarity measure are: (1) only a subset of paths is considered to make it scalable, while Euclidean commute time distance considers all possible paths; (2) it better captures nonconvex-shaped cluster structure, compared to shortest path distance. We define the total cost assigned to a path between nodes as Lp norm of intermediate costs of edges involving the path, showing that minimax path emerges from our Lp norm over paths framework. We also define minimax distance as the intermediate cost of the longest edge on the minimax path, then present a greedy algorithm to compute k smallest minimax distances between a query and N data points in O(logN + k log k) time. Numerical experiments demonstrate that our minimax kNN algorithm reduce the search time by several orders of magnitude, compared to existing methods, while the quality of k-NN search is significantly improved over Euclidean distance. Introduction Given a set of N data points X = {x1, . . . ,xN}, k-nearest neighbor (k-NN) search in metric spaces involves finding k closest points in the dataset X to a query xq . Dissimilarity measure defines distance duv between two data points (or nodes of a weighted graph) xu and xv in the corresponding metric space, and the performance of k-NN search depends on distance metric. Euclidean distance \u2016xu \u2212 xv\u20162 is the most popular measure for k-NN search but it does not work well when data points X lie on a curved manifold with nonconvex-shaped clusters (see Fig. 1(a)). Metric learning (Xing et al. 2003; Goldberger et al. 2005; Weinberger and Saul 2009) optimizes parameters involving the Mahalanobis distance using Copyright c \u00a9 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. labeled dataset, such that points in the same cluster become close together and points in the different cluster become far apart. Most of metric learning methods are limited to linear embedding, so that the nonconvex-shaped cluster structure is not well captured (see Fig. 1(b)). Link-based (dis)similarity measures (Fouss et al. 2007; Yen, Mantrach, and Shimbo 2008; Yen et al. 2009; Mantrach et al. 2010; Chebotarev 2011) rely on paths between nodes of a weighted graph, on which nodes are associated with data points and intermediate costs (for instance Euclidean distance) are assigned to edge weights. Distance between nodes depends on the total cost that is computed by aggregating edge weights on a path connecting those nodes of interest. Total cost associated with a path is often assumed to be additive (Yen, Mantrach, and Shimbo 2008), so the aggregation reduces to summation. Dissimilarity between two nodes is calculated by integrating the total costs assigned to all possible paths between them. Such integration is often determined by computing the pseudo-inverse of the graph Laplacian, leading to Euclidean commute time distance (ECTD), regularized Laplacian kernel, and Markov diffusion kernel (see (Fouss et al. 2007; Yen, Mantrach, and Shimbo 2008; Fouss et al. 2012) and references therein), which are known to better capture the nonconvex-shaped cluster structure (see Fig. 1(c)). However, all possible paths between nodes need to be considered to compute the distances and the inversion of N \u00d7 N matrix requires O(N) time and O(N) space, so it does not scale well to the problems of interest. Shortest path distance (Dijkstra 1959) is also a popular link-based dissimilarity measure (Tenenbaum, de Silva, and Langford 2000), where only shortest paths are considered to compute the distances between two nodes. Computational cost is reduced, but the cluster structure is not well captured when shortest path distance is used for k-NN search (see Fig. 1(d)). The distance between two nodes is computed along the shortest path only, Randomized shortest path (RSP) dissimilarity measures were proposed as a family of distance measures depending on a single parameter, which has interesting property of reducing, on one end, to the standard shortest path distance when the parameter is large, on the other hand, to the commute time distance when the parameter is near zero (Yen, Mantrach, and Shimbo 2008). In this paper we present a new link-based k-NN search method with minimax paths (Pollack 1960; Gower and Ross (a) Euclidean (b) LMNN (c) ECTD (d) Shortest path (e) Minimax distance Figure 1: Real-world datasets usually contain nonconvex-shaped clusters. For example, two curved clusters (dots) and a query point (star) are given. Heat map shows the closeness to the query at each point, in terms of the distance measure used (best viewed in color; more reddish means closer to the query). (a) Euclidean distance cannot reflect the underlying cluster structure at all. (b) Metric learning (Weinberger and Saul 2009) cannot capture the nonconvex shapes. (c) Link-based distance (Fouss et al. 2007) captures the underlying cluster correctly, but requires a large amount of computation. (d) Shortest path distance is efficient to compute, but it is not reliable near the boundary between different clusters. (e) Our method is as efficient as computing the shortest path distance, while capturing the entire cluster structure correctly. 1969), which is well-known to capture the underlying cluster structure of data (e.g., Fig. 1(e)) (Kim and Choi 2007; Luo et al. 2008; Zadeh and Ben-David 2009). We develop a fast k-NN search algorithm that computes the minimax distance efficiently through the minimax paths between data points. Our method is as efficient as the shortest-path distance computation. More specifically, the overall time for kNN search with N data points is onlyO(logN+k log k). In image retrieval experiments on some public image datasets, our method was several orders of magnitude faster, while achieving the comparable search quality (in terms of precision). The main contributions of this paper are summarized: 1. We develop a novel framework for link-based (dis)similarity measures where we define the total cost (corresponding to dissimilarity) assigned to a path between nodes as Lp norm of intermediate costs of edges involving the path, showing that minimax path emerges from our Lp norm over paths framework. This framework has two main benefits: (1) it can reduce the number of paths considered for the link-based (dis)similarity computation, which improves the scalability greatly; (2) even using a small number of paths, it can capture any nonconvex-shaped cluster structure successfully. 2. We define minimax distance as the intermediate cost of the longest edge on the minimax path, then present a greedy algorithm to compute k smallest minimax distances between a query and N data points in O(logN + k log k) time, whereas the state of the arts, minimax message passing (Kim and Choi 2007) requires O(N) time for k-NN search. Aggregation over Paths: Lp Norm Link-based (dis)similarity is defined on a weighted graph, denoted by G = (X , E): \u2022 The set of nodes, X = {x1, . . . ,xN}, are associated with N data points. E = {(i, j) | i 6= j \u2208 {1, . . . , N}} is a set of edges between the nodes, excluding self-loop edges. \u2022 The graph is usually assumed to be sparse such that the number of edges, |E|, is limited to O(N). A well-known example is the K-NN graph, where an edge (i, j) exists only if xi is one of the K nearest neighbors of xj or xj is one of the K nearest neighbors of xi in the Euclidean space. The value of K is set to a small constant (usually 5-20) for ensuring sparsity. Then a link-based (dis)similarity depends on the total cost associated with paths on the graph. In this section, we describe our Lp norm approach to the cost aggregation over paths to compute the total cost along paths. Let a = (a0, a1, . . . , am) be a path with m hops, connecting the nodes xa0 ,xa1 , . . . ,xam \u2208 X through the consecutive edges {(a0, a1), (a1, a2), . . . , (am\u22121, am)} \u2286 E . We denote a set of paths with m hops between an initial node xu and a destination node xv by Auv = { a \u2223\u2223 a0 = u, am = v, (a`, a`+1) \u2208 E , a` 6= v, \u2200` = 0, . . . ,m\u2212 1 } , (1) and denote a set of all possible paths between xu and xv by Auv = \u222am=1Auv. (2) A weight c(i, j) is assigned to each edge (i, j), representing the intermediate cost of following edge (i, j), usually defined as the Euclidean distance, i.e., c(i, j) = \u2016xi \u2212 xj\u2016. We define the total cost associated with a path a as", "title": "Walking on Minimax Paths for k-NN Search"}, "55206f0b5f57ce17358999145506cd01e570358c": {"paper_id": "55206f0b5f57ce17358999145506cd01e570358c", "abstract": "Stochastic modelling of non-stationary vector timeseries based on HMMs has been very successful for speech applications [5]. Recently it has been applied to a range of image recognition problems [7, 9]. Previously reported work [6] has investigated the use of HMMs to model human faces for identi cation purposes. Faces can be intuitively divided into regions such as the mouth, eyes, nose, etc., and these regions can be associated with the states of an HMM. The identi cation performance of a top-bottom HMM compares favourably with some of the well-known algorithms, for example eigenfaces as detailed in [8]. However, the HMM parameterisation in the work presented so far was arrived at by subjective intuition. This paper presents experimental results which show how identi cation rates vary with HMM parameters, and which indicate the most sensible choice of parameters. The paper is organised as follows: section 2 gives an overview of the HMM-based approach; section 3 details the training and recognition processes; section 4 describes the experimental setup; section 5 presents the identi cation results; section 6 concludes the paper.", "title": "Parameterisation of a stochastic model for human face identification"}, "831a20c9e6c25ad0e9408af5d5942d150e218cd3": {"paper_id": "831a20c9e6c25ad0e9408af5d5942d150e218cd3", "abstract": "There is an ongoing debate over the capabilities of hierarchical neural feedforward architectures for performing real-world invariant object recognition. Although a variety of hierarchical models exists, appropriate supervised and unsupervised learning methods are still an issue of intense research. We propose a feedforward model for recognition that shares components like weight sharing, pooling stages, and competitive nonlinearities with earlier approaches but focuses on new methods for learning optimal feature-detecting cells in intermediate stages of the hierarchical network. We show that principles of sparse coding, which were previously mostly applied to the initial feature detection stages, can also be employed to obtain optimized intermediate complex features. We suggest a new approach to optimize the learning of sparse features under the constraints of a weight-sharing or convolutional architecture that uses pooling operations to achieve gradual invariance in the feature hierarchy. The approach explicitly enforces symmetry constraints like translation invariance on the feature set. This leads to a dimension reduction in the search space of optimal features and allows determining more efficiently the basis representatives, which achieve a sparse decomposition of the input. We analyze the quality of the learned feature representation by investigating the recognition performance of the resulting hierarchical network on object and face databases. We show that a hierarchy with features learned on a single object data set can also be applied to face recognition without parameter changes and is competitive with other recent machine learning recognition approaches. To investigate the effect of the interplay between sparse coding and processing nonlinearities, we also consider alternative feedforward pooling nonlinearities such as presynaptic maximum selection and sum-of-squares integration. The comparison shows that a combination of strong competitive nonlinearities with sparse coding offers the best recognition performance in the difficult scenario of segmentation-free recognition in cluttered surround. We demonstrate that for both learning and recognition, a precise segmentation of the objects is not necessary.", "title": "Learning Optimized Features for Hierarchical Models of Invariant Object Recognition"}, "9be6f01fc4943f5ffa458adc3b09d0db4b4e5a7c": {"paper_id": "9be6f01fc4943f5ffa458adc3b09d0db4b4e5a7c", "abstract": "We propose a family of learning algorithms based on a new form of regularization which allows us to incorporate both labeled and unlabeled data in a general-purpose learner. Transductive graph learning algorithms and standard methods including SVM and Regularized Least Squares can be obtained as special cases of our framework.", "title": "On Manifold Regularization"}, "2ad0ee93d029e790ebb50574f403a09854b65b7e": {"paper_id": "2ad0ee93d029e790ebb50574f403a09854b65b7e", "abstract": "Previous work has demonstrated that the image variation of many objects (human faces in particular) under variable lighting can be effectively modeled by low-dimensional linear spaces, even when there are multiple light sources and shadowing. Basis images spanning this space are usually obtained in one of three ways: a large set of images of the object under different lighting conditions is acquired, and principal component analysis (PCA) is used to estimate a subspace. Alternatively, synthetic images are rendered from a 3D model (perhaps reconstructed from images) under point sources and, again, PCA is used to estimate a subspace. Finally, images rendered from a 3D model under diffuse lighting based on spherical harmonics are directly used as basis images. In this paper, we show how to arrange physical lighting so that the acquired images of each object can be directly used as the basis vectors of a low-dimensional linear space and that this subspace is close to those acquired by the other methods. More specifically, there exist configurations of k point light source directions, with k typically ranging from 5 to 9, such that, by taking k images of an object under these single sources, the resulting subspace is an effective representation for recognition under a wide range of lighting conditions. Since the subspace is generated directly from real images, potentially complex and/or brittle intermediate steps such as 3D reconstruction can be completely avoided; nor is it necessary to acquire large numbers of training images or to physically construct complex diffuse (harmonic) light fields. We validate the use of subspaces constructed in this fashion within the context of face recognition.", "title": "Acquiring linear subspaces for face recognition under variable lighting"}, "a6f1dfcc44277d4cfd8507284d994c9283dc3a2f": {"paper_id": "a6f1dfcc44277d4cfd8507284d994c9283dc3a2f", "abstract": "We have developed a near-real-time computer system that can locate and track a subject's head, and then recognize the person by comparing characteristics of the face to those of known individuals. The computational approach taken in this system is motivated by both physiology and information theory, as well as by the practical requirements of near-real-time performance and accuracy. Our approach treats the face recognition problem as an intrinsically two-dimensional (2-D) recognition problem rather than requiring recovery of three-dimensional geometry, taking advantage of the fact that faces are normally upright and thus may be described by a small set of 2-D characteristic views. The system functions by projecting face images onto a feature space that spans the significant variations among known face images. The significant features are known as \"eigenfaces,\" because they are the eigenvectors (principal components) of the set of faces; they do not necessarily correspond to features such as eyes, ears, and noses. The projection operation characterizes an individual face by a weighted sum of the eigenface features, and so to recognize a particular face it is necessary only to compare these weights to those of known individuals. Some particular advantages of our approach are that it provides for the ability to learn and later recognize new faces in an unsupervised manner, and that it is easy to implement using a neural network architecture.", "title": "Eigenfaces for Recognition"}, "88d527a4fa5c1b48dc72159932c48dc155c2d608": {"paper_id": "88d527a4fa5c1b48dc72159932c48dc155c2d608", "abstract": "We consider the rendering of diffuse objects under distant illumination, as specified by an environment map. Using an analytic expression for the irradiance in terms of spherical harmonic coefficients of the lighting, we show that one needs to compute and use only 9 coefficients, corresponding to the lowest-frequency modes of the illumination, in order to achieve average errors of only 1%. In other words, the irradiance is insensitive to high frequencies in the lighting, and is well approximated using only 9 parameters. In fact, we show that the irradiance can be procedurally represented simply as a quadratic polynomial in the cartesian components of the surface normal, and give explicit formulae. These observations lead to a simple and efficient procedural rendering algorithm amenable to hardware implementation, a prefiltering method up to three orders of magnitude faster than previous techniques, and new representations for lighting design and image-based rendering.", "title": "An efficient representation for irradiance environment maps"}, "4b507a161af8a7dd41e909798b9230f4ac779315": {"paper_id": "4b507a161af8a7dd41e909798b9230f4ac779315", "abstract": "Imaging of objects under variable lighting directions is an important and frequent practice in computer vision and image-based rendering. We introduce an approach that significantly improves the quality of such images. Traditional methods for acquiring images under variable illumination directions use only a single light source per acquired image. In contrast, our approach is based on a multiplexing principle, in which multiple light sources illuminate the object simultaneously from different directions. Thus, the object irradiance is much higher. The acquired images are then computationally demultiplexed. The number of image acquisitions is the same as in the single-source method. The approach is useful for imaging dim object areas. We give the optimal code by which the illumination should be multiplexed to obtain the highest quality output. For n images corresponding to n light sources, the noise is reduced by \u221a n/2 relative to the signal. This noise reduction translates to a faster acquisition time or an increase in density of illumination direction samples. It also enables one to use lighting with high directional resolution using practical setups, as we demonstrate in our experiments.", "title": "A Theory of Multiplexed Illumination"}, "e14d9f962f3f329f911df7481b94ace927483412": {"paper_id": "e14d9f962f3f329f911df7481b94ace927483412", "abstract": "This paper presents a survey as well as an empirical comparison and evaluation of seven kernels on graphs and two related similarity matrices, that we globally refer to as \"kernels on graphs\" for simplicity. They are the exponential diffusion kernel, the Laplacian exponential diffusion kernel, the von Neumann diffusion kernel, the regularized Laplacian kernel, the commute-time (or resistance-distance) kernel, the random-walk-with-restart similarity matrix, and finally, a kernel first introduced in this paper (the regularized commute-time kernel) and two kernels defined in some of our previous work and further investigated in this paper (the Markov diffusion kernel and the relative-entropy diffusion matrix). The kernel-on-graphs approach is simple and intuitive. It is illustrated by applying the nine kernels to a collaborative-recommendation task, viewed as a link prediction problem, and to a semisupervised classification task, both on several databases. The methods compute proximity measures between nodes that help study the structure of the graph. Our comparisons suggest that the regularized commute-time and the Markov diffusion kernels perform best on the investigated tasks, closely followed by the regularized Laplacian kernel.", "title": "An experimental investigation of kernels on graphs for collaborative recommendation and semisupervised classification"}, "0f17c798597372ad819b4c87181b2bfd7ebf38bb": {"paper_id": "0f17c798597372ad819b4c87181b2bfd7ebf38bb", "abstract": null, "title": "Semi-Supervised Learning Literature Survey"}, "1e20f9de45d26950ecd11965989d2b15a5d0d86b": {"paper_id": "1e20f9de45d26950ecd11965989d2b15a5d0d86b", "abstract": "Model-based methods and deep neural networks have both been tremendously successful paradigms in machine learning. In model-based methods, we can easily express our problem domain knowledge in the constraints of the model at the expense of difficulties during inference. Deterministic deep neural networks are constructed in such a way that inference is straightforward, but we sacrifice the ability to easily incorporate problem domain knowledge. The goal of this paper is to provide a general strategy to obtain the advantages of both approaches while avoiding many of their disadvantages. The general idea can be summarized as follows: given a model-based approach that requires an iterative inference method, we unfold the iterations into a layer-wise structure analogous to a neural network. We then de-couple the model parameters across layers to obtain novel neural-network-like architectures that can easily be trained discriminatively using gradient-based methods. The resulting formula combines the expressive power of a conventional deep network with the internal structure of the model-based approach, while allowing inference to be performed in a fixed number of layers that can be optimized for best performance. We show how this framework can be applied to the non-negative matrix factorization to obtain a novel non-negative deep neural network architecture, that can be trained with a multiplicative back-propagation-style update algorithm. We present experiments in the domain of speech enhancement, where we show that the resulting model is able to outperform conventional neural network while only requiring a fraction of the number of parameters. We believe this is due to the ability afforded by our framework to incorporate problem level assumptions into the architecture of the deep network. arXiv.org This work may not be copied or reproduced in whole or in part for any commercial purpose. Permission to copy in whole or in part without payment of fee is granted for nonprofit educational and research purposes provided that all such whole or partial copies include the following: a notice that such copying is by permission of Mitsubishi Electric Research Laboratories, Inc.; an acknowledgment of the authors and individual contributions to the work; and all applicable portions of the copyright notice. Copying, reproduction, or republishing for any other purpose shall require a license with payment of fee to Mitsubishi Electric Research Laboratories, Inc. All rights reserved. Copyright c \u00a9Mitsubishi Electric Research Laboratories, Inc., 2014 201 Broadway, Cambridge, Massachusetts 02139", "title": "Deep Unfolding: Model-Based Inspiration of Novel Deep Architectures"}, "34e86b693642b41e60e41f7a53bd9821764bda7c": {"paper_id": "34e86b693642b41e60e41f7a53bd9821764bda7c", "abstract": "This letter presents theoretical, algorithmic, and experimental results about nonnegative matrix factorization (NMF) with the Itakura-Saito (IS) divergence. We describe how IS-NMF is underlaid by a well-defined statistical model of superimposed gaussian components and is equivalent to maximum likelihood estimation of variance parameters. This setting can accommodate regularization constraints on the factors through Bayesian priors. In particular, inverse-gamma and gamma Markov chain priors are considered in this work. Estimation can be carried out using a space-alternating generalized expectation-maximization (SAGE) algorithm; this leads to a novel type of NMF algorithm, whose convergence to a stationary point of the IS cost function is guaranteed. We also discuss the links between the IS divergence and other cost functions used in NMF, in particular, the Euclidean distance and the generalized Kullback-Leibler (KL) divergence. As such, we describe how IS-NMF can also be performed using a gradient multiplicative algorithm (a standard algorithm structure in NMF) whose convergence is observed in practice, though not proven. Finally, we report a furnished experimental comparative study of Euclidean-NMF, KL-NMF, and IS-NMF algorithms applied to the power spectrogram of a short piano sequence recorded in real conditions, with various initializations and model orders. Then we show how IS-NMF can successfully be employed for denoising and upmix (mono to stereo conversion) of an original piece of early jazz music. These experiments indicate that IS-NMF correctly captures the semantics of audio and is better suited to the representation of music signals than NMF with the usual Euclidean and KL costs.", "title": "Nonnegative Matrix Factorization with the Itakura-Saito Divergence: With Application to Music Analysis"}, "426521a4b98bcaf4bec027d3f22a119b09c29f92": {"paper_id": "426521a4b98bcaf4bec027d3f22a119b09c29f92", "abstract": "The expectation-maximization (EM) method can facilitate maximizing likelihood functions that arise in statistical estimation problems. In the classical EM paradigm, one iteratively maximizes the conditional log-likelihood of a single unobservable complete data space, rather than maximizing the intractable likelihood function for the measured or incomplete data. EM algorithms update all parameters simultaneously, which has two drawbacks: 1) slow convergence, and 2) difficult maximization steps due to coupling when smoothness penalties are used. This paper describes the space-alternating generalized EM (SAGE) method, which updates the parameters sequentially by alternating between several small hidden-data spaces defined by the algorithm designer. We prove that the sequence of estimates monotonically increases the penalized-likelihood objective, we derive asymptotic convergence rates, and we provide sufficient conditions for monotone convergence in norm. Two signal processing applications illustrate the method: estimation of superimposed signals in Gaussian noise, and image reconstruction from Poisson measurements. In both applications, our SAGE algorithms easily accommodate smoothness penalties and converge faster than the EM algorithms.", "title": "Space-alternating generalized expectation-maximization algorithm"}, "a7bf67353c538687b7833f756fc19405713b41f9": {"paper_id": "a7bf67353c538687b7833f756fc19405713b41f9", "abstract": "The categorical compositional approach to meaning has been successfully applied in natural language processing, outperforming other models in mainstream empirical language processing tasks. We show how this approach can be generalized to conceptual space models of cognition. In order to do this, first we introduce the category of convex relations as a new setting for categorical compositional semantics, emphasizing the convex structure important to conceptual space applications. We then show how to construct conceptual spaces for various types such as nouns, adjectives and verbs. Finally we show by means of examples how concepts can be systematically combined to establish the meanings of composite phrases from the meanings of their constituent parts. This provides the mathematical underpinnings of a new compositional approach to cognition.", "title": "Interacting Conceptual Spaces I : Grammatical Composition of Concepts"}, "f3999b9749d5f0b71f638bcc998dc14f2e0a6cbe": {"paper_id": "f3999b9749d5f0b71f638bcc998dc14f2e0a6cbe", "abstract": "Commonsense reasoning patterns such as interpolation and a fortiori inference have proven useful for dealing with gaps in structured knowledge bases. An important di culty in applying these reasoning patterns in practice is that they rely on fine-grained knowledge of how di\u21b5erent concepts and entities are semantically related. In this paper, we show how the required semantic relations can be learned from a large collection of text documents. To this end, we first induce a conceptual space from the text documents, using multi-dimensional scaling. We then rely on the key insight that the required semantic relations correspond to qualitative spatial relations in this conceptual space. Among others, in an entirely unsupervised way, we identify salient directions in the conceptual space which correspond to interpretable relative properties such as \u2018more fruity than\u2019 (in a space of wines), resulting in a symbolic and interpretable representation of the conceptual space. To evaluate the quality of our semantic relations, we show how they can be exploited by a number of commonsense reasoning based classifiers. We experimentally show that these classifiers can outperform standard approaches, while being able to provide intuitive explanations of classification decisions. A number of crowdsourcing experiments provide further insights into the nature of the extracted semantic relations.", "title": "Inducing semantic relations from conceptual spaces: A data-driven approach to plausible reasoning"}, "36adffb9c5f15b6459e4a4ec796737a09b673697": {"paper_id": "36adffb9c5f15b6459e4a4ec796737a09b673697", "abstract": "This work investigates cluster labeling enhancement by utilizing Wikipedia, the free on-line encyclopedia. We describe a general framework for cluster labeling that extracts candidate labels from Wikipedia in addition to important terms that are extracted directly from the text. The \"labeling quality\" of each candidate is then evaluated by several independent judges and the top evaluated candidates are recommended for labeling.\n Our experimental results reveal that the Wikipedia labels agree with manual labels associated by humans to a cluster, much more than with significant terms that are extracted directly from the text. We show that in most cases even when human's associated label appears in the text, pure statistical methods have difficulty in identifying them as good descriptors. Furthermore, our experiments show that for more than 85% of the clusters in our test collection, the manual label (or an inflection, or a synonym of it) appears in the top five labels recommended by our system.", "title": "Enhancing cluster labeling using wikipedia"}, "137379d9422f9b9596db61a38d80b17fcc34cca6": {"paper_id": "137379d9422f9b9596db61a38d80b17fcc34cca6", "abstract": "Many machine reading approaches, from shallow information extraction to deep semantic parsing, map natural language to symbolic representations of meaning. Representations such as first-order logic capture the richness of natural language and support complex reasoning, but often fail in practice due to their reliance on logical background knowledge and the difficulty of scaling up inference. In contrast, low-dimensional embeddings (i.e. distributional representations) are efficient and enable generalization, but it is unclear how reasoning with embeddings could support the full power of symbolic representations such as first-order logic. In this proof-ofconcept paper we address this by learning embeddings that simulate the behavior of first-order logic.", "title": "Low-Dimensional Embeddings of Logic"}, "19747a1b06e6577603c9f34972e93a8f722913a6": {"paper_id": "19747a1b06e6577603c9f34972e93a8f722913a6", "abstract": "The overall goal is to show that conceptual spaces are more promising than other ways of modelling the semantics of natural language. In particular, I will show how they can be used to model actions and events. I will also outline how conceptual spaces provide a cognitive grounding for word classes, including nouns, adjectives, prepositions and verbs.", "title": "Semantics Based on Conceptual Spaces"}, "1750a3716a03aaacdfbb0e25214beaa5e1e2b6ee": {"paper_id": "1750a3716a03aaacdfbb0e25214beaa5e1e2b6ee", "abstract": "1 Why develop an ontology? In recent years the development of ontologies\u2014explicit formal specifications of the terms in the domain and relations among them (Gruber 1993)\u2014has been moving from the realm of ArtificialIntelligence laboratories to the desktops of domain experts. Ontologies have become common on the World-Wide Web. The ontologies on the Web range from large taxonomies categorizing Web sites (such as on Yahoo!) to categorizations of products for sale and their features (such as on Amazon.com). The WWW Consortium (W3C) is developing the Resource Description Framework (Brickley and Guha 1999), a language for encoding knowledge on Web pages to make it understandable to electronic agents searching for information. The Defense Advanced Research Projects Agency (DARPA), in conjunction with the W3C, is developing DARPA Agent Markup Language (DAML) by extending RDF with more expressive constructs aimed at facilitating agent interaction on the Web (Hendler and McGuinness 2000). Many disciplines now develop standardized ontologies that domain experts can use to share and annotate information in their fields. Medicine, for example, has produced large, standardized, structured vocabularies such as SNOMED (Price and Spackman 2000) and the semantic network of the Unified Medical Language System (Humphreys and Lindberg 1993). Broad general-purpose ontologies are emerging as well. For example, the United Nations Development Program and Dun & Bradstreet combined their efforts to develop the UNSPSC ontology which provides terminology for products and services (www.unspsc.org). An ontology defines a common vocabulary for researchers who need to share information in a domain. It includes machine-interpretable definitions of basic concepts in the domain and relations among them. Why would someone want to develop an ontology? Some of the reasons are:", "title": "Ontology Development 101 : A Guide to Creating Your First Ontology"}, "fae3e0c9232676c345530abc1764a8dec8cb8d4e": {"paper_id": "fae3e0c9232676c345530abc1764a8dec8cb8d4e", "abstract": "1. Associate Professor of Oncology of the State University of Cear\u00e1; Clinical Director of the Cancer Hospital of Cear\u00e1 2. Resident in Urology of Urology Department of the Federal University of Cear\u00e1 3. Associate Professor of Urology of the State University of Cear\u00e1; Assistant of the Division of Uro-Oncology, Cancer Hospital of Cear\u00e1 4. Professor of Urology Department of the Federal University of Cear\u00e1; Chief of Division of Uro-Oncology, Cancer Hospital of Cear\u00e1", "title": "Partial glansectomy for invasive glans penis cancer and immediate reconstruction with preputial flap: preliminary results"}, "51b299700380c374a184363339f845a00ae471ab": {"paper_id": "51b299700380c374a184363339f845a00ae471ab", "abstract": "While most existing video summarization approaches aim to identify important frames of a video from either a global or local perspective, we propose a top-down approach consisting of scene identification and scene summarization. For scene identification, we represent each frame with global features and utilize a scalable clustering method. We then formulate scene summarization as choosing those frames that best cover a set of local descriptors with minimal redundancy. In addition, we develop a visual word-based approach to make our approach more computationally scalable. Experimental results on two benchmark datasets demonstrate that our proposed approach clearly outperforms the state-of-the-art.", "title": "A Top-Down Approach for Video Summarization"}, "e0d2cddc34edd3db9c9a854c937575095acda756": {"paper_id": "e0d2cddc34edd3db9c9a854c937575095acda756", "abstract": "In this paper, we propose a novel technique for video summarization based on the Singular Value Decomposition (SVD). For the input video sequence, we create a feature-frame matrix A, and perform the SVD on it. From this SVD, we are able to not only derive the reened feature space to better cluster visually similar frames, but also deene a metric to measure the amount of visual content contained in each frame cluster using its degree of visual changes. Then, in the reened feature space, we nd the most static frame cluster, deene it as the content unit, and use the content value computed from it as the threshold to cluster the rest of the frames. Based on this clustering result, either the optimal set of keyframes, or a summarized motion video with the user speciied time length can be generated to support diierent user requirements for video browsing and content overview. Our approach ensures that the summarized video representation contains little redundancy , and gives equal attention to the same amount of contents.", "title": "Video Summarization Using Singular Value Decomposition"}, "54d3acb7db574c9bedde8b33805eb78d2d54d169": {"paper_id": "54d3acb7db574c9bedde8b33805eb78d2d54d169", "abstract": "X-ray computed tomography is a widely used method for nondestructive visualization of the interior of different samples - also of wooden material. Different to usual applications very high resolution is needed to use such CT images in dendrochronology and to evaluate wood species. In dendrochronology big samples (up to 50 cm) are necessary to scan. The needed resolution is - depending on the species - about 20 mum. In wood identification usually very small samples have to be scanned, but wood anatomical characters of less than 1 mum in width have to be visualized. This paper deals with four examples of X-ray CT scanned images to be used for dendrochronology and wood identification.", "title": "The need of high resolution \u03bc-X-ray CT in dendrochronology and in wood identification"}, "68c4363024a846f89a1e170a9ddd0ff8bddbd6c2": {"paper_id": "68c4363024a846f89a1e170a9ddd0ff8bddbd6c2", "abstract": "The spring loaded inverted pendulum (SLIP) model has been extensively shown to be fundamental for legged locomotion. However, the way this low-order template model dynamics is anchored in high-dimensional articulated multibody systems describing compliantly actuated robots (and animals) is not obvious and has not been shown so far. In this letter, an articulated leg mechanism and a corresponding quadrupedal robot design are introduced, for which the natural oscillation dynamics is structurally equivalent to the SLIP. On the basis of this property, computationally simple and robust control methods are proposed, which implement the gaits of pronking, trotting, and dynamic walking in the real robotic system. Experiments with a compliantly actuated quadruped featuring only low-performance electrical drives validate the effectiveness of the proposed approach.", "title": "Dynamic Locomotion Gaits of a Compliantly Actuated Quadruped With SLIP-Like Articulated Legs Embodied in the Mechanical Design"}, "d045b5ecf769f4a8823a4ed89b67850d971240de": {"paper_id": "d045b5ecf769f4a8823a4ed89b67850d971240de", "abstract": "This paper presents the design principles for highly efficient legged robots, the implementation of the principles in the design of the MIT Cheetah, and the analysis of the high-speed trotting experimental results. The design principles were derived by analyzing three major energy-loss mechanisms in locomotion: heat losses from the actuators, friction losses in transmission, and the interaction losses caused by the interface between the system and the environment. Four design principles that minimize these losses are discussed: employment of high torque-density motors, energy regenerative electronic system, low loss transmission, and a low leg inertia. These principles were implemented in the design of the MIT Cheetah; the major design features are large gap diameter motors, regenerative electric motor drivers, single-stage low gear transmission, dual coaxial motors with composite legs, and the differential actuated spine. The experimental results of fast trotting are presented; the 33-kg robot runs at 22 km/h (6 m/s). The total power consumption from the battery pack was 973 W and resulted in a total cost of transport of 0.5, which rivals running animals' at the same scale. 76% of the total energy consumption is attributed to heat loss from the motor, and the remaining 24% is used in mechanical work, which is dissipated as interaction loss as well as friction losses at the joint and transmission.", "title": "Design Principles for Energy-Efficient Legged Locomotion and Implementation on the MIT Cheetah Robot"}, "0f0064eb75d6ac42c47f08062035e674320a2817": {"paper_id": "0f0064eb75d6ac42c47f08062035e674320a2817", "abstract": "The biological hypothesis of spinal engine states that locomotion is mainly achieved by the spine, while the legs may serve as assistance. Inspired by this hypothesis, a compliant, multiple degree-of-freedom, biologically-inspired spine has been embedded into a quadruped robot, named Kitty, which has no actuation on the legs. In this paper, we demonstrate how versatile behaviors (bounding, trotting, and turning) can be generated exclusively by the spine's movements through dynamical interaction between the controller, the body, and the environment, known as embodiment. Moreover, we introduce information theoretic approach to quantitatively study the spine internal dynamics and its effect on the bounding gait based on three spinal morphologies. These three morphologies differ in the position of virtual spinal joint where the spine is easier to get bent. The experimental results reveal that locomotion can be enhanced by using the spine featuring a rear virtual spinal joint, which offers more freedom for the rear legs to move forward. In addition, the information theoretic analysis shows that, according to the morphological differences of the spine, the information structure changes. The relationship between the observed behavior of the robot and the corresponding information structure is discussed in detail.", "title": "Embodiment enables the spinal engine in quadruped robot locomotion"}, "01437af976e0add013fd0924d8259145d5f3ffbf": {"paper_id": "01437af976e0add013fd0924d8259145d5f3ffbf", "abstract": "Facing new tasks, the conventional rigid design of robotic joints has come to its limits. Operating in unknown environments current robots are prone to failure when hitting unforeseen rigid obstacles. Moreover, safety constraints are a major aspect for robots interacting with humans. In order to operate safely, existing robotic systems in this field are slow and have a lack of performance. To circumvent these limitations, a new robot joint with a variable stiffness approach (VS-Joint) is presented. It combines a compact and highly integrated design with high performance actuation. The VS- Joint features a highly dynamic stiffness adjustment along with a mechanically programmable system behavior. This allows an easy adaption to a big variety of tasks. A benefit of the joint is its intrinsic robustness against impacts and hard contacts, which permits faster trajectories and handling. Thus, it provides excellent attributes for the use in shoulder and elbow joints of an anthropomorphic robot arm.", "title": "A new variable stiffness design: Matching requirements of the next robot generation"}, "86cf98c8ad25b3cb185b7524bccf32f77a91b616": {"paper_id": "86cf98c8ad25b3cb185b7524bccf32f77a91b616", "abstract": "Less than half the Earth's landmass is accessible to existing wheeled and tracked vehicles. But people and animals using their legs can go almost anywhere. Our mission at Boston Dynamics is to develop a new breed of rough-terrain robots that capture the mobility, autonomy and speed of living creatures. Such robots will travel in outdoor terrain that is too steep, rutted, rocky, wet, muddy, and snowy for conventional vehicles. They will travel in cities and in our homes, doing chores and providing care, where steps, stairways and household clutter limit the utility of wheeled vehicles. Robots meeting these goals will have terrain sensors, sophisticated computing and power systems, advanced actuators and dynamic controls. We will give a status report on BigDog, an example of such rough-terrain robots.", "title": "BigDog , the Rough-Terrain Quadruped Robot"}, "e9493cfe1dd1fd3f9042da6a2385e889119b23f1": {"paper_id": "e9493cfe1dd1fd3f9042da6a2385e889119b23f1", "abstract": "The basic mechanics of human locomotion are associated with vaulting over stiff legs in walking and rebounding on compliant legs in running. However, while rebounding legs well explain the stance dynamics of running, stiff legs cannot reproduce that of walking. With a simple bipedal spring-mass model, we show that not stiff but compliant legs are essential to obtain the basic walking mechanics; incorporating the double support as an essential part of the walking motion, the model reproduces the characteristic stance dynamics that result in the observed small vertical oscillation of the body and the observed out-of-phase changes in forward kinetic and gravitational potential energies. Exploring the parameter space of this model, we further show that it not only combines the basic dynamics of walking and running in one mechanical system, but also reveals these gaits to be just two out of the many solutions to legged locomotion offered by compliant leg behaviour and accessed by energy or speed.", "title": "Compliant leg behaviour explains basic dynamics of walking and running."}, "5a53e6ce20e4d591b9ab02c6645c9dafc2480c61": {"paper_id": "5a53e6ce20e4d591b9ab02c6645c9dafc2480c61", "abstract": "In this paper, the authors describe the design and control of RHex, a power autonomous, untethered, compliant-legged hexapod robot. RHex has only six actuators\u2014one motor located at each hip\u2014achieving mechanical simplicity that promotes reliable and robust operation in real-world tasks. Empirically stable and highly maneuverable locomotion arises from a very simple clock-driven, openloop tripod gait. The legs rotate full circle, thereby preventing the common problem of toe stubbing in the protraction (swing) phase. An extensive suite of experimental results documents the robot\u2019s significant \u201cintrinsic mobility\u201d\u2014the traversal of rugged, broken, and obstacle-ridden ground without any terrain sensing or actively controlled adaptation. RHex achieves fast and robust forward locomotion traveling at speeds up to one body length per second and traversing height variations well exceeding its body clearance.", "title": "RHex: A Simple and Highly Mobile Hexapod Robot"}, "c198b2eab19ff6d12273c6869d457bc1bac663b0": {"paper_id": "c198b2eab19ff6d12273c6869d457bc1bac663b0", "abstract": "Although the potential benefits of a powered ankle-foot prosthesis have been well documented, no one has successfully developed and verified that such a prosthesis can improve amputee gait compared to a conventional passive-elastic prosthesis. One of the main hurdles that hinder such a development is the challenge of building an ankle-foot prosthesis that matches the size and weight of the intact ankle, but still provides a sufficiently large instantaneous power output and torque to propel an amputee. In this paper, we present a novel, powered ankle-foot prosthesis that overcomes these design challenges. The prosthesis comprises an unidirectional spring, configured in parallel with a force-controllable actuator with series elasticity. With this architecture, the ankle-foot prosthesis matches the size and weight of the human ankle, and is shown to be satisfying the restrictive design specifications dictated by normal human ankle walking biomechanics.", "title": "Biomechanical Design of a Powered Ankle-Foot Prosthesis"}, "442cef27a2e2edb1754abfbfdb69238c45b5270d": {"paper_id": "442cef27a2e2edb1754abfbfdb69238c45b5270d", "abstract": "Walking like an inverted pendulum reduces muscle-force and work demands during single support, but it also unavoidably requires mechanical work to redirect the body's center of mass in the transition between steps, when one pendular motion is substituted by the next. Production of this work exacts a proportional metabolic cost that is a major determinant of the overall cost of walking.", "title": "Energetic consequences of walking like an inverted pendulum: step-to-step transitions."}, "089a211f16e62a16c3d67678b99439638b5d0a1e": {"paper_id": "089a211f16e62a16c3d67678b99439638b5d0a1e", "abstract": "Passive dynamic walking refers to a class of bipedal machines that are able to walk down a gentle slope with no external control or energy input. The legs swing naturally as pendula, and conservation of angular momentum governs the contact of the swing foot with the ground. Previous machines have been limited to planar motions. We extend the planar motions to allow for tilting side to side (roll motion). Passive walking cycles exist, but the roll motion is unstable, resembling that of an inverted pendulum. The instability is due to mismatching of roll velocity with the ground contact conditions. Several strategies are presented for stabilizing this motion, of which the quasi-static control of step width is determined to be both simple", "title": "Stabilization of Lateral Motion in Passive Dynamic Walking"}, "f7916d592aa644b01f3f061cc210f10236c734bc": {"paper_id": "f7916d592aa644b01f3f061cc210f10236c734bc", "abstract": "Although below-knee prostheses have been commercially available for some time, today's devices are completely passive, and consequently, their mechanical properties remain fixed with walking speed and terrain. A lack of understanding of the ankle-foot biomechanics and the dynamic interaction between an amputee and a prosthesis is one of the main obstacles in the development of a biomimetic ankle-foot prosthesis. In this paper, we present a novel ankle-foot emulator system for the study of human walking biomechanics. The emulator system is comprised of a high performance, force-controllable, robotic ankle-foot worn by an amputee interfaced to a mobile computing unit secured around his waist. We show that the system is capable of mimicking normal ankle-foot walking behaviour. An initial pilot study supports the hypothesis that the emulator may provide a more natural gait than a conventional passive prosthesis", "title": "An ankle-foot emulation system for the study of human walking biomechanics"}, "90af18138a8e3af7c16ea5ab474d854f044eb9b8": {"paper_id": "90af18138a8e3af7c16ea5ab474d854f044eb9b8", "abstract": "In this article, we review the recent history of the development of dental CAD/CAM systems for the fabrication of crowns and fixed partial dentures (FPDs), based on our 20 years of experience in this field. The current status of commercial dental CAD/CAM systems developed around the world is evaluated, with particular focus on the field of ceramic crowns and FPDs. Finally, we discuss the future perspectives applicable to dental CAD/CAM. The use of dental CAD/CAM systems is promising not only in the field of crowns and FPDs but also in other fields of dentistry, even if the contribution is presently limited. CAD/CAM technology will contribute to patients' health and QOL in the aging society.", "title": "A review of dental CAD/CAM: current status and future perspectives from 20 years of experience."}, "acfc6af67863e885dd7fda2881a33380e9626431": {"paper_id": "acfc6af67863e885dd7fda2881a33380e9626431", "abstract": "This paper describes a generalisation of the unscented transformation (UT) which allows sigma points to be scaled to an arbitrary dimension. The UT is a method for predicting means and covariances in nonlinear systems. A set of samples are deterministically chosen which match the mean and covariance of a (not necessarily Gaussian-distributed) probability distribution. These samples can be scaled by an arbitrary constant. The method guarantees that the mean and covariance second order accuracy in mean and covariance, giving the same performance as a second order truncated filter but without the need to calculate any Jacobians or Hessians. The impacts of scaling issues are illustrated by considering conversions from polar to Cartesian coordinates with large angular uncertainties.", "title": "The scaled unscented transformation"}, "65484334a5cd4cabf6e5f7a17f606f07e2acf625": {"paper_id": "65484334a5cd4cabf6e5f7a17f606f07e2acf625", "abstract": "An algorithm, the bootstrap filter, is proposed for implementing recursive Bayesian filters. The required density of the state vector is represented as a set of random samples, which are updated and propagated by the algorithm. The method is not restricted by assumptions of linearity or Gaussian noise: it may be applied to any state transition or measurement model. A simulation example of the bearings only tracking problem is presented. This simulation includes schemes for improving the efficiency of the basic algorithm. For this example, the performance of the bootstrap filter is greatly superior to the standard extended Kalman filter.", "title": "Novel approach to nonlinear / non-Gaussian Bayesian state estimation"}, "880f681d1ad0fd8aa2c7a8eb8a16ccade1c25147": {"paper_id": "880f681d1ad0fd8aa2c7a8eb8a16ccade1c25147", "abstract": "We present part of a vision system for blind and visually impaired people. It detects obstacles on sidewalks and provides guidance to avoid them. Obstacles are trees, light poles, trash cans, holes, branches, stones and other objects at a distance of 3 to 5 meters from the camera position. The system first detects the sidewalk borders, using edge information in combination with a tracking mask, to obtain straight lines with their slopes and the vanishing point. Once the borders are found, a rectangular window is defined within which two obstacle detection methods are applied. The first determines the variation of the maxima and minima of the gray levels of the pixels. The second uses the binary edge image and searches in the vertical and horizontal histograms for discrepancies of the number of edge points. Together, these methods allow to detect possible obstacles with their position and size, such that the user can be alerted and informed about the best way to avoid them. The system works in realtime and complements normal navigation with the cane.", "title": "Obstacle Detection and Avoidance on Sidewalks"}, "c31382156fe9f4e3096a390a475361b72f9f3e78": {"paper_id": "c31382156fe9f4e3096a390a475361b72f9f3e78", "abstract": "Owing to the uncertainty of transmission opportunities between mobile nodes, the routing in delay-tolerant networks (DTNs) exploits the mechanism of opportunistic forwarding. Energy-efficient algorithms and policies for DTN are crucial to maximizing the message delivery probability while reducing the delivery cost. In this contribution, we investigate the problem of energy-efficient optimal beaconing control in a DTN. We model the message dissemination under variable beaconing rate with a continuous-time Markov model. Based on this model, we then formulate the optimization problem of the optimal beaconing control for epidemic routing and obtain the optimal threshold policy from the solution of this optimization problem. Furthermore, through extensive numerical results, we demonstrate that the proposed optimal threshold policy significantly outperforms the static policy with constant beaconing rate in terms of system energy consumption savings.", "title": "Optimal Beaconing Control for Epidemic Routing in Delay-Tolerant Networks"}, "06fd86110dbcdc37f298ac5f35c5cb9ccdb1ac08": {"paper_id": "06fd86110dbcdc37f298ac5f35c5cb9ccdb1ac08", "abstract": "The highly successful architecture and protocols of today's Internet may operate poorly in environments characterized by very long delay paths and frequent network partitions. These problems are exacerbated by end nodes with limited power or memory resources. Often deployed in mobile and extreme environments lacking continuous connectivity, many such networks have their own specialized protocols, and do not utilize IP. To achieve interoperability between them, we propose a network architecture and application interface structured around optionally-reliable asynchronous message forwarding, with limited expectations of end-to-end connectivity and node resources. The architecture operates as an overlay above the transport layers of the networks it interconnects, and provides key services such as in-network data storage and retransmission, interoperable naming, authenticated forwarding and a coarse-grained class of service.", "title": "A delay-tolerant network architecture for challenged internets"}, "119fc4418b5d12b5af3bc698eebd50952134e2a8": {"paper_id": "119fc4418b5d12b5af3bc698eebd50952134e2a8", "abstract": "This paper investigates an application of mobile sensing: detecting and reporting the surface conditions of roads. We describe a system and associated algorithms to monitor this important civil infrastructure using a collection of sensor-equipped vehicles. This system, which we call the Pothole Patrol (P2), uses the inherent mobility of the participating vehicles, opportunistically gathering data from vibration and GPS sensors, and processing the data to assess road surface conditions. We have deployed P2 on 7 taxis running in the Boston area. Using a simple machine-learning approach, we show that we are able to identify potholes and other severe road surface anomalies from accelerometer data. Via careful selection of training data and signal features, we have been able to build a detector that misidentifies good road segments as having potholes less than 0.2% of the time. We evaluate our system on data from thousands of kilometers of taxi drives, and show that it can successfully detect a number of real potholes in and around the Boston area. After clustering to further reduce spurious detections, manual inspection of reported potholes shows that over 90% contain road anomalies in need of repair.", "title": "The pothole patrol: using a mobile sensor network for road surface monitoring"}, "bfe7f183bc191665b39edd5fe62a66ae24de8319": {"paper_id": "bfe7f183bc191665b39edd5fe62a66ae24de8319", "abstract": "In de novo drug design, computational strategies are used to generate novel molecules with good affinity to the desired biological target. In this work, we show that recurrent neural networks can be trained as generative models for molecular structures, similar to statistical language models in natural language processing. We demonstrate that the properties of the generated molecules correlate very well with the properties of the molecules used to train the model. In order to enrich libraries with molecules active towards a given biological target, we propose to fine-tune the model with small sets of molecules, which are known to be active against that target. Against Staphylococcus aureus, the model reproduced 14% of 6051 hold-out test molecules that medicinal chemists designed, whereas against Plasmodium falciparum (Malaria) it reproduced 28% of 1240 test molecules. When coupled with a scoring function, our model can perform the complete de novo drug design cycle to generate large sets of novel molecules for drug discovery.", "title": "Generating Focussed Molecule Libraries for Drug Discovery with Recurrent Neural Networks"}, "5955fbf8604e2c6845f73b155b68223eeed96710": {"paper_id": "5955fbf8604e2c6845f73b155b68223eeed96710", "abstract": "Predicting properties of molecules requires functions that take graphs as inputs. Molecular graphs are usually preprocessed using hash-based functions to produce fixed-size fingerprint vectors, which are used as features for making predictions. We introduce a convolutional neural network that operates directly on graphs, allowing end-to-end learning of the feature pipeline. This architecture generalizes standard molecular fingerprints. We show that these data-driven features are more interpretable, and have better predictive performance on a variety of tasks.", "title": "Convolutional Networks on Graphs for Learning Molecular Fingerprints"}, "ec2b2569b3a0d70a5b45d48b041dec9060d85eb7": {"paper_id": "ec2b2569b3a0d70a5b45d48b041dec9060d85eb7", "abstract": "This paper presents a new approach for learning in structured domains (SDs) using a constructive neural network for graphs (NN4G). The new model allows the extension of the input domain for supervised neural networks to a general class of graphs including both acyclic/cyclic, directed/undirected labeled graphs. In particular, the model can realize adaptive contextual transductions, learning the mapping from graphs for both classification and regression tasks. In contrast to previous neural networks for structures that had a recursive dynamics, NN4G is based on a constructive feedforward architecture with state variables that uses neurons with no feedback connections. The neurons are applied to the input graphs by a general traversal process that relaxes the constraints of previous approaches derived by the causality assumption over hierarchical input data. Moreover, the incremental approach eliminates the need to introduce cyclic dependencies in the definition of the system state variables. In the traversal process, the NN4G units exploit (local) contextual information of the graphs vertices. In spite of the simplicity of the approach, we show that, through the compositionality of the contextual information developed by the learning, the model can deal with contextual information that is incrementally extended according to the graphs topology. The effectiveness and the generality of the new approach are investigated by analyzing its theoretical properties and providing experimental results.", "title": "Neural Network for Graphs: A Contextual Constructive Approach"}, "03ff3f8f4d5a700fbe8f3a3e63a39523c29bb60f": {"paper_id": "03ff3f8f4d5a700fbe8f3a3e63a39523c29bb60f", "abstract": "The ability to accurately represent sentences is central to language understanding. We describe a convolutional architecture dubbed the Dynamic Convolutional Neural Network (DCNN) that we adopt for the semantic modelling of sentences. The network uses Dynamic k-Max Pooling, a global pooling operation over linear sequences. The network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations. The network does not rely on a parse tree and is easily applicable to any language. We test the DCNN in four experiments: small scale binary and multi-class sentiment prediction, six-way question classification and Twitter sentiment prediction by distant supervision. The network achieves excellent performance in the first three tasks and a greater than 25% error reduction in the last task with respect to the strongest baseline.", "title": "A Convolutional Neural Network for Modelling Sentences"}, "60c36d469c48541ec06f92cf982de0f7e12093cf": {"paper_id": "60c36d469c48541ec06f92cf982de0f7e12093cf", "abstract": "Few types of signal streams are as ubiquitous as music. Here we consider the problem of extracting essential ingredients of music signals, such as well-defined global temporal structure in the form of nested periodicities (or meter). Can we construct an adaptive signal processing device that learns by example how to generate new instances of a given musical style? Because recurrent neural networks can in principle learn the temporal structure of a signal, they are good candidates for such a task. Unfortunately, music composed by standard recurrent neural networks (RNNs) often lacks global coherence. The reason for this failure seems to be that RNNs cannot keep track of temporally distant events that indicate global music structure. Long Short-Term Memory (LSTM) has succeeded in similar domains where other RNNs have failed, such as timing & counting and learning of context sensitive languages. In the current study we show that LSTM is also a good mechanism for learning to compose music. We present experimental results showing that LSTM successfully learns a form of blues music and is able to compose novel (and we believe pleasing) melodies in that style. Remarkably, once the network has found the relevant structure it does not drift from it: LSTM is able to play the blues with good timing and proper structure as long as one is willing to listen.", "title": "Finding temporal structure in music: blues improvisation with LSTM recurrent networks"}, "590999a3b8e6e484260b671682c4f704e90bfdac": {"paper_id": "590999a3b8e6e484260b671682c4f704e90bfdac", "abstract": "Rumelhart, Hinton and Williams [Rumelhart et al. 86] describe a learning procedure for layered networks of deterministic, neuron-like units. This paper describes further research on the learning procedure. We start by describing the units, the way they are connected, the learning procedure, and the extension to iterative nets. We then give an example in which a network learns a set of filters that enable it to discriminate formant-like patterns in the presence of noise. The speed of learning is strongly dependent on the shape of the surface formed by the error measure in \"weight space.\" We give examples of the shape of the error surface for a typical task and illustrate how an acceleration method speeds up descent in weight space. The main drawback of the learning procedure is the way it scales as the size of the task and the network increases. We give some preliminary results on scaling and show how the magnitude of the optimal weight changes depends on the fan-in of the units. Additional results illustrate the effects on learning speed of the amount of interaction between the weights. A variation of the learning procedure that back-propagates desired state information rather than error gradients is developed and compared with the standard procedure. Finally, we discuss the relationship between our iterative networks and the \"analog\" networks described by Hopfield and Tank [Hopfield and Tank 85]. The learning procedure can discover appropriate weights in their kind of network, as well as determine an optimal schedule for varying the nonlinearity of the units during a search.", "title": "Experiments on Learning by Back Propagation."}, "8804402bd9bd1013d1a67f0f9fb26a9c678b6c78": {"paper_id": "8804402bd9bd1013d1a67f0f9fb26a9c678b6c78", "abstract": "D3EGF(FIH)J KMLONPEGQSRPETN UCV.WYX(Z R.[ V R6\\M[ X N@]_^O\\`JaNcb V RcQ W d EGKeL(^(QgfhKeLOE?i)^(QSj ETNPfPQkRl[ V R)m\"[ X ^(KeLOEG^ npo qarpo m\"[ X ^(KeLOEG^tsAu EGNPb V ^ v wyx zlwO{(|(}<~O\u007fC}\u0081\u0080(\u0082(xp{a\u0083y\u0084.~A}\u0086\u0085\u0088\u0087_~ \u0089C\u008al\u00833\u0089#|<\u0080Az\u0086w#|l\u00806\u0087 \u008b(| \u008c JpfhL X\u008dV\u008f\u008e EG^O\u0090 QgJ \u0091 ETFOR\u0086\u0092\u0093] ^O\\\u0094J\u0095NPb V RcQ\u0097\u0096 X E)ETR \u00986EGKeLOETNcKMLOE\u009a\u0099 F\u0088\u009b ETN V RcQgJp^(^OE ZgZ E i ^(Qkj EGNPfhQSRO\u009b E \u009cOE2m1Jp^ RcNY\u009b E V\u0095Z sO\u009d\u009f\u009e! \u008d\u00a1 q.n sCD X KGKa\u00928\u009d\u00a2EG^ RPNhE\u00a4\u00a3 \u00a5\u00a6Q ZgZ E\u0095s m\u00a7J\u0095^ RPNO\u009b E V\u0095Z s( \u0308 X \u009b EG\u00a9#E\u0081Kas#\u009d V ^ V \u009c V s(H a \u009d\u00aba\u0095\u00ac3\u00ad \u00ae#|.\u0080Y \u0304y} xa\u00b0O\u007fC}l{\u008dx\u0093\u0087 \u0089 \u0083yxl\u0080Y~3{\u008d| \u0084 \u00b12\u0087Pz \u0084 \u009e V J Z J U N V fhKTJp^(Q \u0091 ETFOR\u0086\u0092 J\u0095\\ D vYf3RPEGb \u0301f V ^(\u009c\u00a7\u009d\u0088Jpb\u008fF X RPETN@D KTQ\u0097EG^(KTE i ^(QSjpEGNPfhQSR4v\u03bcJ\u0095\\ U\u00b6Z JaNPEG^(K\u00b7E jYQ V \u009c(Q \u0327D V ^ R V m V N3R V aOs#1 o \u00a1Ga r U Q\u0097NhE\u0081^OoTE1\u20444\u00bb,] R V\u0095Z vC1\u20442 3\u20444 \u0084 x \u00b1 x \u007f \u008b#\u00bf }\u00c0\u0087 \u00893\u0080t}l\u0082C}2\u0087P}<~ \u00act[ X NP\u0090\u0095E\u0081^\u00a7D KeL(b \u0301Qg\u009c(L X \u00a9yETN ] \u0091 DY]_\u00c1 \u009d\u0088J\u0095NPfhJ\u00c3\u00c2 Z j ETo\u0081Q V a\u0095 rpopo2\u00c4 X \u0090 V ^(J(sCD \u00c5)QSRPoTEGN ZgV ^(\u009c \u00c6 \u0089#|\u0095{3 \u0304\u008d|.\u0080(\u007fC}.\u008bC\u00bfY}p\u0084 \u0087Pz\u0086w", "title": "Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies"}, "1a736409c7711f8673f31d366f583ddc8759547f": {"paper_id": "1a736409c7711f8673f31d366f583ddc8759547f", "abstract": "The long short-term memory (LSTM) network trained by gradient descent solves difficult problems which traditional recurrent neural networks in general cannot. We have recently observed that the decoupled extended Kalman filter training algorithm allows for even better performance, reducing significantly the number of training steps when compared to the original gradient descent training algorithm. In this paper we present a set of experiments which are unsolvable by classical recurrent networks but which are solved elegantly and robustly and quickly by LSTM combined with Kalman filters.", "title": "Kalman filters improve LSTM network performance in problems unsolvable by traditional recurrent nets"}, "6f3e53fca180b1701e18b14b983d3259c692c92e": {"paper_id": "6f3e53fca180b1701e18b14b983d3259c692c92e", "abstract": "Full Metal Jacket is a general-purpose visual dataflow language currently being developed on top of Emblem, a Lisp dialect strongly influenced by Common Lisp but smaller and more type-aware, and with support for CLOS-style object orientation, graphics, event handling and multi-threading. Methods in Full Metal Jacket Jacket are directed acyclic graphs. Data arriving at ingates from the calling method flows along edges through vertices, at which it gets transformed by applying Emblem functions or methods, or methods defined in Full Metal Jacket, before it finally arrives at outgates where it is propagated back upwards to the calling method. The principal difference between Full Metal Jacket and existing visual dataflow languages such as Prograph is that Full Metal Jacket is a pure dataflow language, with no special syntax being required for control constructs such as loops or conditionals, which resemble ordinary methods except in the number of times they generate outputs. This uniform syntax means that, like Lisp and Prolog, methods in Full Metal Jacket are themselves data structures and can be manipulated as such.", "title": "Full Metal Jacket : A Pure Visual Dataflow Language Built on Top of Lisp"}, "11291b24e7ef097593f7960d66a5863a97f996aa": {"paper_id": "11291b24e7ef097593f7960d66a5863a97f996aa", "abstract": "To explore issues of developmental structure, physical embodiment, integration of multiple sensory and motor systems, and social interaction, we have constructed an upper-torso humanoid robot called Cog. The robot has twenty-one degrees of freedom and a variety of sensory systems, including visual, auditory, vestibular, kinesthetic, and tactile senses. This chapter gives a background on the methodology that we have used in our investigations, highlights the research issues that have been raised during this project, and provides a summary of both the current state of the project and our long-term goals. We report on a variety of implemented visual-motor routines (smooth-pursuit tracking, saccades, binocular vergence, and vestibular-ocular and opto-kinetic reflexes), orientation behaviors, motor control techniques, and social behaviors (pointing to a visual target, recognizing joint attention through face and eye finding, imitation of head nods, and regulating interaction through expressive feedback). We further outline a number of areas for future research that will be necessary to build a complete embodied system.", "title": "The Cog Project : Building a Humanoid Robot"}, "035f444e5e801d1a375e2836b0c7c9ab90542428": {"paper_id": "035f444e5e801d1a375e2836b0c7c9ab90542428", "abstract": "This report documents the design and implementation of a binocular, foveated active vision system as part of the Cog project at the MIT Artificial Intelligence Laboratory. The active vision system features a 3 degree of freedom mechanical platform that supports four color cameras, a motion control system, and a parallel network of digital signal processors for image processing. To demonstrate the capabilities of the system, we present results from four sample visual-motor tasks. The author receives support from a National Defense Science and Engineering Graduate Fellowship. Support for this project is provided in part by an ONR/ARPA Vision MURI Grant (No. N00014-95-1-0600).", "title": "A Binocular, Foveated Active Vision System"}, "f8f2b7c91e1039d221a33e7796952943ca50a277": {"paper_id": "f8f2b7c91e1039d221a33e7796952943ca50a277", "abstract": "The locomotive motion in animals is produced in some central neural units, and basically no sensory signal from peripheral receptors is necessary to induce it. The rhythm generators do not only produce rhythms but also alter their frequencies and patterns. This paper presents some methematical models of the neural rhythm generators and discusses various aspects of the frequency and pattern control in them.", "title": "Mechanisms of frequency and pattern control in the neural rhythm generators"}, "cf4cc1c9c1dc6ecefeacb1f5bbe9e11e22212eef": {"paper_id": "cf4cc1c9c1dc6ecefeacb1f5bbe9e11e22212eef", "abstract": "A new architecture for controlling mobile robots is described. Layers of control system are built to let the robot operate at increasing levels of competence. Layers are made up of asynchronous modules that communicate over low-bandwidth channels. Each module is an instance of a fairly simple computational machine. Higher-level layers can subsume the roles of lower levels by suppressing their outputs. However, lower levels continue to function as higher levels are added. The result is a robust and flexible robot control system. The system has been used to control a mobile robot wandering around unconstrained laboratory areas and computer machine rooms. Eventually it is intended to control a robot that wanders the office areas of our laboratory, building maps of its surroundings using an onboard arm to perform simple tasks.", "title": "A robust layered control system for a mobile robot"}, "1b9efda9b971b7784f73e6960a95bba1b10fcbe8": {"paper_id": "1b9efda9b971b7784f73e6960a95bba1b10fcbe8", "abstract": "Complex systems and complex missions take years of planning and force launches to become incredibly expensive. The longer the planning and the more expensive the mission, the more catastrophic if it fails. The solution has always been to plan better, add redundancy, test thoroughly and use high quality components. Based on our experience in building ground based mobile robots (legged and wheeled) we argue here for cheap, fast missions using large numbers of mass produced simple autonomous robots that are small by today's standards (1 to 2 Kg). We argue that the time between mission conception and implementation can be radically reduced, that launch mass can be slashed, that totally autonomous robots can be more reliable than ground controlled robots, and that large numbers of robots can change the tradeoff between reliability of individual components and overall mission success. Lastly, we suggest that within a few years it will be possible at modest cost to invade a planet with millions of tiny robots.", "title": "FAST , CHEAP AND OUT OF CONTROL : A ROBOT INVASION OF THE SOLAR SYSTEM"}, "291c3f4393987f67cded328e984dbae84af643cb": {"paper_id": "291c3f4393987f67cded328e984dbae84af643cb", "abstract": "OF THESIS FASTER DYNAMIC PROGRAMMING FOR MARKOV DECISION PROCESSES Markov decision processes (MDPs) are a general framework used by Artificial Intelligence (AI) researchers to model decision theoretic planning problems. Solving real world MDPs has been a major and challenging research topic in the AI literature. This paper discusses two main groups of approaches in solving MDPs. The first group of approaches combines the strategies of heuristic search and dynamic programming to expedite the convergence process. The second makes use of graphical structures in MDPs to decrease the effort of classic dynamic programming algorithms. Two new algorithms proposed by the author, MBLAO* and TVI, are described here.", "title": "Faster Dynamic Programming for Markov Decision Processes"}, "6a7384bf0d319d19bbbbf578ac7052bb72ef940c": {"paper_id": "6a7384bf0d319d19bbbbf578ac7052bb72ef940c", "abstract": "Value Iteration is an inefficient algorithm for Markov decision processes (MDPs) because it puts the majority of its effort into backing up the entire state space, which turns out to be unnecessary in many cases. In order to overcome this problem, many approaches have been proposed. Among them, LAO*, LRTDP and HDP are state-of-theart ones. All of these use reachability analysis and heuristics to avoid some unnecessary backups. However, none of these approaches fully exploit the graphical features of the MDPs or use these features to yield the best backup sequence of the state space. We introduce an algorithm named Topological Value Iteration (TVI) that can circumvent the problem of unnecessary backups by detecting the structure of MDPs and backing up states based on topological sequences. We prove that the backup sequence TVI applies is optimal. Our experimental results show that TVI outperforms VI, LAO*, LRTDP and HDP on our benchmark MDPs.", "title": "Topological Value Iteration Algorithm for Markov Decision Processes"}, "097ced4aaed127da59d125f9103bb8d90d78ca96": {"paper_id": "097ced4aaed127da59d125f9103bb8d90d78ca96", "abstract": "A number of proposals have been put forth in recent years for the solution of Markov decision processes (MDPs) whose state (and sometimes action) spaces are factored. One recent class of methods involves linear value function approximation, where the optimal value function is assumed to be a linear combination of some set of basis functions, with the aim of finding suitable weights. While sophisticated techniques have been developed for finding the best approximation within this constrained space, few methods have been proposed for choosing a suitable basis set, or modifying it if solution quality is found wanting. We propose a general framework, and specific proposals, that address both of these questions. In particular, we examine <i>weakly coupled MDPs</i> where a number of subtasks can be viewed independently modulo resource constraints. We then describe methods for constructing a piecewise linear combination of the subtask value functions, using greedy decision tree techniques. We argue that this architecture is suitable for many types of MDPs whose combinatorics are determined largely by the existence multiple conflicting objectives.", "title": "Piecewise Linear Value Function Approximation for Factored MDPs"}, "2e268b70c7dcae58de2c8ff7bed1e58a5e58109a": {"paper_id": "2e268b70c7dcae58de2c8ff7bed1e58a5e58109a", "abstract": "This is an updated version of Chapter 4 of the author\u2019s Dynamic Programming and Optimal Control, Vol. II, 4th Edition, Athena Scientific, 2012. It includes new material, and it is substantially revised and expanded (it has more than doubled in size). The new material aims to provide a unified treatment of several models, all of which lack the contractive structure that is characteristic of the discounted problems of Chapters 1 and 2: positive and negative cost models, deterministic optimal control (including adaptive DP), stochastic shortest path models, and risk-sensitive models. Here is a summary of the new material:", "title": "Dynamic Programming and Optimal Control"}, "708e5e17564c8c6501bcbe9297e0298c3314800b": {"paper_id": "708e5e17564c8c6501bcbe9297e0298c3314800b", "abstract": "\u00d0This paper investigates the influence of the cost function on the optimal match between two graphs. It is shown that, for a given cost function, there are an infinite number of other cost functions that lead, for any given pair of graphs, to the same optimal error correcting matching. Furthermore, it is shown that well-known concepts from graph theory, such as graph isomorphism, subgraph isomorphism, and maximum common subgraph, are special cases of optimal error correcting graph matching under particular cost functions. Index Terms\u00d0Graph, subgraph, maximum common subgraph, graph isomorphism, subgraph isomorphism, graph matching, error correcting graph matching, cost function, edit operation, graph edit distance.", "title": "Error Correcting Graph Matching: On the Influence of the Underlying Cost Function"}, "ea65866e3d1fd3e3e630005147e373ff7a886009": {"paper_id": "ea65866e3d1fd3e3e630005147e373ff7a886009", "abstract": "A method to determine a distance measure between two nonhierarchical attributed relational graphs is presented. In order to apply this distance measure, the graphs are characterised by descriptive graph grammars (DGG). The proposed distance measure is based on the computation of the minimum number of modifications required to transform an input graph into the reference one. Specifically, the distance measure is defined as the cost of recognition of nodes plus the number of transformations which include node insertion, node deletion, branch insertion, branch deletion, node label substitution and branch label substitution. The major difference between the proposed distance measure and the other ones is the consideration of the cost of recognition of nodes in the distance computation. In order to do this, the principal features of the nodes are described by one or several cost functions which are used to compute the similarity between the input nodes and the reference ones. Finally, an application of this distance measure to the recognition of lower case handwritten English characters is presented.", "title": "A distance measure between attributed relational graphs for pattern recognition"}, "b3f2bd7f89fdfdc118432f4105e7e6c00a6073f5": {"paper_id": "b3f2bd7f89fdfdc118432f4105e7e6c00a6073f5", "abstract": "The tree-to-tree correctmn problem Is to determine, for two labeled ordered trees T and T', the distance from T to T' as measured by the mlmmum cost sequence of edit operaUons needed to transform T into T' The edit operations investigated allow changing one node of a tree into another node, deleting one node from a tree, or inserting a node into a tree An algorithm Is presented which solves this problem m time O(V* V'*LZ* L'2), where V and V' are the numbers of nodes respectively of T and T', and L and L' are the maximum depths respectively of T and T' Possible apphcatmns are to the problems of measuring the similarity between trees, automatic error recovery and correction for programming languages, and determining the largest common substructure of two trees", "title": "The Tree-to-Tree Correction Problem"}, "455e1168304e0eb2909093d5ab9b5ec85cda5028": {"paper_id": "455e1168304e0eb2909093d5ab9b5ec85cda5028", "abstract": "The string-to-string correction problem is to determine the distance between two strings as measured by the minimum cost sequence of \u201cedit operations\u201d needed to change the one string into the other. The edit operations investigated allow changing one symbol of a string into another single symbol, deleting one symbol from a string, or inserting a single symbol into a string. An algorithm is presented which solves this problem in time proportional to the product of the lengths of the two strings. Possible applications are to the problems of automatic spelling correction and determining the longest subsequence of characters common to two strings.", "title": "The String-to-String Correction Problem"}, "db565b24e7f134c85afe541b0ad531c435fae6b5": {"paper_id": "db565b24e7f134c85afe541b0ad531c435fae6b5", "abstract": "Phishing is an unlawful activity of making gullible people to reveal their insightful information into fake websites. The Aim of these phishing websites is to acquire confidential information such as usernames, passwords, banking credentials and some other personal information. Phishing website looks similar to legitimate website therefore people cannot make difference among them. Today users are heavily relying on the internet for online purchasing, ticket booking, bill payments, etc. As technology advances, the phishing approaches being used are also getting progressed and hence it stimulates anti-phishing methods to be upgraded. In this paper, we have implemented two algorithms named Adaline and Backpropion along with the support vector machine to enhance the detection rate and classification.", "title": "Phishing websites detection through supervised learning networks"}, "2b877d697fb8ba947fe3f964824098c25636fa0e": {"paper_id": "2b877d697fb8ba947fe3f964824098c25636fa0e", "abstract": "Phishing is a plague in cyberspace. Typically, phish detection methods either use human-verified URL blacklists or exploit Web page features via machine learning techniques. However, the former is frail in terms of new phish, and the latter suffers from the scarcity of effective features and the high false positive rate (FP). To alleviate those problems, we propose a layered anti-phishing solution that aims at (1) exploiting the expressiveness of a rich set of features with machine learning to achieve a high true positive rate (TP) on novel phish, and (2) limiting the FP to a low level via filtering algorithms.\n Specifically, we proposed CANTINA+, the most comprehensive feature-based approach in the literature including eight novel features, which exploits the HTML Document Object Model (DOM), search engines and third party services with machine learning techniques to detect phish. Moreover, we designed two filters to help reduce FP and achieve runtime speedup. The first is a near-duplicate phish detector that uses hashing to catch highly similar phish. The second is a login form filter, which directly classifies Web pages with no identified login form as legitimate.\n We extensively evaluated CANTINA+ with two methods on a diverse spectrum of corpora with 8118 phish and 4883 legitimate Web pages. In the randomized evaluation, CANTINA+ achieved over 92% TP on unique testing phish and over 99% TP on near-duplicate testing phish, and about 0.4% FP with 10% training phish. In the time-based evaluation, CANTINA+ also achieved over 92% TP on unique testing phish, over 99% TP on near-duplicate testing phish, and about 1.4% FP under 20% training phish with a two-week sliding window. Capable of achieving 0.4% FP and over 92% TP, our CANTINA+ has been demonstrated to be a competitive anti-phishing solution.", "title": "CANTINA+: A Feature-Rich Machine Learning Framework for Detecting Phishing Web Sites"}, "5938ca3697f9c55bd28b7d2a0d5460cf99ffba50": {"paper_id": "5938ca3697f9c55bd28b7d2a0d5460cf99ffba50", "abstract": "Phishing is form of identity theft that combines social engineering techniques and sophisticated attack vectors to harvest financial information from unsuspecting consumers. Often a phisher tries to lure her victim into clicking a URL pointing to a rogue page. In this paper, we focus on studying the structure of URLs employed in various phishing attacks. We find that it is often possible to tell whether or not a URL belongs to a phishing attack without requiring any knowledge of the corresponding page data. We describe several features that can be used to distinguish a phishing URL from a benign one. These features are used to model a logistic regression filter that is efficient and has a high accuracy. We use this filter to perform thorough measurements on several million URLs and quantify the prevalence of phishing on the Internet today", "title": "A framework for detection and measurement of phishing attacks"}, "151c9a0e8e31ee17a43bdd66091f49324f36dbdc": {"paper_id": "151c9a0e8e31ee17a43bdd66091f49324f36dbdc", "abstract": "Web spoofing is a significant problem involving fraudulent email and web sites that trick unsuspecting users into revealing private information. We discuss some aspects of common attacks and propose a framework for client-side defense: a browser plug-in that examines web pages and warns the user when requests for data may be part of a spoof attack. While the plugin, SpoofGuard, has been tested using actual sites obtained through government agencies concerned about the problem, we expect that web spoofing and other forms of identity theft will be continuing problems in", "title": "Client-Side Defense Against Web-Based Identity Theft"}, "1805e19e9fa6a4c140c531bc0dca8016ee75257b": {"paper_id": "1805e19e9fa6a4c140c531bc0dca8016ee75257b", "abstract": "The tutorial starts with an overview of the concepts of VC dimension and structural risk minimization. We then describe linear Support Vector Machines (SVMs) for separable and non-separable data, working through a non-trivial example in detail. We describe a mechanical analogy, and discuss when SVM solutions are unique and when they are global. We describe how support vector training can be practically implemented, and discuss in detail the kernel mapping technique which is used to construct SVM solutions which are nonlinear in the data. We show how Support Vector machines can have very large (even infinite) VC dimension by computing the VC dimension for homogeneous polynomial and Gaussian radial basis function kernels. While very high VC dimension would normally bode ill for generalization performance, and while at present there exists no theory which shows that good generalization performance is guaranteed for SVMs, there are several arguments which support the observed high accuracy of SVMs, which we review. Results of some experiments which were inspired by these arguments are also presented. We give numerous examples and proofs of most of the key theorems. There is new material, and I hope that the reader will find that even old material is cast in a fresh light.", "title": "A Tutorial on Support Vector Machines for Pattern Recognition"}, "88d6898d6167b0687bc921da35b4ff44bd091e6e": {"paper_id": "88d6898d6167b0687bc921da35b4ff44bd091e6e", "abstract": "Detecting and identifying any phishing websites in real-time, particularly for e-banking is really a complex and dynamic problem involving many factors and criteria. Because of the subjective considerations and the ambiguities involved in the detection, Fuzzy Data Mining (DM) Techniques can be an effective tool in assessing and identifying phishing websites for e-banking since it offers a more natural way of dealing with quality factors rather than exact values. In this paper, we present novel approach to overcome the \u2018fuzziness\u2019 in the e-banking phishing website assessment and propose an intelligent resilient and effective model for detecting e-banking phishing websites. The proposed model is based on Fuzzy logic (FL) combined with Data Mining algorithms to characterize the e-banking phishing website factors and to investigate its techniques by classifying there phishing types and defining six e-banking phishing website attack criteria\u2019s with a layer structure. The proposed e-banking phishing website model showed the significance importance of the phishing website two criteria\u2019s (URL & Domain Identity) and (Security & Encryption) in the final phishing detection rate result, taking into consideration its characteristic association and relationship with each others as showed from the fuzzy data mining classification and association rule algorithms. Our phishing model also showed the insignificant trivial influence of the (Page Style & Content) criteria along with (Social Human Factor) criteria in the phishing detection final rate result.", "title": "Modelling Intelligent Phishing Detection System for E-banking Using Fuzzy Data Mining"}, "3c398007c04eb12c0b7417f5d135919a300a470d": {"paper_id": "3c398007c04eb12c0b7417f5d135919a300a470d", "abstract": "In recent years we have seen a tremendous growth in the volume of text documents available on the Internet, digital libraries, news sources, and company-wide intrane ts. Automatic text categorization, which is the task of assigning text documents to pre-specified classes (topics o r themes) of documents, is an important task that can help both in organizing as well as in finding information on these h uge resources. Text categorization presents unique challenges due to the large number of attributes present in t he data set, large number of training samples, and attribute dependencies. In this paper we focus on a simple linear-time centroid-based document classification algorithm, that despite its simplicity and robust performance, has not been extensively studied and analyzed. Our extensive experiments show that this centroid-based classifier consi stently and substantially outperforms other algorithms su ch as Naive Bayesian, k-nearest-neighbors, and C4.5, on a wide range of datasets. O ur analysis shows that the similarity measure used by the centroid-based scheme allows it to class ify new document based on how closely its behavior matches the behavior of the documents belonging to differen t classes, as measured by the average similarity between the documents. This matching allows it to dynamically adjus t for classes with different densities. Furthermore, our analysis shows that the similarity measure of the centroidbased scheme accounts for dependencies between the terms in the different classes. We believe that this feature is the reason why it consistently outperforms other classifiers th at cannot take these dependencies into account.", "title": "Centroid-Based Document Classification: Analysis and Experimental Results"}, "80d82331b1d5af68d7e8e0e30dda7a8cfa050c0e": {"paper_id": "80d82331b1d5af68d7e8e0e30dda7a8cfa050c0e", "abstract": "The decision tree output of Quinlan's ID3 algorithm is one of its major weaknesses. Not only can it be incomprehensible and difficult to manipulate, but its use in expert systems frequently demands irrelevant information to be supplied. This report argues that the problem lies in the induction algorithm itself and can only be remedied by radically altering the underlying strategy. It describes a new algorithm, PRISM which, although based on ID3, uses a different induction strategy to induce rules which are modular, thus avoiding many of the problems associated with decision trees.", "title": "PRISM: An Algorithm for Inducing Modular Rules"}, "dad5a7b4d1eb0e51805090e3289955de38275991": {"paper_id": "dad5a7b4d1eb0e51805090e3289955de38275991", "abstract": "Phishing is the third cyber-security threat globally and the first cyber-security threat in China. There were 61.69 million phishing victims in China alone from June 2011 to June 2012, with the total annual monetary loss more than 4.64 billion US dollars. These phishing attacks were highly concentrated in targeting at a few major Websites. Many phishing Webpages had a very short life span. In this paper, we assume the Websites to protect against phishing attacks are known, and study the effectiveness of machine learning based phishing detection using only lexical and domain features, which are available even when the phishing Webpages are inaccessible. We propose several novel highly effective features, and use the real phishing attack data against Taobao and Tencent, two main phishing targets in China, in studying the effectiveness of each feature, and each group of features. We then select an optimal set of features in our phishing detector, which has achieved a detection rate better than 98%, with a false positive rate of 0.64% or less. The detector is still effective when the distribution of phishing URLs changes.", "title": "Protect sensitive sites from phishing attacks using features extractable from inaccessible phishing URLs"}, "85b625341fd7f4588dfc9e5e6082582aabbbe619": {"paper_id": "85b625341fd7f4588dfc9e5e6082582aabbbe619", "abstract": "Malicious URLs have been widely used to mount various cyber attacks including spamming, phishing and malware. Detection of malicious URLs and identification of threat types are critical to thwart these attacks. Knowing the type of a threat enables estimation of severity of the attack and helps adopt an effective countermeasure. Existing methods typically detect malicious URLs of a single attack type. In this paper, we propose method using machine learning to detect malicious URLs of all the popular attack types and identify the nature of attack a malicious URL attempts to launch. Our method uses a variety of discriminative features including textual properties, link structures, webpage contents, DNS information, and network traffic. Many of these features are novel and highly effective. Our experimental studies with 40,000 benign URLs and 32,000 malicious URLs obtained from real-life Internet sources show that our method delivers a superior performance: the accuracy was over 98% in detecting malicious URLs and over 93% in identifying attack types. We also report our studies on the effectiveness of each group of discriminative features, and discuss their evadability.", "title": "Detecting Malicious Web Links and Identifying Their Attack Types"}, "6f3f0ff81994f2566ba29a67448a8096ec52e3de": {"paper_id": "6f3f0ff81994f2566ba29a67448a8096ec52e3de", "abstract": "Phishing has been easy and effective way for trickery and deception on the Internet. While solutions such as URL blacklisting have been effective to some degree, their reliance on exact match with the blacklisted entries makes it easy for attackers to evade. We start with the observation that attackers often employ simple modifications (e.g., changing top level domain) to URLs. Our system, PhishNet, exploits this observation using two components. In the first component, we propose five heuristics to enumerate simple combinations of known phishing sites to discover new phishing URLs. The second component consists of an approximate matching algorithm that dissects a URL into multiple components that are matched individually against entries in the blacklist. In our evaluation with real-time blacklist feeds, we discovered around 18,000 new phishing URLs from a set of 6,000 new blacklist entries. We also show that our approximate matching algorithm leads to very few false positives (3%) and negatives (5%).", "title": "PhishNet: Predictive Blacklisting to Detect Phishing Attacks"}, "e769c99f63f526ec88e74d9683bdbc12a6e55359": {"paper_id": "e769c99f63f526ec88e74d9683bdbc12a6e55359", "abstract": "Phishing websites, fraudulent sites that impersonate a trusted third party to gain access to private data, continue to cost Internet users over a billion dollars each year. In this paper, we describe the design and performance characteristics of a scalable machine learning classifier we developed to detect phishing websites. We use this classifier to maintain Google\u2019s phishing blacklist automatically. Our classifier analyzes millions of pages a day, examining the URL and the contents of a page to determine whether or not a page is phishing. Unlike previous work in this field, we train the classifier on a noisy dataset consisting of millions of samples from previously collected live classification data. Despite the noise in the training data, our classifier learns a robust model for identifying phishing pages which correctly classifies more than 90% of phishing pages several weeks after training concludes.", "title": "Large-Scale Automatic Classification of Phishing Pages"}, "addd84f7cbf5e5626a1ebf08f5edb58fc61147f5": {"paper_id": "addd84f7cbf5e5626a1ebf08f5edb58fc61147f5", "abstract": "Despite the many solutions proposed by industry and the research community to address phishing attacks, this problem continues to cause enormous damage. Because of our inability to deter phishing attacks, the research community needs to develop new approaches to anti-phishing solutions. Most of today's anti-phishing technologies focus on automatically detecting and preventing phishing attacks. While automation makes anti-phishing tools user-friendly, automation also makes them suffer from false positives, false negatives, and various practical hurdles. As a result, attackers often find simple ways to escape automatic detection.\n This paper presents iTrustPage - an anti-phishing tool that does not rely completely on automation to detect phishing. Instead, iTrustPage relies on user input and external repositories of information to prevent users from filling out phishing Web forms. With iTrustPage, users help to decide whether or not a Web page is legitimate. Because iTrustPage is user-assisted, iTrustPage avoids the false positives and the false negatives associated with automatic phishing detection. We implemented iTrustPage as a downloadable extension to FireFox. After being featured on the Mozilla website for FireFox extensions, iTrustPage was downloaded by more than 5,000 users in a two week period. We present an analysis of our tool's effectiveness and ease of use based on our examination of usage logs collected from the 2,050 users who used iTrustPage for more than two weeks. Based on these logs, we find that iTrustPage disrupts users on fewer than 2% of the pages they visit, and the number of disruptions decreases over time.", "title": "Itrustpage: a user-assisted anti-phishing tool"}, "42c1751640f6e37e7058b94adfcbf5b5fd0ac056": {"paper_id": "42c1751640f6e37e7058b94adfcbf5b5fd0ac056", "abstract": "Alendronate, an inhibitor of bone resorption, is widely used in osteoporosis treatment. However, concerns have been raised about potential oversuppression of bone turnover during long-term use. We report on nine patients who sustained spontaneous nonspinal fractures while on alendronate therapy, six of whom displayed either delayed or absent fracture healing for 3 months to 2 yr during therapy. Histomorphometric analysis of the cancellous bone showed markedly suppressed bone formation, with reduced or absent osteoblastic surface in most patients. Osteoclastic surface was low or low-normal in eight patients, and eroded surface was decreased in four. Matrix synthesis was markedly diminished, with absence of double-tetracycline label and absent or reduced single-tetracycline label in all patients. The same trend was seen in the intracortical and endocortical surfaces. Our findings raise the possibility that severe suppression of bone turnover may develop during long-term alendronate therapy, resulting in increased susceptibility to, and delayed healing of, nonspinal fractures. Although coadministration of estrogen or glucocorticoids appears to be a predisposing factor, this apparent complication can also occur with monotherapy. Our observations emphasize the need for increased awareness and monitoring for the potential development of excessive suppression of bone turnover during long-term alendronate therapy.", "title": "Severely suppressed bone turnover: a potential complication of alendronate therapy."}, "07c00639d498de8b3aa0c3ee50dd939cd03fce7d": {"paper_id": "07c00639d498de8b3aa0c3ee50dd939cd03fce7d", "abstract": "OBJECTIVES/HYPOTHESIS\nTo establish the rate of inflammatory reaction to hyaluronic acid (HA) in vocal fold injection augmentation, determine the most common presenting signs and symptoms, and propose an etiology.\n\n\nSTUDY DESIGN\nRetrospective chart review.\n\n\nMETHODS\nPatients injected with HA over a 5-year period were reviewed to identify those who had a postoperative inflammatory reaction. Medical records were reviewed for patient demographic information, subjective complaints, Voice Handicap Index-10 (VHI-10) scores, medical intervention, and resolution time. Videolaryngostroboscopy examinations were also evaluated.\n\n\nRESULTS\nA total of 186 patients (245 vocal folds) were injected with HA over a 5-year period, with a postoperative inflammatory reaction rate of 3.8%. The most common complaints in these patients were odynophagia, dysphonia, and dyspnea with vocal fold erythema, edema, and loss of pliability on videolaryngostroboscopy. All patients were treated with corticosteroids. Return of vocal fold vibration ranged from 3 weeks to 26 months, with VHI-10 scores normalizing in 50% of patients.\n\n\nCONCLUSIONS\nThis reaction may be a form of hypersensitivity related to small amounts of protein linked to HA. Alternatively, extravascular compression from the HA could lead to venous congestion of the vocal fold. The possibility of equipment contamination is also being investigated. Further studies are needed to determine the etiology and best treatment.\n\n\nLEVEL OF EVIDENCE\n4 Laryngoscope, 2016 127:445-449, 2017.", "title": "Inflammatory reaction to hyaluronic acid: A newly described complication in vocal fold augmentation."}, "3fc5f81865c50c584263bf00802ebb4cb4e9e99c": {"paper_id": "3fc5f81865c50c584263bf00802ebb4cb4e9e99c", "abstract": "Soft tissue filler injection has been a very common procedure worldwide since filler injection was first introduced for soft tissue augmentation. Currently, filler is used in various medical fields with satisfactory results, but the number of complications is increasing due to the increased use of filler. The complications after filler injection can occur at any time after the procedure, early and delayed, and they range from minor to severe. In this review, based on our experience and previously published other articles, we suggest a treatment algorithm to help wound healing and tissue regeneration and generate good aesthetic results with early treatment in response to the side effects of filler. Familiarity with the treatment of these rare complications is essential for achieving the best possible outcome.", "title": "Treatment Algorithm of Complications after Filler Injection: Based on Wound Healing Process"}, "2ddb3574653e59979f10c1e35ef2f26f877f10db": {"paper_id": "2ddb3574653e59979f10c1e35ef2f26f877f10db", "abstract": "All fillers are associated with the risk of both early and late complications. Early side effects such as swelling, redness, and bruising occur after intradermal or subdermal injections. The patient has to be aware of and accept these risks. Adverse events that last longer than 2 weeks can be attributable to technical shortcomings (e.g., too superficial an implantation of a long-lasting filler substance). Such adverse events can be treated with intradermal 5-fluorouracil, steroid injections, vascular lasers, or intense pulsed light, and later with dermabrasion or shaving. Late adverse events also include immunologic phenomena such as late-onset allergy and nonallergic foreign body granuloma. Both react well to intralesional steroid injections, which often have to be repeated to establish the right dose. Surgical excisions shall remain the last option and are indicated for hard lumps in the lips and visible hard nodules or hard granuloma in the subcutaneous fat.", "title": "Avoiding and treating dermal filler complications."}, "e9cd502c380875f35dae33648744fb1deb4253f4": {"paper_id": "e9cd502c380875f35dae33648744fb1deb4253f4", "abstract": "For over five decades, liquid injectable silicone has been used for soft-tissue augmentation. Its use has engendered polarized reactions from the public and from physicians. Adherents of this product tout its inert chemical structure, ease of use, and low cost. Opponents of silicone cite the many reports of complications, including granulomas, pneumonitis, and disfiguring nodules that are usually the result of large-volume injection and/or industrial grade or adulterated material. Unfortunately, as recently as 2006, reports in The New England Journal of Medicine and The New York Times failed to distinguish between the use of medical grade silicone injected by physicians trained in the microdroplet technique and the use of large volumes of industrial grade products injected by unlicensed or unskilled practitioners. This review separates these two markedly different procedures. In addition, it provides an overview of the chemical structure of liquid injectable silicone, the immunology of silicone reactions within the body, treatment for cosmetic improvement including human immunodeficiency virus lipoatrophy, technical considerations for its injection, complications seen following injections, and some considerations of the future for silicone soft-tissue augmentation.", "title": "Liquid injectable silicone: a review of its history, immunology, technical considerations, complications, and potential."}, "a55d01c2a6d33e8dff01d4eab89b941465d288ba": {"paper_id": "a55d01c2a6d33e8dff01d4eab89b941465d288ba", "abstract": "IMPORTANCE\nEven when administered by experienced hands, injectable soft-tissue fillers can cause various unintended reactions, ranging from minor and self-limited responses to severe complications requiring prompt treatment and close follow-up.\n\n\nOBJECTIVES\nTo review the complications associated with injectable soft-tissue filler treatments administered in the Williams Rejuva Center during a 5-year period and to discuss their management.\n\n\nDESIGN AND SETTING\nRetrospective medical record review in a private practice setting.\n\n\nPARTICIPANTS\nPatients receiving injectable soft-tissue fillers and having a treatment-related complication.\n\n\nINTERVENTIONS\nInjectable soft-tissue filler treatments.\n\n\nMAIN OUTCOME MEASURES\nA retrospective medical record review was conducted of patients undergoing treatment with injectable soft-tissue fillers between January 1, 2007, and December 31, 2011, and identified as having a treatment-related complication.\n\n\nRESULTS\nA total of 2089 injectable soft-tissue filler treatments were performed during the study period, including 1047 with hyaluronic acid, 811 with poly-L-lactic acid, and 231 with calcium hydroxylapatite. Fourteen complications were identified. The most common complication was nodule or granuloma formation. Treatment with calcium hydroxylapatite had the highest complication rate.\n\n\nCONCLUSIONS AND RELEVANCE\nComplications are rare following treatment with injectable soft-tissue fillers. Nevertheless, it is important to be aware of the spectrum of potential adverse sequelae and to be comfortable with their proper management.\n\n\nLEVEL OF EVIDENCE\n4.", "title": "Complications associated with injectable soft-tissue fillers: a 5-year retrospective review."}, "187f0aa5cb20d91de4bfdf8f15236aa8c28b25d2": {"paper_id": "187f0aa5cb20d91de4bfdf8f15236aa8c28b25d2", "abstract": "The requirements for the interoperability of semantics and knowledge have become increasingly important in Product Lifecycle Management (PLM), in the drive towards knowledge-driven decision support in the manufacturing industry. This article presents a novel concept, based on the Model Driven Architecture (MDA). The concept has been implemented under the Interoperable Manufacturing Knowledge Systems (IMKS) project in order to understand the extent to which manufacturing system interoperability can be supported using radically new methods of knowledge sharing. The concept exploits the capabilities of semantically well-defined core concepts formalised in a Common Logic-based ontology language. The core semantics can be specialised to configure multiple application-specific knowledge bases, as well as product and manufacturing information platforms. Furthermore, the utilisation of the expressive ontology language and the generic nature of core concepts help support the specification of system mechanisms to enable the verification of knowledge across multiple platforms. An experimental demonstration, using a test case based on the design and manufacture of an aerospace part, has been realised. This has led to the identification of several benefits of the approach, its current limitations as well as the areas to be considered for further work. Crown Copyright 2013 Published by Elsevier B.V. All rights reserved.", "title": "A model-driven ontology approach for manufacturing system interoperability and knowledge sharing"}, "24b4076e2f58325f5d86ba1ca1f00b08a56fb682": {"paper_id": "24b4076e2f58325f5d86ba1ca1f00b08a56fb682", "abstract": "This paper is intended to serve as a comprehensive introduction to the emerging eld concerned with the design and use of ontologies. We observe that disparate backgrounds, languages, tools, and techniques are a major barrier to e ective communication among people, organisations, and/or software systems. We show how the development and implementation of an explicit account of a shared understanding (i.e. an `ontology') in a given subject area, can improve such communication, which in turn, can give rise to greater reuse and sharing, inter-operability, and more reliable software. After motivating their need, we clarify just what ontologies are and what purposes they serve. We outline a methodology for developing and evaluating ontologies, rst discussing informal techniques, concerning such issues as scoping, handling ambiguity, reaching agreement and producing de nitions. We then consider the bene ts of and describe, a more formal approach. We re-visit the scoping phase, and discuss the role of formal languages and techniques in the speci cation, implementation and evaluation of ontologies. Finally, we review the state of the art and practice in this emerging eld, considering various case studies, software tools for ontology development, key research issues and future prospects. AIAI-TR-191 Ontologies Page i", "title": "Ontologies: principles, methods and applications"}, "1524aab9e8d3dcf24f5bb5431fbf8d7d2ab1d438": {"paper_id": "1524aab9e8d3dcf24f5bb5431fbf8d7d2ab1d438", "abstract": "This paper aims at identifying some of the key factors in adopting or running a company-wide software reuse program. Key factors are derived from empirical evidence of reuse practices, as emerged from a survey of projects for the introduction of reuse in European companies: 24 such projects performed from 1994 to 1997 were analysed using structured interviews. The projects were undertaken in both large and small companies, working in a variety of business domains, and using both object-oriented and procedural development approaches . Most of them produce software with high commonality between applications, and have at least reasonably mature processes . Despite that apparent potential for success, around one-third of the projects failed. Three main causes of ailure were not introducing reuse-specific processes, not modifying non-reuse processes , and not considering human factors. The root cause was a lack of commitment by top management, or non-awareness of the importance of those factors, often coupled with the belief that using the object-oriented approach or setting up a repository seamlessly is all that is necessary to achieve success in reuse. Conversely, successes were achieved when, given a potential for reuse because of commonality among applications, management committed to introducing reuse processes, modifying non-reuse processes, and addressing human factors. While addressing those three issues turned out to be essential, the lower-level details of how to address them varied greatly: for instance, companies produced large-grained or small-grained reusable assets, did or did not perform domain analysis , did or did not use dedicated reuse groups, used specific tools for the repository or no tools. As far as these choices are concerned, the key point seems to be the sustainability of the approach and its suitability to the context of the company.", "title": "Success and Failure Factors in Software Reuse"}, "f30fe8b675bd0d11dc4c2bd4dcaa05b8b7599010": {"paper_id": "f30fe8b675bd0d11dc4c2bd4dcaa05b8b7599010", "abstract": "Cooperative driving behavior is essential for driving in traffic, especially for ramp merging, lane changing or navigating intersections. Autonomous vehicles should also manage these situations by behaving cooperatively and naturally. The challenge of cooperative driving is estimating other vehicles' intentions. In this paper, we present a novel method to estimate other human-driven vehicles' intentions with the aim of achieving a natural and amenable cooperative driving behavior, without using wireless communication. The new approach allows the autonomous vehicle to cooperate with multiple observable merging vehicles on the ramp with a leading vehicle ahead of the autonomous vehicle in the same lane. To avoid calculating trajectories, simplify computation, and take advantage of mature Level-3 components, the new method reacts to merging cars by determining a following target for an off-the-shelf distance keeping module (ACC) which governs speed control of the autonomous vehicle. We train and evaluate the proposed model using real traffic data. Results show that the new approach has a lower collision rate than previous methods and generates more human driver-like behaviors in terms of trajectory similarity and time-to-collision to leading vehicles.", "title": "Interactive ramp merging planning in autonomous driving: Multi-merging leading PGM (MML-PGM)"}, "fb65afff7fdc672af3bb2b9a75bc804beb57cf90": {"paper_id": "fb65afff7fdc672af3bb2b9a75bc804beb57cf90", "abstract": "Human drivers use nonverbal communication and anticipation of other drivers' actions to master conflicts occurring in everyday driving situations. Without a high penetration of vehicle-to-vehicle communication an autonomous vehicle has to have the possibility to understand intentions of others and share own intentions with the surrounding traffic participants. This paper proposes a cooperative combinatorial motion planning algorithm without the need for inter vehicle communication based on Monte Carlo Tree Search (MCTS). We motivate why MCTS is particularly suited for the autonomous driving domain. Furthermore, adoptions to the MCTS algorithm are presented as for example simultaneous decisions, the usage of the Intelligent Driver Model as microscopic traffic simulation, and a cooperative cost function. We further show simulation results of merging scenarios in highway-like situations to underline the cooperative nature of the approach.", "title": "Tactical cooperative planning for autonomous highway driving using Monte-Carlo Tree Search"}, "37f70e2caf630db60558df055f0c45377dfa9581": {"paper_id": "37f70e2caf630db60558df055f0c45377dfa9581", "abstract": "Cooperative behavior planning for automated vehicles is getting more and more attention in the research community. This paper introduces two dimensions to structure cooperative driving tasks. The authors suggest to distinguish driving tasks by the used communication channels and by the hierarchical level of cooperative skills and abilities. In this manner, this paper presents the cooperative behavior skills of \"Jack\", our automated vehicle driving from Stanford to Las Vegas in January 2015.", "title": "Structuring Cooperative Behavior Planning Implementations for Automated Driving"}, "feb16cadac0f1c02d678283525e12f7fee2ae259": {"paper_id": "feb16cadac0f1c02d678283525e12f7fee2ae259", "abstract": "Automated driving is predicted to enhance traffic safety, transport efficiency, and driver comfort. To extend the capability of current advanced driver assistance systems, and eventually realize fully automated driving, the intelligent vehicle system must have the ability to plan different maneuvers while adapting to the surrounding traffic environment. This paper presents an algorithm for longitudinal and lateral trajectory planning for automated driving maneuvers where the vehicle does not have right of way, i.e., yielding maneuvers. Such maneuvers include, e.g., lane change, roundabout entry, and intersection crossing. In the proposed approach, the traffic environment which the vehicle must traverse is incorporated as constraints on its longitudinal and lateral positions. The trajectory planning problem can thereby be formulated as two loosely coupled low-complexity model predictive control problems for longitudinal and lateral motion. Simulation results demonstrate the ability of the proposed trajectory planning algorithm to generate smooth collision-free maneuvers which are appropriate for various traffic situations.", "title": "Longitudinal and Lateral Control for Automated Yielding Maneuvers"}, "f95577df76d3a78de71ff0ced34140708bc3e2b4": {"paper_id": "f95577df76d3a78de71ff0ced34140708bc3e2b4", "abstract": "Safe handling of dynamic highway and inner city scenarios with autonomous vehicles involves the problem of generating traffic-adapted trajectories. In order to account for the practical requirements of the holistic autonomous system, we propose a semi-reactive trajectory generation method, which can be tightly integrated into the behavioral layer. The method realizes long-term objectives such as velocity keeping, merging, following, stopping, in combination with a reactive collision avoidance by means of optimal-control strategies within the Fren\u00e9t-Frame [12] of the street. The capabilities of this approach are demonstrated in the simulation of a typical high-speed highway scenario.", "title": "Optimal trajectory generation for dynamic street scenarios in a Fren\u00e9t Frame"}, "03bf26d72d8cc0cf401c31e31c242e1894bd0890": {"paper_id": "03bf26d72d8cc0cf401c31e31c242e1894bd0890", "abstract": "On-road motion planning for autonomous vehicles is in general a challenging problem. Past efforts have proposed solutions for urban and highway environments individually. We identify the key advantages/shortcomings of prior solutions, and propose a novel two-step motion planning system that addresses both urban and highway driving in a single framework. Reference Trajectory Planning (I) makes use of dense lattice sampling and optimization techniques to generate an easy-to-tune and human-like reference trajectory accounting for road geometry, obstacles and high-level directives. By focused sampling around the reference trajectory, Tracking Trajectory Planning (II) generates, evaluates and selects parametric trajectories that further satisfy kinodynamic constraints for execution. The described method retains most of the performance advantages of an exhaustive spatiotemporal planner while significantly reducing computation.", "title": "Focused Trajectory Planning for autonomous on-road driving"}, "9041de4e57910263ba3e8ea4cdafbe319071051c": {"paper_id": "9041de4e57910263ba3e8ea4cdafbe319071051c", "abstract": "A system, particularly a decision-making concept, that facilitates highly automated driving on freeways in real traffic is presented. The system is capable of conducting fully automated lane change (LC) maneuvers with no need for driver approval. Due to the application in real traffic, a robust functionality and the general safety of all traffic participants are among the main requirements. Regarding these requirements, the consideration of measurement uncertainties demonstrates a major challenge. For this reason, a fully integrated probabilistic concept is developed. By means of this approach, uncertainties are regarded in the entire process of determining driving maneuvers. While this also includes perception tasks, this contribution puts a focus on the driving strategy and the decision-making process for the execution of driving maneuvers. With this approach, the BMW Group Research and Technology managed to drive 100% automated in real traffic on the freeway A9 from Munich to Ingolstadt, showing a robust, comfortable, and safe driving behavior, even during multiple automated LC maneuvers.", "title": "Highly Automated Driving on Freeways in Real Traffic Using a Probabilistic Framework"}, "25115494d4bd188885fea21aab58618f804a9f50": {"paper_id": "25115494d4bd188885fea21aab58618f804a9f50", "abstract": "This paper describes the design of an optimal-control-based active safety framework that performs trajectory planning, threat assessment, and semiautonomous control of passenger vehicles in hazard avoidance scenarios. The vehicle navigation problem is formulated as a constrained optimal control problem with constraints bounding a navigable region of the road surface. A model predictive controller iteratively plans an optimal vehicle trajectory through the constrained corridor. Metrics from this \u201cbest-case\u201d scenario establish the minimum threat posed to the vehicle given its current state. Based on this threat assessment, the level of controller intervention required to prevent departure from the navigable corridor is calculated and driver/controller inputs are scaled accordingly. This approach minimizes controller intervention while ensuring that the vehicle does not depart from a navigable corridor of travel. It also allows for multiple actuation modes, diverse trajectory-planning objectives, and varying levels of autonomy. Experimental results are presented here to demonstrate the framework\u2019s semiautonomous performance in hazard avoidance scenarios.", "title": "Experimental Study of an Optimal-Control- Based Framework for Trajectory Planning, Threat Assessment, and Semi-Autonomous Control of Passenger Vehicles in Hazard Avoidance Scenarios"}, "68c0f511121d61ae8089be2f162246f5cac7f40f": {"paper_id": "68c0f511121d61ae8089be2f162246f5cac7f40f", "abstract": "This paper presents the design and first test on a simulator of a vehicle trajectory-planning algorithm that adapts to traffic on a lane-structured infrastructure such as highways. The proposed algorithm is designed to run on a fail-safe embedded environment with low computational power, such as an engine control unit, to be implementable in commercial vehicles of the near future. The target platform has a clock frequency of less than 150 MHz, 150 kB RAM of memory, and a 3-MB program memory. The trajectory planning is performed by a two-step algorithm. The first step defines the feasible maneuvers with respect to the environment, aiming at minimizing the risk of a collision. The output of this step is a target group of maneuvers in the longitudinal direction (accelerating or decelerating), in the lateral direction (changing lanes), and in the combination of both directions. The second step is a more detailed evaluation of several possible trajectories within these maneuvers. The trajectories are optimized to additional performance indicators such as travel time, traffic rules, consumption, and comfort. The output of this module is a trajectory in the vehicle frame that represents the recommended vehicle state (position, heading, speed, and acceleration) for the following seconds.", "title": "Maneuver-Based Trajectory Planning for Highly Autonomous Vehicles on Real Road With Traffic and Driver Interaction"}, "1c0fb6b1bbfde0f9bab6268f5609cce2bd3bc5bd": {"paper_id": "1c0fb6b1bbfde0f9bab6268f5609cce2bd3bc5bd", "abstract": "Chris Urmson, Joshua Anhalt, Drew Bagnell, Christopher Baker, Robert Bittner, M. N. Clark, John Dolan, Dave Duggins, Tugrul Galatali, Chris Geyer, Michele Gittleman, Sam Harbaugh, Martial Hebert, Thomas M. Howard, Sascha Kolski, Alonzo Kelly, Maxim Likhachev, Matt McNaughton, Nick Miller, Kevin Peterson, Brian Pilnick, Raj Rajkumar, Paul Rybski, Bryan Salesky, Young-Woo Seo, Sanjiv Singh, Jarrod Snider, Anthony Stentz, William \u201cRed\u201d Whittaker, Ziv Wolkowicki, and Jason Ziglar Carnegie Mellon University Pittsburgh, Pennsylvania 15213 e-mail: curmson@ri.cmu.edu", "title": "Autonomous Driving in Urban Environments: Boss and the Urban Challenge"}, "07186dd168d2a8add853cc9fdedf1376a77a7ac8": {"paper_id": "07186dd168d2a8add853cc9fdedf1376a77a7ac8", "abstract": "Future driver assistance systems are likely to use a multisensor approach with heterogeneous sensors for tracking dynamic objects around the vehicle. The quality and type of data available for a data fusion algorithm depends heavily on the sensors detecting an object. This article presents a general framework which allows the use sensor specific advantages while abstracting the specific details of a sensor. Different tracking models are used depending on the current set of sensors detecting the object. A sensor independent algorithm for classifying objects regarding their current and past movement state is presented. The described architecture and algorithms have been successfully implemented in Tartan racingpsilas autonomous vehicle for the urban grand challenge. Results are presented and discussed.", "title": "Classification and tracking of dynamic objects with multiple sensors for autonomous driving in urban environments"}, "551fe3c247e2201f659ccd4e6fe933beb7d452ea": {"paper_id": "551fe3c247e2201f659ccd4e6fe933beb7d452ea", "abstract": "Classical object tracking approaches use a Kalman-filter with a single dynamic model which is therefore optimised to a single driving maneuver. In contrast the interacting multiple model (IMM) filter allows for several parallel models which are combined to a weighted estimate. Choosing models for different driving modes, such as constant speed, acceleration and strong acceleration changes, the object state estimation can be optimised for highly dynamic driving maneuvers. The paper describes the analysis of Stop&Go situations and the systematic parametrisation of the IMM method based on these statistics. The evaluation of the IMM approach is presented based on real sensor measurements of laser scanners, a radar and a video image processing unit.", "title": "IMM object tracking for high dynamic driving maneuvers"}, "502cb1014c79b40fb3bbe465c302d1f93965e69a": {"paper_id": "502cb1014c79b40fb3bbe465c302d1f93965e69a", "abstract": "In this paper, we propose a novel planning framework that can greatly improve the level of intelligence and driving quality of autonomous vehicles. A reference planning layer first generates kinematically and dynamically feasible paths assuming no obstacles on the road, then a behavioral planning layer takes static and dynamic obstacles into account. Instead of directly commanding a desired trajectory, it searches for the best directives for the controller, such as lateral bias and distance keeping aggressiveness. It also considers the social cooperation between the autonomous vehicle and surrounding cars. Based on experimental results from both simulation and a real autonomous vehicle platform, the proposed behavioral planning architecture improves the driving quality considerably, with a 90.3% reduction of required computation time in representative scenarios.", "title": "A behavioral planning framework for autonomous driving"}, "83a292a56e5df5c39e249719b78112588d34990d": {"paper_id": "83a292a56e5df5c39e249719b78112588d34990d", "abstract": "For decades, humans have dreamed of making cars that could drive themselves, so that travel would be less taxing, and the roads safer for everyone. Toward this goal, we have made strides in motion planning algorithms for autonomous cars, using a powerful new computing tool, the parallel graphics processing unit (GPU). We propose a novel five-dimensional search space formulation that includes both spatial and temporal dimensions, and respects the kinematic and dynamic constraints on a typical automobile. With this formulation, the search space grows linearly with the length of the path, compared to the exponential growth of other methods. We also propose a parallel search algorithm, using the GPU to tackle the curse of dimensionality directly and increase the number of plans that can be evaluated by an order of magnitude compared to a CPU implementation. With this larger capacity, we can evaluate a dense sampling of plans combining lateral swerves and accelerations that represent a range of effective responses to more on-road driving scenarios than have previously been addressed in the literature. We contribute a cost function that evaluates many aspects of each candidate plan, ranking them all, and allowing the behavior of the vehicle to be fine-tuned by changing the ranking. We show that the cost function can be changed on-line by a behavioral planning layer to express preferred vehicle behavior without the brittleness induced by top-down planning architectures. Our method is particularly effective at generating robust merging behaviors, which have traditionally required a delicate and failure-prone coordination between multiple planning layers. Finally, we demonstrate our proposed planner in a variety of on-road driving scenarios in both simulation and on an autonomous SUV, and make a detailed comparison with prior work.", "title": "Parallel Algorithms for Real-time Motion Planning"}, "6ac15e819701cd0d077d8157711c4c402106722c": {"paper_id": "6ac15e819701cd0d077d8157711c4c402106722c", "abstract": "This technical report describes Team MIT's approach to the DARPA Urban Challenge. We have developed a novel strategy for using many inexpensive sensors, mounted on the vehicle periphery, and calibrated with a new cross\u00admodal calibration technique. Lidar, camera, and radar data streams are processed using an innovative, locally smooth state representation that provides robust perception for real\u00ad time autonomous control. A resilient planning and control architecture has been developed for driving in traffic, comprised of an innovative combination of well\u00adproven algorithms for mission planning, situational planning, situational interpretation, and trajectory control. These innovations are being incorporated in two new robotic vehicles equipped for autonomous driving in urban environments, with extensive testing on a DARPA site visit course. Experimental results demonstrate all basic navigation and some basic traffic behaviors, including unoccupied autonomous driving, lane following using pure\u00adpursuit control and our local frame perception strategy, obstacle avoidance using kino\u00addynamic RRT path planning, U\u00adturns, and precedence evaluation amongst other cars at intersections using our situational interpreter. We are working to extend these approaches to advanced navigation and traffic scenarios. \u2020 Executive Summary This technical report describes Team MIT's approach to the DARPA Urban Challenge. We have developed a novel strategy for using many inexpensive sensors, mounted on the vehicle periphery, and calibrated with a new cross-modal calibration technique. Lidar, camera, and radar data streams are processed using an innovative, locally smooth state representation that provides robust perception for real-time autonomous control. A resilient planning and control architecture has been developed for driving in traffic, comprised of an innovative combination of well-proven algorithms for mission planning, situational planning, situational interpretation, and trajectory control. These innovations are being incorporated in two new robotic vehicles equipped for autonomous driving in urban environments, with extensive testing on a DARPA site visit course. Experimental results demonstrate all basic navigation and some basic traffic behaviors, including unoccupied autonomous driving, lane following using pure-pursuit control and our local frame perception strategy, obstacle avoidance using kino-dynamic RRT path planning, U-turns, and precedence evaluation amongst other cars at intersections using our situational interpreter. We are working to extend these approaches to advanced navigation and traffic scenarios. DISCLAIMER: The information contained in this paper does not represent the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency (DARPA) or the Department of Defense. DARPA does not guarantee the accuracy or reliability of the information in this paper. Additional support \u2026", "title": "Team MIT Urban Challenge Technical Report"}, "059dd922dbe3cf55a67b25619d927798976df841": {"paper_id": "059dd922dbe3cf55a67b25619d927798976df841", "abstract": "Deep learning usually requires large amounts of labeled training data, but annotating data is costly and tedious. The framework of semi-supervised learning provides the means to use both labeled data and arbitrary amounts of unlabeled data for training. Recently, semisupervised deep learning has been intensively studied for standard CNN architectures. However, Fully Convolutional Networks (FCNs) set the state-of-the-art for many image segmentation tasks. To the best of our knowledge, there is no existing semi-supervised learning method for such FCNs yet. We lift the concept of auxiliary manifold embedding for semisupervised learning to FCNs with the help of Random Feature Embedding. In our experiments on the challenging task of MS Lesion Segmentation, we leverage the proposed framework for the purpose of domain adaptation and report substantial improvements over the baseline model.", "title": "Semi-supervised Deep Learning for Fully Convolutional Networks"}, "1b2c6fb85e7e776b533fda63f92fabb04ed5e887": {"paper_id": "1b2c6fb85e7e776b533fda63f92fabb04ed5e887", "abstract": "MatConvNet is an open source implementation of Convolutional Neural Networks (CNNs) with a deep integration in the MATLAB environment. The toolbox is designed with an emphasis on simplicity and flexibility. It exposes the building blocks of CNNs as easy-to-use MATLAB functions, providing routines for computing convolutions with filter banks, feature pooling, normalisation, and much more. MatConvNet can be easily extended, often using only MATLAB code, allowing fast prototyping of new CNN architectures. At the same time, it supports efficient computation on CPU and GPU, allowing to train complex models on large datasets such as ImageNet ILSVRC containing millions of training examples", "title": "MatConvNet: Convolutional Neural Networks for MATLAB"}, "1827de6fa9c9c1b3d647a9d707042e89cf94abf0": {"paper_id": "1827de6fa9c9c1b3d647a9d707042e89cf94abf0", "abstract": "Training Deep Neural Networks is complicated by the fact that the distribution of each layer\u2019s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a stateof-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.", "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"}, "376b078694f0c183e4832900debda4dfed021a9a": {"paper_id": "376b078694f0c183e4832900debda4dfed021a9a", "abstract": "This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [5], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [13].", "title": "Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps"}, "80bcdd5d82f03e4d2bca28cbc1399424dac138f9": {"paper_id": "80bcdd5d82f03e4d2bca28cbc1399424dac138f9", "abstract": "VLFeat is an open and portable library of computer vision algorithms. It aims at facilitating fast prototyping and reproducible research for computer vision scientists and students. It includes rigorous implementations of common building blocks such as feature detectors, feature extractors, (hierarchical) k-means clustering, randomized kd-tree matching, and super-pixelization. The source code and interfaces are fully documented. The library integrates directly with MATLAB, a popular language for computer vision research.", "title": "Vlfeat: an open and portable library of computer vision algorithms"}, "71b5e8d9970f3b725127eb7da3d5183b921ad017": {"paper_id": "71b5e8d9970f3b725127eb7da3d5183b921ad017", "abstract": "Magnetic resonance (MR) imaging is often used to characterize and quantify multiple sclerosis (MS) lesions in the brain and spinal cord. The number and volume of lesions have been used to evaluate MS disease burden, to track the progression of the disease and to evaluate the effect of new pharmaceuticals in clinical trials. Accurate identification of MS lesions in MR images is extremely difficult due to variability in lesion location, size and shape in addition to anatomical variability between subjects. Since manual segmentation requires expert knowledge, is time consuming and is subject to intra- and inter-expert variability, many methods have been proposed to automatically segment lesions. The objective of this study was to carry out a systematic review of the literature to evaluate the state of the art in automated multiple sclerosis lesion segmentation. From 1240 hits found initially with PubMed and Google scholar, our selection criteria identified 80 papers that described an automatic lesion segmentation procedure applied to MS. Only 47 of these included quantitative validation with at least one realistic image. In this paper, we describe the complexity of lesion segmentation, classify the automatic MS lesion segmentation methods found, and review the validation methods applied in each of the papers reviewed. Although many segmentation solutions have been proposed, including some with promising results using MRI data obtained on small groups of patients, no single method is widely employed due to performance issues related to the high variability of MS lesion appearance and differences in image acquisition. The challenge remains to provide segmentation techniques that work in all cases regardless of the type of MS, duration of the disease, or MRI protocol, and this within a comprehensive, standardized validation framework. MS lesion segmentation remains an open problem.", "title": "Review of automatic segmentation methods of multiple sclerosis white matter lesions on conventional magnetic resonance imaging"}, "7d5aedecdfc4e8f83638bac47eb7cf2f860ec51c": {"paper_id": "7d5aedecdfc4e8f83638bac47eb7cf2f860ec51c", "abstract": "The finite mixture (FM) model is the most commonly used model for statistical segmentation of brain magnetic resonance (MR) images because of its simple mathematical form and the piecewise constant nature of ideal brain MR images. However, being a histogram-based model, the FM has an intrinsic limitation-no spatial information is taken into account. This causes the FM model to work only on well-defined images with low levels of noise; unfortunately, this is often not the the case due to artifacts such as partial volume effect and bias field distortion. Under these conditions, FM model-based methods produce unreliable results. Here, the authors propose a novel hidden Markov random field (HMRF) model, which is a stochastic process generated by a MRF whose state sequence cannot be observed directly but which can be indirectly estimated through observations. Mathematically, it can be shown that the FM model is a degenerate version of the HMRF model. The advantage of the HMRF model derives from the way in which the spatial information is encoded through the mutual influences of neighboring sites. Although MRF modeling has been employed in MR image segmentation by other researchers, most reported methods are limited to using MRF as a general prior in an FM model-based approach. To fit the HMRF model, an EM algorithm is used. The authors show that by incorporating both the HMRF model and the EM algorithm into a HMRF-EM framework, an accurate and robust segmentation can be achieved. More importantly, the HMRF-EM framework can easily be combined with other techniques. As an example, the authors show how the bias field correction algorithm of Guillemaud and Brady (1997) can be incorporated into this framework to achieve a three-dimensional fully automated approach for brain MR image segmentation.", "title": "Segmentation of brain MR images through a hidden Markov random field model and the expectation-maximization algorithm"}, "b6cfa7fdeeaa1607ea96f92b5d198fe5461187cb": {"paper_id": "b6cfa7fdeeaa1607ea96f92b5d198fe5461187cb", "abstract": "We present a novel algorithm for fuzzy segmentation of magnetic resonance imaging (MRI) data and estimation of intensity inhomogeneities using fuzzy logic. MRI intensity inhomogeneities can be attributed to imperfections in the radio-frequency coils or to problems associated with the acquisition sequences. The result is a slowly varying shading artifact over the image that can produce errors with conventional intensity-based classification. Our algorithm is formulated by modifying the objective function of the standard fuzzy c-means (FCM) algorithm to compensate for such inhomogeneities and to allow the labeling of a pixel (voxel) to be influenced by the labels in its immediate neighborhood. The neighborhood effect acts as a regularizer and biases the solution toward piecewise-homogeneous labelings. Such a regularization is useful in segmenting scans corrupted by salt and pepper noise. Experimental results on both synthetic images and MR data are given to demonstrate the effectiveness and efficiency of the proposed algorithm.", "title": "A modified fuzzy c-means algorithm for bias field estimation and segmentation of MRI data"}, "0c7f1d285ce069b2f7a807a4b2750695098bffe6": {"paper_id": "0c7f1d285ce069b2f7a807a4b2750695098bffe6", "abstract": "We present a semi-supervised learning framework based on graph embeddings. Given a graph between instances, we train an embedding for each instance to jointly predict the class label and the neighborhood context in the graph. We develop both transductive and inductive variants of our method. In the transductive variant of our method, the class labels are determined by both the learned embeddings and input feature vectors, while in the inductive variant, the embeddings are defined as a parametric function of the feature vectors, so predictions can be made on instances not seen during training. On a large and diverse set of benchmark tasks, including text classification, distantly supervised entity extraction, and entity classification, we show improved performance over many of the existing models.", "title": "Revisiting Semi-Supervised Learning with Graph Embeddings"}, "04b52c8230c3f9f4f4032b06458069d81c8f07b2": {"paper_id": "04b52c8230c3f9f4f4032b06458069d81c8f07b2", "abstract": "We consider the problem of embedding entities and relationships of multirelational data in low-dimensional vector spaces. Our objective is to propose a canonical model which is easy to train, contains a reduced number of parameters and can scale up to very large databases. Hence, we propose TransE, a method which models relationships by interpreting them as translations operating on the low-dimensional embeddings of the entities. Despite its simplicity, this assumption proves to be powerful since extensive experiments show that TransE significantly outperforms state-of-the-art methods in link prediction on two knowledge bases. Besides, it can be successfully trained on a large scale data set with 1M entities, 25k relationships and more than 17M training samples.", "title": "Translating Embeddings for Modeling Multi-relational Data"}, "7b355eafb5e28dfec84dba88f435fc5cc30a3e2f": {"paper_id": "7b355eafb5e28dfec84dba88f435fc5cc30a3e2f", "abstract": "Because of polysemy, distant labeling for information extraction leads to noisy training data. We describe a procedure for reducing this noise by using label propagation on a graph in which the nodes are entity mentions, and mentions are coupled when they occur in coordinate list structures. We show that this labeling approach leads to good performance even when off-the-shelf classifiers are used on the distantly-labeled data.", "title": "Improving Distant Supervision for Information Extraction Using Label Propagation Through Lists"}, "105c153817724a8c5eaffe7065fb8e0cb1cb4b15": {"paper_id": "105c153817724a8c5eaffe7065fb8e0cb1cb4b15", "abstract": "Network models are widely used to represent relations between interacting units or actors. Network data often exhibit transitivity, meaning that two actors that have ties to a third actor are more likely to be tied than actors that do not, homophily by attributes of the actors or dyads, and clustering. Interest often focuses on finding clusters of actors or ties, and the number of groups in the data is typically unknown. We propose a new model, the latent position cluster model , under which the probability of a tie between two actors depends on the distance between them in an unobserved Euclidean \u2018social space\u2019, and the actors\u2019 locations in the latent social space arise from a mixture of distributions, each corresponding to a cluster. We propose two estimation methods: a two-stage maximum likelihood method and a fully Bayesian method that uses Markov chain Monte Carlo sampling. The former is quicker and simpler, but the latter performs better. We also propose a Bayesian way of determining the number of clusters that are present by using approximate conditional Bayes factors. Our model represents transitivity, homophily by attributes and clustering simultaneously and does not require the number of clusters to be known. The model makes it easy to simulate realistic networks with clustering, which are potentially useful as inputs to models of more complex systems of which the network is part, such as epidemic models of infectious disease. We apply the model to two networks of social relations. A free software package in the R statistical language, latentnet, is available to analyse data by using the model.", "title": "Model-based clustering for social networks"}, "0834e74304b547c9354b6d7da6fa78ef47a48fa8": {"paper_id": "0834e74304b547c9354b6d7da6fa78ef47a48fa8", "abstract": "This paper studies the problem of embedding very large information networks into low-dimensional vector spaces, which is useful in many tasks such as visualization, node classification, and link prediction. Most existing graph embedding methods do not scale for real world information networks which usually contain millions of nodes. In this paper, we propose a novel network embedding method called the ``LINE,'' which is suitable for arbitrary types of information networks: undirected, directed, and/or weighted. The method optimizes a carefully designed objective function that preserves both the local and global network structures. An edge-sampling algorithm is proposed that addresses the limitation of the classical stochastic gradient descent and improves both the effectiveness and the efficiency of the inference. Empirical experiments prove the effectiveness of the LINE on a variety of real-world information networks, including language networks, social networks, and citation networks. The algorithm is very efficient, which is able to learn the embedding of a network with millions of vertices and billions of edges in a few hours on a typical single machine. The source code of the LINE is available online\\footnote{\\url{https://github.com/tangjianpku/LINE}}.", "title": "LINE: Large-scale Information Network Embedding"}, "19bb0dce99466077e9bc5a2ad4941607fc28b40c": {"paper_id": "19bb0dce99466077e9bc5a2ad4941607fc28b40c", "abstract": "We propose a family of learning algorithms based on a new form f regularization that allows us to exploit the geometry of the marginal distribution. We foc us on a semi-supervised framework that incorporates labeled and unlabeled data in a general-p u pose learner. Some transductive graph learning algorithms and standard methods including Suppor t Vector Machines and Regularized Least Squares can be obtained as special cases. We utilize pr op rties of Reproducing Kernel Hilbert spaces to prove new Representer theorems that provide theor e ical basis for the algorithms. As a result (in contrast to purely graph-based approaches) we ob tain a natural out-of-sample extension to novel examples and so are able to handle both transductive and truly semi-supervised settings. We present experimental evidence suggesting that our semiupervised algorithms are able to use unlabeled data effectively. Finally we have a brief discuss ion of unsupervised and fully supervised learning within our general framework.", "title": "Manifold Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples"}, "02485a373142312c354b79552b3d326913eaf86d": {"paper_id": "02485a373142312c354b79552b3d326913eaf86d", "abstract": "Active and semi-supervised learning are important techniques when labeled data are scarce. We combine the two under a Gaussian random field model. Labeled and unlabeled data are represented as vertices in a weighted graph, with edge weights encoding the similarity between instances. The semi-supervised learning problem is then formulated in terms of a Gaussian random field on this graph, the mean of which is characterized in terms of harmonic functions. Active learning is performed on top of the semisupervised learning scheme by greedily selecting queries from the unlabeled data to minimize the estimated expected classification error (risk); in the case of Gaussian fields the risk is efficiently computed using matrix methods. We present experimental results on synthetic data, handwritten digit recognition, and text classification tasks. The active learning scheme requires a much smaller number of queries to achieve high accuracy compared with random query selection. 1. Introduction Semi-supervised learning targets the common situation where labeled data are scarce but unlabeled data are abundant. Under suitable assumptions, it uses unlabeled data to help supervised learning tasks. Various semi-supervised learning methods have been proposed and show promising results; Seeger (2001) gives a survey. These methods typically assume that the labeled data set is given and fixed. In practice, it may make sense to utilize active learning in conjunction with semi-supervised learning. That is, we might allow the learning algorithm to pick a set of unlabeled instances to be labeled by a domain expert, which will then be used as (or to augment) the labeled data set. In other words, if we have to label a few instances for semisupervised learning, it may be attractive to let the learning algorithm tell us which instances to label, rather than selecting them randomly. We will limit the range of query selection to the unlabeled data set, a practice known as poolbased active learning or selective sampling. There has been a great deal of research in active learning. For example, Tong and Koller (2000) select queries to minimize the version space size for support vector machines; Cohn et al. (1996) minimize the variance component of the estimated generalization error; Freund et al. (1997) employ a committee of classifiers, and query a point whenever the committee members disagree. Most of the active learning methods do not take further advantage of the large amount of unlabeled data once the queries are selected. Exceptions include McCallum and Nigam (1998) who use EM with unlabeled data integrated into the active learning, and Muslea et al. (2002) who use a semi-supervised learning method during training. In addition to this body of work from the machine learning community, there is a large literature on the closely related topic of experimental design in statistics; Chaloner and Verdinelli (1995) give a survey of experimental design from a Bayesian perspective. Recently Zhu et al. (2003) introduced a semi-supervised learning framework which is based on Gaussian random fields and harmonic functions. In this paper we demonstrate how this framework allows a combination of active learning and semi-supervised learning. In brief, the framework allows one to efficiently estimate the expected generalization error after querying a point, which leads to a better query selection criterion than naively selecting the point with maximum label ambiguity. Then, once the queries are selected and added to the labeled data set, the classifier can be trained using both the labeled and remaining unlabeled data. We present results on synthetic data, text classificaProceedings of the ICML-2003 Workshop on The Continuum from Labeled to Unlabeled Data, Washington DC, 2003. tion and image classification tasks that indicate the combination of these techniques can result in highly effective learning schemes. 2. Gaussian random fields and harmonic energy minimizing functions We begin by briefly describing the semi-supervised learning framework of Zhu et al. (2003). There are labeled points , and unlabeled points ; usually . We will use , ! to denote the labeled and unlabeled set, and \"$#% '&( the total number of points. We assume the labels are binary: *),+.-0/1 243 . We assume a connected graph 56#6 87' 9: is given with nodes 7 corresponding to the \" data points, of which the nodes in are labeled with the corresponding \u2019s. The edges 9 are represented by an \"<;=\" weight matrix > which is given. For example if ?+A@CB , > can be the radial basis function (RBF): DFEHG #JI KML NPO 2 Q R B STVU E T O G T R W (1) so that nearby points in Euclidean space are assigned large edge weights. Other weightings are possible, of course, and may be more appropriate when is discrete or symbolic. For our purposes the matrix > fully specifies the data manifold structure. We note that a method for learning the scale parameter Q is proposed in (Zhu et al., 2003). The semi-supervised algorithm in this paper is based on a relaxation of the requirement that labels are binary, resulting in a simple and tractable family of algorithms. We allow continuous labels on unlabeled nodes MXZY[7]\\^@ . The labels on labeled data are still constrained to be 0 or 1: _` C#a 4)b c_ d+A-e/ 243 for _P#(24 f . We also denote the constraint by g ) U h i . Since we want unlabeled points that are nearby in the graph to have similar labels, we define the energy to be 9j c 1 C# 2 k S E l G D EmG _` O onp R (2) so that low energy corresponds to a slowly varying function over the graph. Define the diagonal matrix q6#sr*_`t*u[ 8r E whose entries r E #wv G DFEmG are the weighted degrees of the nodes in 5 . The combinatorial Laplacian is the \"x;y\" matrix z{#aq O > . We can rewrite the energy function in matrix form as 9| c 1 #( } z: . We consider the Gaussian random field ~ c 1 C# 2 \u007fP\u0080 I KML< OF\u0081 9j c 1 f where \u0081 is an \u201cinverse temperature\u201d parameter, and \u007f \u0080 is the partition function \u007f\u0082\u0080 #s\u0083 h \u0084 i U h i I KpL< OF\u0081 9j M \u0085r4 . The Gaussian random field differs from the \u201cstandard\u201d discrete Markov random field in that the field configurations are over a continuous state space. Moreover, for a Gaussian field the joint probability distribution over unlabeled nodes is Gaussian with covariance matrix \u0080 z|\u0086 0 . z\u0087 0 is the submatrix of z corresponding to unlabeled data. The minimum energy function \u0088\u0089# arg min h \u0084 i U h i 9| c 1 of the Gaussian field is harmonic; that is, it satisfies z\u008a\u0088A#\u008b/ on unlabeled data points ! , and is equal to ) on the labeled data points . The harmonic property means that the value at each unlabeled node is the average of neighboring nodes: \u0088b onp C# 2 r G S E\u008d\u008c\u008eG D\u0085EmG \u0088b _` for n\u0087#Z &a24 f &? which is consistent with our prior notion of smoothness with respect to the graph. Because of the maximum principle of harmonic functions (Doyle & Snell, 1984), \u0088 is unique. Furthermore, \u0088 satisfies /\u0089\u008f(\u0088b onp \u0090\u008f\u00912 for n\u0092+\u0093! when ! is connected and labeled nodes from both classes are present at the boundary (the usual case; otherwise \u0088 takes on the extremum 0 or 1). By definition \u0088 is the mode of the Gaussian random field, but since the joint distribution is Gaussian, \u0088 is also the mean of the field. The harmonic energy minimizing function \u0088 can be computed with matrix methods. We partition the Laplacian matrix z into blocks for labeled and unlabeled nodes, z\u0091#\u0095\u0094 z\u0087 m z\u0087 z e z 0 \u0097\u0096 and let \u0088,# \u0094 \u00880 \u0088 \u0096 where \u0088 \u0090#\u0098 ) , and \u0088 denotes the mean values on the unlabeled data points. The solution is given by \u0088 # O z \u0086 0 z e \u0088 (3) It is not hard to show that the Gaussian field, conditioned on labeled data, is a multivariate normal: * y\u0099a\u009a\u008b \u009b\u0088 \u008e fz \u0086 0 . To carry out classification with a Gaussian field, we note that the harmonic energy minimizing function \u0088 is the mean of the field. Therefore the Bayes classification rule is to label node _ as class 1 in case \u0088b c_ d\u009cJ/1 H\u009d , and to label node _ class 0 otherwise. The harmonic function \u0088 has many nice interpretations, of which the random walk view is particularly relevant here. Define the transition matrix \u009e\u009f# q \u0086 > . Consider the random walk on graph 5 by a particle. Starting from an unlabeled node _ , it moves to a node n with probability \u009e EHG after one step. The walk continues until the particle reaches a labeled node. Then \u0088 E is the probability that the particle, starting from node _ , reaches a labeled node with label 1. The labeled data are the absorbing boundary for the random walk. More on this semi-supervised learning framework can be found in (Zhu et al., 2003). 3. Active learning We propose to perform active learning with the Gaussian random field model by greedily selecting queries from the unlabeled data to minimize the risk of the harmonic energy minimization function. The risk is the estimated generalization error of the Bayes classifier, and can be efficiently computed with matrix methods. We define the true risk \u009b\u0088 of the Bayes classifier based on the harmonic function \u0088 to be \u009b\u0088 # S E U S h U l sgn \u009b\u0088 E # E ~ E g where sgn 8\u0088 E is the Bayes decision rule, where (with a slight abuse of notation) sgn \u009b\u0088 E \u0097# 2 if \u0088 E \u009c^/ \u009d and sgn \u009b\u0088 E #,/ otherwise. Here ~ E g is the unknown true label distribution at node _ , given the labeled data. Because of this 8\u0088 is not computable. In order to proceed, it is necessary to make assumptions. We begin by assuming that we can estimate the unknown distribution ~ c E g d with the mean of the Gaussian field model: ~ c E # 2pg d Z\u0088 E Intuitively, recalling \u0088 E is the probability of reaching 1 in a random walk on the graph, our assumption is that we can approximate the distribution using a biased coin at each node, whose probability of heads is \u0088 E . With this assumption we can compute the estimated risk 8\u0088 as \u0092 8\u0088 # S E U sgn \u009b\u0088 E # / 2 O \u0088 E & sgn 8\u0088 E #(2 \u0088 E # S E U \u009b\u0088 E 2 O \u0088 E (4) If we perform active learning and query an unlabeled node , we will receive an answer (0 or 1). Adding this point to the tr", "title": "Semi-Supervised Learning Using Gaussian Fields and Harmonic Functions"}, "4e901eab50bc6a73c555fd68f721a3d8f4c669f1": {"paper_id": "4e901eab50bc6a73c555fd68f721a3d8f4c669f1", "abstract": "The increasing availability and affordability of wireless building and home automation networks has increased interest in residential and commercial building energy management. This interest has been coupled with an increased awareness of the environmental impact of energy generation and usage. Residential appliances and equipment account for 30% of all energy consumption in OECD countries and indirectly contribute to 12% of energy generation related carbon dioxide (CO2) emissions (International Energy Agency, 2003). The International Energy Association also predicts that electricity usage for residential appliances would grow by 12% between 2000 and 2010, eventually reaching 25% by 2020. These figures highlight the importance of managing energy use in order to improve stewardship of the environment. They also hint at the potential gains that are available through smart consumption strategies targeted at residential and commercial buildings. The challenge is how to achieve this objective without negatively impacting people\u2019s standard of living or their productivity. The three primary purposes of building energy management are the reduction/management of building energy use; the reduction of electricity bills while increasing occupant comfort and productivity; and the improvement of environmental stewardship without adversely affecting standards of living. Building energy management systems provide a centralized platform for managing building energy usage. They detect and eliminate waste, and enable the efficient use electricity resources. The use of widely dispersed sensors enables the monitoring of ambient temperature, lighting, room occupancy and other inputs required for efficient management of climate control (heating, ventilation and air conditioning), security and lighting systems. Lighting and HVAC account for 50% of commercial and 40% of residential building electricity expenditure respectively, indicating that efficiency improvements in these two areas can significantly reduce energy expenditure. These savings can be made through two avenues: the first is through the use of energy-efficient lighting and HVAC systems; and the second is through the deployment of energy management systems which utilize real time price information to schedule loads to minimize energy bills. The latter scheme requires an intelligent power grid or smart grid which can provide bidirectional data flows between customers and utility companies. The smart grid is characterized by the incorporation of intelligenceand bidirectional flows of information and electricity throughout the power grid. These enhancements promise to revolutionize the grid by enabling customers to not only consume but also supply power.", "title": "6 Energy Management for Intelligent Buildings"}, "bc8e53c1ef837531126886520ce155e14140beb4": {"paper_id": "bc8e53c1ef837531126886520ce155e14140beb4", "abstract": "In this paper, we present a number of robust methodologies for an underwater robot to visually detect, follow, and interact with a diver for collaborative task execution. We design and develop two autonomous diver-following algorithms, the first of which utilizes both spatialand frequency-domain features pertaining to human swimming patterns in order to visually track a diver. The second algorithm uses a convolutional neural network-based model for robust tracking-by-detection. In addition, we propose a hand gesture-based human-robot communication framework that is syntactically simpler and computationally more efficient than the existing grammar-based frameworks. In the proposed interaction framework, deep visual detectors are used to provide accurate hand gesture recognition; subsequently, a finite-state machine performs robust and efficient gesture-to-instruction mapping. The distinguishing feature of this framework is that it can be easily adopted by divers for communicating with underwater robots without using artificial markers or requiring memorization of complex language rules. Furthermore, we validate the performance and effectiveness of the proposed methodologies through extensive field experiments in closedand open-water environments. Finally, we perform a user interaction study to demonstrate the usability benefits of our proposed interaction framework compared to existing methods.", "title": "Understanding Human Motion and Gestures for Underwater Human-Robot Collaboration"}, "0626908dd710b91aece1a81f4ca0635f23fc47f3": {"paper_id": "0626908dd710b91aece1a81f4ca0635f23fc47f3", "abstract": "Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2% top-1 and 5:6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5% top-5 error and 17:3% top-1 error on the validation set and 3:6% top-5 error on the official test set.", "title": "Rethinking the Inception Architecture for Computer Vision"}, "8ecc044d920df247fbd455b752fd7cc0f7363ad7": {"paper_id": "8ecc044d920df247fbd455b752fd7cc0f7363ad7", "abstract": "Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned. Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods su ce for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.", "title": "On the importance of initialization and momentum in deep learning"}, "1121ff5cdeaa470521b8dff084ba1424dd613cc1": {"paper_id": "1121ff5cdeaa470521b8dff084ba1424dd613cc1", "abstract": "Deep convolutional neural networks take GPU-days of computation to train on large data sets. Pedestrian detection for self driving cars requires very low latency. Image recognition for mobile phones is constrained by limited processing resources. The success of convolutional neural networks in these situations is limited by how fast we can compute them. Conventional FFT based convolution is fast for large filters, but state of the art convolutional neural networks use small, 3 3 filters. We introduce a new class of fast algorithms for convolutional neural networks using Winograd's minimal filtering algorithms. The algorithms compute minimal complexity convolution over small tiles, which makes them fast with small filters and small batch sizes. We benchmark a GPU implementation of our algorithm with the VGG network and show state of the art throughput at batch sizes from 1 to 64.", "title": "Fast Algorithms for Convolutional Neural Networks"}, "b2180fc4f5cb46b5b5394487842399c501381d67": {"paper_id": "b2180fc4f5cb46b5b5394487842399c501381d67", "abstract": "In this paper, we study the challenging problem of tracking the trajectory of a moving object in a video with possibly very complex background. In contrast to most existing trackers which only learn the appearance of the tracked object online, we take a different approach, inspired by recent advances in deep learning architectures, by putting more emphasis on the (unsupervised) feature learning problem. Specifically, by using auxiliary natural images, we train a stacked denoising autoencoder offline to learn generic image features that are more robust against variations. This is then followed by knowledge transfer from offline training to the online tracking process. Online tracking involves a classification neural network which is constructed from the encoder part of the trained autoencoder as a feature extractor and an additional classification layer. Both the feature extractor and the classifier can be further tuned to adapt to appearance changes of the moving object. Comparison with the state-of-the-art trackers on some challenging benchmark video sequences shows that our deep learning tracker is more accurate while maintaining low computational cost with real-time performance when our MATLAB implementation of the tracker is used with a modest graphics processing unit (GPU).", "title": "Learning a Deep Compact Image Representation for Visual Tracking"}, "b183947ee15718b45546eda6b01e179b9a95421f": {"paper_id": "b183947ee15718b45546eda6b01e179b9a95421f", "abstract": "The use of object proposals is an effective recent approach for increasing the computational efficiency of object detection. We propose a novel method for generating object bounding box proposals using edges. Edges provide a sparse yet informative representation of an image. Our main observation is that the number of contours that are wholly contained in a bounding box is indicative of the likelihood of the box containing an object. We propose a simple box objectness score that measures the number of edges that exist in the box minus those that are members of contours that overlap the box\u2019s boundary. Using efficient data structures, millions of candidate boxes can be evaluated in a fraction of a second, returning a ranked set of a few thousand top-scoring proposals. Using standard metrics, we show results that are significantly more accurate than the current state-of-the-art while being faster to compute. In particular, given just 1000 proposals we achieve over 96% object recall at overlap threshold of 0.5 and over 75% recall at the more challenging overlap of 0.7. Our approach runs in 0.25 seconds and we additionally demonstrate a near real-time variant with only minor loss in accuracy.", "title": "Edge Boxes: Locating Object Proposals from Edges"}, "28f9cf85ebbff86207e1f6067880bb23daff0878": {"paper_id": "28f9cf85ebbff86207e1f6067880bb23daff0878", "abstract": "Generic object detection is the challenging task of proposing windows that localize all the objects in an image, regardless of their classes. Such detectors have recently been shown to benefit many applications such as speeding-up class-specific object detection, weakly supervised learning of object detectors and object discovery. In this paper, we introduce a novel and very efficient method for generic object detection based on a randomized version of Prim's algorithm. Using the connectivity graph of an image's super pixels, with weights modelling the probability that neighbouring super pixels belong to the same object, the algorithm generates random partial spanning trees with large expected sum of edge weights. Object localizations are proposed as bounding-boxes of those partial trees. Our method has several benefits compared to the state-of-the-art. Thanks to the efficiency of Prim's algorithm, it samples proposals very quickly: 1000 proposals are obtained in about 0.7s. With proposals bound to super pixel boundaries yet diversified by randomization, it yields very high detection rates and windows that tightly fit objects. In extensive experiments on the challenging PASCAL VOC 2007 and 2012 and SUN2012 benchmark datasets, we show that our method improves over state-of-the-art competitors for a wide range of evaluation scenarios.", "title": "Prime Object Proposals with Randomized Prim's Algorithm"}, "18fe8a42cb3b7ce14e29a11faa3d7cc45f1be22f": {"paper_id": "18fe8a42cb3b7ce14e29a11faa3d7cc45f1be22f", "abstract": "We propose a category-independent method to produce a bag of regions and rank them, such that top-ranked regions are likely to be good segmentations of different objects. Our key objectives are completeness and diversity: Every object should have at least one good proposed region, and a diverse set should be top-ranked. Our approach is to generate a set of segmentations by performing graph cuts based on a seed region and a learned affinity function. Then, the regions are ranked using structured learning based on various cues. Our experiments on the Berkeley Segmentation Data Set and Pascal VOC 2011 demonstrate our ability to find most objects within a small bag of proposed regions.", "title": "Category-Independent Object Proposals with Diverse Ranking"}, "d63793f01b863c01fa336b2193c2c4560c2342ee": {"paper_id": "d63793f01b863c01fa336b2193c2c4560c2342ee", "abstract": "An anonymous transaction environment and the advantage of virtual property have resulted in trust playing an important role in the rapid growth of online shopping in China. To satisfy this trust issue, Alibaba (China) Co., Ltd. (Hangzhou, China) invented Alipay, the largest third-party online payment service. Using a structural equation model (SEM), this paper attempts to determine whether Alipay\u2019s service quality factors are truly sustainable. The results indicate that only two of five factors\u2014convenience and security\u2014are significantly mediated by the sustainable performance of customer satisfaction as a mediator. The other three factors\u2014usefulness, responsiveness and economy\u2014were rejected for the role of customer satisfaction, even if they are accepted regarding the direct effect on reuse intention. This result implies that Chinese web companies need to make greater efforts not to ensure initial success, but instead to ensure sustainable performance.", "title": "Reuse Intention of Third-Party Online Payments : A Focus on the Sustainable Factors of Alipay"}, "7752e0835506a6629c1b06e67f2afb1e5d2bb714": {"paper_id": "7752e0835506a6629c1b06e67f2afb1e5d2bb714", "abstract": "Content Memory (Learning Ability) As Comprehension 82 Vocabulary Cs .30 ( ) .23 .31 ( ) .31 .31 .35 ( ) .29 .48 .35 .38 ( ) .30 .40 .47 .58 .48 ( ) As judged against these latter values, comprehension (.48) and vocabulary (.47), but not memory (.31), show some specific validity. This transmutability of the validation matrix argues for the comparisons within the heteromethod block as the most generally relevant validation data, and illustrates the potential interchangeability of trait and method components. Some of the correlations in Chi's (1937) prodigious study of halo effect in ratings are appropriate to a multitrait-multimethod matrix in which each rater might be regarded as representing a different method. While the published report does not make these available in detail because it employs averaged values, it is apparent from a comparison of his Tables IV and VIII that the ratings generally failed to meet the requirement that ratings of the same trait by different raters should correlate higher than ratings of different traits by the same rater. Validity is shown to the extent that of the correlations in the heteromethod block, those in the validity diagonal are higher than the average heteromethod-heterotrait values. A conspicuously unsuccessful multitrait-multimethod matrix is provided by Campbell (1953, 1956) for rating of the leadership behavior of officers by themselves and by their subordinates. Only one of 11 variables (Recognition Behavior) met the requirement of providing a validity diagonal value higher than any of the heterotrait-heteromethod values, that validity being .29. For none of the variables were the validities higher than heterotrait-monomethod values. A study of attitudes toward authority and nonauthority figures by Burwen and Campbell (1957) contains a complex multitrait-multimethod matrix, one symmetrical excerpt from which is shown in Table 6. Method variance was strong for most of the procedures in this study. Where validity was found, it was primarily at the level of validity diagonal values higher than heterotrait-heteromethod values. As illustrated in Table 6, attitude toward father showed this kind of validity, as did attitude toward peers to a lesser degree. Attitude toward boss showed no validity. There was no evidence of a generalized attitude toward authority which would include father and boss, although such values as the VALIDATION BY THE MULTITRAIT-MULTIMETHOD MATRIX", "title": "Convergent and discriminant validation by the multitrait-multimethod matrix."}, "c7d59c8e3e5745fef1473df8f4f78a86fc4f7c87": {"paper_id": "c7d59c8e3e5745fef1473df8f4f78a86fc4f7c87", "abstract": "Using the means-end framework as a theoretical foundation, this article conceptualizes, constructs, refines, and tests a multiple-item scale (E-S-QUAL) for measuring the service quality delivered by Web sites on which customers shop online. Two stages of empirical data collection revealed that two different scales were necessary for capturing electronic service quality. The basic E-S-QUAL scale developed in the research is a 22-item scale of four dimensions: efficiency, fulfillment, system availability, and privacy. The second scale, E-RecS-QUAL, is salient only to customers who had nonroutine encounters with the sites and contains 11 items in three dimensions: responsiveness, compensation, and contact. Both scales demonstrate good psychometric properties based on findings from a variety of reliability and validity tests and build on the research already conducted on the topic. Directions for further research on electronic service quality are offered. Managerial implications stemming from the empirical findings about E-S-QUAL are also discussed.", "title": "E-S-QUAL A Multiple-Item Scale for Assessing Electronic Service Quality"}, "0b990a9c6000b80dc00b69b68f6091844b898215": {"paper_id": "0b990a9c6000b80dc00b69b68f6091844b898215", "abstract": "This paper addresses the role of marketing in hypermedia computer-mediated environments (CMEs). Our approach considers hypermedia CMEs to be large-scale (i.e. national or global) networked environments, of which the World Wide Web on the Internet is the first and current global implementation. We introduce marketers to this revolutionary new medium, and propose two structural models of consumer behavior in a CME. Then we examine the set of consequent testable research propositions and marketing implications that flow from the models. Marketing in Hypermedia Computer-Mediated Environments: Conceptual Foundations 1) Introduction Firms communicate with their customers through various media. Traditionally, these media follow a passive one-to-many communication model whereby a firm reaches many current and potential customers, segmented or not, through marketing efforts that allow only limited forms of feedback on the part of the customer. For several years now, a revolution has been developing that is dramatically altering this traditional view of advertising and communication media. This revolution is the Internet, the massive global network of interconnected packet-switched computer networks, and as a new marketing medium, has the potential to radically change the way firms do business with their customers. The Internet operationalizes a model of distributed computing that facilitates interactive multimedia many-to-many communication. As such, the Internet supports discussion groups (e.g. USENET news and moderated and unmoderated mailing lists), multi-player games and communications systems (e.g. MUDs, irc, chat, MUSEs), file transfer, electronic mail, and global information access and retrieval systems (e.g. archie, gopher, and the World Wide Web). The business implications of this model \"[where] the engine of democratization sitting on so many desktops is already out of control, is already creating new players in a new game\" (Carroll 1994), will be played out in as yet unknown ways for years to come. This paper is concerned with the marketing implications of commercializing hypermedia computer-mediated environments (CMEs), of which the World Wide Web (Berners-Lee et. al. 1992, 1993) on the Internet is the first and current networked global implementation. While we provide a formal definition subsequently, at this point we informally define a hypermedia CME as a distributed computer network used to access and provide hypermedia content (i.e., multimedia content connected across the network with hypertext links). Though other CMEs are relevant to marketers, including private bulletin board systems (Bunch 1994); public conferencing systems such as the WELL (Figallo 1993; Rheingold 1992, 1993) and ECHO; and commercial online services such as America On-Line, Prodigy, and CompuServe, we restrict our current focus to marketing activities in hypermedia CMEs accessible via the \"Web\" on the Internet. The Internet is an important focus for marketers because consumers and firms are conducting business on the Internet in proportions that dwarf the commercial provider base of the other CMEs combined. There are over 21,700 commercial Internet addressess (Verity and Hof 1994), and an increasing percentage of these commercial addresses are providing Web services. As of December 28, 1994, 1465 firms were listed in Open Market\u2019s (1994) directory of \"Commercial Services on the Net,\" and there were 6370 entries in the \"Business/Corporations\" directory of the Yahoo Guide to WWW (Filo and Yang 1994). The central thesis driving this research is that hypermedia CMEs, such as but not limited to the World Wide Web on the Internet, require the development and application of new marketing concepts and models. This is because hypermedia CMEs possess unique characteristics, including machine-interactivity, telepresence, hypermedia, and network navigation, which distinguish them from traditional media and some interactive multimedia, on which conventional concepts and models are based. Hoffman & Novak (1995), \"Marketing in Hypermedia CMEs: Conceptual Foundations\" page 1", "title": "Marketing in hypermedia computer-mediated environment: Conceptual foundations"}, "76d5a90f26e1270c952eac1fa048a83d63f1dd39": {"paper_id": "76d5a90f26e1270c952eac1fa048a83d63f1dd39", "abstract": "In this article, we attempt to distinguish between the properties of moderator and mediator variables at a number of levels. First, we seek to make theorists and researchers aware of the importance of not using the terms moderator and mediator interchangeably by carefully elaborating, both conceptually and strategically, the many ways in which moderators and mediators differ. We then go beyond this largely pedagogical function and delineate the conceptual and strategic implications of making use of such distinctions with regard to a wide range of phenomena, including control and stress, attitudes, and personality traits. We also provide a specific compendium of analytic procedures appropriate for making the most effective use of the moderator and mediator distinction, both separately and in terms of a broader causal system that includes both moderators and mediators.", "title": "The moderator-mediator variable distinction in social psychological research: conceptual, strategic, and statistical considerations."}, "423b941641728a21e37f41359a691815cdd84ceb": {"paper_id": "423b941641728a21e37f41359a691815cdd84ceb", "abstract": "In this work, we propose a novel Reversible Recursive Instance-level Object Segmentation (R2-IOS) framework to address the challenging instance-level object segmentation task. R2-IOS consists of a reversible proposal refinement sub-network that predicts bounding box offsets for refining the object proposal locations, and an instance-level segmentation sub-network that generates the foreground mask of the dominant object instance in each proposal. By being recursive, R2-IOS iteratively optimizes the two subnetworks during joint training, in which the refined object proposals and improved segmentation predictions are alternately fed into each other to progressively increase the network capabilities. By being reversible, the proposal refinement sub-network adaptively determines an optimal number of refinement iterations required for each proposal during both training and testing. Furthermore, to handle multiple overlapped instances within a proposal, an instance-aware denoising autoencoder is introduced into the segmentation sub-network to distinguish the dominant object from other distracting instances. Extensive experiments on the challenging PASCAL VOC 2012 benchmark well demonstrate the superiority of R2-IOS over other state-of-the-art methods. In particular, the APr over 20 classes at 0:5 IoU achieves 66:7%, which significantly outperforms the results of 58:7% by PFN [17] and 46:3% by [22].", "title": "Reversible Recursive Instance-Level Object Segmentation"}, "0161e4348a7079e9c37434c5af47f6372d4b412d": {"paper_id": "0161e4348a7079e9c37434c5af47f6372d4b412d", "abstract": "We propose a method to identify and localize object classes in images. Instead of operating at the pixel level, we advocate the use of superpixels as the basic unit of a class segmentation or pixel localization scheme. To this end, we construct a classifier on the histogram of local features found in each superpixel. We regularize this classifier by aggregating histograms in the neighborhood of each superpixel and then refine our results further by using the classifier in a conditional random field operating on the superpixel graph. Our proposed method exceeds the previously published state-of-the-art on two challenging datasets: Graz-02 and the PASCAL VOC 2007 Segmentation Challenge.", "title": "Class segmentation and object localization with superpixel neighborhoods"}, "03adaf0497fdd9fb32ef0ef925db9bc7da4f2b4f": {"paper_id": "03adaf0497fdd9fb32ef0ef925db9bc7da4f2b4f", "abstract": "We propose a two-class classification model for grouping. Human segmented natural images are used as positive examples. Negative examples of grouping are constructed by randomly matching human segmentations and images. In a preprocessing stage an image is oversegmented into superpixels. We define a variety of features derived from the classical Gestalt cues, including contour, texture, brightness and good continuation. Information-theoretic analysis is applied to evaluate the power of these grouping cues. We train a linear classifier to combine these features. To demonstrate the power of the classification model, a simple algorithm is used to randomly search for good segmentations. Results are shown on a wide range of images.", "title": "Learning a Classification Model for Segmentation"}, "f5a52b69dde106cb69cb7c35dd8ca23071966876": {"paper_id": "f5a52b69dde106cb69cb7c35dd8ca23071966876", "abstract": "While there has been a lot of recent work on object recognition and image understanding, the focus has been on carefully establishing mathematical models for images, scenes, and objects. In this paper, we propose a novel, nonparametric approach for object recognition and scene parsing using a new technology we name label transfer. For an input image, our system first retrieves its nearest neighbors from a large database containing fully annotated images. Then, the system establishes dense correspondences between the input image and each of the nearest neighbors using the dense SIFT flow algorithm [28], which aligns two images based on local image structures. Finally, based on the dense scene correspondences obtained from SIFT flow, our system warps the existing annotations and integrates multiple cues in a Markov random field framework to segment and recognize the query image. Promising experimental results have been achieved by our nonparametric scene parsing system on challenging databases. Compared to existing object recognition approaches that require training classifiers or appearance models for each object category, our system is easy to implement, has few parameters, and embeds contextual information naturally in the retrieval/alignment procedure.", "title": "Nonparametric Scene Parsing via Label Transfer"}, "0690ba31424310a90028533218d0afd25a829c8d": {"paper_id": "0690ba31424310a90028533218d0afd25a829c8d", "abstract": "Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification (also called \u201dsemantic image segmentation\u201d). We show that responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our \u201cDeepLab\u201d system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6% IOU accuracy in the test set. We show how these results can be obtained efficiently: Careful network re-purposing and a novel application of the \u2019hole\u2019 algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU.", "title": "Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs"}, "0da75bded3ae15e255f5bd376960cfeffa173b4e": {"paper_id": "0da75bded3ae15e255f5bd376960cfeffa173b4e", "abstract": "In this paper we study the role of context in existing state-of-the-art detection and segmentation approaches. Towards this goal, we label every pixel of PASCAL VOC 2010 detection challenge with a semantic category. We believe this data will provide plenty of challenges to the community, as it contains 520 additional classes for semantic segmentation and object detection. Our analysis shows that nearest neighbor based approaches perform poorly on semantic segmentation of contextual classes, showing the variability of PASCAL imagery. Furthermore, improvements of existing contextual models for detection is rather modest. In order to push forward the performance in this difficult scenario, we propose a novel deformable part-based model, which exploits both local context around each candidate detection as well as global context at the level of the scene. We show that this contextual reasoning significantly helps in detecting objects at all scales.", "title": "The Role of Context for Object Detection and Semantic Segmentation in the Wild"}, "6074c1108997e0c1f97dc3c199323a162ffe978d": {"paper_id": "6074c1108997e0c1f97dc3c199323a162ffe978d", "abstract": "Torch7 is a versatile numeric computing framework and machine learning library that extends Lua. Its goal is to provide a flexible environment to design and train learning machines. Flexibility is obtained via Lua, an extremely lightweight scripting language. High performance is obtained via efficient OpenMP/SSE and CUDA implementations of low-level numeric routines. Torch7 can easily be interfaced to third-party software thanks to Lua\u2019s light interface.", "title": "Torch7: A Matlab-like Environment for Machine Learning"}, "2f7ad26514bce4df6c8ebc42c90383ef3a974df4": {"paper_id": "2f7ad26514bce4df6c8ebc42c90383ef3a974df4", "abstract": "Pylearn2 is a machine learning research library. This does n t just mean that it is a collection of machine learning algorithms that share a comm n API; it means that it has been designed for flexibility and extensibility in ord e to facilitate research projects that involve new or unusual use cases. In this paper we give a brief history of the library, an overview of its basic philosophy, a summar y of the library\u2019s architecture, and a description of how the Pylearn2 communi ty functions socially.", "title": "Pylearn2: a machine learning research library"}, "1109b663453e78a59e4f66446d71720ac58cec25": {"paper_id": "1109b663453e78a59e4f66446d71720ac58cec25", "abstract": "We present an integrated framework for using Convolutional Networks for classification , localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.", "title": "OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks"}, "4af785bf8a5959d7e8eb37ca87c45db2ac6a544c": {"paper_id": "4af785bf8a5959d7e8eb37ca87c45db2ac6a544c", "abstract": "The style of an image plays a significant role in how it is viewed, but style has received little attention in computer vision research. We describe an approach to predicting style of images, and perform a thorough evaluation of different image features for these tasks. We find that features learned in a multi-layer network generally perform best \u2013 even when trained with object class (not style) labels. Our large-scale learning methods results in the best published performance on an existing dataset of aesthetic ratings and photographic style annotations. We present two novel datasets: 80K Flickr photographs annotated with 20 curated style labels, and 85K paintings annotated with 25 style/genre labels. Our approach shows excellent classification performance on both datasets. We use the learned classifiers to extend traditional tag-based image search to consider stylistic constraints, and demonstrate cross-dataset understanding of style.", "title": "Recognizing Image Style"}, "3b0c88cb414455b7f86528cf22e58ee83ee0b1c6": {"paper_id": "3b0c88cb414455b7f86528cf22e58ee83ee0b1c6", "abstract": "Recent object detection systems rely on two critical steps: (1) a set of object proposals is predicted as efficiently as possible, and (2) this set of candidate proposals is then passed to an object classifier. Such approaches have been shown they can be fast, while achieving the state of the art in detection performance. In this paper, we propose a new way to generate object proposals, introducing an approach based on a discriminative convolutional network. Our model is trained jointly with two objectives: given an image patch, the first part of the system outputs a class-agnostic segmentation mask, while the second part of the system outputs the likelihood of the patch being centered on a full object. At test time, the model is efficiently applied on the whole test image and generates a set of segmentation masks, each of them being assigned with a corresponding object likelihood score. We show that our model yields significant improvements over state-of-theart object proposal algorithms. In particular, compared to previous approaches, our model obtains substantially higher object recall using fewer proposals. We also show that our model is able to generalize to unseen categories it has not seen during training. Unlike all previous approaches for generating object masks, we do not rely on edges, superpixels, or any other form of low-level segmentation.", "title": "Learning to Segment Object Candidates"}, "88bde2b1b4522c726d331218ce7fc6e1b27d8dde": {"paper_id": "88bde2b1b4522c726d331218ce7fc6e1b27d8dde", "abstract": "Successful methods for visual object recognition typically rely on training datasets containing lots of richly annotated images. Detailed image annotation, e.g. by object bounding boxes, however, is both expensive and often subjective. We describe a weakly supervised convolutional neural network (CNN) for object classification that relies only on image-level labels, yet can learn from cluttered scenes containing multiple objects. We quantify its object classification and object location prediction performance on the Pascal VOC 2012 (20 object classes) and the much larger Microsoft COCO (80 object classes) datasets. We find that the network (i) outputs accurate image-level labels, (ii) predicts approximate locations (but not extents) of objects, and (iii) performs comparably to its fully-supervised counterparts using object bounding box annotation for training.", "title": "Is object localization for free? - Weakly-supervised learning with convolutional neural networks"}, "06a23ffbd9752ce204197df59812b2ebd1a097ff": {"paper_id": "06a23ffbd9752ce204197df59812b2ebd1a097ff", "abstract": "We introduce a purely feed-forward architecture for semantic segmentation. We map small image elements (superpixels) to rich feature representations extracted from a sequence of nested regions of increasing extent. These regions are obtained by \u201czooming out\u201d from the superpixel all the way to scene-level resolution. This approach exploits statistical structure in the image and in the label space without setting up explicit structured prediction mechanisms, and thus avoids complex and expensive inference. Instead superpixels are classified by a feedforward multilayer network. Our architecture achieves 69.6% average accuracy on the PASCAL VOC 2012 test set.", "title": "Feedforward semantic segmentation with zoom-out features"}, "7e383307edacb0bb53e57772fdc1ffa2825eba91": {"paper_id": "7e383307edacb0bb53e57772fdc1ffa2825eba91", "abstract": "Many problems in real-world applications involve predicting several random variables that are statistically related. Markov random fields (MRFs) are a great mathematical tool to encode such dependencies. The goal of this paper is to combine MRFs with deep learning to estimate complex representations while taking into account the dependencies between the output random variables. Towards this goal, we propose a training algorithm that is able to learn structured models jointly with deep features that form the MRF potentials. Our approach is efficient as it blends learning and inference and makes use of GPU acceleration. We demonstrate the effectiveness of our algorithm in the tasks of predicting words from noisy images, as well as tagging of Flickr photographs. We show that joint learning of the deep features and the MRF parameters results in significant performance gains.", "title": "Learning Deep Structured Models"}, "3461f06617b42dadd1ce240a93ffe420513b3399": {"paper_id": "3461f06617b42dadd1ce240a93ffe420513b3399", "abstract": "Yann Le Cun AT&T Bell Labs Holmdel NJ 07733 We introduce a new approach for on-line recognition of handwritten words written in unconstrained mixed style. The preprocessor performs a word-level normalization by fitting a model of the word structure using the EM algorithm. Words are then coded into low resolution \"annotated images\" where each pixel contains information about trajectory direction and curvature. The recognizer is a convolution network which can be spatially replicated. From the network output, a hidden Markov model produces word scores. The entire system is globally trained to minimize word-level errors.", "title": "Globally Trained Handwritten Word Recognizer Using Spatial Representation, Convolutional Neural Networks, and Hidden Markov Models"}, "71f98c3f7a5b02ab193110d5ae9f9d48a1c5ec38": {"paper_id": "71f98c3f7a5b02ab193110d5ae9f9d48a1c5ec38", "abstract": "In this work, the human parsing task, namely decomposing a human image into semantic fashion/body regions, is formulated as an active template regression (ATR) problem, where the normalized mask of each fashion/body item is expressed as the linear combination of the learned mask templates, and then morphed to a more precise mask with the active shape parameters, including position, scale and visibility of each semantic region. The mask template coefficients and the active shape parameters together can generate the human parsing results, and are thus called the structure outputs for human parsing. The deep Convolutional Neural Network (CNN) is utilized to build the end-to-end relation between the input human image and the structure outputs for human parsing. More specifically, the structure outputs are predicted by two separate networks. The first CNN network is with max-pooling, and designed to predict the template coefficients for each label mask, while the second CNN network is without max-pooling to preserve sensitivity to label mask position and accurately predict the active shape parameters. For a new image, the structure outputs of the two networks are fused to generate the probability of each label for each pixel, and super-pixel smoothing is finally used to refine the human parsing result. Comprehensive evaluations on a large dataset well demonstrate the significant superiority of the ATR framework over other state-of-the-arts for human parsing. In particular, the F1-score reaches 64.38 percent by our ATR framework, significantly higher than 44.76 percent based on the state-of-the-art algorithm [28].", "title": "Deep Human Parsing with Active Template Regression"}, "5087d9bdde0ba5440eb8658be7183bf5074a2a94": {"paper_id": "5087d9bdde0ba5440eb8658be7183bf5074a2a94", "abstract": "We propose an object detection system that relies on a multi-region deep convolutional neural network (CNN) that also encodes semantic segmentation-aware features. The resulting CNN-based representation aims at capturing a diverse set of discriminative appearance factors and exhibits localization sensitivity that is essential for accurate object localization. We exploit the above properties of our recognition module by integrating it on an iterative localization mechanism that alternates between scoring a box proposal and refining its location with a deep CNN regression model. Thanks to the efficient use of our modules, we detect objects with very high localization accuracy. On the detection challenges of PASCAL VOC2007 and PASCAL VOC2012 we achieve mAP of 78.2% and 73.9% correspondingly, surpassing any other published work by a significant margin.", "title": "Object Detection via a Multi-region and Semantic Segmentation-Aware CNN Model"}, "32c45df9e11e6751bcea1b928f398f6c134d22c6": {"paper_id": "32c45df9e11e6751bcea1b928f398f6c134d22c6", "abstract": "Object detection and semantic segmentation are two strongly correlated tasks, yet typically solved separately or sequentially with substantially different techniques. Motivated by the complementary effect observed from the typical failure cases of the two tasks, we propose a unified framework for joint object detection and semantic segmentation. By enforcing the consistency between final detection and segmentation results, our unified framework can effectively leverage the advantages of leading techniques for these two tasks. Furthermore, both local and global context information are integrated into the framework to better distinguish the ambiguous samples. By jointly optimizing the model parameters for all the components, the relative importance of different component is automatically learned for each category to guarantee the overall performance. Extensive experiments on the PASCAL VOC 2010 and 2012 datasets demonstrate encouraging performance of the proposed unified framework for both object detection and semantic segmentation tasks.", "title": "Towards Unified Object Detection and Semantic Segmentation"}, "2e1dab46b0547f4a08adf8d4dfffc9e8cd6b0054": {"paper_id": "2e1dab46b0547f4a08adf8d4dfffc9e8cd6b0054", "abstract": "Previous studies propose that associative classification has high classification accuracy and strong flexibility at handling unstructured data. However, it still suffers from the huge set of mined rules and sometimes biased classification or overfitting since the classification is based on only single high-confidence rule. In this study, we propose a new associative classification method, CMAR, i.e., Classification based on Multiple Association Rules. The method extends an efficient frequent pattern mining method, FP-growth, constructs a class distribution-associated FP-tree, and mines large database efficiently. Moreover, it applies a CR-tree structure to store and retrieve mined association rules efficiently, and prunes rules effectively based on confidence, correlation and database coverage. The classification is performed based on a weighted analysis using multiple strong association rules. Our extensive experiments on databases from UCI machine learning database repository show that CMAR is consistent, highly effective at classification of various kinds of databases and has better average classification accuracy in comparison with CBA and C4.5. Moreover, our performance study shows that the method is highly efficient and scalable in comparison with other reported associative classification methods.", "title": "CMAR: Accurate and Efficient Classification Based on Multiple Class-Association Rules"}, "28ff8b42fd60c34821ab694359d270ccd6b4da24": {"paper_id": "28ff8b42fd60c34821ab694359d270ccd6b4da24", "abstract": "Twenty-two decision tree, nine statistical, and two neural network algorithms are compared on thirty-two datasets in terms of classification accuracy, training time, and (in the case of trees) number of leaves. Classification accuracy is measured by mean error rate and mean rank of error rate. Both criteria place a statistical, spline-based, algorithm called POLYCLSSS at the top, although it is not statistically significantly different from twenty other algorithms. Another statistical algorithm, logistic regression, is second with respect to the two accuracy criteria. The most accurate decision tree algorithm is QUEST with linear splits, which ranks fourth and fifth, respectively. Although spline-based statistical algorithms tend to have good accuracy, they also require relatively long training times. POLYCLASS, for example, is third last in terms of median training time. It often requires hours of training compared to seconds for other algorithms. The QUEST and logistic regression algorithms are substantially faster. Among decision tree algorithms with univariate splits, C4.5, IND-CART, and QUEST have the best combinations of error rate and speed. But C4.5 tends to produce trees with twice as many leaves as those from IND-CART and QUEST.", "title": "A Comparison of Prediction Accuracy, Complexity, and Training Time of Thirty-Three Old and New Classification Algorithms"}, "21a84fe894493e9cd805bf64f1dc342d4b5ce17a": {"paper_id": "21a84fe894493e9cd805bf64f1dc342d4b5ce17a", "abstract": "Unlike a univariate decision tree, a multivariate decision tree is not restricted to splits of the instance space that are orthogonal to the features' axes. This article addresses several issues for constructing multivariate decision trees: representing a multivariate test, including symbolic and numeric features, learning the coefficients of a multivariate test, selecting the features to include in a test, and pruning of multivariate decision trees. We present several new methods for forming multivariate decision trees and compare them with several well-known methods. We compare the different methods across a variety of learning tasks, in order to assess each method's ability to find concise, accurate decision trees. The results demonstrate that some multivariate methods are in general more effective than others (in the context of our experimental assumptions). In addition, the experiments confirm that allowing multivariate tests generally improves the accuracy of the resulting decision tree over a univariate tree.", "title": "Multivariate decision trees"}, "3096595380cfc118bb163b74897e13a84d094432": {"paper_id": "3096595380cfc118bb163b74897e13a84d094432", "abstract": "Mining frequent patterns in transaction databases, time-series databases, and many other kinds of databases has been studied popularly in data mining research. Most of the previous studies adopt an Apriori-like candidate set generation-and-test approach. However, candidate set generation is still costly, especially when there exist prolific patterns and/or long patterns.\nIn this study, we propose a novel frequent pattern tree (FP-tree) structure, which is an extended prefix-tree structure for storing compressed, crucial information about frequent patterns, and develop an efficient FP-tree-based mining method, FP-growth, for mining the complete set of frequent patterns by pattern fragment growth. Efficiency of mining is achieved with three techniques: (1) a large database is compressed into a highly condensed, much smaller data structure, which avoids costly, repeated database scans, (2) our FP-tree-based mining adopts a pattern fragment growth method to avoid the costly generation of a large number of candidate sets, and (3) a partitioning-based, divide-and-conquer method is used to decompose the mining task into a set of smaller tasks for mining confined patterns in conditional databases, which dramatically reduces the search space. Our performance study shows that the FP-growth method is efficient and scalable for mining both long and short frequent patterns, and is about an order of magnitude faster than the Apriori algorithm and also faster than some recently reported new frequent pattern mining methods.", "title": "Mining Frequent Patterns without Candidate Generation"}, "678de5508a6809a52d22d69cd524952275dc342f": {"paper_id": "678de5508a6809a52d22d69cd524952275dc342f", "abstract": "We present a pattern-mining algorithm that scales roughly linearly in the number of maximal patterns embedded in a database irrespective of the length of the longest pattern. In comparison, previous algorithms based on Apriori scale exponentially with longest pattern length. Experiments on real data show that when the patterns are long, our algorithm is more efficient by an order of magnitude or more.", "title": "Efficiently Mining Long Patterns from Databases"}, "188fcdd5aa1e5ba92805461241916bcf6e0cf198": {"paper_id": "188fcdd5aa1e5ba92805461241916bcf6e0cf198", "abstract": "Mining for association rules in market basket data has proved a fruitful area of research. Measures such as conditional probability (confidence) and correlation have been used to infer rules of the form \u201cthe existence of item A implies the existence of item B.\u201d However, such rules indicate only a statistical relationship between A and B. They do not specify the nature of the relationship: whether the presence of A causes the presence of B, or the converse, or some other attribute or phenomenon causes both to appear together. In applications, knowing such causal relationships is extremely useful for enhancing understanding and effecting change. While distinguishing causality from correlation is a truly difficult problem, recent work in statistics and Bayesian learning provide some avenues of attack. In these fields, the goal has generally been to learn complete causal models, which are essentially impossible to learn in large-scale data mining applications with a large number of variables. In this paper, we consider the problem of determining casual relationships, instead of mere associations, when mining market basket data. We identify some problems with the direct application of Bayesian learning ideas to mining large databases, concerning both the scalability of algorithms and the appropriateness of the statistical techniques, and introduce some initial ideas for dealing with these problems. We present experimental results from applying our algorithms on several large, real-world data sets. The results indicate that the approach proposed here is both computationally feasible and successful in identifying interesting causal structures. An interesting outcome is that it is perhaps easier to infer the lack of causality than to infer causality, information that is useful in preventing erroneous decision making.", "title": "Scalable Techniques for Mining Causal Structures"}, "524e3c670dfcb0dfc4be4a5a8fc2f4fe6d67473c": {"paper_id": "524e3c670dfcb0dfc4be4a5a8fc2f4fe6d67473c", "abstract": "The problem of discovering association rules has received considerable research attention and several fast algorithms for mining association rules have been developed. In practice, users are often interested in a subset of association rules. For example, they may only want rules that contain a specific item or rules that contain children of a specific item in a hierarchy. While such constraints can be applied as a postprocessing step, integrating them into the mining algorithm can dramatically reduce the execution time. We consider the problem of integrating constraints that n..,, l.....l,.... ,....,,....:,,, -1.~.. cl., -..s..a..-m e.. ..l.\u201c,\u201c, CUG Y\u201d\u201cAc;Qu GnpLz:I)DIVua \u201cYGI \u201cUs: pGYaLcG \u201cI OLJDciliLG of items into the association discovery algorithm. We present three integrated algorithms for mining association rules with item constraints and discuss their tradeoffs.", "title": "Mining Association Rules with Item Constraints"}, "2b0750d16db1ecf66a3c753264f207c2cb480bde": {"paper_id": "2b0750d16db1ecf66a3c753264f207c2cb480bde", "abstract": "We are given a large database of customer transactions, where each transaction consists of customer-id, transaction time, and the items bought in the transaction. We introduce the problem of mining sequential patterns over such databases. We present three algorithms to solve this problem, and empirically evaluate their performance using synthetic data. Two of the proposed algorithms, AprioriSome and AprioriAll, have comparable performance, albeit AprioriSome performs a little better when the minimum number of customers that must support a sequential pattern is low. Scale-up experiments show that both AprioriSome and AprioriAll scale linearly with the number of customer transactions. They also have excellent scale-up properties with respect to the number of transactions per customer and the number of items in a transaction.", "title": "Mining Sequential Patterns"}, "7ec5f9694bc3d061b376256320eacb8ec3566b77": {"paper_id": "7ec5f9694bc3d061b376256320eacb8ec3566b77", "abstract": "Systems for inducing concept descriptions from examples are valuable tools for assisting in the task of knowledge acquisition for expert systems. This paper presents a description and empirical evaluation of a new induction system, CN2, designed for the efficient induction of simple, comprehensible production rules in domains where problems of poor description language and/or noise may be present. Implementations of the CN2, ID3, and AQ algorithms are compared on three medical classification tasks.", "title": "The CN2 Induction Algorithm"}, "ea4bdeb0ba42ed22c85290528372941678680755": {"paper_id": "ea4bdeb0ba42ed22c85290528372941678680755", "abstract": "Many inductive knowledge acquisition algorithms generate classifiers in the form of decision trees. This paper describes a technique for transforming such trees to small sets of production rules, a common formalism for expressing knowledge in expert systems. The method makes use of the training set of cases from which the decision tree was generated, first to generalize and assess the reliability of individual rules extracted from the tree, and subsequently to refine the collection of rules as a whole. The final set of production rules is usually both simpler than the decision tree from which it was obtained, and more accurate when classifying unseen cases. Transformation to production rules also provides a way of combining different decision trees for the same classification domain.", "title": "Generating Production Rules from Decision Trees"}, "00e752adb3c2e3715c6f2c37756d75d1e9678877": {"paper_id": "00e752adb3c2e3715c6f2c37756d75d1e9678877", "abstract": "Classification rule mining aims to discover a small set of rules in the database that forms an accurate classifier. Association rule mining finds all the rules existing in the database that satisfy some minimum support and minimum confidence constraints. For association rule mining, the target of discovery is not pre-determined, while for classification rule mining there is one and only one predetermined target. In this paper, we propose to integrate these two mining techniques. The integration is done by focusing on mining a special subset of association rules, called class association rules (CARs). An eff icient algorithm is also given for building a classifier based on the set of discovered CARs. Experimental results show that the classifier built this way is, in general, more accurate than that produced by the state-of-the-art classification system C4.5. In addition, this integration helps to solve a number of problems that exist in the current classification systems.", "title": "Integrating Classification and Association Rule Mining"}, "39b58ef6487c893219c77c61c762eee5694d0e36": {"paper_id": "39b58ef6487c893219c77c61c762eee5694d0e36", "abstract": "Classi cation is an important problem in the emerging eld of data mining. Although classi cation has been studied extensively in the past, most of the classi cation algorithms are designed only for memory-resident data, thus limiting their suitability for data mining large data sets. This paper discusses issues in building a scalable classier and presents the design of SLIQ, a new classi er. SLIQ is a decision tree classi er that can handle both numeric and categorical attributes. It uses a novel pre-sorting technique in the tree-growth phase. This sorting procedure is integrated with a breadthrst tree growing strategy to enable classi cation of disk-resident datasets. SLIQ also uses a new tree-pruning algorithm that is inexpensive, and results in compact and accurate trees. The combination of these techniques enables SLIQ to scale for large data sets and classify data sets irrespective of the number of classes, attributes, and examples (records), thus making it an attractive tool for data mining.", "title": "SLIQ: A Fast Scalable Classifier for Data Mining"}, "fec1e79c7d8b72a3640e6d161b10275a81b53e83": {"paper_id": "fec1e79c7d8b72a3640e6d161b10275a81b53e83", "abstract": "This paper studies the energy conversion efficiency for a rectified piezoelectric power harvester. An analytical model is proposed, and an expression of efficiency is derived under steady-state operation. In addition, the relationship among the conversion efficiency, electrically induced damping and ac\u2013dc power output is established explicitly. It is shown that the optimization criteria are different depending on the relative strength of the coupling. For the weak electromechanical coupling system, the optimal power transfer is attained when the efficiency and induced damping achieve their maximum values. This result is consistent with that observed in the recent literature. However, a new finding shows that they are not simultaneously maximized in the strongly coupled electromechanical system.", "title": "Efficiency of energy conversion for a piezoelectric power harvesting system"}, "5ea76cb2a00101cbfec83916d217c3b495c4c3cb": {"paper_id": "5ea76cb2a00101cbfec83916d217c3b495c4c3cb", "abstract": "This paper describes an approach to harvesting electrical energy from a mechanically excited piezoelectric element. A vibrating piezoelectric device differs from a typical electrical power source in that it has a capacitive rather than inductive source impedance, and may be driven by mechanical vibrations of varying amplitude. An analytical expression for the optimal power flow from a rectified piezoelectric device is derived, and an \u201cenergy harvesting\u201d circuit is proposed which can achieve this optimal power flow. The harvesting circuit consists of an ac\u2013dc rectifier with an output capacitor, an electrochemical battery, and a switch-mode dc\u2013dc converter that controls the energy flow into the battery. An adaptive control technique for the dc\u2013dc converter is used to continuously implement the optimal power transfer theory and maximize the power stored by the battery. Experimental results reveal that use of the adaptive dc\u2013dc converter increases power transfer by over 400% as compared to when the dc\u2013dc converter is not used.", "title": "Adaptive Piezoelectric Energy Harvesting Circuit for Wireless Remote Power Supply"}, "6519bf5580fcdcc9c50fd72c6c8dc5d040d443e8": {"paper_id": "6519bf5580fcdcc9c50fd72c6c8dc5d040d443e8", "abstract": "Energy harvesting allows low-power embedded devices to be powered from naturally-ocurring or unwanted environmental energy (e.g. light, vibration, or temperature difference). While a number of systems incorporating energy harvesters are now available commercially, they are specific to certain types of energy source. Energy availability can be a temporal as well as spatial effect. To address this issue, 'hybrid' energy harvesting systems combine multiple harvesters on the same platform, but the design of these systems is not straightforward. This paper surveys their design, including trade-offs affecting their efficiency, applicability, and ease of deployment. This survey, and the taxonomy of multi-source energy harvesting systems that it presents, will be of benefit to designers of future systems. Furthermore, we identify and comment upon the current and future research directions in this field.", "title": "A survey of multi-source energy harvesting systems"}, "34feeafb5ff7757b67cf5c46da0869ffb9655310": {"paper_id": "34feeafb5ff7757b67cf5c46da0869ffb9655310", "abstract": "Environmental energy is an attractive power source for low power wireless sensor networks. We present Prometheus, a system that intelligently manages energy transfer for perpetual operation without human intervention or servicing. Combining positive attributes of different energy storage elements and leveraging the intelligence of the microprocessor, we introduce an efficient multi-stage energy transfer system that reduces the common limitations of single energy storage systems to achieve near perpetual operation. We present our design choices, tradeoffs, circuit evaluations, performance analysis, and models. We discuss the relationships between system components and identify optimal hardware choices to meet an application's needs. Finally we present our implementation of a real system that uses solar energy to power Berkeley's Telos Mote. Our analysis predicts the system will operate for 43 years under 1% load, 4 years under 10% load, and 1 year under 100% load. Our implementation uses a two stage storage system consisting of supercapacitors (primary buffer) and a lithium rechargeable battery (secondary buffer). The mote has full knowledge of power levels and intelligently manages energy transfer to maximize lifetime.", "title": "Perpetual environmentally powered sensor networks"}, "61c1d66defb225eda47462d1bc393906772c9196": {"paper_id": "61c1d66defb225eda47462d1bc393906772c9196", "abstract": "The enormous potential for wireless sensor networks to make a positive impact on our society has spawned a great deal of research on the topic, and this research is now producing environment-ready systems. Current technology limits coupled with widely-varying application requirements lead to a diversity of hardware platforms for different portions of the design space. In addition, the unique energy and reliability constraints of a system that must function for months at a time without human intervention mean that demands on sensor network hardware are different from the demands on standard integrated circuits. This paper describes our experiences designing sensor nodes and low level software to control them.\n In the ZebraNet system we use GPS technology to record fine-grained position data in order to track long term animal migrations [14]. The ZebraNet hardware is composed of a 16-bit TI microcontroller, 4 Mbits of off-chip flash memory, a 900 MHz radio, and a low-power GPS chip. In this paper, we discuss our techniques for devising efficient power supplies for sensor networks, methods of managing the energy consumption of the nodes, and methods of managing the peripheral devices including the radio, flash, and sensors. We conclude by evaluating the design of the ZebraNet nodes and discussing how it can be improved. Our lessons learned in developing this hardware can be useful both in designing future sensor nodes and in using them in real systems.", "title": "Hardware design experiences in ZebraNet"}, "3689220c58f89e9e19cc0df51c0a573884486708": {"paper_id": "3689220c58f89e9e19cc0df51c0a573884486708", "abstract": "AmbiMax is an energy harvesting circuit and a supercapacitor based energy storage system for wireless sensor nodes (WSN). Previous WSNs attempt to harvest energy from various sources, and some also use supercapacitors instead of batteries to address the battery aging problem. However, they either waste much available energy due to impedance mismatch, or they require active digital control that incurs overhead, or they work with only one specific type of source. AmbiMax addresses these problems by first performing maximum power point tracking (MPPT) autonomously, and then charges supercapacitors at maximum efficiency. Furthermore, AmbiMax is modular and enables composition of multiple energy harvesting sources including solar, wind, thermal, and vibration, each with a different optimal size. Experimental results on a real WSN platform, Eco, show that AmbiMax successfully manages multiple power sources simultaneously and autonomously at several times the efficiency of the current state-of-the-art for WSNs", "title": "AmbiMax: Autonomous Energy Harvesting Platform for Multi-Supply Wireless Sensor Nodes"}, "4833d690f7e0a4020ef48c1a537dbb5b8b9b04c6": {"paper_id": "4833d690f7e0a4020ef48c1a537dbb5b8b9b04c6", "abstract": "A low-power low-cost highly efficient maximum power point tracker (MPPT) to be integrated into a photovoltaic (PV) panel is proposed. This can result in a 25% energy enhancement compared to a standard photovoltaic panel, while performing functions like battery voltage regulation and matching of the PV array with the load. Instead of using an externally connected MPPT, it is proposed to use an integrated MPPT converter as part of the PV panel. It is proposed that this integrated MPPT uses a simple controller in order to be cost effective. Furthermore, the converter has to be very efficient, in order to transfer more energy to the load than a directly coupled system. This is achieved by using a simple soft-switched topology. A much higher conversion efficiency at lower cost will then result, making the MPPT an affordable solution for small PV energy systems.", "title": "Integrated photovoltaic maximum power point tracking converter"}, "60adb8b4be9e2ac7f5cdbbd12ef3390ac1da6b17": {"paper_id": "60adb8b4be9e2ac7f5cdbbd12ef3390ac1da6b17", "abstract": "This paper gives a security analysis of Microsoft's ASP.NET technology. The main part of the paper is a list of threats which is structured according to an architecture of Web services and attack points. We also give a reverse table of threats against security requirements as well as a summary of security guidelines for IT developers. This paper has been worked out in collaboration with five University teams each of which is focussing on a different security problem area. We use the same architecture for Web services and attack points.", "title": "Threat Modelling for ASP.NET - Designing Secure Applications"}, "111d09d3d383a8167d74fc342514b2e7a48f9b16": {"paper_id": "111d09d3d383a8167d74fc342514b2e7a48f9b16", "abstract": "Cloud computing provides a revolutionary model for the deployment of enterprise applications and Web services alike. In this new model, cloud users save on the cost of purchasing and managing base infrastructure, while the cloud providers save on the cost of maintaining underutilized CPU, memory, and network resources. In migrating to this new model, users face a variety of issues. Commercial clouds provide several support models to aide users in resolving the reported issues This paper arises from our quest to understand how to design IaaS support models for more efficient user troubleshooting. Using a data driven approach, we start our exploration into this issue with an investigation into the problems encountered by users and the methods utilized by the cloud support\u2019s staff to resolve these problems. We examine message threads appearing in the forum of a large IaaS provider over a 3 year period. We argue that the lessons derived from this study point to a set of principles that future IaaS offerings can implement to provide users with a more efficient support model. This data driven approach enables us to propose a set of principles that are pertinent to the experiences of users and that we believe could vastly improve the SLA observed by the users.", "title": "A First Look at Problems in the Cloud"}, "02cbb22e2011938d8d2c0a42b175e96d59bb377f": {"paper_id": "02cbb22e2011938d8d2c0a42b175e96d59bb377f", "abstract": "Cloud Computing, the long-held dream of computing as a utility, has the potential to transform a large part of the IT industry, making software even more attractive as a service and shaping the way IT hardware is designed and purchased. Developers with innovative ideas for new Internet services no longer require the large capital outlays in hardware to deploy their service or the human expense to operate it. They need not be concerned about overprovisioning for a service whose popularity does not meet their predictions, thus wasting costly resources, or underprovisioning for one that becomes wildly popular, thus missing potential customers and revenue. Moreover, companies with large batch-oriented tasks can get results as quickly as their programs can scale, since using 1000 servers for one hour costs no more than using one server for 1000 hours. This elasticity of resources, without paying a premium for large scale, is unprecedented in the history of IT. Cloud Computing refers to both the applications delivered as services over the Internet and the hardware and systems software in the datacenters that provide those services. The services themselves have long been referred to as Software as a Service (SaaS). The datacenter hardware and software is what we will call a Cloud. When a Cloud is made available in a pay-as-you-go manner to the general public, we call it a Public Cloud; the service being sold is Utility Computing. We use the term Private Cloud to refer to internal datacenters of a business or other organization, not made available to the general public. Thus, Cloud Computing is the sum of SaaS and Utility Computing, but does not include Private Clouds. People can be users or providers of SaaS, or users or providers of Utility Computing. We focus on SaaS Providers (Cloud Users) and Cloud Providers, which have received less attention than SaaS Users. From a hardware point of view, three aspects are new in Cloud Computing.", "title": "Above the Clouds: A Berkeley View of Cloud Computing"}, "0f44833eb9047158221e7b3128cde1347b58ccd6": {"paper_id": "0f44833eb9047158221e7b3128cde1347b58ccd6", "abstract": "Energy-proportional designs would enable large energy savings in servers, potentially doubling their efficiency in real-life use. Achieving energy proportionality will require significant improvements in the energy usage profile of every system component, particularly the memory and disk subsystems.", "title": "The Case for Energy-Proportional Computing"}, "a256647d65b0c81e60e232f4212abcc9a6730aaf": {"paper_id": "a256647d65b0c81e60e232f4212abcc9a6730aaf", "abstract": "Bigtable is a distributed storage system for managing structured data that is designed to scale to a very large size: petabytes of data across thousands of commodity servers. Many projects at Google store data in Bigtable, including web indexing, Google Earth, and Google Finance. These applications place very different demands on Bigtable, both in terms of data size (from URLs to web pages to satellite imagery) and latency requirements (from backend bulk processing to real-time data serving). Despite these varied demands, Bigtable has successfully provided a flexible, high-performance solution for all of these Google products. In this article, we describe the simple data model provided by Bigtable, which gives clients dynamic control over data layout and format, and we describe the design and implementation of Bigtable.", "title": "Bigtable: A Distributed Storage System for Structured Data"}, "7f589cfc94781843e9d01cac74baecf8f4889840": {"paper_id": "7f589cfc94781843e9d01cac74baecf8f4889840", "abstract": "In this work, we present a novel RGB-D SLAM algorithm. The novelty of the proposed algorithm lies in the use of both feature points and plane patches for pose estimation. A plane patch is defined as a small-sized patch constructed by using a feature point with small curvature. The feature points with small curvature are called plane points. The remaining feature points are classified as either smooth points which represent smooth surfaces or structural points which represent edges or corners of some objects. Two criteria (coplanarity and overlap) are used to match the plane patches from two frames. The proposed algorithm is able to weight various types of feature points in order to improve the robustness of SLAM algorithms in favouring plane patches but not the isolated feature points, and maintain the real-time performance of SLAM algorithms in favouring small plane patches, but not the large size of planes. We evaluate the proposed algorithm on multiple benchmark datasets. A large scale experiment is also presented to show the robustness of our algorithm.", "title": "A novel RGB-D SLAM algorithm based on points and plane-patches"}, "24d1afc81644877f6fc34a5a15d7a41e03a4e522": {"paper_id": "24d1afc81644877f6fc34a5a15d7a41e03a4e522", "abstract": "Many popular problems in robotics and computer vision including various types of simultaneous localization and mapping (SLAM) or bundle adjustment (BA) can be phrased as least squares optimization of an error function that can be represented by a graph. This paper describes the general structure of such problems and presents g2o, an open-source C++ framework for optimizing graph-based nonlinear error functions. Our system has been designed to be easily extensible to a wide range of problems and a new problem typically can be specified in a few lines of code. The current implementation provides solutions to several variants of SLAM and BA. We provide evaluations on a wide range of real-world and simulated datasets. The results demonstrate that while being general g2o offers a performance comparable to implementations of state-of-the-art approaches for the specific problems.", "title": "G2o: A general framework for graph optimization"}, "1f2ee7714c3018c1687f9b17f2bc889c5c017439": {"paper_id": "1f2ee7714c3018c1687f9b17f2bc889c5c017439", "abstract": "This paper is a survey of the theory and methods of photogrammetric bundle adjustment, aimed at potential implementors in the computer vision community. Bundle adjustment is the problem of refining a visual reconstruction to produce jointly optimal structure and viewing parameter estimates. Topics covered include: the choice of cost function and robustness; numerical optimization including sparse Newton methods, linearly convergent approximations, updating and recursive methods; gauge (datum) invariance; and quality control. The theory is developed for general robust cost functions rather than restricting attention to traditional nonlinear least squares.", "title": "Bundle Adjustment - A Modern Synthesis"}, "af4c42009bd535d70c6597aef2afe56d10ea3d33": {"paper_id": "af4c42009bd535d70c6597aef2afe56d10ea3d33", "abstract": "CHOLMOD is a set of routines for factorizing sparse symmetric positive definite matrices of the form A or AAT, updating/downdating a sparse Cholesky factorization, solving linear systems, updating/downdating the solution to the triangular system Lx\u2009=\u2009b, and many other sparse matrix functions for both symmetric and unsymmetric matrices. Its supernodal Cholesky factorization relies on LAPACK and the Level-3 BLAS, and obtains a substantial fraction of the peak performance of the BLAS. Both real and complex matrices are supported. CHOLMOD is written in ANSI/ISO C, with both C and MATLABTM interfaces. It appears in MATLAB 7.2 as x\u2009=\u2009A\\b when A is sparse symmetric positive definite, as well as in several other sparse matrix functions.", "title": "Algorithm 887: CHOLMOD, Supernodal Sparse Cholesky Factorization and Update/Downdate"}, "6ca3c5ee075c463f2914e8ec211e041955502ec6": {"paper_id": "6ca3c5ee075c463f2914e8ec211e041955502ec6", "abstract": "We present the design and implementation of a new inexact Newton type algorithm for solving large-scale bundle adjustment problems with tens of thousands of images. We explore the use of Conjugate Gradients for calculating the Newton step and its performance as a function of some simple and computationally efficient preconditioners. We show that the common Schur complement trick is not limited to factorization-based methods and that it can be interpreted as a form of preconditioning. Using photos from a street-side dataset and several community photo collections, we generate a variety of bundle adjustment problems and use them to evaluate the performance of six different bundle adjustment algorithms. Our experiments show that truncated Newton methods, when paired with relatively simple preconditioners, offer state of the art performance for large-scale bundle adjustment. The code, test problems and detailed performance data are available at http://grail.cs.washington.edu/projects/bal.", "title": "Bundle Adjustment in the Large"}, "38e81eae60199fa1ea8f4d8b0af835df14055efb": {"paper_id": "38e81eae60199fa1ea8f4d8b0af835df14055efb", "abstract": "Simultaneous localization and mapping with infinite planes is attractive because of the reduced complexity with respect to both sparse point-based and dense volumetric methods. We show how to include infinite planes into a least-squares formulation for mapping, using a homogeneous plane parametrization with a corresponding minimal representation for the optimization. Because it is a minimal representation, it is suitable for use with Gauss-Newton, Powell's Dog Leg and incremental solvers such as iSAM. We also introduce a relative plane formulation that improves convergence. We evaluate our proposed approach on simulated data to show its advantages over alternative solutions. We also introduce a simple mapping system and present experimental results, showing real-time mapping of select indoor environments with a hand-held RGB-D sensor.", "title": "Simultaneous localization and mapping with infinite planes"}, "2f31ae1aa489430ad5a96a4f3a7633091e819e37": {"paper_id": "2f31ae1aa489430ad5a96a4f3a7633091e819e37", "abstract": "This paper describes a system for performing multisession visual mapping in large-scale environments. Multi-session mapping considers the problem of combining the results of multiple Simultaneous Localisation and Mapping (SLAM) missions performed repeatedly over time in the same environment. The goal is to robustly combine multiple maps in a common metrical coordinate system, with consistent estimates of uncertainty. Our work employs incremental Smoothing and Mapping (iSAM) as the underlying SLAM state estimator and uses an improved appearance-based method for detecting loop closures within single mapping sessions and across multiple sessions. To stitch together pose graph maps from multiple visual mapping sessions, we employ spatial separator variables, called anchor nodes, to link together multiple relative pose graphs. We provide experimental results for multi-session visual mapping in the MIT Stata Center, demonstrating key capabilities that will serve as a foundation for future work in large-scale persistent visual mapping.", "title": "6-DOF Multi-session Visual SLAM using Anchor Nodes"}, "379f61c6f01b70848c89e49370ce999631007d61": {"paper_id": "379f61c6f01b70848c89e49370ce999631007d61", "abstract": "In this paper we describe a method that estimates the motion of a calibrated camera (settled on an experimental vehicle) and the tridimensional geometry of the environment. The only data used is a video input. In fact, interest points are tracked and matched between frames at video rate. Robust estimates of the camera motion are computed in real-time, key-frames are selected and permit the features 3D reconstruction. The algorithm is particularly appropriate to the reconstruction of long images sequences thanks to the introduction of a fast and local bundle adjustment method that ensures both good accuracy and consistency of the estimated camera poses along the sequence. It also largely reduces computational complexity compared to a global bundle adjustment. Experiments on real data were carried out to evaluate speed and robustness of the method for a sequence of about one kilometer long. Results are also compared to the ground truth measured with a differential GPS.", "title": "Real Time Localization and 3D Reconstruction"}, "5205c4e95d5acf639ae21dede605abfe51bce853": {"paper_id": "5205c4e95d5acf639ae21dede605abfe51bce853", "abstract": "Vision-based tracking systems for augmented reality often require that artificial fiducials be placed in the scene. In this paper we utilize our approach for robust detection and tracking of natural features such as textures or corners. The tracked natural features are automatically calibrated to the fiducials that are used to initialize and facilitate normal tracking. Once calibrated, the natural features are used to extend the system\u2019s tracking range and to stabilize the tracked pose against occlusions and noise. The emphasis of this paper is the integration of natural feature tracking with fiducial tracking to increase the range and robustness of vision-based augmented reality tracking.", "title": "Natural feature tracking for extendible robust augmented realities"}, "a7a3a16355838b62c0fdc69f56d09f70e4ff9dab": {"paper_id": "a7a3a16355838b62c0fdc69f56d09f70e4ff9dab", "abstract": "Estimating the pose of a camera (virtual or real) in which some augmentation takes place is one of the most important parts of an augmented reality (AR) system. Availability of powerful processors and fast frame grabbers have made vision-based trackers commonly used due to their accuracy as well as flexibility and ease of use. Current vision-based trackers are based on tracking of markers. The use of markers increases robustness and reduces computational requirements. However, their use can be very complicated, as they require certain maintenance. Direct use of scene features for tracking, therefore, is desirable. To this end, we describe a general system that tracks the position and orientation of a camera observing a scene without any visual markers. Our method is based on a two-stage process. In the first stage, a set of features is learned with the help of an external tracking system while in action. The second stage uses these learned features for camera tracking when the system in the first stage decides that it is possible to do so. The system is very general so that it can employ any available feature tracking and pose estimation system for learning and tracking. We experimentally demonstrate the viability of the method in real-life examples.", "title": "Marker-less Tracking for AR: A Learning-Based Approach"}, "514fa8d4981cc2b2aecfc02e0e3a8f4be717bcd7": {"paper_id": "514fa8d4981cc2b2aecfc02e0e3a8f4be717bcd7", "abstract": "This paper presents a novel version of the five-point relative orientation algorithm given in Nist\u00e9r (2004). The name of the algorithm arises from the fact that it can operate even on the minimal five point correspondences required for a finite number of solutions to relative orientation. For the minimal five correspondences the algorithm returns up to ten real solutions. The algorithm can also operate on many points. Like the previous version of the five-point algorithm, our method can operate correctly even in the face of critical surfaces, including planar and ruled quadric scenes. The paper presents comparisons with other direct methods, including the previously developed five-point method, two different six-point methods, the seven-point method, and the eight-point method. It is shown that the five-point method is superior in most cases among the direct methods. The new version of the algorithm was developed from the perspective of algebraic geometry and is presented in the context of computing a Gr\u00f6bner basis. The constraints are formulated in terms of polynomial equations in the entries of the fundamental matrix. The polynomial equations generate an algebraic ideal for which a Gr\u00f6bner basis is computed. The Gr\u00f6bner basis is used to compute the action matrix for multiplication by a single variable monomial. The eigenvectors of the action matrix give the solutions for all the variables and thereby also relative orientation. Using a Gr\u00f6bner basis makes the solution clear and easy to explain.", "title": "Recent Developments on Direct Relative Orientation"}, "230ad73e8bd1d3268d56c66a83442d24176b864d": {"paper_id": "230ad73e8bd1d3268d56c66a83442d24176b864d", "abstract": "This paper presents ORB-SLAM, a feature-based monocular simultaneous localization and mapping (SLAM) system that operates in real time, in small and large indoor and outdoor environments. The system is robust to severe motion clutter, allows wide baseline loop closing and relocalization, and includes full automatic initialization. Building on excellent algorithms of recent years, we designed from scratch a novel system that uses the same features for all SLAM tasks: tracking, mapping, relocalization, and loop closing. A survival of the fittest strategy that selects the points and keyframes of the reconstruction leads to excellent robustness and generates a compact and trackable map that only grows if the scene content changes, allowing lifelong operation. We present an exhaustive evaluation in 27 sequences from the most popular datasets. ORB-SLAM achieves unprecedented performance with respect to other state-of-the-art monocular SLAM approaches. For the benefit of the community, we make the source code public.", "title": "ORB-SLAM: A Versatile and Accurate Monocular SLAM System"}, "2ee07f241c725023494b2a9a61b00a409e5b9708": {"paper_id": "2ee07f241c725023494b2a9a61b00a409e5b9708", "abstract": "Received August 6,1986; accepted November 25,1986 Finding the relationship between two coordinate systems using pairs of measurements of the coordinates of a number of points in both systems is a classic photogrammetric task. It finds applications in stereophotogrammetry and in robotics. I present here a closed-form solution to the least-squares problem for three or more points. Currently various empirical, graphical, and numerical iterative methods are in use. Derivation of the solution is simplified by use of unit quaternions to represent rotation. I emphasize a symmetry property that a solution to this problem ought to possess. The best translational offset is the difference between the centroid of the coordinates in one system and the rotated and scaled centroid of the coordinates in the other system. The best scale is equal to the ratio of the root-mean-square deviations of the coordinates in the two systems from their respective centroids. These exact results are to be preferred to approximate methods based on measurements of a few selected points. The unit quaternion representing the best rotation is the eigenvector associated with the most positive eigenvalue of a symmetric 4 X 4 matrix. The elements of this matrix are combinations of sums of products of corresponding coordinates of the points.", "title": "Closed-form solution of absolute orientation using unit quaternions"}, "6c5c61bc780b9a696ef72fb8f27873fa7ae33215": {"paper_id": "6c5c61bc780b9a696ef72fb8f27873fa7ae33215", "abstract": "We propose a novel and fast multiscale feature detection and description approach that exploits the benefits of nonlinear scale spaces. Previous attempts to detect and describe features in nonlinear scale spaces are highly time consuming due to the computational burden of creating the nonlinear scale space. In this paper we propose to use recent numerical schemes called Fast Explicit Diffusion (FED) embedded in a pyramidal framework to dramatically speed-up feature detection in nonlinear scale spaces. In addition, we introduce a Modified-Local Difference Binary (M-LDB) descriptor that is highly efficient, exploits gradient information from the nonlinear scale space, is scale and rotation invariant and has low storage requirements. We present an extensive evaluation that shows the excellent compromise between speed and performance of our approach compared to state-of-the-art methods such as BRISK, ORB, SURF, SIFT and KAZE.", "title": "Fast Explicit Diffusion for Accelerated Features in Nonlinear Scale Spaces"}, "92747e1b72eedc38416e836fbb82d236bcd9fb32": {"paper_id": "92747e1b72eedc38416e836fbb82d236bcd9fb32", "abstract": "HAL is a multi-disciplinary open access archive for the deposit and dissemination of scientific research documents, whether they are published or not. The documents may come from teaching and research institutions in France or abroad, or from public or private research centers. L\u2019archive ouverte pluridisciplinaire HAL, est destin\u00e9e au d\u00e9p\u00f4t et \u00e0 la diffusion de documents scientifiques de niveau recherche, publi\u00e9s ou non, \u00e9manant des \u00e9tablissements d\u2019enseignement et de recherche fran\u00e7ais ou \u00e9trangers, des laboratoires publics ou priv\u00e9s. Listen and Translate: A Proof of Concept for End-to-End Speech-to-Text Translation Alexandre B\u00e9rard, Olivier Pietquin, Laurent Besacier, Christophe Servan", "title": "Listen and Translate: A Proof of Concept for End-to-End Speech-to-Text Translation"}, "17aa78bd4331ef490f24bdd4d4cd21d22a18c09c": {"paper_id": "17aa78bd4331ef490f24bdd4d4cd21d22a18c09c", "abstract": "We consider the problem of building high-level, class-specific feature detectors from only unlabeled data. For example, is it possible to learn a face detector using only unlabeled images? To answer this, we train a deep sparse autoencoder on a large dataset of images (the model has 1 billion connections, the dataset has 10 million 200\u00d7200 pixel images downloaded from the Internet). We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three days. Contrary to what appears to be a widely-held intuition, our experimental results reveal that it is possible to train a face detector without having to label images as containing a face or not. Control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation. We also find that the same network is sensitive to other high-level concepts such as cat faces and human bodies. Starting from these learned features, we trained our network to recognize 22,000 object categories from ImageNet and achieve a leap of 70% relative improvement over the previous state-of-the-art.", "title": "Building high-level features using large scale unsupervised learning"}, "c9ab6adcf149c740b03ef3759260719af4f7ce07": {"paper_id": "c9ab6adcf149c740b03ef3759260719af4f7ce07", "abstract": "This paper describes the University of Edinburgh\u2019s (UEDIN) phrase-based submissions to the translation and medical translation shared tasks of the 2014 Workshop on Statistical Machine Translation (WMT). We participated in all language pairs. We have improved upon our 2013 system by i) using generalized representations, specifically automatic word clusters for translations out of English, ii) using unsupervised character-based models to translate unknown words in RussianEnglish and Hindi-English pairs, iii) synthesizing Hindi data from closely-related Urdu data, and iv) building huge language on the common crawl corpus.", "title": "Edinburgh's Phrase-based Machine Translation Systems for WMT-14"}, "0894b06cff1cd0903574acaa7fcf071b144ae775": {"paper_id": "0894b06cff1cd0903574acaa7fcf071b144ae775", "abstract": "Recent work has shown success in using neural network language models (NNLMs) as features in MT systems. Here, we present a novel formulation for a neural network joint model (NNJM), which augments the NNLM with a source context window. Our model is purely lexicalized and can be integrated into any MT decoder. We also present several variations of the NNJM which provide significant additive improvements. Although the model is quite simple, it yields strong empirical results. On the NIST OpenMT12 Arabic-English condition, the NNJM features produce a gain of +3.0 BLEU on top of a powerful, featurerich baseline which already includes a target-only NNLM. The NNJM features also produce a gain of +6.3 BLEU on top of a simpler baseline equivalent to Chiang\u2019s (2007) original Hiero implementation. Additionally, we describe two novel techniques for overcoming the historically high cost of using NNLM-style models in MT decoding. These techniques speed up NNJM computation by a factor of 10,000x, making the model as fast as a standard back-off LM. This work was supported by DARPA/I2O Contract No. HR0011-12-C-0014 under the BOLT program (Approved for Public Release, Distribution Unlimited). The views, opinions, and/or findings contained in this article are those of the author and should not be interpreted as representing the official views or policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the Department of Defense.", "title": "Fast and Robust Neural Network Joint Models for Statistical Machine Translation"}, "b289b0485f98a1fa0c22c9176998535937227e5d": {"paper_id": "b289b0485f98a1fa0c22c9176998535937227e5d", "abstract": "For many low-resource languages, spoken language resources are more likely to be annotated with translations than transcriptions. This bilingual speech data can be used for word-spotting, spoken document retrieval, and even for documentation of endangered languages. We experiment with the neural, attentional model applied to this data. On phoneto-word alignment and translation reranking tasks, we achieve large improvements relative to several baselines. On the more challenging speech-to-word alignment task, our model nearly matches GIZA++\u2019s performance on gold transcriptions, but without recourse to transcriptions or to a lexicon.", "title": "An Attentional Model for Speech Translation Without Transcription"}, "592a6d781309423ceb95502e92e577ef5656de0d": {"paper_id": "592a6d781309423ceb95502e92e577ef5656de0d", "abstract": "Neural encoder-decoder models of machine translation have achieved impressive results, rivalling traditional translation models. However their modelling formulation is overly simplistic, and omits several key inductive biases built into traditional models. In this paper we extend the attentional neural translation model to include structural biases from word based alignment models, including positional bias, Markov conditioning, fertility and agreement over translation directions. We show improvements over a baseline attentional model and standard phrase-based model over several language pairs, evaluating on difficult languages in a low resource setting.", "title": "Incorporating Structural Alignment Biases into an Attentional Neural Translation Model"}, "dbca66128d360286c6a5e99214101ae356b93ef2": {"paper_id": "dbca66128d360286c6a5e99214101ae356b93ef2", "abstract": "In this paper, we describe a new model for word alignment in statistical translation and present experimental results. The idea of the model is to make the alignment probabilities dependent on the differences in the alignment positions rather than on the absolute positions. To achieve this goal, the approach uses a first-order Hidden Markov model (HMM) for the word alignment problem as they are used successfully in speech recognition for the time alignment problem. The difference to the time alignment HMM is that there is no monotony constraint for the possible word orderings. We describe the details of the model and test the model on several bilingual corpora.", "title": "HMM-Based Word Alignment in Statistical Translation"}, "f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97": {"paper_id": "f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97", "abstract": "We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, and machine translation.", "title": "Recurrent Neural Network Regularization"}, "4902805fe1e2f292f6beed7593154e686d7f6dc2": {"paper_id": "4902805fe1e2f292f6beed7593154e686d7f6dc2", "abstract": "Recognizing lines of unconstrained handwritten text is a challenging task. The difficulty of segmenting cursive or overlapping characters, combined with the need to exploit surrounding context, has led to low recognition rates for even the best current recognizers. Most recent progress in the field has been made either through improved preprocessing or through advances in language modeling. Relatively little work has been done on the basic recognition algorithms. Indeed, most systems rely on the same hidden Markov models that have been used for decades in speech and handwriting recognition, despite their well-known shortcomings. This paper proposes an alternative approach based on a novel type of recurrent neural network, specifically designed for sequence labeling tasks where the data is hard to segment and contains long-range bidirectional interdependencies. In experiments on two large unconstrained handwriting databases, our approach achieves word recognition accuracies of 79.7 percent on online data and 74.1 percent on offline data, significantly outperforming a state-of-the-art HMM-based system. In addition, we demonstrate the network's robustness to lexicon size, measure the individual influence of its hidden layers, and analyze its use of context. Last, we provide an in-depth discussion of the differences between the network and HMMs, suggesting reasons for the network's superior performance.", "title": "A Novel Connectionist System for Unconstrained Handwriting Recognition"}, "b118e5ca88647661f27438a76c324db5f04a1b21": {"paper_id": "b118e5ca88647661f27438a76c324db5f04a1b21", "abstract": "Preventing feature co-adaptation by encouraging independent contributions from different features often improves classification and regression performance. Dropout training (Hinton et al., 2012) does this by randomly dropping out (zeroing) hidden units and input features during training of neural networks. However, repeatedly sampling a random subset of input features makes training much slower. Based on an examination of the implied objective function of dropout training, we show how to do fast dropout training by sampling from or integrating a Gaussian approximation, instead of doing Monte Carlo optimization of this objective. This approximation, justified by the central limit theorem and empirical evidence, gives an order of magnitude speedup and more stability. We show how to do fast dropout training for classification, regression, and multilayer neural networks. Beyond dropout, our technique is extended to integrate out other types of noise and small image transformations.", "title": "Fast dropout training"}, "965c9aec5e68d49142c5af6a9f0a984f6c2c743a": {"paper_id": "965c9aec5e68d49142c5af6a9f0a984f6c2c743a", "abstract": "We recently showed that Long Short-Term Memory (LSTM) recurrent neural networks (RNNs) outperform state-of-the-art deep neural networks (DNNs) for large scale acoustic modeling where the models were trained with the cross-entropy (CE) criterion. It has also been shown that sequence discriminative training of DNNs initially trained with the CE criterion gives significant improvements. In this paper, we investigate sequence discriminative training of LSTM RNNs in a large scale acoustic modeling task. We train the models in a distributed manner using asynchronous stochastic gradient descent optimization technique. We compare two sequence discriminative criteria \u2013 maximum mutual information and state-level minimum Bayes risk, and we investigate a number of variations of the basic training strategy to better understand issues raised by both the sequential model, and the objective function. We obtain significant gains over the CE trained LSTM RNN model using sequence discriminative training techniques.", "title": "Sequence discriminative distributed training of long short-term memory recurrent neural networks"}, "5b156a3749b4110e328bcf2b147dc3d71ac09fc7": {"paper_id": "5b156a3749b4110e328bcf2b147dc3d71ac09fc7", "abstract": "When a user has watched, say, 70 romance movies and 30 action movies, then it is reasonable to expect the personalized list of recommended movies to be comprised of about 70% romance and 30% action movies as well. This important property is known as calibration, and recently received renewed attention in the context of fairness in machine learning. In the recommended list of items, calibration ensures that the various (past) areas of interest of a user are reflected with their corresponding proportions. Calibration is especially important in light of the fact that recommender systems optimized toward accuracy (e.g., ranking metrics) in the usual offline-setting can easily lead to recommendations where the lesser interests of a user get crowded out by the user's main interests-which we show empirically as well as in thought-experiments. This can be prevented by calibrated recommendations. To this end, we outline metrics for quantifying the degree of calibration, as well as a simple yet effective re-ranking algorithm for post-processing the output of recommender systems.", "title": "Calibrated recommendations"}, "64edd2c5c41e856695e1dcb950c0512c3a87edec": {"paper_id": "64edd2c5c41e856695e1dcb950c0512c3a87edec", "abstract": "In this work we present topic diversification, a novel method designed to balance and diversify personalized recommendation lists in order to reflect the user's complete spectrum of interests. Though being detrimental to average accuracy, we show that our method improves user satisfaction with recommendation lists, in particular for lists generated using the common item-based collaborative filtering algorithm.Our work builds upon prior research on recommender systems, looking at properties of recommendation lists as entities in their own right rather than specifically focusing on the accuracy of individual recommendations. We introduce the intra-list similarity metric to assess the topical diversity of recommendation lists and the topic diversification approach for decreasing the intra-list similarity. We evaluate our method using book recommendation data, including offline analysis on 361, !, 349 ratings and an online study involving more than 2, !, 100 subjects.", "title": "Improving recommendation lists through topic diversification"}, "3219f5733e87afbe1481f7f99c0d06c7c9752c31": {"paper_id": "3219f5733e87afbe1481f7f99c0d06c7c9752c31", "abstract": ".-0/01 2323-04657-98 :<;7:<=>-023: ? @ @ AB;C:<=!? =>DE:<=>DE/F? AG? 465IHJ4 1FK AE-F57L -M5 DE:<N /91 O -98P;Q=>-0/!R 4 D+SUT -0: =>1V=>R @78>1 W AE-02X1 Y#2Z? HJDE4 L[@78>1J5 T /\\=]8>-0/01 2QN 23-9465 ?^=>DE1 4 : 57T 8>DE4 L_?`AEDEO /0T :<=>1 23-98 D(4U=>-98!? /9=>DE1 4a? 4653=>R -9;b?^8>? /\\R DE-0OJD(4 LQK D(5 -0:P@78>-F? 5 :PT /0/0-0:P: DE4acdN<e]1 2323-98>/0-[4 1^K ? 5 ?F;7:0f gh4 =>R DE:`@6? @i-98FjdK -ZDE4JO -0:<=>DEL ?^=>-C:P-9O -98!? Ak=>-0/\\R 4 D(SUT -0:[Yl1 83? 46? AE;7m0DE4 L A(?^8>L -9Nn:P/F? A(-`@ T 8>/!R6? :P-M? 465 @78>-9Yo-\\8>-04 /0-M5 ? =!?QYo1 8p=>R -V@ T 8>@i1 :P-V1 Y @78>1J5 T /0DE4 L%T :P-\\YoT A 8>-0/01 2323-0465 ? =>DE1 4 :3=>1q/0T :<=>1 23-98>:0frg 4s@ ? 8PN =>DE/0T A(? 8Fj]K]? @ @ AB;t?u/01 AEAE-0/9=>DE1 4s1 Y ? AEL 1 8>DB=>R 23:Q:PT /\\Rv? :`=P8!? 5 DBN =>DE1 46? Aw5 ? =!?Q23DE4 D(4 L j 4 -F?^8>-0:<=PNn4 -0DEL R7Wi1 8p/91 AEA(? Wi1 8!?^=>DEO -_x AE=>-\\8>D(4 L j ? 465V5 DE23-04 :PDE1 46? AEDE= ;[8>-F5 T /9=>DE1 4V1 4[= K 1y5 DBz -98>-04U=d5 ? =!? :P-9=>:0fG{ R x 8>:<=.5 ?^=!?`:P-9= K ? :p57-98>DEO -F53Yl8>1 2|=>R -pK]-0W Nn@ T78>/\\R ? :PDE4 LM=P8!? 4 :>? /9N =>DE1 4}1 Y_?qA(? 8>L -~cdN /91 2323-98>/0-a/91 23@6? 4U;tK R -98>-F? :C=>R -u:P-0/91 465 5 ? =!?`:P-\\=.K ? : /01 A(AE-0/\\=>-F5IYl8>1 2\u0080\u007fI1^OJDE-F\u0081#-94 : 231^O7DE-p8>-9/01 2323-04 5 ?^N =>DE1 4~:PDB=>f \u0082 1 8y=>R -M-9\u00837@i-98>DE23-04U=!? A @ T 8>@i1 :PjiK -`5 DEOJD+57-[=>R -V8>-0/9N 1 2323-0465 ? =>DE1 4\u0084L -94 -98!?^=>D(1 4%@ 8>1U/0-9:P:MDE4U=>1I=>R78>-0-Q:PT W~@ 8>1U/0-9:P:P-0:h\u0085 8>-9@ 8>-0:P-04U=!?^=>D(1 4\u00861 Y DE4 @ T =[5 ? =!?Jj 4 -0DEL RJWi1 8>R 1U1J5uYl1 8>2Z? =>DE1 4#j ? 465 8>-9/01 2323-04 5 ?^=>D(1 4ZL -04 -98!? =>DE1 4 fG\u0087q-y5 -0OJDE:P57DEz -\\8>-04U= =>-0/!R 4 D(SUT -9: Yl1 8 5 DBz -98>-04U= :PT WI@ 8>1U/0-0:P:P-9: ? 4 5 ? @ @ AB;b=>R -0DB8./01 2_W DE46?^=>DE1 4 :.1 4 1 T 8`5 ?^=!? :P-9=>:_=>1a/01 23@6?^8>-QYo1 8_8>-0/01 2323-0465 ? =>DE1 4%SUT ? AEDB=h;\u0086? 465 @i-\\8PYo1 8>2Z? 4 /0f 1. INTRODUCTION { R A(? 8>L -0:<=CcGNn/01 2323-98>/0-I:PDB=>-0:Z1 z -98Z23DEA(AEDE1 4 :31 Y[@78>1J5 T /9=>: Yl1 8b:>? AEf}e]R 1U1 :PD(4 L\u0088? 231 4 L\u0084:P1\u00862Z? 4U;%1 @ =>DE1 4 :3DE:b/!R6? AEA(-94 L DE4 L Yl1 8V/01 4 :PT 23-98>:0f[,.-0/01 2323-9465 -\\8p:<;7:<=>-023:yR6?FO -M-023-98>L -05uDE4u8>-9N :P@i1 4 :P-3=>1I=>R D(:M@ 8>1 W AE-02 fC\u0089\u008a8>-9/01 2323-04 5 -98V:<;7:<=>-02\u008bYl1 8`? 4%cdN /91 2323-98>/0-V:PDB=>-V8>-0/0-0DEO -0:yDE4 Yo1 8>2Z? =>DE1 4aY+8>1 2\u008c?Z/91 4 :PT 23-98p? Wi1 T7= K R DE/\\Ra@78>1J5 T /\\=>:y:PR -MDE:.DE4J=>-\\8>-0:<=>-F5 DE4#j ? 465C8>-9/01 2323-04 5 : @78>1J57N T /9=>:V=>R6? =`? 8>-ZAEDEH -0AB;\u0086=>1 x =`R -98_4 -0-F57:0fu{ 1J5 ?F;Uj 8>-0/01 2323-9465 -\\8 :<;7:<=>-023:.?^8>-V5 -0@ A(1F;U-05C1 4IR7T 465J8>-F5 : 1 Y 5 DBz -98>-04U= :PDB=>-0:0j6:P-\\8>O7DE4 L 23DEAEAEDE1 4 :.1 Yw/01 4 :PT 23-98>:0f \u008d 4 -k1 Y =>R -]-F?^8>A(DE-0:<= ? 465V231 :<= :PT /0/0-9:P:<YoT AU8>-0/91 2323-04 5 -98#=>-0/\\R7N 4 1 AE1 L DE-0:pDE:M\u008e>\u008f \u0090o\u0090E\u0091 \u0092>\u008f \u0093<\u0091 \u0094\u0096\u0095o\u0097 \u0098]\u0099 \u0090 \u0094 \u0098\\\u0093\\\u0095o\u009aU\u009bu\u009c \u009dU\u009e j \u009d \u009f7j#\u009e^ Jj \u009e^\u00a1F\u00a2\u00a3f e]1 A(A(? Wi1 8!? N =>DEO x6AB=>-98>DE4 LVK]1 8>HJ:]WU;`W T DEA(5 DE4 L_?V5 ? =!? W ? :P-y1 Y @ 8>-9Yl-98>-04 /0-0:]Yl1 8 @78>1J5 T /9=>:pWU; /91 4 :PT 23-98>:0fy\u0089\u00a44 -9K\u00a5/01 4 :PT 23-98Fji\u00a6y-01 j DE:y2Z? =>/!R -F5 ? L ? D(4 :<=p=>R -V5 ? =!? W6? :P-M=>1357DE:P/01^O -98M\u009a \u0098\\\u0095(\u009b^\u00a7 \u0092>\u008f \u0093! \u0308>jiK R DE/!Ru? 8>-V1 =>R -98 /91 4 :PT 23-98>:VK R 1aR6?FO -ZR D(:<=>1 8>DE/F? AEAB;%R6? 5q:PDE23D(A(?^8`=!? :<=>-3=>1a\u00a6y-01 f Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. EC\u201900, October 17-20, 2000, Minneapolis, Minnesota. Copyright 2000 ACM 1-58113-272-7/00/0010 .. \u00a9 5.00 ad8>1J5 T /\\=>:p=>R6?^=p=>R -M4 -9D(L RJWi1 8>:pAEDEH -`? 8>-_=>R -04 8>-0/01 2323-04657-F5C=>1 \u00a6p-91 jG? :_R -ZK DEAEA]@78>1 W ? W AB;q? AE:P1aAED(H -Z=>R -02 fIe]1 AEA(? Wi1 8!? =>DEO -Cx ABN =>-98>DE4 LIR6? :VWi-0-94qO -98P;\u0084:PT /9/0-0:P:<YoT A]DE4\u0086Wi1 =>R\u00868>-0:P-F?^8>/\\Rq? 465~@ 8!? /\\N =>DE/0f%\u00aby1FK -0O -\\8Fjk=>R -98>-b8>-92Z? DE4\u0088D(23@i1 8P=!? 4U=M8>-0:P-F? 8>/!RvSJT -0:<=>DE1 4 : DE4 1^O -98>/01 23DE4 L`= K 1QYoT 465 ? 23-04U=!? A /!R6? A(AE-04 L -0:.Yo1 8p/01 AEA+? Wi1 8!?^=>D(O x6AB=>-98>DE4 L`8>-0/01 2323-04657-98 :<;7:<=>-023:0f { R x 8>:<=]/\\R ? AEAE-04 L -pDE:k=>1VDE23@ 8>1^O =>R -y:P/F? A(? W DEAEDB=h;Z1 Y#=>R -./91 ABN A(? Wi1 8!? =>DEO -Ix AE=>-\\8>D(4 Lq? A(L 1 8>DB=>R 23:0fs{ R -9:P-a? AEL 1 8>DB=>R 23:3?^8>-a? W A(=>1a:P-F?^8>/\\Rq=>-04 :_1 Y =>R 1 T :>? 4 5 :V1 Y.@i1 =>-94J=>D(? A]4 -9D(L RJWi1 8>:MD(4q8>-0? ABN =>DE23j6W T = =>R -M57-02Z? 4 5 : 1 Y 231J5 -\\8>4acGNn/01 2323-98>/0-[:<;7:<=>-023:.? 8>=>1u:P-F? 8>/!Rq=>-04 :`1 Y.23DEAEAED(1 4 :_1 Y.@i1 =>-04U=>D(? A]4 -0DEL RJWi1 8>:0f\u0086\u0082 T 8P=>R -\\8Fj -9\u00837DE:<=>DE4 Lq? AEL 1 8>DB=>R 23:3R6?FO -I@i-\\8PYo1 8>2Z? 4 /0-I@ 8>1 W AE-023:`K DB=>RsDE4657DBN OJD(5 T6? A /01 4 :PT 23-98>:dYo1 8 K R 1 2\u00a4=>R :PDB=>R6? : A(? 8>L -p? 231 T 4U=>:G1 Y DE47N Yo1 8>2Z? =>DE1 4 fk\u0082 1 8.DE4 :<=!? 4 /0j DBYG?`:PDB=>-[DE: T :PDE4 L_W 8>1FK :PDE4 L3@6?^=P=>-98>4 : ? : DE4 5 DE/F? =>DE1 4 : 1 Y @ 8>1J57T /9= @78>-9Yo-\\8>-04 /0j DB= 2Z?0;3R ?^O -y=>R 1 T :>? 465 : 1 Yd5 ? =!?M@i1 DE4U=>: Yo1 8yDB=>: 231 :<= O ? A(T ? W A(-[/9T :<=>1 23-\\8>:0f { R -0:P\u00ac AE1 4 L /0T :<=>1 23-98d8>1FK :P\u00ad :PAE1FKv5 1FK 4_=>R 4JT 2`Wi-\\8w1 Y 4 -0DEL R7Wi1 8>:d=>R6? =w/F? 4 Wi-[:P-F?^8>/\\R -F5I@i-98 :P-0/91 465 j YlT 8P=>R -98 8>-F5 T /0DE4 LQ:P/F? A(? W DEAEDB=h;Uf { R -y:P-0/01 465Z/\\R ? AEAE-04 L DE: =>1_DE23@ 8>1^O =>R -pSUT6? AEDB=h;Q1 Y =>R -y8>-0/\\N 1 2323-9465 ?^=>DE1 4 :.Yo1 8[=>R -`/91 4 :PT 23-98>:0f`e]1 4 :PT 23-98>: 4 -0-F5a8>-0/01 2QN 23-04 5 ? =>DE1 4 : =>R -9;b/F? 4I=P8>T :<= =>13R -0AE@I=>R -02Xx6465b@ 8>1J57T /9=>: =>R -\\; K DEAEAdA(DEH fygnYk?Z/91 4 :PT 23-98 =P8>T :<=>:p?38>-9/01 2323-04 5 -98 :<;7:<=>-02 j6@ T 8PN /!R6? :P-9:_?b@ 8>1J57T /9=Fj ? 4 5\u0084x64 5 : 1 T =VR -35 1U-0:[4 1 =VAEDEH -`=>R -Q@ 8>1J5JN T /\\=Fj6=>R -M/01 4 :PT 23-98.K DEAEAdWi-_T 4 AEDEH -0AB;I=>1ZT :P-V=>R -[8>-0/91 2323-04 5 -98 :<;7:<=>-02\u00ae? L ? DE4 fk,.-0/01 2323-04657-98 :<;7:<=>-023:0j7AEDEH 1 =>R -98 :P-F?^8>/\\RC:<;7:<N =>-023:0j R6?FO -k=hK 1.=h;7@i-0:w1 Y6/!R6?^8!? /9=>-98>DE:<=>DE/ -98P8>1 8>:0 \u0304 \u00b00\u0091 \u0090  \u03089\u0098]\u009a \u0098\u00a3\u009b \u0091 \u0094\u0096\u0095o\u0097 \u0098\\ \u0308>j K R DE/!R\u0084? 8>-`@78>1J5 T /9=>:y=>R ? =[?^8>-_4 1 =y8>-0/01 2323-9465 -05 j =>R 1 T L Ra=>R /01 4 :PT 23-\\8bK]1 T A+5\u00b1AEDEH -a=>R -92 j ? 4 5%\u00b00\u0091 \u0090  \u03089\u0098327\u008f  \u0308!\u0095l\u0094\u0096\u0095o\u0097 \u0098\\ \u0308>jyK R D(/!R3? 8>@ 8>1J57T /9=>:k=>R6? = ? 8>-y8>-9/01 2323-04 5 -F5 j =>R 1 T L Rb=>R -p/01 4 :PT 23-\\8 5 1U-9: 4 1 =.A(DEH -[=>R -02 f gh4C=>R -VcGNn/01 2323-98>/0-[571 2Z? D(4C=>R -[231 :<= DE23@i1 8PN =!? 4U= -98P8>1 8>:G=>1 ?^O 1 D(5Q? 8>Yo? AE:P@i1 :PDB=>DEO -0:0jU:PDE4 /9=>R -0:P-98P8>1 8>: K DEAEA AE-F? 5 =>1b? 4 L 8P;a/91 4 :PT 23-98>:0j ? 465 :PDE4 /0-M=>R -98>-M?^8>-_T :PT6? AEAB;a2Z? 4J; @ 8>1J57T /9=>:V1 4%? 4\u0088cdNn/01 2323-98>/9-Q:PDB=>-Z=>R ? =_?I/01 4 :PT 23-98VK D(AEAkA(DEH =>1V@ T78>/\\R ? :PjJ:P1V=>R -98>-.DE: 4 1[8>-F? :P1 4Z=>1[8>D(:PHQ8>-9/01 2323-04 5 DE4 Lp1 4 :PR K DEAEA 4 1 = AEDEH f gh4V:P1 23-]K ?0;7: =>R -9:P-]=hK]1y/!R6? A(AE-04 L -0: ?^8>DE4_/01 4  \u0301 D(/\\=Fj :PDE4 /9-]=>R AE-0:P:V=>D(23-Q? 4q? AEL 1 8>DB=>R 2\u03bc:P@i-04 5 :[:P-F?^8>/\\R DE4 L Yo1 8M4 -0DEL RJWi1 8>:0jw=>R 231 8>-V:P/F? A(? W AE-_DE=.K DEAEAdWij ? 4 5I=>R -[K 1 8>:P-`DB=>:ySJT ? AEDB=h;Uf.\u0082 1 8y=>R DE: 8>-F? :P1 4#j DB=bDE:ZD(23@i1 8P=!? 4U=3=>1\u0086=P8>-F?^=3=>R = K 1q/!R6? A(AE-04 L -0:b:PDE2`T AEN =!? 4 -01 T :PAB;s:P1\u0086=>R :P1 AET7=>D(1 4 :b5 DE:P/01^O -\\8>-F5\u00b1? 8>-aWi1 =>RsT :P-9YoT Ap? 465 @ 8!? /9=>DE/F? A\u0096f 1.1 Problem Statement gh4a=>R D(:y@6? @i-98Fj K -V8>-0:P-F? 8>/!R~=>R -0:P-V=hK 1b/!R6? AEAE-04 L -0: =>1 L -9=>R -98Fj WU;I:<=>T 57;7DE4 L34 -9K\u00a4? 465 -9\u00837DE:<=>DE4 Lb? AEL 1 8>DB=>R 23:y=>R ? =yR6?FO -[=>R -V@i1 N =>-04U=>D(? A =>1\u0088DE23@ 8>1^O -uWi1 =>R3:P/F? A(? W DEAEDB=h;*? 4653SUT6? AEDB=h;s1 YM8>-0/01 2QN 23-04 5 -98.:<;7:<=>-023:0f { R -98>-_R ? :.Wi-0-04aAEDB=P=>AE-MK]1 8>HI1 4a-9\u00837@i-98>DE23-04 N =!? A O ? AED(5 ?^=>D(1 4I1 Y 8>-0/01 2323-04657-98 :<;7:<=>-023: ? L ? DE4 :<=.?_:P-\\= 1 Y 8>-0? ABN K]1 8>A(5b5 ? =!? :P-9=>: K DB=>R3=>R 4 1 =!? W AE-p-9\u00837/0-0@7=>DE1 431 Yk\u009c \u00a1^\u00a2 \\fk\u007fI1 8>-y-9\u0083JN @i-\\8>D(23-94J=!? A O ? A(D(5 ? =>DE1 4 DE: 4 -9-F5 -F5C? L ? DE4 :<= 8>-F? AEN\u00a3K]1 8>A(5a5 ?^=!? :P-9=>:0j ? 465ZDB= D(:]D(23@i1 8P=!? 4U= =>R ? = =>R -0:P5 ? =!? :P-\\=>: D(4 /0AET657cdN /91 2323-98>/05 ? =!?Q? :.K]-0AEA ? :y/91 4U=>-04U=.5 ?^=!?7f { R -3Yo1U/0T :`1 Y =>R DE:_@ ? @i-98MDE:_= K 1 N\u00a3Yl1 A(5 fu\u0082wDB8>:<=FjdK]-b@78>1^O7D(57-Z? :<;7:<=>-02Z?^=>DE/ -9\u00837@i-98>DE23-04U=!? A -0O ? AET ? =>DE1 4v1 Y[57DBz -98>-04U=3=>-0/!R 4 D(SUT -9: Yl1 8_8>-0/01 2323-9465 -\\8V:<; :<=>-923:0jG? 4 5q:P-0/01 4 5 jdK]-b@78>-0:P-04U=M4 -9K|? ABN L 1 8>DB=>R 23:_=>R6? =Q? 8>@ ? 8P=>DE/0T A+?^8>AB;\u0088:PT DB=>-F5%Yo1 8Q:P@6? 8>:P5 ? =!?u:P-9=>:0j :PT /\\RI? : =>R 1 :P-p=>R6?^=.? 8>/91 23231 4bDE4 cdNn/01 2323-98>/9-p? @ @ AEDE/F?^=>D(1 4 : 1 Yd8>-0/01 2323-04657-98 =>-0/!R 4 1 AE1 L ;Uf { R -0:P-M? AEL 1 8>DE=>R 23: R6?FO -[/!R6? 8!? /9N =>-\\8>D(:<=>DE/0:Q=>R ? =`2Z? H -Z=>R -02 AEDEH -0AB;%=>1uWi-CY\u0096? :<=>-98ZDE4\u00881 4 AEDE4 -C@i-98PN Yl1 8>2Z? 4 /0=>R ? 4I2Z? 4U;Z@ 8>-0OJDE1 T :PAE;b:<=>T 5 DE-F5I? AEL 1 8>DB=>R 23:0j6? 465bK]:P-9-0Ht=>1\u0086D(4JO -9:<=>D(L ? =>-uR 1FK =>R -uSUT ? AEDB=h;t1 Y[=>R -0DB8b8>-9/01 2323-04 5 ?^N =>DE1 4 : /01 23@6?^8>-0:k=>1V1 =>R -98 ? AEL 1 8>DB=>R 23:]T 4 5 -98]5 DBz -98>-94J=]@ 8!? /9=>DE/F? A /9DE8>/9T 23:<=!? 4 /0-0:0f g 43@i-98PYo1 8>23D(4 L\u00881 T 8I-9\u00837@i-98>DE23-04U=!? AyO ? A(D(5 ? =>DE1 4 j K -\u0084T :P-u=hK]1 5 ? =!? :P-9=>:0f \u0082wDB8>:<=Fj K -[T :P-V5 ? =!?_Y+8>1 2 ?`A(? 8>L -McGNn/01 2323-98>/0/01 2QN @ ? 4U;Uj k\u0095o\u009aJ\u009bU\u0098\\\u0093P\u00a7 J\u0094 \u008f \u0093 27\u008f \u0093<\u0091 \u0094\u0096\u0095\u00a3\u008f \u009a  \u0308>fZ\u0082wDE4 L -\\8>R7T7=V:P-0AEAE:_?ZK D(5 -QO ? 8>DBN -\\=h;Q1 Y R -9=>-\\8>1 L -94 -01 T :]@78>1J5 T /9=>:0j 8!? 4 L DE4 L[DE43@78>D(/9Y+8>1 2X? 8>1 T 465 =>-94u5 1 AEA(? 8>: =>13:P-0O -\\8!? AwRJT 4 578>-F5b5 1 AEA+?^8>:0f 7-0/91 465 j K]-MT :P-M5 ? =!? Y+8>1 2 1 T78 1^K 4_8>-0/01 2323-9465 -\\8G:<", "title": "Analysis of recommendation algorithms for e-commerce"}, "599ebeef9c9d92224bc5969f3e8e8c45bff3b072": {"paper_id": "599ebeef9c9d92224bc5969f3e8e8c45bff3b072", "abstract": "The explosive growth of the world-wide-web and the emergence of e-commerce has led to the development of recommender systems---a personalized information filtering technology used to identify a set of items that will be of interest to a certain user. User-based collaborative filtering is the most successful technology for building recommender systems to date and is extensively used in many commercial recommender systems. Unfortunately, the computational complexity of these methods grows linearly with the number of customers, which in typical commercial applications can be several millions. To address these scalability concerns model-based recommendation techniques have been developed. These techniques analyze the user--item matrix to discover relations between the different items and use these relations to compute the list of recommendations.In this article, we present one such class of model-based recommendation algorithms that first determines the similarities between the various items and then uses them to identify the set of items to be recommended. The key steps in this class of algorithms are (i) the method used to compute the similarity between the items, and (ii) the method used to combine these similarities in order to compute the similarity between a basket of items and a candidate recommender item. Our experimental evaluation on eight real datasets shows that these item-based algorithms are up to two orders of magnitude faster than the traditional user-neighborhood based recommender systems and provide recommendations with comparable or better quality.", "title": "Item-based top-N recommendation algorithms"}, "6fc4aa4bab2a1912a7a1e84b00c459e9e18cff36": {"paper_id": "6fc4aa4bab2a1912a7a1e84b00c459e9e18cff36", "abstract": "Recent discussion in the public sphere about algorithmic cl assification has involved tension between competing notions of what it means for a probabilistic class ification to be fair to different groups. We formalize three fairness conditions that lie at the heart of these debates, and we prove that except in highly constrained special cases, there is no method that can satis fy these three conditions simultaneously. Moreover, even satisfying all three conditions approximat ely requires that the data lie in an approximate version of one of the constrained special cases identified by our theorem. These results suggest some of the ways in which key notions of fairness are incompatible with each other, and hence provide a framework for thinking about the trade-offs between them.", "title": "Inherent Trade-Offs in the Fair Determination of Risk Scores"}, "4556f3f9463166aa3e27b2bec798c0ca7316bd65": {"paper_id": "4556f3f9463166aa3e27b2bec798c0ca7316bd65", "abstract": "In this paper, we investigate how to modify the naive Bayes classifier in order to perform classification that is restricted to be independent with respect to a given sensitive attribute. Such independency restrictions occur naturally when the decision process leading to the labels in the data-set was biased; e.g., due to gender or racial discrimination. This setting is motivated by many cases in which there exist laws that disallow a decision that is partly based on discrimination. Naive application of machine learning techniques would result in huge fines for companies. We present three approaches for making the naive Bayes classifier discrimination-free: (i) modifying the probability of the decision being positive, (ii) training one model for every sensitive attribute value and balancing them, and (iii) adding a latent variable to the Bayesian model that represents the unbiased label and optimizing the model parameters for likelihood using expectation maximization. We present experiments for the three approaches on both artificial and real-life data.", "title": "Three naive Bayes approaches for discrimination-free classification"}, "37c3303d173c055592ef923235837e1cbc6bd986": {"paper_id": "37c3303d173c055592ef923235837e1cbc6bd986", "abstract": "We propose a learning algorithm for fair classification that achieves both group fairness (the proportion of members in a protected group receiving positive classification is identical to the proportion in the population as a whole), and individual fairness (similar individuals should be treated similarly). We formulate fairness as an optimization problem of finding a good representation of the data with two competing goals: to encode the data as well as possible, while simultaneously obfuscating any information about membership in the protected group. We show positive results of our algorithm relative to other known techniques, on three datasets. Moreover, we demonstrate several advantages to our approach. First, our intermediate representation can be used for other classification tasks (i.e., transfer learning is possible); secondly, we take a step toward learning a distance metric which can find important dimensions of the data for classification.", "title": "Learning Fair Representations"}, "306a2f7b702f9aabb9c4750413dd9d8932ffae13": {"paper_id": "306a2f7b702f9aabb9c4750413dd9d8932ffae13", "abstract": "This paper studies the evaluation of policies that recommend an ordered set of items (e.g., a ranking) based on some context\u2014a common scenario in web search, ads, and recommendation. We build on techniques from combinatorial bandits to introduce a new practical estimator that uses logged data to estimate a policy\u2019s performance. A thorough empirical evaluation on real-world data reveals that our estimator is accurate in a variety of settings, including as a subroutine in a learningto-rank task, where it achieves competitive performance. We derive conditions under which our estimator is unbiased\u2014these conditions are weaker than prior heuristics for slate evaluation\u2014and experimentally demonstrate a smaller bias than parametric approaches, even when these conditions are violated. Finally, our theory and experiments also show exponential savings in the amount of required data compared with general unbiased estimators.", "title": "Off-policy evaluation for slate recommendation"}, "517d6e3999bd425069e45346045adcbd2d0c9299": {"paper_id": "517d6e3999bd425069e45346045adcbd2d0c9299", "abstract": "This work shows how to leverage causal inference to understand the behavior of complex learning systems interacting with their environment and predict the consequences of changes to the system. Such predictions allow both humans and algorithms to select the changes that would have improved the system performance. This work is illustrated by experiments on the ad placement system associated with the Bing search engine.", "title": "Counterfactual reasoning and learning systems: the example of computational advertising"}, "48886ea4ee14f0151f186207e1b9ad1d947e83ef": {"paper_id": "48886ea4ee14f0151f186207e1b9ad1d947e83ef", "abstract": "We investigate using gradient descent methods for learning ranking functions; we propose a simple probabilistic cost function, and we introduce RankNet, an implementation of these ideas using a neural network to model the underlying ranking function. We present test results on toy data and on data from a commercial internet search engine.", "title": "Learning to rank using gradient descent"}, "10ceb668f84860bb09fca364125cae4b1ee2e760": {"paper_id": "10ceb668f84860bb09fca364125cae4b1ee2e760", "abstract": "As with any application of machine learning, web search ranking requires labeled data. The labels usually come in the form of relevance assessments made by editors. Click logs can also provide an important source of implicit feedback and can be used as a cheap proxy for editorial labels. The main difficulty however comes from the so called position bias - urls appearing in lower positions are less likely to be clicked even if they are relevant. In this paper, we propose a Dynamic Bayesian Network which aims at providing us with unbiased estimation of the relevance from the click logs. Experiments show that the proposed click model outperforms other existing click models in predicting both click-through rate and relevance.", "title": "A dynamic bayesian network click model for web search ranking"}, "bd0cb6f62619649616316ca3c1348f568c63a852": {"paper_id": "bd0cb6f62619649616316ca3c1348f568c63a852", "abstract": "Nowadays, many decisions are made using predictive models built on historical data. Predictive models may systematically discriminate groups of people even if the computing process is fair and well-intentioned. Discrimination-aware data mining studies how to make predictive models free from discrimination, when historical data, on which they are built, may be biased, incomplete, or even contain past discriminatory decisions. Discrimination refers to disadvantageous treatment of a person based on belonging to a category rather than on individual merit. In this survey we review and organize various discrimination measures that have been used for measuring discrimination in data, as well as in evaluating performance of discrimination-aware predictive models. We also discuss related measures from other disciplines, which have not been used for measuring discrimination, but potentially could be suitable for this purpose. We computationally analyze properties of selected measures. We also review and discuss measuring procedures, and present recommendations for practitioners. The primary target audience is data mining, machine learning, pattern recognition , statistical modeling researchers developing new methods for non-discriminatory predictive modeling. In addition, practitioners and policy makers would use the survey for diagnosing potential discrimination by predictive models.", "title": "A survey on measuring indirect discrimination in machine learning"}, "32093be6d9bf5038adb5b5b8b8e8a9b62001643c": {"paper_id": "32093be6d9bf5038adb5b5b8b8e8a9b62001643c", "abstract": "Multi-label classification methods are increasingly required by modern applications, such as protein function classification, music categorization, and semantic scene classification. This article introduces the task of multi-label classification, organizes the sparse related literature into a structured presentation and performs comparative experimental results of certain multi-label classification methods. It also contributes the definition of concepts for the quantification of the multi-label nature of a data set.", "title": "Multi-Label Classification: An Overview"}, "4a2ebaa9b8ef358349b6b85d5d08fe605f73877f": {"paper_id": "4a2ebaa9b8ef358349b6b85d5d08fe605f73877f", "abstract": "Online marketplaces often contain information not only about products, but also about the people selling the products. In an effort to facilitate trust, many platforms encourage sellers to provide personal profiles and even to post pictures of themselves. However, these features may also facilitate discrimination based on sellers\u2019 race, gender, age, or other aspects of appearance. In this paper, we test for racial discrimination against landlords in the online rental marketplace Airbnb.com. Using a new data set combining pictures of all New York City landlords on Airbnb with their rental prices and information about quality of the rentals, we show that non-black hosts charge approximately 12% more than black hosts for the equivalent rental. These effects are robust when controlling for all information visible in the Airbnb marketplace. These findings highlight the prevalence of discrimination in online marketplaces, suggesting an important unintended consequence of a seemingly-routine mechanism for building trust. 1 Harvard Business School, bedelman@hbs.edu 2 Harvard Business School, mluca@hbs.edu", "title": "Digital Discrimination : The Case of Airbnb"}, "92592b7c3e57b1be2aef8102590e834ea2ea392a": {"paper_id": "92592b7c3e57b1be2aef8102590e834ea2ea392a", "abstract": "Discrimination in decision making is prohibited on many attributes (religion, gender, etc\u2026), but often present in historical decisions. Use of such discriminatory historical decision making as training data can perpetuate discrimination, even if the protected attributes are not directly present in the data. This work focuses on discovering discrimination in instances and preventing discrimination in classification. First, we propose a discrimination discovery method based on modeling the probability distribution of a class using Bayesian networks. This measures the effect of a protected attribute (e.g., gender) in a subset of the dataset using the estimated probability distribution (via a Bayesian network). Second, we propose a classification method that corrects for the discovered discrimination without using protected attributes in the decision process. We evaluate the discrimination discovery and discrimination prevention approaches on two different datasets. The empirical results show that a substantial amount of discrimination identified in instances is prevented in future decisions.", "title": "Combating discrimination using Bayesian networks"}, "59e40d166a46e14b37bc90041864eca26af6ae00": {"paper_id": "59e40d166a46e14b37bc90041864eca26af6ae00", "abstract": "This paper presents a training method that encodes each word into a different vector in semantic space and its relation to low entropy coding. Elman network is employed in the method to process word sequences from literary works. The trained codes possess reduced entropy and are used in ranking, indexing, and categorizing literary works. A modification of the method to train the multi-vector for each polysemous word is also presented where each vector represents a different meaning of its word. These multiple vectors can accommodate several different meanings of their word. This method is applied to the stylish analyses of two Chinese novels, Dream of the Red Chamber and Romance of the Three Kingdoms. & 2014 Elsevier B.V. All rights reserved.", "title": "Autoencoder for words"}, "1e389040dbdb3057ff510df13808be153c459fd0": {"paper_id": "1e389040dbdb3057ff510df13808be153c459fd0", "abstract": "The perceptual recognition of objects is conceptualized to be a process in which the image of the input is segmented at regions of deep concavity into an arrangement of simple geometric components, such as blocks, cylinders, wedges, and cones. The fundamental assumption of the proposed theory, recognition-by-components (RBC), is that a modest set of generalized-cone components, called geons (N \u00a3 36), can be derived from contrasts of five readily detectable properties of edges in a two-dimensional image: curvature, collinearity, symmetry, parallelism, and cotermination. The detection of these properties is generally invariant over viewing position an$ image quality and consequently allows robust object perception when the image is projected from a novel viewpoint or is degraded. RBC thus provides a principled account of the heretofore undecided relation between the classic principles of perceptual organization and pattern recognition: The constraints toward regularization (Pragnanz) characterize not the complete object but the object's components. Representational power derives from an allowance of free combinations of the geons. A Principle of Componential Recovery can account for the major phenomena of object recognition: If an arrangement of two or three geons can be recovered from the input, objects can be quickly recognized even when they are occluded, novel, rotated in depth, or extensively degraded. The results from experiments on the perception of briefly presented pictures by human observers provide empirical support for the theory.", "title": "Recognition-by-components: a theory of human image understanding."}, "02dab93dcdf8cd8671d764e7ef08a454d2c44c3d": {"paper_id": "02dab93dcdf8cd8671d764e7ef08a454d2c44c3d", "abstract": "In open trials, 1-Hz repetitive transcranial magnetic stimulation (rTMS) to the supplementary motor area (SMA) improved symptoms and normalized cortical hyper-excitability of patients with obsessive-compulsive disorder (OCD). Here we present the results of a randomized sham-controlled double-blind study. Medication-resistant OCD patients (n=21) were assigned 4 wk either active or sham rTMS to the SMA bilaterally. rTMS parameters consisted of 1200 pulses/d, at 1 Hz and 100% of motor threshold (MT). Eighteen patients completed the study. Response to treatment was defined as a > or = 25% decrease on the Yale-Brown Obsessive Compulsive Scale (YBOCS). Non-responders to sham and responders to active or sham rTMS were offered four additional weeks of open active rTMS. After 4 wk, the response rate in the completer sample was 67% (6/9) with active and 22% (2/9) with sham rTMS. At 4 wk, patients receiving active rTMS showed on average a 25% reduction in the YBOCS compared to a 12% reduction in those receiving sham. In those who received 8-wk active rTMS, OCD symptoms improved from 28.2+/-5.8 to 14.5+/-3.6. In patients randomized to active rTMS, MT measures on the right hemisphere increased significantly over time. At the end of 4-wk rTMS the abnormal hemispheric laterality found in the group randomized to active rTMS normalized. The results of the first randomized sham-controlled trial of SMA stimulation in the treatment of resistant OCD support further investigation into the potential therapeutic applications of rTMS in this disabling condition.", "title": "Randomized sham-controlled trial of repetitive transcranial magnetic stimulation in treatment-resistant obsessive-compulsive disorder."}, "ed54048cb45e243919e888967d93d6e797b31203": {"paper_id": "ed54048cb45e243919e888967d93d6e797b31203", "abstract": "The staff resources or effort required for a software project are notoriously difficult to estimate in advance. To date most work has focused upon algorithmic cost models such as COCOMO and Function Points. These can suffer from the disadvantage of the need to calibrate the model to each individual measurement environment coupled with very variable accuracy levels even after calibration. An alternative approach is to use analogy for estimation. We demonstrate that this method has considerable promise in that we show it to out perform traditional algorithmic methods for six different datasets. A disadvantage of estimation by analogy is that it requires a considerable amount of computation. The paper describes an automated environment known as ANGEL that supports the collection, storage and identification of the most analogous projects in order to estimate the effort for a new project. ANGEL is based upon the minimisation of Euclidean distance in n-dimensional space. The software is flexible and can deal with differing datasets both in terms of the number of observations (projects) and in the variables collected. Our analogy approach is evaluated with six distinct datasets drawn from a range of different environments and is found to outperform other methods. It is widely accepted that effective software effort estimation demands more than one technique. We have shown that estimating by analogy is a candidate technique and that with the aid of an automated environment is an eminently practical technique.", "title": "Effort Estimation Using Analogy"}, "1eafc6afa58c9e2a903967d5bac69e5d5036166c": {"paper_id": "1eafc6afa58c9e2a903967d5bac69e5d5036166c", "abstract": "Article history: Received 1 October 2007 Received in revised form 13 June 2008 Accepted 1 August 2008", "title": "Performance of feature-selection methods in the classification of high-dimension data"}, "2a5b31aaefd943b8eb3334fda53311b06f0bdbf5": {"paper_id": "2a5b31aaefd943b8eb3334fda53311b06f0bdbf5", "abstract": "A large number of algorithms have been proposed for feature subset selection. Our experimental results show that the sequential forward oating selection (SFFS) algorithm, proposed by Pudil et al., dominates the other algorithms tested. We study the problem of choosing an optimal feature set for land use classi cation based on SAR satellite images using four di erent texture models. Pooling features derived from di erent texture models, followed by a feature selection results in a substantial improvement in the classi cation accuracy. We also illustrate the dangers of using feature selection in small sample size situations.", "title": "Feature Selection: Evaluation, Application, and Small Sample Performance"}, "26cfe4cb725e8cb939cd968f44218b2d9a36a794": {"paper_id": "26cfe4cb725e8cb939cd968f44218b2d9a36a794", "abstract": "In pattern recognition problems it has been noted that beyond a certain point the inclusion of additional parameters (that have been estimated) leads to higher probabilities of error. A simple problem has been formulated where the probability of error approaches zero as the dimensionality increases and all the parameters are known; on the other hand, the probability of error approaches one-half as the dimensionality increases and parameters are estimated.", "title": "A Problem of Dimensionality: A Simple Example"}, "6e34aa0f0444957e63b1c5fbd9d3936b944c33aa": {"paper_id": "6e34aa0f0444957e63b1c5fbd9d3936b944c33aa", "abstract": "Aripiprazole is a novel atypical antipsychotic for the treatment of schizophrenia. It is a D2 receptor partial agonist with partial agonist activity at 5-HT1A receptors and antagonist activity at 5-HT2A receptors. The long-term efficacy and safety of aripiprazole (30 mg/d) relative to haloperidol (10 mg/d) were investigated in two 52-wk, randomized, double-blind, multicentre studies (using similar protocols which were prospectively identified to be pooled for analysis) in 1294 patients in acute relapse with a diagnosis of chronic schizophrenia and who had previously responded to antipsychotic medications. Aripiprazole demonstrated long-term efficacy that was comparable or superior to haloperidol across all symptoms measures, including significantly greater improvements for PANSS negative subscale scores and MADRS total score (p<0.05). The time to discontinuation for any reason was significantly greater with aripiprazole than with haloperidol (p=0.0001). Time to discontinuation due to adverse events or lack of efficacy was significantly greater with aripiprazole than with haloperidol (p=0.0001). Aripiprazole was associated with significantly lower scores on all extrapyramidal symptoms assessments than haloperidol (p<0.001). In summary, aripiprazole demonstrated efficacy equivalent or superior to haloperidol with associated benefits for safety and tolerability. Aripiprazole represents a promising new option for the long-term treatment of schizophrenia.", "title": "Efficacy and safety of aripiprazole vs. haloperidol for long-term maintenance treatment following acute relapse of schizophrenia."}, "7a0e5002fbf02965b30c50540eabcaf6e2117e10": {"paper_id": "7a0e5002fbf02965b30c50540eabcaf6e2117e10", "abstract": "Temporal Text Mining (TTM) is concerned with discovering temporal patterns in text information collected over time. Since most text information bears some time stamps, TTM has many applications in multiple domains, such as summarizing events in news articles and revealing research trends in scientific literature. In this paper, we study a particular TTM task -- discovering and summarizing the evolutionary patterns of themes in a text stream. We define this new text mining problem and present general probabilistic methods for solving this problem through (1) discovering latent themes from text; (2) constructing an evolution graph of themes; and (3) analyzing life cycles of themes. Evaluation of the proposed methods on two different domains (i.e., news articles and literature) shows that the proposed methods can discover interesting evolutionary theme patterns effectively.", "title": "Discovering evolutionary theme patterns from text: an exploration of temporal text mining"}, "21496686f28b5b7b978cc9155bbb164424b55e7b": {"paper_id": "21496686f28b5b7b978cc9155bbb164424b55e7b", "abstract": "We used LSI for both TREC-3 routing and adhoc tasks. For the routing tasks an LSI space was constructed using the training documents. We compared profiles constructed using just the topic words (no training) with profiles constructed using the average of relevant documents (no use of the topic words). Not surprisingly, the centroid of the relevant documents was 30% better than the topic words. This simple feedback method was quite good compared to the routing performance of other systems. Various combinations of information from the topic words and relevant documents provide small additional improvements in performance. For the adhoc task we compared LSI to keyword vector matching (i.e. using no dimension reduction). Small advantages were obtained for LSI even with the long TREC topic statements.", "title": "Latent Semantic Indexing (LSI): TREC-3 Report"}, "0e8d3f68c0a0eb9dab241c63aab319dbf596e697": {"paper_id": "0e8d3f68c0a0eb9dab241c63aab319dbf596e697", "abstract": "Probabilistic Latent Semantic Analysis is a novel statistical technique for the analysis of two-mode and co-occurrence data, which has applications in information retrieval and filtering, natural language processing, ma\u00ad chine learning from text, and in related ar\u00ad eas. Compared to standard Latent Semantic Analysis which stems from linear algebra and performs a Singular Value Decomposition of co-occurrence tables, the proposed method is based on a mixture decomposition derived from a latent class model. This results. in a more principled approach which has a solid foundation in statistics. In order to avoid overfitting, we propose a widely applicable generalization of maximum likelihood model fitting by tempered EM. Our approach yields substantial and consistent improvements over Latent Semantic Analysis in a number of ex\u00ad periments.", "title": "Probabilistic Latent Semantic Analysis"}, "2dff153c498c79232ea5bab523aeca697ae18823": {"paper_id": "2dff153c498c79232ea5bab523aeca697ae18823", "abstract": "The possibilities for data mining from large text collections are virtually untapped. Text expresses a vast, rich range of information, but encodes this information in a form that is difficult to decipher automatically. Perhaps for this reason, there has been little work in text data mining to date, and most people who have talked about it have either conflated it with information access or have not made use of text directly to discover heretofore unknown information. In this paper I will first define data mining, information access, and corpus-based computational linguistics, and then discuss the relationship of these to text data mining. The intent behind these contrasts is to draw attention to exciting new kinds of problems for computational linguists. I describe examples of what I consider to be real text data mining efforts and briefly outline recent ideas about how to pursue exploratory data analysis over text.", "title": "Untangling Text Data Mining"}, "15087da1592f6f631a1428b31312f04818ce4baf": {"paper_id": "15087da1592f6f631a1428b31312f04818ce4baf", "abstract": "The information age is characterized by a rapid growth in the amount of information available in electronic media. Traditional data handling methods are not adequate to cope with this information flood. Knowledge Discovery in Databases (KDD) is a new paradigm that focuses on computerized exploration of large amounts of data and on discovery of relevant and interesting patterns within them. While most work on KDD is concerned with structured databases, it is clear that this paradigm is required for handling the huge amount of information that is available only in unstructured textual form. To apply traditional KDD on texts it is necessary to impose some structure on the data that would be rich enough to allow for interesting KDD operations. On the other hand, we have to consider the severe limitations of current text processing technology and define rather simple structures that can be extracted from texts fairly automatically and in a reasonable cost. We propose using a text categorization paradigm to annotate text articles with meaningful concepts that are organized in hierarchical structure. We suggest that this relatively simple annotation is rich enough to provide the basis for a KDD framework, enabling data summarization, exploration of interesting patterns, and trend analysis. This research combines the KDD and text categorization paradigms and suggests advances to the state of the art in both areas.", "title": "Knowledge Discovery in Textual Databases (KDT)"}, "42e0376c29ad9510464b7a643a49cfc3b60c2cad": {"paper_id": "42e0376c29ad9510464b7a643a49cfc3b60c2cad", "abstract": "A novel method for simultaneous keyphrase extraction and generic text summarization is proposed by modeling text documents as weighted undirected and weighted bipartite graphs. Spectral graph clustering algorithms are useed for partitioning sentences of the documents into topical groups with sentence link priors being exploited to enhance clustering quality. Within each topical group, saliency scores for keyphrases and sentences are generated based on a mutual reinforcement principle. The keyphrases and sentences are then ranked according to their saliency scores and selected for inclusion in the top keyphrase list and summaries of the document. The idea of building a hierarchy of summaries for documents capturing different levels of granularity is also briefly discussed. Our method is illustrated using several examples from news articles, news broadcast transcripts and web documents.", "title": "Generic summarization and keyphrase extraction using mutual reinforcement principle and sentence clustering"}, "373fd1b2149546ff97ea874ae73f8eef989a11b9": {"paper_id": "373fd1b2149546ff97ea874ae73f8eef989a11b9", "abstract": "With the overwhelming volume of online news available today, there is an increasing need for automatic techniques to analyze and present news to the user in a meaningful and efficient manner. Previous research focused only on organizing news stories by their topics into a flat hierarchy. We believe viewing a news topic as a flat collection of stories is too restrictive and inefficient for a user to understand the topic quickly. \n In this work, we attempt to capture the rich structure of events and their dependencies in a news topic through our event models. We call the process of recognizing events and their dependencies <i>event threading</i>. We believe our perspective of modeling the structure of a topic is more effective in capturing its semantics than a flat list of on-topic stories.\n We formally define the novel problem, suggest evaluation metrics and present a few techniques for solving the problem. Besides the standard word based features, our approaches take into account novel features such as temporal locality of stories for event recognition and time-ordering for capturing dependencies. Our experiments on a manually labeled data sets show that our models effectively identify the events and capture dependencies among them.", "title": "Event threading within news topics"}, "21922f216002ed2bc44b886b5e8e3042da45c40c": {"paper_id": "21922f216002ed2bc44b886b5e8e3042da45c40c", "abstract": "Topic Detection and Tracking (TDT) is a DARPA-sponsored initiative to investigate the state of the art in finding and following new events in a stream of broadcast news stories. The TDT problem consists of three major tasks: (1) segmenting a stream of data, especially recognized speech, into distinct stories; (2) identifying those news stories that are the first to discuss a new event occurring in the news; and (3) given a small number of sample news stories about an event, finding all following stories in the stream. The TDT Pilot Study ran from September 1996 through October 1997. The primary participants were DARPA, Carnegie Mellon University, Dragon Systems, and the University of Massachusetts at Amherst. This report summarizes the findings of the pilot study. The TDT work continues in a new project involving larger training and test corpora, more active participants, and a more broadly defined notion of \u201ctopic\u201d than was used in the pilot study. The following individuals participated in the research reported.", "title": "Topic Detection and Tracking Pilot Study Final Report"}, "7e8ce229a27ac45fe58b3c1b08fa64e84bd79b56": {"paper_id": "7e8ce229a27ac45fe58b3c1b08fa64e84bd79b56", "abstract": "Hierarchical Classification refers to assigning of one or more suitable categories from a hierarchical category space to a document. While previous work in hierarchical classification focused on virtual category trees where documents are assigned only to the leaf categories, we propose a topdown level-based classification method that can classify documents to both leaf and internal categories. As the standard performance measures assume independence between categories, they have not considered the documents incorrectly classified into categories that are similar or not far from the correct ones in the category tree. We therefore propose the Category-Similarity Measures and DistanceBased Measures to consider the degree of misclassification in measuring the classification performance. An experiment has been carried out to measure the performance of our proposed hierarchical classification method. The results showed that our method performs well for Reuters text collection when enough training documents are given and the new measures have indeed considered the contributions of misclassified documents.", "title": "Hierarchical Text Classification and Evaluation"}, "19e8aaa1021f829c8ff0378158d9b69699ea4f83": {"paper_id": "19e8aaa1021f829c8ff0378158d9b69699ea4f83", "abstract": "We discuss technology to help a person monitor changes in news coverage over time. We define temporal summaries of news stories as extracting a single sentence from each event within a news topic, where the stories are presented one at a time and sentences from a story must be ranked before the next story can be considered. We explain a method for evaluation, and describe an evaluation corpus that we have built. We also propose several methods for constructing temporal summaries and evaluate their effectiveness in comparison to degenerate cases. We show that simple approaches are effective, but that the problem is far from solved.", "title": "Temporal Summaries of News Topics"}, "44a9484e0b752e7f1218f0c6dd7461a97bc7ab49": {"paper_id": "44a9484e0b752e7f1218f0c6dd7461a97bc7ab49", "abstract": "We consider the problem of modeling the content structureof texts within a specific domain, in terms of the topics the texts address and the order in which these topics appear. We first present an effective knowledge-lean method for learning content models from unannotated documents, utilizing a novel adaptation of algorithms for Hidden Markov Models. We then apply our method to two complementary tasks: information ordering and extractive summarization. Our experiments show that incorporating content models in these applications yields substantial improvement over previously-proposed methods. Publication info: HLT-NAACL 2004: Proceedings of the Main Conference , pp. 113\u2013120.", "title": "Catching the Drift: Probabilistic Content Models, with Applications to Generation and Summarization"}, "71144517c228331d686f5098426ec314a2c8dda1": {"paper_id": "71144517c228331d686f5098426ec314a2c8dda1", "abstract": "The Topic Detection and Tracking (TDT) evaluation program has included a \"cluster detection\" task since its inception in 1996. Systems were required to process a stream of broadcast news stories and partition them into non-overlapping clusters. A system's effectiveness was measured by comparing the generated clusters to \"truth\" clusters created by human annotators. Starting in 2003, TDT is moving to a more realistic model that permits overlapping clusters (stories may be on more than one topic) and encourages the creation of a hierarchy to structure the relationships between clusters (topics). We explore a range of possible evaluation models for this modified TDT clustering task to understand the best approach for mapping between the human-generated \"truth\" clusters and a much richer hierarchical structure. We demonstrate that some obvious evaluation techniques fail for degenerate cases. For a few others we attempt to develop an intuitive sense of what the evaluation numbers mean. We settle on some approaches that incorporate a strong balance between cluster errors (misses and false alarms) and the distance it takes to travel between stories within the hierarchy.", "title": "Flexible intrinsic evaluation of hierarchical clustering for TDT"}, "24de12df6953151ef5cd0379e205eb0f57ff9d1f": {"paper_id": "24de12df6953151ef5cd0379e205eb0f57ff9d1f", "abstract": "Multilabel learning has become a relevant learning paradigm in the past years due to the increasing number of fields where it can be applied and also to the emerging number of techniques that are being developed. This article presents an up-to-date tutorial about multilabel learning that introduces the paradigm and describes the main contributions developed. Evaluation measures, fields of application, trending topics, and resources are also presented.", "title": "A Tutorial on Multilabel Learning"}, "0bfc6add0390f1b4cfbd0e90ac71475cca88b2d5": {"paper_id": "0bfc6add0390f1b4cfbd0e90ac71475cca88b2d5", "abstract": "Multi-label learning studies the problem where each example is represented by a single instance while associated with a set of labels simultaneously. During the past decade, significant amount of progresses have been made toward this emerging machine learning paradigm. This paper aims to provide a timely review on this area with emphasis on state-of-the-art multi-label learning algorithms. Firstly, fundamentals on multi-label learning including formal definition and evaluation metrics are given. Secondly and primarily, eight representative multi-label learning algorithms are scrutinized under common notations with relevant analyses and discussions. Thirdly, several related learning settings are briefly summarized. As a conclusion, online resources and open research problems on multi-label learning are outlined for reference purposes.", "title": "A Review on Multi-Label Learning Algorithms"}, "85fc4c073753365375e0be3175ab92c440611706": {"paper_id": "85fc4c073753365375e0be3175ab92c440611706", "abstract": "Multi-label learning arises in many real-world tasks where an object is naturally associated with multiple concepts. It is well-accepted that, in order to achieve a good performance, the relationship among labels should be exploited. Most existing approaches require the label relationship as prior knowledge, or exploit by counting the label co-occurrence. In this paper, we propose the MAHR approach, which is able to automatically discover and exploit label relationship. Our basic idea is that, if two labels are related, the hypothesis generated for one label can be helpful for the other label. MAHR implements the idea as a boosting approach with a hypothesis reuse mechanism. In each boosting round, the base learner for a label is generated by not only learning on its own task but also reusing the hypotheses from other labels, and the amount of reuse across labels provides an estimate of the label relationship. Extensive experimental results validate that MAHR is able to achieve superior performance and discover reasonable label relationship. Moreover, we disclose that the label relationship is usually asymmetric.", "title": "Multi-label hypothesis reuse"}, "2a68c39e3586f87da501bc2a5ae6138469f50613": {"paper_id": "2a68c39e3586f87da501bc2a5ae6138469f50613", "abstract": "A large body of research in supervised learning deals with the analysis of singlelabel data, where training examples are associated with a single label \u03bb from a set of disjoint labels L. However, training examples in several application domains are often associated with a set of labels Y \u2286 L. Such data are called multi-label. Textual data, such as documents and web pages, are frequently annotated with more than a single label. For example, a news article concerning the reactions of the Christian church to the release of the \u201cDa Vinci Code\u201d film can be labeled as both religion and movies. The categorization of textual data is perhaps the dominant multi-label application. Recently, the issue of learning from multi-label data has attracted significant attention from a lot of researchers, motivated from an increasing number of new applications, such as semantic annotation of images [1, 2, 3] and video [4, 5], functional genomics [6, 7, 8, 9, 10], music categorization into emotions [11, 12, 13, 14] and directed marketing [15]. Table 1 presents a variety of applications that are discussed in the literature. This chapter reviews past and recent work on the rapidly evolving research area of multi-label data mining. Section 2 defines the two major tasks in learning from multi-label data and presents a significant number of learning methods. Section 3 discusses dimensionality reduction methods for multi-label data. Sections 4 and 5 discuss two important research challenges, which, if successfully met, can significantly expand the real-world applications of multi-label learning methods: a) exploiting label structure and b) scaling up to domains with large number of labels. Section 6 introduces benchmark multi-label datasets and their statistics, while Section 7 presents the most frequently used evaluation measures for multi-label learn-", "title": "Mining Multi-label Data"}, "178286f3640f9c5c8c129799d6b00f313481d13a": {"paper_id": "178286f3640f9c5c8c129799d6b00f313481d13a", "abstract": "This paper reports a controlled study with statistical signi cance tests on ve text categorization methods: the Support Vector Machines (SVM), a k-Nearest Neighbor (kNN) classi er, a neural network (NNet) approach, the Linear Leastsquares Fit (LLSF) mapping and a Naive Bayes (NB) classier. We focus on the robustness of these methods in dealing with a skewed category distribution, and their performance as function of the training-set category frequency. Our results show that SVM, kNN and LLSF signi cantly outperform NNet and NB when the number of positive training instances per category are small (less than ten), and that all the methods perform comparably when the categories are su ciently common (over 300 instances).", "title": "A Re-Examination of Text Categorization Methods"}, "0c97e8fcd80d9a3779826f2930724c9d789faa05": {"paper_id": "0c97e8fcd80d9a3779826f2930724c9d789faa05", "abstract": "In this paper, we describe an automated learning approach to text categorization based on perception learning and a new feature selection metric, called correlation coefficient. Our approach has been teated on the standard Reuters text categorization collection. Empirical results indicate that our approach outperforms the best published results on this % uters collection. In particular, our new feature selection method yields comiderable improvement. We also investigate the usability of our automated hxu-n~ approach by actually developing a system that categorizes texts into a treeof categories. We compare tbe accuracy of our learning approach to a rrddmsed, expert system ap preach that uses a text categorization shell built by Cams gie Group. Although our automated learning approach still gives a lower accuracy, by appropriately inmrporating a set of manually chosen worda to use as f~ures, the combined, semi-automated approach yields accuracy close to the * baaed approach.", "title": "Feature Selection, Perceptron Learning, and a Usability Case Study for Text Categorization"}, "e9fd1a7ae0322d417ab2d32017e373dd50efc063": {"paper_id": "e9fd1a7ae0322d417ab2d32017e373dd50efc063", "abstract": "This paper examines the use of inductive learning to categorize natural language documents into predeened content categories. Categorization of text is of increasing importance in information retrieval and natural language processing systems. Previous research on automated text categorization has mixed machine learning and knowledge engineering methods, making it diicult to draw conclusions about the performance of particular methods. In this paper we present empirical results on the performance of a Bayesian classiier and a decision tree learning algorithm on two text categorization data sets. We nd that both algorithms achieve reasonable performance and allow controlled tradeoos between false positives and false negatives. The stepwise feature selection in the decision tree algorithm is particularly eeective in dealing with the large feature sets common in text categorization. However, even this algorithm is aided by an initial preeltering of features, connrming the results found by Almuallim and Dietterich on artiicial data sets. We also demonstrate the impact of the time-varying nature of category deenitions.", "title": "A comparison of two learning algorithms for text categorization"}, "0a4eabea3f727a49f414a036f253ea28d1652104": {"paper_id": "0a4eabea3f727a49f414a036f253ea28d1652104", "abstract": "Most of the multi-label classification (MLC) methods proposed in recent years intended to exploit, in one way or the other, dependencies between the class labels. Comparing to simple binary relevance learning as a baseline, any gain in performance is normally explained by the fact that this method is ignoring such dependencies. Without questioning the correctness of such studies, one has to admit that a blanket explanation of that kind is hiding many subtle details, and indeed, the underlying mechanisms and true reasons for the improvements reported in experimental studies are rarely laid bare. Rather than proposing yet another MLC algorithm, the aim of this paper is to elaborate more closely on the idea of exploiting label dependence, thereby contributing to a better understanding of MLC. Adopting a statistical perspective, we claim that two types of label dependence should be distinguished, namely conditional and marginal dependence. Subsequently, we present three scenarios in which the exploitation of one of these types of dependence may boost the predictive performance of a classifier. In this regard, a close connection with loss minimization is established, showing that the benefit of exploiting label dependence does also depend on the type of loss to be minimized. Concrete theoretical results are presented for two representative loss functions, namely the Hamming loss and the subset 0/1 loss. In addition, we give an overview of state-of-the-art decomposition algorithms for MLC and we try to reveal the reasons for their effectiveness. Our conclusions are supported by carefully designed experiments on synthetic and benchmark data.", "title": "On label dependence and loss minimization in multi-label classification"}, "6c2e32948af0134f0c3d85e8c262345568912208": {"paper_id": "6c2e32948af0134f0c3d85e8c262345568912208", "abstract": "This paper presents a pruned sets method (PS) for multi-label classification. It is centred on the concept of treating sets of labels as single labels. This allows the classification process to inherently take into account correlations between labels. By pruning these sets, PS focuses only on the most important correlations, which reduces complexity and improves accuracy. By combining pruned sets in an ensemble scheme (EPS), new label sets can be formed to adapt to irregular or complex data. The results from experimental evaluation on a variety of multi-label datasets show that [E]PS can achieve better performance and train much faster than other multi-label methods.", "title": "Multi-label Classification Using Ensembles of Pruned Sets"}, "6c8330f9713f76ac9f4a199a31911387af9506d9": {"paper_id": "6c8330f9713f76ac9f4a199a31911387af9506d9", "abstract": "In this paper we present methods of enhancing existing discriminative classifiers for multi-labeled predictions. Discriminative methods like support vector machines perform very well for uni-labeled text classification tasks. Multi-labeled classification is a harder task subject to relatively less attention. In the multi-labeled setting, classes are often related to each other or part of a is-a hierarchy. We present a new technique for combining text features and features indicating relationships between classes, which can be used with any discriminative algorithm. We also present two enhancements to the margin of SVMs for building better models in the presence of overlapping classes. We present results of experiments on real world text benchmark datasets. Our new methods beat accuracy of existing methods with statistically significant improvements.", "title": "Discriminative Methods for Multi-labeled Classification"}, "5b0b7c09be6076a55c92163073049a895693d570": {"paper_id": "5b0b7c09be6076a55c92163073049a895693d570", "abstract": "This paper designs a new five-fingered robotic hand with a camera. Several morphological features of the human hand are integrated to improve the appearance of the hand. The drive system of this hand is under-actuated to eliminate the weight of the hand and to embed all the actuators inside the palm. Despite of this under-actuation, this hand can grasp objects in several different ways. In addition, the two different transmissions are adopted to drive the fingers according to their roles. These transmissions help not only to improve drive efficiency but also to secure the space of the embedded camera.", "title": "Design of a tendon-driven robotic hand with an embedded camera"}, "d4caec47eeabb2eca3ce9e39b1fae5424634c731": {"paper_id": "d4caec47eeabb2eca3ce9e39b1fae5424634c731", "abstract": "Many robotic hands or prosthetic hands have been developed in the last several decades, and many use tendon-driven mechanisms for their transmissions. Robotic hands are now built with underactuated mechanisms, which have fewer actuators than degrees of freedom, to reduce mechanical complexity or to realize a biomimetic motion such as flexion of an index finger. The design is heuristic and it is useful to develop design methods for the underactuated mechanisms. This paper classifies mechanisms driven by tendons into three classes, and proposes a design method for them. The two classes are related to underactuated tendon-driven mechanisms, and these have been used without distinction so far. An index finger robot, which has four active tendons and two passive tendons, is developed and controlled with the proposed method.", "title": "Design and control of underactuated tendon-driven mechanisms"}, "607bd1a88459e5a5fc43bbe1f9a198c920f6cf61": {"paper_id": "607bd1a88459e5a5fc43bbe1f9a198c920f6cf61", "abstract": "Strong motivation for developing new prosthetic hand devices is provided by the fact that low functionality and controllability\u2014in addition to poor cosmetic appearance\u2014are the most important reasons why amputees do not regularly use their prosthetic hands. This paper presents the design of the CyberHand, a cybernetic anthropomorphic hand intended to provide amputees with functional hand replacement. Its design was bio-inspired in terms of its modular architecture, its physical appearance, kinematics, sensorization, and actuation, and its multilevel control system. Its underactuated mechanisms allow separate control of each digit as well as thumb\u2013finger opposition and, accordingly, can generate a multitude of grasps. Its sensory system was designed to provide proprioceptive information as well as to emulate fundamental functional properties of human tactile mechanoreceptors of specific importance for grasp-and-hold tasks. The CyberHand control system presumes just a few efferent and afferent channels and was divided in two main layers: a high-level control that interprets the user\u2019s intention (grasp selection and required force level) and can provide pertinent sensory feedback and a low-level control responsible for actuating specific grasps and applying the desired total force by taking advantage of the intelligent mechanics. The grasps made available by the high-level controller include those fundamental for activities of daily living: cylindrical, spherical, tridigital (tripod), and lateral grasps. The modular and flexible design of the CyberHand makes it suitable for incremental development of sensorization, interfacing, and control strategies and, as such, it will be a useful tool not only for clinical research but also for addressing neuroscientific hypotheses regarding sensorimotor control.", "title": "Design of a cybernetic hand for perception and action"}, "92f8d0d6e5ff48ff55a848bde0b011f4476bd000": {"paper_id": "92f8d0d6e5ff48ff55a848bde0b011f4476bd000", "abstract": "This paper presents a development of an anthropomorphic robot hand, `KITECH Hand' that has 4 full-actuated fingers. Most robot hands have small size simultaneously many joints as compared with robot manipulators. Components of actuator, gear, and sensors used for building robots are not small and are expensive, and those make it difficult to build a small sized robot hand. Differently from conventional development of robot hands, KITECH hand adopts a RC servo module that is cheap, easily obtainable, and easy to handle. The RC servo module that have been already used for several small sized humanoid can be new solution of building small sized robot hand with many joints. The feasibility of KITECH hand in object manipulation is shown through various experimental results. It is verified that the modified RC servo module is one of effective solutions in the development of a robot hand.", "title": "Development of a low cost anthropomorphic robot hand with high capability"}, "7db022345bf776a3aae11393b57335165869eda8": {"paper_id": "7db022345bf776a3aae11393b57335165869eda8", "abstract": "This paper presents an anthropomorphic robot hand called the Gifu hand II, which has a thumb and four fingers, all the joints of which are driven by servomotors built into the fingers and the palm. The thumb has four joints with four-degrees-of-freedom (DOF); the other fingers have four joints with 3-DOF; and two axes of the joints near the palm cross orthogonally at one point, as is the case in the human hand. The Gifu hand II can be equipped with six-axes force sensor at each fingertip and a developed distributed tactile sensor with 624 detecting points on its surface. The design concepts and the specifications of the Gifu hand II, the basic characteristics of the tactile sensor, and the pressure distributions at the time of object grasping are described and discussed herein. Our results demonstrate that the Gifu hand II has a high potential to perform dexterous object manipulations like the human hand.", "title": "Dexterous anthropomorphic robot hand with distributed tactile sensor: Gifu hand II"}, "bc523a4921e83e08790c7709bb5599be82eb790f": {"paper_id": "bc523a4921e83e08790c7709bb5599be82eb790f", "abstract": "Despite the progress since the first attempts of mankind to explore space, it appears that sending man in space remains challenging. While robotic systems are not yet ready to replace human presence, they provide an excellent support for astronauts during maintenance and hazardous tasks. This paper presents the development of a space qualified multi-fingered robotic hand and highlights the most interesting challenges. The design concept, the mechanical structure, the electronics architecture and the control system are presented throughout this overview paper.", "title": "Dexhand: A Space qualified multi-fingered robotic hand"}, "7ab280e2c1bcdcdd2495a336557777c5cea9c899": {"paper_id": "7ab280e2c1bcdcdd2495a336557777c5cea9c899", "abstract": "| this paper outlines the 2nd generation of multisensory hand design at DLR. The results of the use of DLR's Hand I were analyzed and enabled in addition to the big e orts made in grasping technology to design the next generation of dextrous robot hands. An open skeleton structure for better maintenance with semi shell housings and the new automatically recon gurable palm have been equipped with more powerful actuators to reach 30N on the ngertip. Newly designed sensors as the 6 DOF ngertip force torque sensor and integrated electronics together with the new communication architecture which enables a reduction of the cabling to the hand to only 12 lines outline the electronics concept. The Cartesian impedance control of all the ngers completes the new hand with its 13 DOF to what it is: the next step to autonomous and humanoid grasping", "title": "DLR-Hand II Next Generation of a Dextrous Robot Hand"}, "1b845a04a02df6036e34bcfb8f4a75859331b7d1": {"paper_id": "1b845a04a02df6036e34bcfb8f4a75859331b7d1", "abstract": "Subjects were asked to shape the right hand as if to grasp and use a large number of familiar objects. The chosen objects typically are held with a variety of grips, including \"precision\" and \"power\" grips. Static hand posture was measured by recording the angular position of 15 joint angles of the fingers and of the thumb. Although subjects adopted distinct hand shapes for the various objects, the joint angles of the digits did not vary independently. Principal components analysis showed that the first two components could account for >80% of the variance, implying a substantial reduction from the 15 degrees of freedom that were recorded. However, even though they were small, higher-order (more than three) principal components did not represent random variability but instead provided additional information about the object. These results suggest that the control of hand posture involves a few postural synergies, regulating the general shape of the hand, coupled with a finer control mechanism providing for small, subtle adjustments. Because the postural synergies did not coincide with grip taxonomies, the results suggest that hand posture may be regulated independently from the control of the contact forces that are used to grasp an object.", "title": "Postural hand synergies for tool use."}, "82dd3bcbbf024af7b393f61cd64d080438258927": {"paper_id": "82dd3bcbbf024af7b393f61cd64d080438258927", "abstract": "The increasing demand for robotic applications in dynamic unstructured environments is motivating the need for dextrous end-effectors which can cope with the wide variety of tasks and objects encountered in these environments. The human hand is a very complex grasping tool that can handle objects of different sizes and shapes. Many research activities have been carried out to develop artificial robot hands with capabilities similar to the human hand. In this paper the mechanism and design of a new humanoid-type hand (called TUAT/Karlsruhe Humanoid Hand) with human-like manipulation abilities is discussed. The new hand is designed for the humanoid robot ARMAR which has to work autonomously or interactively in cooperation with humans and for an artificial lightweight arm for handicapped persons. The arm is developed as close as possible to the human arm and is driven by spherical ultrasonic motors. The ideal end-effector for such an artificial arm or a humanoid would be able to use the tools and objects that a person uses when working in the same environment. Therefore a new hand is designed for anatomical consistency with the human hand. This includes the number of fingers and the placement and motion of the thumb, the proportions of the link lengths and the shape of the palm. It can also perform most part of human grasping types. The TUAT/Karlsruhe Humanoid Hand possesses 20 DOF and is driven by one actuator which can be placed into or around the hand.", "title": "Design of the TUAT/Karlsruhe humanoid hand"}, "8651476fca593fc925e93067f6fb61bb4a054e5a": {"paper_id": "8651476fca593fc925e93067f6fb61bb4a054e5a", "abstract": "We present EVO, an event-based visual odometry algorithm. Our algorithm successfully leverages the outstanding properties of event cameras to track fast camera motions while recovering a semidense three-dimensional (3-D) map of the environment. The implementation runs in real time on a standard CPU and outputs up to several hundred pose estimates per second. Due to the nature of event cameras, our algorithm is unaffected by motion blur and operates very well in challenging, high dynamic range conditions with strong illumination changes. To achieve this, we combine a novel, event-based tracking approach based on image-to-model alignment with a recent event-based 3-D reconstruction algorithm in a parallel fashion. Additionally, we show that the output of our pipeline can be used to reconstruct intensity images from the binary event stream, though our algorithm does not require such intensity information. We believe that this work makes significant progress in simultaneous localization and mapping by unlocking the potential of event cameras. This allows us to tackle challenging scenarios that are currently inaccessible to standard cameras.", "title": "EVO: A Geometric Approach to Event-Based 6-DOF Parallel Tracking and Mapping in Real Time"}, "30f46fdfe1fdab60bdecaa27aaa94526dfd87ac1": {"paper_id": "30f46fdfe1fdab60bdecaa27aaa94526dfd87ac1", "abstract": "We propose a method which can perform real-time 3D reconstruction from a single hand-held event camera with no additional sensing, and works in unstructured scenes of which it has no prior knowledge. It is based on three decoupled probabilistic filters, each estimating 6-DoF camera motion, scene logarithmic (log) intensity gradient and scene inverse depth relative to a keyframe, and we build a real-time graph of these to track and model over an extended local workspace. We also upgrade the gradient estimate for each keyframe into an intensity image, allowing us to recover a real-time video-like intensity sequence with spatial and temporal super-resolution from the low bit-rate input event stream. To the best of our knowledge, this is the first algorithm provably able to track a general 6D motion along with reconstruction of arbitrary structure including its intensity and the reconstruction of grayscale video that exclusively relies on event camera data.", "title": "Real-Time 3D Reconstruction and 6-DoF Tracking with an Event Camera"}, "7633c7470819061477433fdae15c64c8b49a758b": {"paper_id": "7633c7470819061477433fdae15c64c8b49a758b", "abstract": "DTAM is a system for real-time camera tracking and reconstruction which relies not on feature extraction but dense, every pixel methods. As a single hand-held RGB camera flies over a static scene, we estimate detailed textured depth maps at selected keyframes to produce a surface patchwork with millions of vertices. We use the hundreds of images available in a video stream to improve the quality of a simple photometric data term, and minimise a global spatially regularised energy functional in a novel non-convex optimisation framework. Interleaved, we track the camera's 6DOF motion precisely by frame-rate whole image alignment against the entire dense model. Our algorithms are highly parallelisable throughout and DTAM achieves real-time performance using current commodity GPU hardware. We demonstrate that a dense model permits superior tracking performance under rapid motion compared to a state of the art method using features; and also show the additional usefulness of the dense model for real-time scene interaction in a physics-enhanced augmented reality application.", "title": "DTAM: Dense tracking and mapping in real-time"}, "7b58d89e838799e3ef4967be24a595ccec00d0dd": {"paper_id": "7b58d89e838799e3ef4967be24a595ccec00d0dd", "abstract": "This paper presents a stereo matching approach for a novel multi-perspective panoramic stereo vision system, making use of asynchronous and non-simultaneous stereo imaging towards real-time 3D 360\u00b0 vision. The method is designed for events representing the scenes visual contrast as a sparse visual code allowing the stereo reconstruction of high resolution panoramic views. We propose a novel cost measure for the stereo matching, which makes use of a similarity measure based on event distributions. Thus, the robustness to variations in event occurrences was increased. An evaluation of the proposed stereo method is presented using distance estimation of panoramic stereo views and ground truth data. Furthermore, our approach is compared to standard stereo methods applied on event-data. Results show that we obtain 3D reconstructions of 1024 \u00d7 3600 round views and outperform depth reconstruction accuracy of state-of-the-art methods on event data.", "title": "Event-driven stereo matching for real-time 3D panoramic vision"}, "12099545a31155585a813b840ed711de9d83cace": {"paper_id": "12099545a31155585a813b840ed711de9d83cace", "abstract": "An event camera is a silicon retina which outputs not a sequence of video frames like a standard camera, but a stream of asynchronous spikes, each with pixel location, sign and precise timing, indicating when individual pixels record a threshold log intensity change. By encoding only image change, it offers the potential to transmit the information in a standard video but at vastly reduced bitrate, and with huge added advantages of very high dynamic range and temporal resolution. However, event data calls for new algorithms, and in particular we believe that algorithms which incrementally estimate global scene models are best placed to take full advantages of its properties. Here, we show for the first time that an event stream, with no additional sensing, can be used to track accurate camera rotation while building a persistent and high quality mosaic of a scene which is super-resolution accurate and has high dynamic range. Our method involves parallel camera rotation tracking and template reconstruction from estimated gradients, both operating on an event-by-event basis and based on probabilistic filtering.", "title": "Simultaneous Mosaicing and Tracking with an Event Camera"}, "e151ed990085a41668a88545da12daeeaac11e74": {"paper_id": "e151ed990085a41668a88545da12daeeaac11e74", "abstract": "Event-based vision sensors mimic the operation of biological retina and they represent a major paradigm shift from traditional cameras. Instead of providing frames of intensity measurements synchronously, at artificially chosen rates, event-based cameras provide information on brightness changes asynchronously, when they occur. Such non-redundant pieces of information are called \u201cevents\u201d. These sensors overcome some of the limitations of traditional cameras (response time, bandwidth and dynamic range) but require new methods to deal with the data they output. We tackle the problem of event-based camera localization in a known environment, without additional sensing, using a probabilistic generative event model in a Bayesian filtering framework. Our main contribution is the design of the likelihood function used in the filter to process the observed events. Based on the physical characteristics of the sensor and on empirical evidence of the Gaussian-like distribution of spiked events with respect to the brightness change, we propose to use the contrast residual as a measure of how well the estimated pose of the event-based camera and the environment explain the observed events. The filter allows for localization in the general case of six degrees-of-freedom motions.", "title": "Event-based Camera Pose Tracking using a Generative Event Model"}, "2dcfc3c4e8680374ec3b1e81d1cf6cff84a8dd06": {"paper_id": "2dcfc3c4e8680374ec3b1e81d1cf6cff84a8dd06", "abstract": "In this article, we present an overview of methods for sequential simulation from posterior distributions. These methods are of particular interest in Bayesian filtering for discrete time dynamic models that are typically nonlinear and non-Gaussian. A general importance sampling framework is developed that unifies many of the methods which have been proposed over the last few decades in several different scientific disciplines. Novel extensions to the existing methods are also proposed. We show in particular how to incorporate local linearisation methods similar to those which have previously been employed in the deterministic filtering literature; these lead to very effective importance distributions. Furthermore we describe a method which uses Rao-Blackwellisation in order to take advantage of the analytic structure present in some important classes of state-space models. In a final section we develop algorithms for prediction, smoothing and evaluation of the likelihood in dynamic models.", "title": "On sequential Monte Carlo sampling methods for Bayesian filtering"}, "3ea7120d92e18b41e4b74038806198f924169de1": {"paper_id": "3ea7120d92e18b41e4b74038806198f924169de1", "abstract": "Event-based dynamic vision sensors (DVSs) asynchronously report log intensity changes. Their high dynamic range, sub-ms latency and sparse output make them useful in applications such as robotics and real-time tracking. However they discard absolute intensity information which is useful for object recognition and classification. This paper presents a dynamic and active pixel vision sensor (DAVIS) which addresses this deficiency by outputting asynchronous DVS events and synchronous global shutter frames concurrently. The active pixel sensor (APS) circuits and the DVS circuits within a pixel share a single photodiode. Measurements from a 240\u00d7180 sensor array of 18.5 \u03bcm 2 pixels fabricated in a 0.18 \u03bcm 6M1P CMOS image sensor (CIS) technology show a dynamic range of 130 dB with 11% contrast detection threshold, minimum 3 \u03bcs latency, and 3.5% contrast matching for the DVS pathway; and a 51 dB dynamic range with 0.5% FPN for the APS readout.", "title": "A 240 \u00d7 180 130 dB 3 \u00b5s Latency Global Shutter Spatiotemporal Vision Sensor"}, "313010c775cccf3a51fa49aad15d76e280284eaf": {"paper_id": "313010c775cccf3a51fa49aad15d76e280284eaf", "abstract": "This paper introduces a new methodology to compute dense visual flow using the precise timings of spikes from an asynchronous event-based retina. Biological retinas, and their artificial counterparts, are totally asynchronous and data-driven and rely on a paradigm of light acquisition radically different from most of the currently used frame-grabber technologies. This paper introduces a framework to estimate visual flow from the local properties of events' spatiotemporal space. We will show that precise visual flow orientation and amplitude can be estimated using a local differential approach on the surface defined by coactive events. Experimental results are presented; they show the method adequacy with high data sparseness and temporal resolution of event-based acquisition that allows the computation of motion flow with microsecond accuracy and at very low computational cost.", "title": "Event-Based Visual Flow"}, "5d652f011a0c78594486223cb71f98219363bcad": {"paper_id": "5d652f011a0c78594486223cb71f98219363bcad", "abstract": "I discuss connectivity between neuromorphic chips, which use the timing of fixed-height, fixed-width, pulses to encode information. Address-events\u2014log2(N)-bit packets that uniquely identify one of N neurons\u2014are used to transmit these pulses in real-time on a random-access, time-multiplexed, communication channel. Activity is assumed to consist of neuronal ensembles\u2014spikes clustered in space and in time. I quantify tradeoffs faced in allocating bandwidth, granting access, and queuing, as well as throughput requirements, and conclude that an arbitered channel design is the best choice. I implement the arbitered channel with a formal design methodology for asynchronous digital VLSI CMOS systems, after introducing the reader to this top-down synthesis technique. Following the evolution of three generations of designs, I show how the overhead of arbitrating, and encoding and decoding, can be reduced in area (from N to \u221a N) by organizing neurons into rows and columns, and reduced in time (from log2(N) to 2) by exploiting locality in the arbiter tree and in the row\u2013column architecture, and clustered activity. Throughput is boosted by pipelining and by reading spikes in parallel. Simple techniques that reduce crosstalk in these mixed analog\u2013digital systems are described. Keywords\u2014 Spiking Neurons, Interchip Communication, Asynchronous Logic Synthesis, Virtual Wiring. I. Connectivity in Neuromorphic Systems E are far from matching either the efficacy of neural computation or the efficiency of neural coding. Computers use a million times more energy per operation than brains do [1]. Video cameras uses a thousand times more bandwidth per bit of information than retinas do (see Section II-A). We cannot replace damaged parts of the nervous system because of these shortcomings. To match nature\u2019s computational performance and communication efficiency, we must co-optimize information processing and energy consumption. A small\u2014but growing\u2014community of engineers is attempting to build autonomous sensorimotor systems that match the efficacy and efficiency of their biological counterparts by recreating the function and structure of neural systems in silicon. Taking a structure-to-function approach, these neuromorphic systems go beyond bioinspiration [2], copying biological organization as well as function [3], [4], [5]. Neuromorphic engineers are using garden-variety VLSI CMOS technology to achieve their goal [6]. This effort is facilitated by similarities between VLSI hardware and neural wetware. Both technologies: \u2022 Provide millions of inexpensive, poorly-matched devices. \u2022 Operate in the information-maximizing low-signal-tonoise/high-bandwidth regime. K. A. Boahen morphs brains into silicon at the Bioengineering Dept, University of Pennsylvania, Philadelphia PA 19104-6392. Email: kwabena@neuroengineering.upenn.edu And challenged by these fundamental differences: \u2022 Fan-ins and fan-outs are about ten in VLSI circuits versus several thousand in neural circuits. \u2022 Most digital VLSI circuits are synchronized by an external clock, whereas neurons use the degree of coincidence in their firing times to encode information. Neuromorphic engineers have adopted time-division multiplexing to achieve massive connectivity, inspired by its success in telecommunications [7] and computer networks [8]. The number of layers and pins offered by commercial microfabrication and chip-packaging technologies are severely limited. Multiplexing leverages the 5-decade difference in bandwidth between a neuron (hundreds of Hz) and a digital bus (tens of megahertz), enabling us to replace thousands of dedicated point-to-point connections with a handful of high-speed metal wires and thousands of switches (transistors). It pays off because transistors occupy less area than wires, and are becoming relatively more compact in deep submicron processes. In adapting existing networking solutions, neuromorphic architects are challenged by huge differences between the requirements of computer networks and those of neuromorphic systems. Whereas computer networks connect thousands of computers at the buildingor campus-level, neuromorphic systems need to connect millions of neurons at the chipor circuit-board level. Hence, they must improve the efficiency of traditional computer communication architectures, and protocols, by several orders of magnitude. Mahowald and Sivilotti proposed using an address-event representation to transmit pulses, or spikes, from an array of neurons on one chip to the corresponding location in an array on a second chip [9], [4], [10]. In their scheme, depicted in Figure 1, an address-encoder generates a unique binary address for each neuron whenever it spikes. A bus transmits these addresses to the receiving chip, where an address decoder selects the corresponding location. Eight years after Mahowald and Sivilotti proposed it, the address-event representation (AER) has emerged as the leading candidate for communication between neuromorphic chips. Indeed, at the NSF Neuromorphic Engineering Workshop held in June/July 1997 at Telluride CO, the AER Interchip Communication Workgroup was in the top two\u2014second only to Mindless Robots in popularity [11]! The performance of the original point-to-point protocol has been greatly improved. Efficient hierarchical arbitration circuits have been developed to handle oneand two-dimensional arrays [12], [13], [14]. Sender and receiver interfaces have been combined on a single chip to build a transceiver [15]. Support for multiple senders and receivers [16], [15], [17], one-dimensional nearest-neighbor\u2013 IEEE TRANSACTIONS ON CIRCUITS & SYSTEMS, VOL. XX, NO. Y, MONTH 1999 101", "title": "Point-to-point connectivity between neuromorphic chips using address events"}, "867e94c93524908ad1035542e911c244aaad205b": {"paper_id": "867e94c93524908ad1035542e911c244aaad205b", "abstract": "This work presents a study of three important issues of the color pixel classification approach to skin segmentation: color representation, color quantization, and classification algorithm. Our analysis of several representative color spaces using the Bayesian classifier with the histogram technique shows that skin segmentation based on color pixel classification is largely unaffected by the choice of the color space. However, segmentation performance degrades when only chrominance channels are used in classification. Furthermore, we find that color quantization can be as low as 64 bins per channel, although higher histogram sizes give better segmentation performance. The Bayesian classifier with the histogram technique and the multilayer perceptron classifier are found to perform better compared to other tested classifiers, including three piecewise linear classifiers, three unimodal Gaussian classifiers, and a Gaussian mixture classifier.", "title": "Skin segmentation using color pixel classification: analysis and comparison"}, "7eee628bb49b994dd8c5868a90f46af00770fd92": {"paper_id": "7eee628bb49b994dd8c5868a90f46af00770fd92", "abstract": "This paper addresses our proposed method to automatically segment out the person s face from a given image that consists of a head and shoulders view of the per son and a complex background scene The method involves a fast reliable and e ective algorithm that exploits the spatial distribution characteristics of human skin color A univer sal skin color map is derived and used on the chrominance component of the input image to detect pixels with skin color appearance Then based on the spatial distribution of the detected skin color pixels and their corresponding luminance values the algorithm employs a set of novel reg ularization processes to reinforce regions of skin color pix els that are more likely to belong to the facial regions and eliminate those that are not The performance of the face segmentation algorithm is illustrated by some simulation re sults carried out on various head and shoulders test images The use of face segmentation for video coding in applica tion such as videotelephony is then presented We explain how the face segmentation results can be used to improve the perceptual quality of videophone sequence encoded by the H compliant coder", "title": "Face segmentation using skin-color map in videophone applications"}, "bc0270fd0242d95a10a1c7146dc9b3483e89fa82": {"paper_id": "bc0270fd0242d95a10a1c7146dc9b3483e89fa82", "abstract": "This paper is concerned with estimating a probability density function of human skin color using a nite Gaussian mixture model whose parameters are estimated through the EM algorithm Hawkins statistical test on the normality and homoscedasticity common covariance matrix of the estimated Gaussian mixture models is performed and McLachlan s bootstrap method is used to test the number of components in a mixture Experimental results show that the estimated Gaussian mixture model ts skin images from a large database Applications of the estimated density function in image and video databases are presented", "title": "Gaussian mixture model for human skin color and its applications in image and video databases"}, "ddbb6e0913ac127004be73e2d4097513a8f02d37": {"paper_id": "ddbb6e0913ac127004be73e2d4097513a8f02d37", "abstract": "Detecting and recognizing human faces automatically in digital images strongly enhance content-based video indexing systems. In this paper, a novel scheme for human faces detection in color images under nonconstrained scene conditions, such as the presence of a complex background and uncontrolled illumination, is presented. Color clustering and filtering using approximations of the YCbCr and HSV skin color subspaces are applied on the original image, providing quantized skin color regions. A merging stage is then iteratively performed on the set of homogeneous skin color regions in the color quantized image, in order to provide a set of potential face areas. Constraints related to shape and size of faces are applied, and face intensity texture is analyzed by performing a wavelet packet decomposition on each face area candidate in order to detect human faces. The wavelet coefficients of the band filtered images characterize the face texture and a set of simple statistical deviations is extracted in order to form compact and meaningful feature vectors. Then, an efficient and reliable probabilistic metric derived from the Bhattacharrya distance is used in order to classify the extracted feature vectors into face or nonface areas, using some prototype face area vectors, acquired in a previous training stage.", "title": "Face Detection Using Quantized Skin Color Regions Merging and Wavelet Packet Analysis"}, "0d94a0a51cdecbdec81c97d2040ed28d3e9c96de": {"paper_id": "0d94a0a51cdecbdec81c97d2040ed28d3e9c96de", "abstract": "We describe the Photobook system, which is a set of interactive tools for browsing and searching images and image sequences. These query tools differ from those used in standard image databases in that they make direct use of the image content rather than relying on text annotations. Direct search on image content is made possible by use of semantics-preserving image compression, which reduces images to a small set of perceptually-significant coefficients. We discuss three types of Photobook descriptions in detail: one that allows search based on appearance, one that uses 2-D shape, and a third that allows search based on textural properties. These image content descriptions can be combined with each other and with text-based descriptions to provide a sophisticated browsing and search capability. In this paper we demonstrate Photobook on databases containing images of people, video keyframes, hand tools, fish, texture swatches, and 3-D medical data.", "title": "Photobook: Content-based manipulation of image databases"}, "fa2603efaf717974c77162c93d800defae61a129": {"paper_id": "fa2603efaf717974c77162c93d800defae61a129", "abstract": "This paper proposes a face recognition system, based on probabilistic decision-based neural networks (PDBNN). With technological advance on microelectronic and vision system, high performance automatic techniques on biometric recognition are now becoming economically feasible. Among all the biometric identification methods, face recognition has attracted much attention in recent years because it has potential to be most nonintrusive and user-friendly. The PDBNN face recognition system consists of three modules: First, a face detector finds the location of a human face in an image. Then an eye localizer determines the positions of both eyes in order to generate meaningful feature vectors. The facial region proposed contains eyebrows, eyes, and nose, but excluding mouth (eye-glasses will be allowed). Lastly, the third module is a face recognizer. The PDBNN can be effectively applied to all the three modules. It adopts a hierarchical network structures with nonlinear basis functions and a competitive credit-assignment scheme. The paper demonstrates a successful application of PDBNN to face recognition applications on two public (FERET and ORL) and one in-house (SCR) databases. Regarding the performance, experimental results on three different databases such as recognition accuracies as well as false rejection and false acceptance rates are elaborated. As to the processing speed, the whole recognition process (including PDBNN processing for eye localization, feature extraction, and classification) consumes approximately one second on Sparc10, without using hardware accelerator or co-processor.", "title": "Face recognition/detection by probabilistic decision-based neural network"}, "002aaf4412f91d0828b79511f35c0863a1a32c47": {"paper_id": "002aaf4412f91d0828b79511f35c0863a1a32c47", "abstract": "We present a real-time face tracker in this paper The system has achieved a rate of 30% frameshecond using an HP-9000 workstation with a framegrabber and a Canon VC-CI camera. It can track a person 'sface while the person moves freely (e.g., walks, jumps, sits down and stands up) in a room. Three types of models have been employed in developing the system. First, we present a stochastic model to characterize skin-color distributions of human faces. The information provided by the model is sufJicient for tracking a human face in various poses and views. This model is adaptable to different people and different lighting conditions in real-time. Second, a motion model e's used to estimate image motion and to predict search window. Third, a camera model is used toprediet and to compensate for camera motion. The system can be applied to tele-conferencing and many HCI applications including lip-reading and gaze tracking. The principle in developing this system can be extended to other tracking problems such as tracking the human hand.", "title": "A real-time face tracker"}, "0a11cd64f46b34fee230840684dae3cc8e1905a8": {"paper_id": "0a11cd64f46b34fee230840684dae3cc8e1905a8", "abstract": "Computer vision is embracing a new research focus in which the aim is to develop visual skills for robots that allow them to interact with a dynamic, realistic environment. To achieve this aim, new kinds of vision algorithms need to be developed which run in real time and subserve the robot's goals. Two fundamental goals are determining the location of a known object. Color can be successfully used for both tasks. This article demonstrates that color histograms of multicolored objects provide a robust, efficient cue for indexing into a large database of models. It shows that color histograms are stable object representations in the presence of occlusion and over change in view, and that they can differentiate among a large number of objects. For solving the identification problem, it introduces a technique calledHistogram Intersection, which matches model and image histograms and a fast incremental version of Histogram Intersection, which allows real-time indexing into a large database of stored models. For solving the location problem it introduces an algorithm calledHistogram Backprojection, which performs this task efficiently in crowded scenes.", "title": "Color indexing"}, "cc470d34b6d76518ef4435b627ba1ec01ac55c03": {"paper_id": "cc470d34b6d76518ef4435b627ba1ec01ac55c03", "abstract": "Skin detection is an important process in many of computer vision algorithms. It usually is a process that starts at a pixel-level, and that involves a pre-process of colorspace transformation followed by a classification process. A colorspace transformation is assumed to increase separability between skin and non-skin classes, to increase similarity among different skin tones, and to bring a robust performance under varying illumination conditions, without any sound reasonings. In this work, we examine if the colorspace transformation does bring those benefits by measuring four separability measurements on a large dataset of 805 images with different skin tones and illumination. Surprising results indicate that most of the colorspace transformations do not bring the benefits which have been assumed.", "title": "Does Colorspace Transformation Make Any Difference on Skin Detection?"}, "55da1b4c2cd371a3c4e74400f8960a2eef97cbfd": {"paper_id": "55da1b4c2cd371a3c4e74400f8960a2eef97cbfd", "abstract": "The performance of brain-machine interfaces (BMIs) that continuously control upper limb neuroprostheses may benefit from distinguishing periods of posture and movement so as to prevent inappropriate movement of the prosthesis. Few studies, however, have investigated how decoding behavioral states and detecting the transitions between posture and movement could be used autonomously to trigger a kinematic decoder. We recorded simultaneous neuronal ensemble and local field potential (LFP) activity from microelectrode arrays in primary motor cortex (M1) and dorsal (PMd) and ventral (PMv) premotor areas of two male rhesus monkeys performing a center-out reach-and-grasp task, while upper limb kinematics were tracked with a motion capture system with markers on the dorsal aspect of the forearm, hand, and fingers. A state decoder was trained to distinguish four behavioral states (baseline, reaction, movement, hold), while a kinematic decoder was trained to continuously decode hand end point position and 18 joint angles of the wrist and fingers. LFP amplitude most accurately predicted transition into the reaction (62%) and movement (73%) states, while spikes most accurately decoded arm, hand, and finger kinematics during movement. Using an LFP-based state decoder to trigger a spike-based kinematic decoder [r = 0.72, root mean squared error (RMSE) = 0.15] significantly improved decoding of reach-to-grasp movements from baseline to final hold, compared with either a spike-based state decoder combined with a spike-based kinematic decoder (r = 0.70, RMSE = 0.17) or a spike-based kinematic decoder alone (r = 0.67, RMSE = 0.17). Combining LFP-based state decoding with spike-based kinematic decoding may be a valuable step toward the realization of BMI control of a multifingered neuroprosthesis performing dexterous manipulation.", "title": "State-based decoding of hand and finger kinematics using neuronal ensemble and LFP activity during dexterous reach-to-grasp movements."}, "9c9cea8d717a63ed233918f7b059861fb69c943b": {"paper_id": "9c9cea8d717a63ed233918f7b059861fb69c943b", "abstract": "Currently, multiple data vendors utilize the cloud-computing paradigm for trading raw data, associated analytical services, and analytic results as a commodity good. We observe that these vendors often move the functionality of data warehouses to cloud-based platforms. On such platforms, vendors provide services for integrating and analyzing data from public and commercial data sources. We present insights from interviews with seven established vendors about their key challenges with regard to pricing strategies in different market situations and derive associated research problems for the business intelligence community.", "title": "Pricing Approaches for Data Markets"}, "b40567c71269c01c57330cce321d9147bc62a2e8": {"paper_id": "b40567c71269c01c57330cce321d9147bc62a2e8", "abstract": "Research tools critical for exploratory search success involve the creation of new interfaces that move the process beyond predictable fact retrieval.", "title": "Exploratory search: from finding to understanding"}, "076077a5771747ad7355120f1ba64cfd603141c6": {"paper_id": "076077a5771747ad7355120f1ba64cfd603141c6", "abstract": "Written communication of ideas is carried out on the basis of statistical probability in that a writer chooses that level of subject specificity and that combination of words which he feels will convey the most meaning. Since this process varies among individuals and since similar ideas are therefore relayed at different levels of specificity and by means of different words, the problem of literature searching by machines still presents major difficulties. A statistical approach to this problem will be outlined and the various steps of a system based on this approach will be described. Steps include the statistical analysis of a collection of documents in a field of interest, the establishment of a set of \"notions\" and the vocabulary by which they are expressed, the compilation of a thesaurus-type dictionary and index, the automatic encoding of documents by machine with the aid of such a dictionary, the encoding of topological notations (such as branched structures), the recording of the coded information, the establishment of a searching pattern for finding pertinent information, and the programming of appropriate machines to carry out a search.", "title": "A Statistical Approach to Mechanized Encoding and Searching of Literary Information"}, "5350676fae09092b42731448acae3469cba8919c": {"paper_id": "5350676fae09092b42731448acae3469cba8919c", "abstract": "Software tools designed for disk analysis play a critical role today in digital forensics investigations. However, these digital forensics tools are often difficult to use, usually task specific, and generally require professionally trained users with IT backgrounds. The relevant tools are also often open source requiring additional technical knowledge and proper configuration. This makes it difficult for investigators without some computer science background to easily conduct the needed disk analysis. In this dissertation, we present AUDIT, a novel automated disk investigation toolkit that supports investigations conducted by non-expert (in IT and disk technology) and expert investigators. Our system design and implementation of AUDIT intelligently integrates open source tools and guides non-IT professionals while requiring minimal technical knowledge about the disk structures and file systems of the target disk image. We also present a new hierarchical disk investigation model which leads AUDIT to systematically examine the disk in its totality based on its physical and logical structures. AUDIT\u2019s capabilities as an intelligent digital assistant are evaluated through a series of experiments comparing it with a human investigator as well as against standard benchmark disk images.", "title": "Building an Intelligent Assistant for Digital Forensics"}, "2cb72b036d11c13ea07ce95a9b946b4d88d26ea5": {"paper_id": "2cb72b036d11c13ea07ce95a9b946b4d88d26ea5", "abstract": "In this paper it is shown that the computation of the optical flow from a sequence of timevarying images is not, in general, an underconstrained problem. A local algorithm for the computation of the optical flow which uses second order derivatives of the image brightness pattern, and that avoids the aperture problem, is presented. The obtained optical flow is very similar to the true motion field \u2014 which is the vector field associated with moving features on the image plane \u2014 and can be used to recover 3D motion information. Experimental results on sequences of real images, together with estimates of relevant motion parameters, like time-to-crash for translation and angular velocity for rotation, are presented and discussed. Due to the remarkable accuracy which can be achieved in estimating motion parameters, the proposed method is likely to be very useful in a number of computer vision applications.", "title": "A computational approach to motion perception"}, "00a7370518a6174e078df1c22ad366a2188313b5": {"paper_id": "00a7370518a6174e078df1c22ad366a2188313b5", "abstract": "Optical flow cannot be computed locally, since only one independent measurement is available from the image sequence at a point, while the flow velocity has two components. A second constraint is needed. A method for finding the optical flow pattern is presented which assumes that the apparent velocity of the brightness pattern varies smoothly almost everywhere in the image. An iterative implementation is shown which successfully computes the optical flow for a number of synthetic image sequences. The algorithm is robust in that it can handle image sequences that are quantized rather coarsely in space and time. It is also insensitive to quantization of brightness levels and additive noise. Examples are included where the assumption of smoothness is violated at singular points or along lines in the image.", "title": "Determining Optical Flow"}, "67301c286439a7c24368300ea13e9785bd666aed": {"paper_id": "67301c286439a7c24368300ea13e9785bd666aed", "abstract": "A method is described which quantifies the speed and direction of several moving objects in a sequence of digital images. A relationship between the time variation of intensity, the spatial gradient, and velocity has been developed which allows the determination of motion using clustering techniques. This paper describes these relationships, the clustering technique, and provides examples of the technique on real images containing several moving objects.", "title": "Velocity determination in scenes containing several moving objects"}, "ec56d14345ec85256a6eab9380477eb9da507e3f": {"paper_id": "ec56d14345ec85256a6eab9380477eb9da507e3f", "abstract": null, "title": "Usability: Lessons Learned ... and Yet to Be Learned"}, "5cf7d5bfb380b9d3d7459ae14aedc0672fb3ab78": {"paper_id": "5cf7d5bfb380b9d3d7459ae14aedc0672fb3ab78", "abstract": "Understanding the relation between usability measures seems crucial to deepen our conception of usability and to select the right measures for usability studies. We present a meta-analysis of correlations among usability measures calculated from the raw data of 73 studies. Correlations are generally low: effectiveness measures (e.g., errors) and efficiency measures (e.g., time) have a correlation of .247 \u00b1 .059 (Pearson's product-moment correlation with 95% confidence interval), efficiency and satisfaction (e.g., preference) one of .196 \u00b1 .064, and effectiveness and satisfaction one of .164 \u00b1 .062. Changes in task complexity do not influence these correlations, but use of more complex measures attenuates them. Standard questionnaires for measuring satisfaction appear more reliable than homegrown ones. Measures of users' perceptions of phenomena are generally not correlated with objective measures of the phenomena. Implications for how to measure usability are drawn and common models of usability are criticized.", "title": "Meta-analysis of correlations among usability measures"}, "8b3f1a417c76badd2893ee5a655b39cd7aecf5ac": {"paper_id": "8b3f1a417c76badd2893ee5a655b39cd7aecf5ac", "abstract": "How to measure usability is an important question in HCI research and user interface evaluation. We review current practice in measuring usability by categorizing and discussing usability measures from 180 studies published in core HCI journals and proceedings. The discussion distinguish several problems with the measures, including whether they actually measure usability, if they cover usability broadly, how they are reasoned about, and if they meet recommendations on how to measure usability. In many studies, the choice of and reasoning about usability measures fall short of a valid and reliable account of usability as quality-in-use of the user interface being studied. Based on the review, we discuss challenges for studies of usability and for research into how to measure usability. The challenges are to distinguish and empirically compare subjective and objective measures of usability; to focus on developing and employing measures of learning and retention; to study long-term use and usability; to extend measures of satisfaction beyond post-use questionnaires; to validate and standardize the host of subjective satisfaction questionnaires used; to study correlations between usability measures as a means for validation; and to use both micro and macro tasks and corresponding measures of usability. In conclusion, we argue that increased attention to the problems identified and challenges discussed may strengthen studies of usability and usability research. r 2005 Elsevier Ltd. All rights reserved.", "title": "Current practice in measuring usability: Challenges to usability studies and research"}, "81a784a05fedcb1a1fa00a83741d25f8b1edd789": {"paper_id": "81a784a05fedcb1a1fa00a83741d25f8b1edd789", "abstract": "The aircraft industry is developing the more electric aircraft (MEA) with an ultimate goal of distributing only electrical power across the airframe. The replacement of existing systems with electric equivalents has, and will continue to, significantly increase the electrical power requirement. This has created a need for the enhancement of generation capacity and changes to distribution systems. The higher powers will push distribution voltages higher in order to limit conduction losses and reduce cable size, and hence weight. A power electronic interface may be required to regulate generator output into the distributed power form.", "title": "Electrical generation and distribution for the more electric aircraft"}, "5d8ca70a0448998162f91e3ce84fac92e10d4ccf": {"paper_id": "5d8ca70a0448998162f91e3ce84fac92e10d4ccf", "abstract": "The LogAnswer system is an application of automated reasoning to the field of open domain question answering. In order to find answers to natural language questions regarding arbitrary topics, the system integrates an automated theorem prover in a framework of natural language processing tools. The latter serve to construct an extensive knowledge base automatically from given textual sources, while the automated theorem prover makes it possible to derive answers by deductive reasoning. In the paper, we discuss the requirements to the prover that arise in this application, especially concerning efficiency and robustness. The proposed solution rests on incremental reasoning, relaxation of the query (if no proof of the full query is found), and other techniques. In order to improve the robustness of the approach to gaps of the background knowledge, the results of deductive processing are combined with shallow linguistic features by machine learning.", "title": "An application of automated reasoning in natural language question answering"}, "454c67eea55fc97d41a13bdb9c29cf669be7eaf5": {"paper_id": "454c67eea55fc97d41a13bdb9c29cf669be7eaf5", "abstract": "Since 1984, a person-century of effort has gone into building CYC, a universal schema of roughly 105 general concepts spanning human reality. Most of the time has been spent codifying knowledge about these concepts; approximately 106 commonsense axioms have been handcrafted for and entered into CYC's knowledge base, and millions more have been inferred and cached by CYC. This article examines the fundamental assumptions of doing such a large-scale project, reviews the technical lessons learned by the developers, and surveys the range of applications that are or soon will be enabled by the technology.", "title": "CYC: A Large-Scale Investment in Knowledge Infrastructure"}, "909273dc44ce4c4f2cf4adfe5d60c3d421635909": {"paper_id": "909273dc44ce4c4f2cf4adfe5d60c3d421635909", "abstract": "Cyc is a bold attempt to assemble a massive knowledge base (on the order of 108 axioms) spanning human consensus knowledge. This article examines the need for such an undertaking and reviews the authos' efforts over the past five years to begin its construction. The methodology and history of the project are briefly discussed, followed by a more developed treatment of the current state of the representation language used (epistemological level), techniques for efficient inferencing and default reasoning (heuristic level), and the content and organization of the knowledge base.", "title": "CYC: Toward Programs With Common Sense"}, "1a1b53f6253d9f9d586775f879ced9d6abaaa2a1": {"paper_id": "1a1b53f6253d9f9d586775f879ced9d6abaaa2a1", "abstract": "With the prevalence of server blades and systems-on-a-chip (SoCs), interconnection networks are becoming an important part of the microprocessor landscape. However, there is limited tool support available for their design. While performance simulators have been built that enable performance estimation while varying network parameters, these cover only one metric of interest in modern designs. System power consumption is increasingly becoming equally, if not more important than performance. It is now critical to get detailed power-performance tradeoff information early in the microarchitectural design cycle. This is especially so as interconnection networks consume a significant fraction of total system power. It is exactly this gap that the work presented in this paper aims to fill.We present Orion, a power-performance interconnection network simulator that is capable of providing detailed power characteristics, in addition to performance characteristics, to enable rapid power-performance trade-offs at the architectural-level. This capability is provided within a general framework that builds a simulator starting from a microarchitectural specification of the interconnection network. A key component of this construction is the architectural-level parameterized power models that we have derived as part of this effort. Using component power models and a synthesized efficient power (and performance) simulator, a microarchitect can rapidly explore the design space. As case studies, we demonstrate the use of Orion in determining optimal system parameters, in examining the effect of diverse traffic conditions, as well as evaluating new network microarchitectures. In each of the above, the ability to simultaneously monitor power and performance is key in determining suitable microarchitectures.", "title": "Orion: a power-performance simulator for interconnection networks"}, "2857fd5657b58701dc6545ae9a1871999b9fdf30": {"paper_id": "2857fd5657b58701dc6545ae9a1871999b9fdf30", "abstract": "Static power dissipation due to transistor leakage constitutes an increasing fraction of the total power in modern semiconductor technologies. Current technology trends indicate that the contribution will increase rapidly, reaching one half of total power dissipation within three process generations. Developing power efficient products will require consideration of static power in the earliest phases of design, including architecture and microarchitecture definition. We propose a simple equation for estimating static power consumption at the architectural level: , where VCC is the supply voltage, N is the number of transistors, kdesign is a design dependent parameter, and is a technology dependent parameter. This model enables high-level reasoning about the likely static power demands of alternative microarchitectures. Reasonably accurate values for the factors within the equation may be obtained directly from the high-level designs or by straightforward scaling arguments. The factors within the equation also suggest opportunities for static power optimization, including reducing the total number of devices, partitioning the design to allow for lower supply voltages or slower, less leaky transistors, turning off unused devices, favoring certain design styles, and favoring high bandwidth over low latency. Speculation is also examined as a means to employ slower transistors without a significant performance penalty.", "title": "A static power model for architects"}, "9530e3328f614371c45ee2324c45ff6dc7390760": {"paper_id": "9530e3328f614371c45ee2324c45ff6dc7390760", "abstract": "Evaluating in Massive Open Online Courses (MOOCs) is a difficult task because of the huge number of students involved in the courses. Peer grading is an effective method to cope with this problem, but something must be done to lessen the effect of the subjective evaluation. In this paper we present a matrix factorization approach able to learn from the order of the subset of exams evaluated by each grader. We tested this method on a data set provided by a real peer review process. By using a tailored graphical representation, the induced model could also allow the detection of peculiarities in the peer review process.", "title": "Peer Assessment in MOOCs Using Preference Learning via Matrix Factorization"}, "f09c129ffbb0d23e0270dc0e567b8778b9136821": {"paper_id": "f09c129ffbb0d23e0270dc0e567b8778b9136821", "abstract": "Music prediction tasks range from predicting tags given a song or clip of audio, predicting the name of the artist, or predicting related songs given a song, clip, artist name or tag. That is, we are interested in every semantic relationship between the different musical concepts in our database. In realistically sized databases, the number of songs is measured in the hundreds of thousands or more, and the number of artists in the tens of thousands or more, providing a considerable challenge to standard machine learning techniques. In this work, we propose a method that scales to such datasets which attempts to capture the semantic similarities between the database items by modeling audio, artist names, and tags in a single low-dimensional semantic embedding space. This choice of space is learnt by optimizing the set of prediction tasks of interest jointly using multi-task learning. Our single model learnt by training on the joint objective function is shown experimentally to have improved accuracy over training on each task alone. Our method also outperforms the baseline methods tried and, in comparison to them, is faster and consumes less memory. We also demonstrate how our method learns an interpretable model, where the semantic space captures well the similarities of interest.", "title": "Multi-Tasking with Joint Semantic Spaces for Large-Scale Music Annotation and Retrieval"}, "86e951e190586b84c530f9f03504f9ad70cc650a": {"paper_id": "86e951e190586b84c530f9f03504f9ad70cc650a", "abstract": "Feature extraction is a crucial part of many MIR tasks. In this work, we present a system that can automatically extract relevant features from audio for a given task. The feature extraction system consists of a Deep Belief Network (DBN) on Discrete Fourier Transforms (DFTs) of the audio. We then use the activations of the trained network as inputs for a non-linear Support Vector Machine (SVM) classifier. In particular, we learned the features to solve the task of genre recognition. The learned features perform significantly better than MFCCs. Moreover, we obtain a classification accuracy of 84.3% on the Tzanetakis dataset, which compares favorably against state-of-the-art genre classifiers using frame-based features. We also applied these same features to the task of auto-tagging. The autotaggers trained with our features performed better than those that were trained with timbral and temporal features.", "title": "Learning Features from Music Audio with Deep Belief Networks"}, "0025b963134b1c0b64c1389af19610d038ab7072": {"paper_id": "0025b963134b1c0b64c1389af19610d038ab7072", "abstract": "There are many applications in which it is desirable to order rather than classify instances. Here we consider the problem of learning how to order, given feedback in the form of preference judgments, i.e., statements to the effect that one instance should be ranked ahead of another. We outline a two-stage approach in which one first learns by conventional means a preference function, of the form PREF , which indicates whether it is advisable to rank before . New instances are then ordered so as to maximize agreements with the learned preference function. We show that the problem of finding the ordering that agrees best with a preference function is NP-complete, even under very restrictive assumptions. Nevertheless, we describe a simple greedy algorithm that is guaranteed to find a good approximation. We then discuss an on-line learning algorithm, based on the \u201cHedge\u201d algorithm, for finding a good linear combination of ranking \u201cexperts.\u201d We use the ordering algorithm combined with the on-line learning algorithm to find a combination of \u201csearch experts,\u201d each of which is a domain-specific query expansion strategy for a WWW search engine, and present experimental results that demonstrate the merits of our approach.", "title": "Learning to Order Things"}, "7a59927fb02d28f61356ea5c23d2ac819881b5f9": {"paper_id": "7a59927fb02d28f61356ea5c23d2ac819881b5f9", "abstract": "Peer and self-assessment offer an opportunity to scale both assessment and learning to global classrooms. This article reports our experiences with two iterations of the first large online class to use peer and self-assessment. In this class, peer grades correlated highly with staff-assigned grades. The second iteration had 42.9% of students\u2019 grades within 5% of the staff grade, and 65.5% within 10%. On average, students assessed their work 7% higher than staff did. Students also rated peers\u2019 work from their own country 3.6% higher than those from elsewhere. We performed three experiments to improve grading accuracy. We found that giving students feedback about their grading bias increased subsequent accuracy. We introduce short, customizable feedback snippets that cover common issues with assignments, providing students more qualitative peer feedback. Finally, we introduce a data-driven approach that highlights high-variance items for improvement. We find that rubrics that use a parallel sentence structure, unambiguous wording, and well-specified dimensions have lower variance. After revising rubrics, median grading error decreased from 12.4% to 9.9%.", "title": "Peer and self assessment in massive online classes"}, "36ea668bb7617b9c1e6e98aebe96a0aaf90b569e": {"paper_id": "36ea668bb7617b9c1e6e98aebe96a0aaf90b569e", "abstract": "This paper presents an approach to automatically optimizing the retrieval quality of search engines using clickthrough data. Intuitively, a good information retrieval system should present relevant documents high in the ranking, with less relevant documents following below. While previous approaches to learning retrieval functions from examples exist, they typically require training data generated from relevance judgments by experts. This makes them difficult and expensive to apply. The goal of this paper is to develop a method that utilizes clickthrough data for training, namely the query-log of the search engine in connection with the log of links the users clicked on in the presented ranking. Such clickthrough data is available in abundance and can be recorded at very low cost. Taking a Support Vector Machine (SVM) approach, this paper presents a method for learning retrieval functions. From a theoretical perspective, this method is shown to be well-founded in a risk minimization framework. Furthermore, it is shown to be feasible even for large sets of queries and features. The theoretical results are verified in a controlled experiment. It shows that the method can effectively adapt the retrieval function of a meta-search engine to a particular group of users, outperforming Google in terms of retrieval quality after only a couple of hundred training examples.", "title": "Optimizing search engines using clickthrough data"}, "0420c25cbf57da8ceeff7e2d204466270db210cb": {"paper_id": "0420c25cbf57da8ceeff7e2d204466270db210cb", "abstract": "Indexing systems for the World Wide Web, such as Lycos and Alta Vista, play an essential role in making the Web useful and usable. These systems are based on Information Retrieval methods for indexing plain text documents, but also include heuristics for adjusting their document rankings based on the special HTML structure of Web documents. In this paper, we describe a wide range of such heuristics|including a novel one inspired by reinforcement learning techniques for propagating rewards through a graph|which can be used to a ect a search engine's rankings. We then demonstrate a system which learns to combine these heuristics automatically, based on feedback collected unintrusively from users, resulting in much improved rankings.", "title": "A Machine Learning Architecture for Optimizing Web Search Engines"}, "21dd2790b76a57b42191b19a54505837f3969141": {"paper_id": "21dd2790b76a57b42191b19a54505837f3969141", "abstract": "In massive open-access online courses (MOOCs), peer grading serves as a critical tool for scaling the grading of complex, open-ended assignments to courses with tens or hundreds of thousands of students. But despite promising initial trials, it does not always deliver accurate results compared to human experts. In this paper, we develop algorithms for estimating and correcting for grader biases and reliabilities, showing significant improvement in peer grading accuracy on real data with 63,199 peer grades from Coursera\u2019s HCI course offerings \u2014 the largest peer grading networks analysed to date. We relate grader biases and reliabilities to other student factors such as engagement, performance as well as commenting style. We also show that our model can lead to more intelligent assignment of graders to gradees.", "title": "Tuned Models of Peer Assessment in MOOCs"}, "6953420c593842697dd09bc2cf7ffbbaf67a6e8e": {"paper_id": "6953420c593842697dd09bc2cf7ffbbaf67a6e8e", "abstract": "Modern machine learning-based approaches to computer vision require very large databases of hand labeled images. Some contemporary vision systems already require on the order of millions of images for training (e.g., Omron face detector [9]). New Internet-based services allow for a large number of labelers to collaborate around the world at very low cost. However, using these services brings interesting theoretical and practical challenges: (1) The labelers may have wide ranging levels of expertise which are unknown a priori, and in some cases may be adversarial; (2) images may vary in their level of difficulty; and (3) multiple labels for the same image must be combined to provide an estimate of the actual label of the image. Probabilistic approaches provide a principled way to approach these problems. In this paper we present a probabilistic model and use it to simultaneously infer the label of each image, the expertise of each labeler, and the difficulty of each image. On both simulated and real data, we demonstrate that the model outperforms the commonly used \u201cMajority Vote\u201d heuristic for inferring image labels, and is robust to both noisy and adversarial labelers.", "title": "Whose Vote Should Count More: Optimal Integration of Labels from Labelers of Unknown Expertise"}, "57619d0962814f38a447fe9e26e8bcc74b5fc1bb": {"paper_id": "57619d0962814f38a447fe9e26e8bcc74b5fc1bb", "abstract": "We show how machine learning and inference can be harnessed to leverage the complementary strengths of humans and computational agents to solve crowdsourcing tasks. We construct a set of Bayesian predictive models from data and describe how the models operate within an overall crowdsourcing architecture that combines the efforts of people and machine vision on the task of classifying celestial bodies defined within a citizens\u2019 science project named Galaxy Zoo. We show how learned probabilistic models can be used to fuse human and machine contributions and to predict the behaviors of workers. We employ multiple inferences in concert to guide decisions on hiring and routing workers to tasks so as to maximize the efficiency of large-scale crowdsourcing processes based on expected utility.", "title": "Combining human and machine intelligence in large-scale crowdsourcing"}, "95ef74f3d40ab11163f41edaa757b71de4550a52": {"paper_id": "95ef74f3d40ab11163f41edaa757b71de4550a52", "abstract": "We propose a new probabilistic graphical model that jointly models the difficulties of questions, the abilities of participants and the correct answers to questions in aptitude testing and crowdsourcing settings. We devise an active learning/adaptive testing scheme based on a greedy minimization of expected model entropy, which allows a more efficient resource allocation by dynamically choosing the next question to be asked based on the previous responses. We present experimental results that confirm the ability of our model to infer the required parameters and demonstrate that the adaptive testing scheme requires fewer questions to obtain the same accuracy as a static test scenario.", "title": "How To Grade a Test Without Knowing the Answers - A Bayesian Graphical Model for Adaptive Crowdsourcing and Aptitude Testing"}, "1ae43fde986515f0effad3d62b1cb9eff9032003": {"paper_id": "1ae43fde986515f0effad3d62b1cb9eff9032003", "abstract": "Hadapt is a start-up company currently commercializing the Yale University research project called HadoopDB. The company focuses on building a platform for Big Data analytics in the cloud by introducing a storage layer optimized for structured data and by providing a framework for executing SQL queries efficiently. This work considers processing data warehousing queries over very large datasets. Our goal is to maximize perfor mance while, at the same time, not giving up fault tolerance and scalability. We analyze the complexity of this problem in the split execution environment of HadoopDB. Here, incoming queries are examined; parts of the query are pushed down and executed inside the higher performing database layer; and the rest of the query is processed in a more generic MapReduce framework.\n In this paper, we discuss in detail performance-oriented query execution strategies for data warehouse queries in split execution environments, with particular focus on join and aggregation operations. The efficiency of our techniques is demonstrated by running experiments using the TPC-H benchmark with 3TB of data. In these experiments we compare our results with a standard commercial parallel database and an open-source MapReduce implementation featuring a SQL interface (Hive). We show that HadoopDB successfully competes with other systems.", "title": "Efficient processing of data warehousing queries in a split execution environment"}, "4c7bfa933c11c7a802c2fa9c1dc475dba36a2bd5": {"paper_id": "4c7bfa933c11c7a802c2fa9c1dc475dba36a2bd5", "abstract": "The size of data sets being collected and analyzed in the industry for business intelligence is growing rapidly, making traditional warehousing solutions prohibitively expensive. Hadoop [1] is a popular open-source map-reduce implementation which is being used in companies like Yahoo, Facebook etc. to store and process extremely large data sets on commodity hardware. However, the map-reduce programming model is very low level and requires developers to write custom programs which are hard to maintain and reuse. In this paper, we present Hive, an open-source data warehousing solution built on top of Hadoop. Hive supports queries expressed in a SQL-like declarative language - HiveQL, which are compiled into map-reduce jobs that are executed using Hadoop. In addition, HiveQL enables users to plug in custom map-reduce scripts into queries. The language includes a type system with support for tables containing primitive types, collections like arrays and maps, and nested compositions of the same. The underlying IO libraries can be extended to query data in custom formats. Hive also includes a system catalog - Metastore - that contains schemas and statistics, which are useful in data exploration, query optimization and query compilation. In Facebook, the Hive warehouse contains tens of thousands of tables and stores over 700TB of data and is being used extensively for both reporting and ad-hoc analyses by more than 200 users per month.", "title": "Hive - a petabyte scale data warehouse using Hadoop"}, "8c4e02205ab4dcfa428ecfa2db866a38b12c199c": {"paper_id": "8c4e02205ab4dcfa428ecfa2db866a38b12c199c", "abstract": "The MapReduce framework is increasingly being used to analyze large volumes of data. One important type of data analysis done with MapReduce is log processing, in which a click-stream or an event log is filtered, aggregated, or mined for patterns. As part of this analysis, the log often needs to be joined with reference data such as information about users. Although there have been many studies examining join algorithms in parallel and distributed DBMSs, the MapReduce framework is cumbersome for joins. MapReduce programmers often use simple but inefficient algorithms to perform joins. In this paper, we describe crucial implementation details of a number of well-known join strategies in MapReduce, and present a comprehensive experimental comparison of these join techniques on a 100-node Hadoop cluster. Our results provide insights that are unique to the MapReduce platform and offer guidance on when to use a particular join algorithm on this platform.", "title": "A comparison of join algorithms for log processing in MaPreduce"}, "6f77626160f3366e5ea72d636f6a80825ebf5881": {"paper_id": "6f77626160f3366e5ea72d636f6a80825ebf5881", "abstract": "This case study describes frostbite, a previously unreported complication following cryolipolysis, which resulted in substantial necrosis of the flank. Medical attention was not sought until one week after treatment. On examination, two distinct areas of significant frostbite in the left flank with surrounding erythema were revealed. Surgical intervention was avoided, as is recommended in cases of frostbite, and conservative treatment resulted in recovery of the affected area. Here, the authors highlight the adverse effects related to cryolipolysis, analysing the pathogenesis, clinical manifestations and management of this injury. The necessity of regulation within the cosmetic sector and the challenges associated with its implementation are also described. The authors believe emphasis must be placed on increasing patient awareness on the potential hazards of seeking cosmetic treatment from unregulated providers.", "title": "Frostbite following cryolipolysis treatment in a beauty salon: a case study."}, "75eb7fab69f89edf48f62de41ded0b3bc5f4af83": {"paper_id": "75eb7fab69f89edf48f62de41ded0b3bc5f4af83", "abstract": "IMPORTANCE\nCryolipolysis is the noninvasive reduction of fat with localized cutaneous cooling. Since initial introduction, over 650,000 cryolipolysis treatment cycles have been performed worldwide. We present a previously unreported, rare adverse effect following cryolipolysis: paradoxical adipose hyperplasia.\n\n\nOBSERVATIONS\nA man in his 40s underwent a single cycle of cryolipolysis to his abdomen. Three months following his treatment, a gradual enlargement of the treatment area was noted. This enlargement was a large, well-demarcated subcutaneous mass, slightly tender to palpation. Imaging studies revealed accumulation of adipose tissue with normal signal intensity within the treatment area.\n\n\nCONCLUSIONS AND RELEVANCE\nParadoxical adipose hyperplasia is a rare, previously unreported adverse effect of cryolipolysis with an incidence of 0.0051%. No single unifying risk factor has been identified. The phenomenon seems to be more common in male patients undergoing cryolipolysis. At this time, there is no evidence of spontaneous resolution. Further studies are needed to characterize the pathogenesis and histologic findings of this rare adverse event.", "title": "Paradoxical adipose hyperplasia after cryolipolysis."}, "8f041ec5eb741992b636e79b18ddc9ce4e5b2a7a": {"paper_id": "8f041ec5eb741992b636e79b18ddc9ce4e5b2a7a", "abstract": "A study was conducted to evaluate user performance andsatisfaction in completion of a set of text creation tasks usingthree commercially available continuous speech recognition systems.The study also compared user performance on similar tasks usingkeyboard input. One part of the study (Initial Use) involved 24users who enrolled, received training and carried out practicetasks, and then completed a set of transcription and compositiontasks in a single session. In a parallel effort (Extended Use),four researchers used speech recognition to carry out real worktasks over 10 sessions with each of the three speech recognitionsoftware products. This paper presents results from the Initial Usephase of the study along with some preliminary results from theExtended Use phase. We present details of the kinds of usabilityand system design problems likely in current systems and severalcommon patterns of error correction that we found.", "title": "Patterns of Entry and Correction in Large Vocabulary Continuous Speech Recognition System"}, "78e88e5801226496e109b7d5ded86a955b5a01d6": {"paper_id": "78e88e5801226496e109b7d5ded86a955b5a01d6", "abstract": "ID: 2423 Y. M. S. Al-Wesabi, Avishek Choudhury, Daehan Won Binghamton University, USA", "title": "Classification of Cervical Cancer Dataset"}, "fd11e32fdce081978aff8fa5d3f7629527627bd1": {"paper_id": "fd11e32fdce081978aff8fa5d3f7629527627bd1", "abstract": "Cervical cancer, as the fourth most common cause of death from cancer among women, has no symptoms in the early stage. There are few methods to diagnose cervical cancer precisely at present. Support vector machine (SVM) approach is introduced in this paper for cervical cancer diagnosis. Two improved SVM methods, support vector machine-recursive feature elimination and support vector machine-principal component analysis (SVM-PCA), are further proposed to diagnose the malignant cancer samples. The cervical cancer data are represented by 32 risk factors and 4 target variables: Hinselmann, Schiller, Cytology, and Biopsy. All four targets have been diagnosed and classified by the three SVM-based approaches, respectively. Subsequently, we make the comparison among these three methods and compare our ranking result of risk factors with the ground truth. It is shown that SVM-PCA method is superior to the others.", "title": "Data-Driven Diagnosis of Cervical Cancer With Support Vector Machine-Based Approaches"}, "8cc5f500be7688053a1abc1feab8a47e5581bfc5": {"paper_id": "8cc5f500be7688053a1abc1feab8a47e5581bfc5", "abstract": "Feature selection has been the focus of interest for quite some time and much work has been done. With the creation of huge databases and the consequent requirements for good machine learning techniques, new problems arise and novel approaches to feature selection are in demand. This survey is a comprehensive overview of many existing methods from the 1970\u2019s to the present. It identifies four steps of a typical feature selection method, and categorizes the different existing methods in terms of generation procedures and evaluation functions, and reveals hitherto unattempted combinations of generation procedures and evaluation functions. Representative methods are chosen from each category for detailed explanation and discussion via example. Benchmark datasets with different characteristics are used for comparative study. The strengths and weaknesses of different methods are explained. Guidelines for applying feature selection methods are given based on data types and domain characteristics. This survey identifies the future research areas in feature selection, introduces newcomers to this field, and paves the way for practitioners who search for suitable methods for solving domain-specific real-world applications. (Intelligent Data Analysis, Vol. I, no. 3, http:llwwwelsevier.co&ocate/ida)", "title": "Feature Selection for Classification"}, "a83bddb34618cc68f1014ca12eef7f537825d104": {"paper_id": "a83bddb34618cc68f1014ca12eef7f537825d104", "abstract": "We address the problem of finding a subset of features tha t allows a supervised induction algori thm to induce small high-accuracy concepts. We examine notions of relevance and irrelevance, and show tha t the definitions used in the machine learning literature do not adequately part i t ion the features into useful categories of relevance. We present definitions for irrelevance and for two degrees of relevance. These definitions improve our understanding of the behavior of previous subset selection algorithms, and help define the subset of features tha t should be sought. The features selected should depend not only on the features and the target concept, but also on the induction algorithm. We describe a method for feature subset selection using cross-validation tha t is applicable to any induction algorithm, and discuss experiments conducted with ID3 and C4.5 on artificial and real datasets . 1 I N T R O D U C T I O N In supervised learning, one is given a training set containing labelled instances. The instances are typically specified by assigning values to a set of features, and the task is to induce a hypothesis tha t accurately predicts the label of novel instances. Following Occam's razor (Blumer et al. 1987), min imum description length (Rissanen 1986), and min imum message length (Wallace & Freeman 1987), one usually a t t empts to find structures tha t correctly classify a large subset of the training set, and yet are not so complex tha t they begin to overfit the da ta . Ideally, the induction algor i thm should use only the subset of features tha t leads to the best performance. Since induction of minimal structures is NP-hard in many cases (Hancock 1989; Blum & Rivest 1992), algorithms usually conduct a heuristic search in the Figure 1: An example where ID3 picks a bad relevant feature (correlated) for the root, and an irrelevant feature (irrelevant). space of possible hypotheses. This heuristic search may lead to induced concepts which depend on irrelevant features, or in some cases even relevant features tha t hurt the overall accuracy. Figure 1 shows such a choice of a non-optimal split at the root made by ID3 (Quinlan 1986). The Boolean target concept is (AO A Al ) V (BO A Bl). The feature named \"irrelevant\" is uniformly random, and the feature \"correlated\" matches the class label 75% of the t ime. The left subtree is the correct decision tree, which is correctly induced if the \"correlated\" feature is removed from the da ta . C4.5 (Quinlan 1992) and CART (Breiman et al. 1984) induce similar trees with the \"correlated\" feature at the root. Such a split causes all these induction algorithms to generate trees tha t are less accurate than if this feature is completely removed. The problem of feature subset selection involves finding a \"good\" set of features under some objective function. Common objective functions are prediction accuracy, 122 John, Kohavi, and Pfleger structure size, and minimal use of input features (e.g., when features are tests tha t have an associated cost). In this paper we chose to investigate the possibility of improving prediction accuracy or decreasing the size of the structure without significantly decreasing prediction accuracy. This specific problem has been thoroughly investigated in the statistics literature, but under assumptions tha t do not apply to most learning algorithms (see Section 5). We begin by describing the notions of relevance and irrelevance tha t have been previously defined by researchers. We show tha t the definitions are too coarse-grained, and tha t better understanding can be achieved by looking at two degrees of relevance. Section 3 looks at two models for feature subset selection: the filter model and the wrapper model. We claim tha t the wrapper model is more appropriate than the filter model, which has received more attention in machine learning. Section 4 presents our experimental results, Section 5 describes related work, and Section 6 provides a summary and discussion of future work. 2 D E F I N I N G R E L E V A N C E In this section we present definitions of relevance tha t have been suggested in the literature. We then show a single example where the definitions give unexpected answers, and we suggest tha t two degrees of relevance are needed: weak and strong. The input to a supervised learning algorithm is a set of n training instances. Each instance X is an element of the set F\\ x F2 x \u2022 \u2022 \u2022 x Fm, where Fi is the domain of the i th feature. Training instances are tuples (X, Y) where Y is the label, or output . Given an instance, we denote the value of feature Xi by X{. The task of the induction algorithm is to induce a structure (e.g., a decision tree or a neural net) such tha t , given a new instance, it is possible to accurately predict the label Y. We assume a probability measure p on the space F\\ x F2 x \u2022 \u2022 \u2022 x Fm x Y. Our general discussion does not make any assumptions on the features or on the label; they can be discrete, continuous, linear, or structured, and the label may be single-valued or a multi-valued vector of arbitrary dimension. 2.1 E X I S T I N G D E F I N I T I O N S Almuallim and Dietterich (1991, p . 548) define relevance under the assumption tha t all features and the label are Boolean and tha t there is no noise. D e f i n i t i o n 1 A feature X{ is said to be re levant to a concept C if X{ appears in every Boolean formula that represents C and irrelevant otherwise. Gennari et al. (1989, Section 5.5) define relevance a s 1 xThe definition given is a formalization of their stateDefinition Relevant Irrelevant Definition 1 Xx X2,X3,X4,Xs Definition 2 None All Definition 3 All None Definition 4 X2,Xs,X4,X^ Table 1: Feature relevance for the Correlated XOR problem under the four definitions. Def in i t i on 2 Xi is relevant iff there exists some X{ and y for which p(X{ = X{) > 0 such that p ( Y = y \\ X i = xi)j:p(Y = y) . Under this definition, Xi is relevant if knowing its value can change the estimates for Y, or in other words, if Y is conditionally dependent of X{. Note tha t this definition fails to capture the relevance of features in the parity concept, and may be changed as follows. Let Si be the set of all features except Xi, i.e., Si = {Xi,..., X{-1, Xi+i,..., Xm }. Denote by Si a valueassignment to all features in Si. Def in i t i on 3 Xi is relevant iff there exists some Xi, y, and Si for which p(Xi = Xi) > 0 such that p(Y = y,Si = S i I Xi = *,.) ^ p(Y = y,Si = Si) . Under the following definition, Xi is relevant if the probability of the label (given all features) can change when we eliminate knowledge about the value of Xi. Def in i t i on 4 Xi is relevant iff there exists some Xi, y, and Si for which p(Xi = Si =$,-)>() such that p(Y = y\\ Xi = Xi,Si = Si)^p(Y = y\\Si = Si) The following example shows tha t all the definitions above give unexpected results. E x a m p l e 1 ( C o r r e l a t e d X O R ) Let features X\\,..., X*> be Boolean. The instance space is such that X2 and X3 are negatively correlated with X4 and X\u00a7, respectively, i.e., X4 = X2, X$ = X3. There are only eight possible instances, and we assume they are equiprobable. The (deterministic) target concept is y = I I \u00a9 I 2 (0 denotes XOR) . Note tha t the target concept has an equivalent Boolean expression, namely, Y = X\\ 0 I 4 . The features X3 and X5 are irrelevant in the strongest possible sense. Xi is indispensable, and one of X2, X4 can be disposed ment: \"Features are relevant if their values vary systematically with category membership.\" Irrelevant Features and the Subset Selection Problem 123", "title": "Irrelevant Features and the Subset Selection Problem"}, "f4ed7a2c1ce10b08ad338217743d737de64b056b": {"paper_id": "f4ed7a2c1ce10b08ad338217743d737de64b056b", "abstract": "Cervical cancer threatens the lives of many women in our world today. In 2014, the number of women infected with this disease in the United States was 12,578, of which 4,115 died, with a death rate of nearly 32%. Cancer data, including cervical cancer datasets, represent a significant challenge data mining techniques because absence of different costs for error cases. The proposed model present a cost sensitive classifiers that has three main stages; the first stage is prepressing the original data to prepare it for classification model which is build based on decision tree classifier with cost selectivity and finallyevaluation the proposed model based on many metrics in addition to apply a cross validation.The proposed model provides more accurate result in both binary class and multi class classification. It has a TP rate (0.429) comparing with (0.160) for typical decision tree in binary class task.", "title": "Enhanced Classification Model for Cervical Cancer Dataset based on Cost Sensitive Classifier Hayder"}, "9bd362017e2592eec65da61d758d2c5d55237706": {"paper_id": "9bd362017e2592eec65da61d758d2c5d55237706", "abstract": "We examined the hypothesis that threat alters the cognitive strategies used by high authoritarians in seeking out new political information from the environment. In a laboratory experiment, threat was manipulated through a \u201cmortality salience\u201d manipulation used in research on terror management theory (Pyszczynski, Solomon, & Greenberg, 2003). Subjects (N = 92) were then invited to read one of three editorial articles on the topic of capital punishment. We found that in the absence of threat, both low and high authoritarians were responsive to salient norms of evenhandedness in information selection, preferring exposure to a two-sided article that presents the merits of both sides of an issue to an article that selectively touts the benefits of the pro or con side of the issue. However, in the presence of threat, high but not low authoritarians became significantly more interested in exposure to an article containing uniformly pro-attitudinal arguments, and significantly less interested in a balanced, two-sided article. Finally, a path analysis indicated that selective exposure to attitude-congruent information led to more internally consistent policy attitudes and inhibited attitude change. Discussion focuses on the role of threat in conditioning the cognitive and attitudinal effects of authoritarianism.", "title": "Threat , Authoritarianism , and Selective Exposure to Information"}, "4a3b2851bfb946a314731433651c5de5bfa28630": {"paper_id": "4a3b2851bfb946a314731433651c5de5bfa28630", "abstract": "Analyzing political conservatism as motivated social cognition integrates theories of personality (authoritarianism, dogmatism-intolerance of ambiguity), epistemic and existential needs (for closure, regulatory focus, terror management), and ideological rationalization (social dominance, system justification). A meta-analysis (88 samples, 12 countries, 22,818 cases) confirms that several psychological variables predict political conservatism: death anxiety (weighted mean r = .50); system instability (.47); dogmatism-intolerance of ambiguity (.34); openness to experience (-.32); uncertainty tolerance (-.27); needs for order, structure, and closure (.26); integrative complexity (-.20); fear of threat and loss (.18); and self-esteem (-.09). The core ideology of conservatism stresses resistance to change and justification of inequality and is motivated by needs that vary situationally and dispositionally to manage uncertainty and threat.", "title": "Political conservatism as motivated social cognition."}, "60a488e29b5b64c44f6ce124bce7ced9602636d4": {"paper_id": "60a488e29b5b64c44f6ce124bce7ced9602636d4", "abstract": "Main memories are becoming sufficiently large that most OLTP databases can be stored entirely in main memory, but this may not be the best solution. OLTP workloads typically exhibit skewed access patterns where some records are hot (frequently accessed) but many records are cold (infrequently or never accessed). It is more economical to store the coldest records on secondary storage such as flash. As a first step towards managing cold data in databases optimized for main memory we investigate how to efficiently identify hot and cold data. We propose to log record accesses - possibly only a sample to reduce overhead - and perform offline analysis to estimate record access frequencies. We present four estimation algorithms based on exponential smoothing and experimentally evaluate their efficiency and accuracy. We find that exponential smoothing produces very accurate estimates, leading to higher hit rates than the best caching techniques. Our most efficient algorithm is able to analyze a log of 1B accesses in sub-second time on a workstation-class machine.", "title": "Identifying hot and cold data in main-memory databases"}, "0ad3358ffc0d5e44311160767cc0fb65ccd25b00": {"paper_id": "0ad3358ffc0d5e44311160767cc0fb65ccd25b00", "abstract": "Although LRU replacement policy has been commonly used in the buffer cache management, it is well known for its inability to cope with access patterns with weak locality. Previous work, such as LRU-K and 2Q, attempts to enhance LRU capacity by making use of additional history information of previous block references other than only the recency information used in LRU. These algorithms greatly increase complexity and/or can not consistently provide performance improvement. Many recently proposed policies, such as UBM and SEQ, improve replacement performance by exploiting access regularities in references. They only address LRU problems on certain specific and well-defined cases such as access patterns like sequences and loops. Motivated by the limits of previous studies, we propose an efficient buffer cache replacement policy, called Low Inter-reference Recency Set (LIRS). LIRS effectively addresses the limits of LRU by using recency to evaluate Inter-Reference Recency (IRR) for making a replacement decision. This is in contrast to what LRU does: directly using recency to predict next reference timing. At the same time, LIRS almost retains the same simple assumption of LRU to predict future access behavior of blocks. Our objectives are to effectively address the limits of LRU for a general purpose, to retain the low overhead merit of LRU, and to outperform those replacement policies relying on the access regularity detections. Conducting simulations with a variety of traces and a wide range of cache sizes, we show that LIRS significantly outperforms LRU, and outperforms other existing replacement algorithms in most cases. Furthermore, we show that the additional cost for implementing LIRS is trivial in comparison with LRU.", "title": "LIRS: an efficient low inter-reference recency set replacement policy to improve buffer cache performance"}, "252384bae49e1f2092e6a9553cd9a67f41134ded": {"paper_id": "252384bae49e1f2092e6a9553cd9a67f41134ded", "abstract": "Sam H. Nohs Sang Lyul Mint t Department of Computer Engineering Seoul National University Seoul 151-742, Korea http://ssrnet.snu.ac.kr http://archi.snu.ac.kr We show that there exists a spectrum of block replacement policies that subsumes both the Least Recently Used (LRU) and the Least Frequently Used (LFU) policies. The spectrum is formed according to how much more weight we give to the recent history than to the older history, and is referred to as the LRFU (Least Recently/Frequently Used) policy. Unlike many previous policies that use limited history to make block replacement decisions, the LRFU policy uses the complete reference history of blocks recorded during their cache residency. Nevertheless, the LRFU requires only a few words for each block to maintain such history. This paper also describes an implementation of the LRFU that again subsumes the LRU and LFU implementations. The LRFU policy is applied to buffer caching, and results from trace-driven simulations show that the LRFU performs better than previously known policies for the workloads we considered. This point is reinforced by results from our integration of the LRFU into the FreeBSD operating system.", "title": "On the Existence of a Spectrum of Policies that Subsumes the Least Recently Used (LRU) and Least Frequently Used (LFU) Policies"}, "4b4ee1ee9bbfd9527fba0bbd761bd61a59f96a48": {"paper_id": "4b4ee1ee9bbfd9527fba0bbd761bd61a59f96a48", "abstract": "This paper introduces a new approach to database disk buffering, called the LRU-K method. The basic idea of LRU-K is to keep track of the times of the last K references to popular database pages, using this information to statistically estimate the interarrival times of references on a page by page basis. Although the LRU-K approach performs optimal statistical inference under relatively standard assumptions, it is fairly simple and incurs little bookkeeping overhead. As we demonstrate with simulation experiments, the LRU-K algorithm surpasses conventional buffering algorithms in discriminating between frequently and infrequently referenced pages. In fact, LRU-K can approach the behavior of buffering algorithms in which page sets with known access frequencies are manually assigned to different buffer pools of specifically tuned sizes. Unlike such customized buffering algorithms however, the LRU-K method is self-tuning, and does not rely on external hints about workload characteristics. Furthermore, the LRU-K algorithm adapts in real time to changing patterns of access.", "title": "The LRU-K Page Replacement Algorithm For Database Disk Buffering"}, "5fa357b43c8351a5d8e7124429e538ad7d687abc": {"paper_id": "5fa357b43c8351a5d8e7124429e538ad7d687abc", "abstract": "In a path-breaking paper last year Pat and Betty O\u2019Neil and Gerhard Weikum pro posed a self-tuning improvement to the Least Recently Used (LRU) buffer management algorithm[l5]. Their improvement is called LRU/k and advocates giving priority to buffer pages baaed on the kth most recent access. (The standard LRU algorithm is denoted LRU/l according to this terminology.) If Pl\u2019s kth most recent access is more more recent than P2\u2019s, then Pl will be replaced after P2. Intuitively, LRU/k for k > 1 is a good strategy, because it gives low priority to pages that have been scanned or to pages that belong to a big randomly accessed file (e.g., the account file in TPC/A). They found that LRU/S achieves most of the advantage of their method. The one problem of LRU/S is the processor *Supported by U.S. Office of Naval Research #N00014-91-E 1472 and #N99914-92-J-1719, U.S. National Science Foundation grants #CC%9103953 and IFlI-9224691, and USBA #5555-19. Part of this work was performed while Theodore Johnson was a 1993 ASEE Summer Faculty Fellow at the National Space Science Data Center of NASA Goddard Space Flight Center. t Authors\u2019 e-mail addresses : ted@cis.ufi.edu and", "title": "2Q: A Low Overhead High Performance Buffer Management Replacement Algorithm"}, "e75c5d1b7ecd71cd9f1fdc3d07f56290517ef1e5": {"paper_id": "e75c5d1b7ecd71cd9f1fdc3d07f56290517ef1e5", "abstract": "The two areas of online transaction processing (OLTP) and online analytical processing (OLAP) present different challenges for database architectures. Currently, customers with high rates of mission-critical transactions have split their data into two separate systems, one database for OLTP and one so-called data warehouse for OLAP. While allowing for decent transaction rates, this separation has many disadvantages including data freshness issues due to the delay caused by only periodically initiating the Extract Transform Load-data staging and excessive resource consumption due to maintaining two separate information systems. We present an efficient hybrid system, called HyPer, that can handle both OLTP and OLAP simultaneously by using hardware-assisted replication mechanisms to maintain consistent snapshots of the transactional data. HyPer is a main-memory database system that guarantees the ACID properties of OLTP transactions and executes OLAP query sessions (multiple queries) on the same, arbitrarily current and consistent snapshot. The utilization of the processor-inherent support for virtual memory management (address translation, caching, copy on update) yields both at the same time: unprecedentedly high transaction rates as high as 100000 per second and very fast OLAP query response times on a single system executing both workloads in parallel. The performance analysis is based on a combined TPC-C and TPC-H benchmark.", "title": "HyPer: A hybrid OLTP&OLAP main memory database system based on virtual memory snapshots"}, "5d3158674e1a0fedf69299a905151949fb8b01a5": {"paper_id": "5d3158674e1a0fedf69299a905151949fb8b01a5", "abstract": "RDF is a data model for schema-free structured information that is gaining momentum in the context of Semantic-Web data, life sciences, and also Web 2.0 platforms. The \u201cpay-as-you-go\u201d nature of RDF and the flexible pattern-matching capabilities of its query language SPARQL entail efficiency and scalability challenges for complex queries including long join paths. This paper presents the RDF-3X engine, an implementation of SPARQL that achieves excellent performance by pursuing a RISC-style architecture with streamlined indexing and query processing. The physical design is identical for all RDF-3X databases regardless of their workloads, and completely eliminates the need for index tuning by exhaustive indexes for all permutations of subject-property-object triples and their binary and unary projections. These indexes are highly compressed, and the query processor can aggressively leverage fast merge joins with excellent performance of processor caches. The query optimizer is able to choose optimal join orders even for complex queries, with a cost model that includes statistical synopses for entire join paths. Although RDF-3X is optimized for queries, it also provides good support for efficient online updates by means of a staging architecture: direct updates to the main database indexes are deferred, and instead applied to compact differential indexes which are later merged into the main indexes in a batched manner. Experimental studies with several large-scale datasets with more than 50 million RDF triples and benchmark queries that include pattern matching, manyway star-joins, and long path-joins demonstrate that RDF-3X can outperform the previously best alternatives by one or two orders of magnitude.", "title": "The RDF-3X engine for scalable management of RDF data"}, "43856bfd02a6eb82791e6beed21cba59eea273ae": {"paper_id": "43856bfd02a6eb82791e6beed21cba59eea273ae", "abstract": "With the availability of very large, relatively inexpensive main memories, it is becoming possible keep large databases resident in main memory In this paper we consider the changes necessary to permit a relational database system to take advantage of large amounts of main memory We evaluate AVL vs B+-tree access methods for main memory databases, hash-based query processing strategies vs sort-merge, and study recovery issues when most or all of the database fits in main memory As expected, B+-trees are the preferred storage mechanism unless more than 80--90% of the database fits in main memory A somewhat surprising result is that hash based query processing strategies are advantageous for large memory situations", "title": "Implementation Techniques for Main Memory Database Systems"}, "73b6c5d9db8fc9b862b4c9d00c96c627df365812": {"paper_id": "73b6c5d9db8fc9b862b4c9d00c96c627df365812", "abstract": "Database technology is one of the cornerstones for the new millennium\u2019s IT landscape. However, database systems as a unit of code packaging and deployment are at a crossroad: commercial systems have been adding features for a long time and have now reached complexity that makes them a difficult choice, in terms of their \"gain/pain ratio\", as a central platform for value-added information services such as ERP or e-commerce. It is critical that database systems be easy to manage, predictable in their performance characteristics, and ultimately self-tuning. For this elusive goal, RISC-style simplification of server functionality and interfaces is absolutely crucial. We suggest a radical architectural departure in which database technology is packaged into much smaller RISC-style data managers with lean, specialized APIs, and with built-in self-assessment and auto-tuning capabilities 1. The Need for a New Departure Database technology has an extremely successful track record as a backbone of information technology (IT) throughout the last three decades. High-level declarative query languages like SQL and atomic transactions are key assets in the cost-effective development and maintenance of information systems. Furthermore, database technology continues to play a major role in the trends of our modern cyberspace society with applications ranging from webbased applications/services, and digital libraries to information mining on business as well as scientific data. Thus, database technology has impressively proven its benefits and seems to remain crucially relevant in the new millennium as well. Success is a lousy teacher (to paraphrase Bill Gates), and therefore we should not conclude that the database system, as the unit of engineering, deploying, and operating packaged database technology, is in good shape. A closer look at some important application areas and major trends in the software industry strongly indicates that database systems have an overly low \u201cgain/pain ratio\u201d. First, with the dramatic drop of hardware and software prices, the expenses due to human administration and tuning staff dominate the cost of ownership for a database system. The complexity and cost of these feed-and-care tasks is likely to prohibit database systems from further playing their traditionally prominent role in the future IT infrastructure. Next, database technology is more likely to be adopted in unbundled and dispersed form within higher-level application services. Both of the above problems stem from packaging all database technology into a single unit of development, maintenance, deployment, and operation. We argue that this architecture is no longer appropriate for the new age of cyberspace applications. The alternative approach that we envision and advocate in this paper is to provide RISC-style, functionally restricted, specialized data managers that have a narrow interface as well as a smaller footprint and are more amenable to automatic tuning. The rest of the paper is organized as follows. Section 2 puts together some important observations indicating that database systems in their traditional form are in crisis. Section 3 briefly reviews earlier attempts for a new architectural departure along the lines of the current paper, and discusses why they did not catch on. Section 4 outlines the envisioned architecture with emphasis on RISC-style simplification of data-management components and consequences for the viability of autotuning. Section 5 outlines a possible research agenda towards our vision. 2. Crisis Indicators To begin our analysis, let us put together a few important observations on how database systems are perceived by customers, vendors, and the research community. Observation 1: Featurism drives products beyond manageability. Database systems offer more and more features, leading to extremely broad and thus complex Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Very Large Data Base Endowment. To copy otherwise, or to republish, requires a fee and/or special permission from the Endowment. Proceedings of the 26th International Conference on Very Large Databases, Cairo, Egypt, 2000 interfaces. Quite often novel features are more a marketing issue rather than a real application need or technological advance; for example, a database system vendor may decide to support a fancy type of join or spatial index in the next product release because the major competitors have already announced this feature. As a result, database systems become overloaded with functionality, increasing the complexity of maintaining the system\u2019s code base as well as installing and managing the system. The irony of this trend lies in the fact that each individual customer (e.g., a small enterprise) only makes use of a tiny fraction of the system\u2019s features and many high-end features are hardly ever exercised at all. Observation 2: SQL is painful. A big headache that comes with a database system is the SQL language. It is the union of all conceivable features (many of which are rarely used or should be discouraged to use anyway) and is way too complex for the typical application developer. Its core, say selection-projection-join queries and aggregation, is extremely useful, but we doubt that there is wide and wise use of all the bells and whistles. Understanding semantics of SQL (not even of SQL-92), covering all combinations of nested (and correlated) subqueries, null values, triggers, ADT functions, etc. is a nightmare. Teaching SQL typically focuses on the core, and leaves the featurism as a \u201clearning-on-the-job\u201d life experience. Some trade magazines occasionally pose SQL quizzes where the challenge is to express a complicated information request in a single SQL statement. Those statements run over several pages, and are hardly comprehensible. When programmers adopt this style in real applications and given the inherent difficulty of debugging a very high-level \u201cdeclarative\u201d statement, it is extremely hard if not impossible to gain high confidence that the query is correct in capturing the users\u2019 information needs. In fact, good SQL programming in many cases decomposes complex requests into a sequence of simpler SQL statements. Observation 3: Performance is unpredictable. Commercial database engines are among the most sophisticated pieces of software that have ever been built in the history of computer technology. Furthermore, as product releases have been driven by the time-to-market pressure for quite a few years, these systems have little leeway for redesigning major components so that adding features and enhancements usually increases the code size and complexity and, ultimately, the general \u201csoftware entropy\u201d of the system. The scary consequence is that database systems become inherently unpredictable in their exact behavior and, especially, performance. Individual components like query optimizers may already have crossed the critical complexity barrier. There is probably no single person in the world who fully understands all subtleties of the complex interplay of rewrite rules, approximate cost models, and search-space traversal heuristics that underlie the optimization of complex queries. Contrast this dilemma with the emerging need for performance and service quality guarantees in ecommerce, digital libraries, and other Internet applications. The PTAC report has rightly emphasized: \u201cour ability to analyze and predict the performance of the enormously complex software systems that lie at the core of our economy is painfully inadequate\u201d [18]. Observation 4: Tuning is a nightmare and auto-tuning is wishful thinking at this stage. The wide diversity of applications for a given database system makes it impossible to provide universally good performance by solely having a well-engineered product. Rather all commercial database systems offer a variety of \u201ctuning knobs\u201d that allow the customer to adjust certain system parameters to the specific workload characteristics of the application. These knobs include index selection, data placement across parallel disks, and other aspects of physical database design, query optimizer hints, thresholds that govern the partitioning of memory or multiprogramming level in a multi-user environment. Reasonable settings for such critical parameters for a complex application often depend on the expertise and experience of highly skilled tuning gurus and/or timeconsuming trial-and-error experimentation; both ways are expensive and tend to dominate the cost of ownership for a database system. \u201cAuto-tuning\u201d capabilities and \u201czeroadmin\u201d systems have been put on the research and development agenda as high priority topics for several years (see, e.g., [2]), but despite some advances on individual issues (e.g., [4,7,8,10,24]) progress on the big picture of self-tuning system architectures is slow and a breakthrough is not nearly in sight. Although commercial systems have admittedly improved on ease of use, many tuning knobs are merely disguised by introducing internal thresholds that still have to be carefully considered, e.g., at packaging or installation time to take into account the specific resources and the application environment. In our experience, robust, universally working default settings for complex tuning knobs are wishful thinking. Despite the common myth is that a few rules of thumb could be sufficient for most tuning concerns, with complex, highly diverse workloads whose characteristics evolve over time it is quite a nightmare to find appropriate settings for physical design and the various run-time parameters of a database server to ensure at least decent performance. Observation 5: We are not alone in the universe. Database systems are not (or no long", "title": "Rethinking Database System Architecture: Towards a Self-Tuning RISC-Style Database System"}, "ed77247681d7059db42996231bddb09cfa69c440": {"paper_id": "ed77247681d7059db42996231bddb09cfa69c440", "abstract": "Sequentiality of access is an inherent characteristic of many database systems. We use this observation to develop an algorithm which selectively prefetches data blocks ahead of the point of reference. The number of blocks prefetched is chosen by using the empirical run length distribution and conditioning on the observed number of sequential block references immediately preceding reference to the current block. The optimal number of blocks to prefetch is estimated as a function of a number of \u201ccosts,\u201d including the cost of accessing a block not resident in the buffer (a miss), the cost of fetching additional data blocks at fault times, and the cost of fetching blocks that are never referenced. We estimate this latter cost, described as memory pollution, in two ways. We consider the treatment (in the replacement algorithm) of prefetched blocks, whether they are treated as referenced or not, and find that it makes very little difference. Trace data taken from an operational IMS database system is analyzed and the results are presented. We show how to determine optimal block sizes. We find that anticipatory fetching of data can lead to significant improvements in system operation.", "title": "Sequentiality and Prefetching in Database Systems"}, "5046a718f92447642939f5c93414dc97225d726a": {"paper_id": "5046a718f92447642939f5c93414dc97225d726a", "abstract": "In this paper, we describe a main memory hybrid database system called HYRISE, which automatically partitions tables into vertical partitions of varying widths depending on how the columns of the table are accessed. For columns accessed as a part of analytical queries (e.g., via sequential scans), narrow partitions perform better, because, when scanning a single column, cache locality is improved if the values of that column are stored contiguously. In contrast, for columns accessed as a part of OLTP-style queries, wider partitions perform better, because such transactions frequently insert, delete, update, or access many of the fields of a row, and co-locating those fields leads to better cache locality. Using a highly accurate model of cache misses, HYRISE is able to predict the performance of different partitionings, and to automatically select the best partitioning using an automated database design algorithm. We show that, on a realistic workload derived from customer applications, HYRISE can achieve a 20% to 400% performance improvement over pure all-column or all-row designs, and that it is both more scalable and produces better designs than previous vertical partitioning approaches for main memory systems.", "title": "HYRISE - A Main Memory Hybrid Storage Engine"}, "347920406c9a9a3846adf485e2b864d4523a0652": {"paper_id": "347920406c9a9a3846adf485e2b864d4523a0652", "abstract": "In addition to indexes and materialized views, horizontal and vertical partitioning are important aspects of physical design in a relational database system that significantly impact performance. Horizontal partitioning also provides manageability; database administrators often require indexes and their underlying tables partitioned identically so as to make common operations such as backup/restore easier. While partitioning is important, incorporating partitioning makes the problem of automating physical design much harder since: (a) The choices of partitioning can strongly interact with choices of indexes and materialized views. (b) A large new space of physical design alternatives must be considered. (c) Manageability requirements impose a new constraint on the problem. In this paper, we present novel techniques for designing a scalable solution to this integrated physical design problem that takes both performance and manageability into account. We have implemented our techniques and evaluated it on Microsoft SQL Server. Our experiments highlight: (a) the importance of taking an integrated approach to automated physical design and (b) the scalability of our techniques.", "title": "Integrating Vertical and Horizontal Partitioning Into Automated Physical Database Design"}, "174148018456e391ee06adc21ea0535c825e8df3": {"paper_id": "174148018456e391ee06adc21ea0535c825e8df3", "abstract": "When SQL and the relational data model were introduced 25 years ago as a general data management concept, enterprise software migrated quickly to this new technology. It is fair to say that SQL and the various implementations of RDBMSs became the backbone of enterprise systems. In those days. we believed that business planning, transaction processing and analytics should reside in one single system. Despite the incredible improvements in computer hardware, high-speed networks, display devices and the associated software, speed and flexibility remained an issue.\n The nature of RDBMSs, being organized along rows, prohibited us from providing instant analytical insight and finally led to the introduction of so-called data warehouses. This paper will question some of the fundamentals of the OLAP and OLTP separation. Based on the analysis of real customer environments and experience in some prototype implementations, a new proposal for an enterprise data management concept will be presented.\n In our proposal, the participants in enterprise applications, customers, orders, accounting documents, products, employees etc. will be modeled as objects and also stored and maintained as such. Despite that, the vast majority of business functions will operate on an in memory representation of their objects. Using the relational algebra and a column-based organization of data storage will allow us to revolutionize transactional applications while providing an optimal platform for analytical data processing. The unification of OLTP and OLAP workloads on a shared architecture and the reintegration of planning activities promise significant gains in application development while simplifying enterprise systems drastically.\n The latest trends in computer technology -- e.g. blade architecture, multiple CPUs per blade with multiple cores per CPU allow for a significant parallelization of application processes. The organization of data in columns supports the parallel use of cores for filtering and aggregation. Elements of application logic can be implemented as highly efficient stored procedures operating on columns. The vast increase in main memory combined with improvements in L1--, L2--, L3--caching, together with the high data compression rate column storage will allow us to support substantial data volumes on one single blade. Distributing data across multiple blades using a shared nothing approach provides further scalability.", "title": "A common database approach for OLTP and OLAP using an in-memory column database"}, "2f11a5686253b2089dcaef773ee508f8ef63ab42": {"paper_id": "2f11a5686253b2089dcaef773ee508f8ef63ab42", "abstract": "Replicated Softmax model, a well-known undirected topic model, is powerful in extracting semantic representations of documents. Traditional learning strategies such as Contrastive Divergence are very inefficient. This paper provides a novel estimator to speed up the learning based on Noise Contrastive Estimate, extended for documents of variant lengths and weighted inputs. Experiments on two benchmarks show that the new estimator achieves great learning efficiency and high accuracy on document retrieval and classification.", "title": "Efficient Learning for Undirected Topic Models"}, "44c977c18752d8913746efc7ea8635b0e4be4e47": {"paper_id": "44c977c18752d8913746efc7ea8635b0e4be4e47", "abstract": null, "title": "A Practical Guide to Training Restricted Boltzmann Machines"}, "1ad4d974e4732a9e0a3c857eb182275fb296e62d": {"paper_id": "1ad4d974e4732a9e0a3c857eb182275fb296e62d", "abstract": "The uniformity of the cortical architecture and the ability of functions to move to different areas of cortex following early damage strongly suggest that there is a single basic learning algorithm for extracting underlying structure from richly structured, high-dimensional sensory data. There have been many attempts to design such an algorithm, but until recently they all suffered from serious computational weaknesses. This chapter describes several of the proposed algorithms and shows how they can be combined to produce hybrid methods that work efficiently in networks with many layers and millions of adaptive connections.", "title": "To recognize shapes, first learn to generate images."}, "4fdd812505a362c6e7e6b1857f1d9be699d1b112": {"paper_id": "4fdd812505a362c6e7e6b1857f1d9be699d1b112", "abstract": "We introduce a new type of top-level model for Deep Belief Nets and evaluate it on a 3D object recognition task. The top-level model is a third-order Boltzmann machine, trained using a hybrid algorithm that combines both generative and discriminative gradients. Performance is evaluated on the NORB database (normalized-uniform version), which contains stereo-pair images of objects under different lighting conditions and viewpoints. Our model achieves 6.5% error on the test set, which is close to the best published result for NORB (5.9%) using a convolutional neural net that has built-in knowledge of translation invariance. It substantially outperforms shallow models such as SVMs (11.6%). DBNs are especially suited for semi-supervised learning, and to demonstrate this we consider a modified version of the NORB recognition task in which additional unlabeled images are created by applying small translations to the images in the database. With the extra unlabeled data (and the same amount of labeled data as before), our model achieves 5.2% error.", "title": "3D Object Recognition with Deep Belief Nets"}, "9cb3def8283837cd223ce4cdd7293fc57f365542": {"paper_id": "9cb3def8283837cd223ce4cdd7293fc57f365542", "abstract": "Hidden Markov Models (HMMs) have been the state-of-the-art techniques for acoustic modeling despite their unrealistic independence assumptions and the very limited representational capacity of their hidden states. There are many proposals in the research community for deeper models that are capable of modeling the many types of variability present in the speech generation p r cess. Deep Belief Networks (DBNs) have recently proved to be very effective fo r a variety of machine learning problems and this paper applies DBNs to acous ti modeling. On the standard TIMIT corpus, DBNs consistently outperform ot her techniques and the best DBN achieves a phone error rate (PER) of 23.0% on the T IMIT core test set.", "title": "Deep Belief Networks for phone recognition"}, "643d6bb5d80c833da51fdd941d836be44f27c19b": {"paper_id": "643d6bb5d80c833da51fdd941d836be44f27c19b", "abstract": "A new reference collection of patent documents for training and testing automated categorization systems is established and described in detail. This collection is tailored for automating the attribution of international patent classification codes to patent applications and is made publicly available for future research work. We report the results of applying a variety of machine learning algorithms to the automated categorization of English-language patent documents. This procedure involves a complex hierarchical taxonomy, within which we classify documents into 114 classes and 451 subclasses. Several measures of categorization success are described and evaluated. We investigate how best to resolve the training problems related to the attribution of multiple classification codes to each patent document.", "title": "Automated categorization in the international patent classification"}, "04b58bb061196d566acb2985659445f7a5985be2": {"paper_id": "04b58bb061196d566acb2985659445f7a5985be2", "abstract": "This article describes the implementation of a system that is able to organize vast document collections according to textual similarities. It is based on the self-organizing map (SOM) algorithm. As the feature vectors for the documents statistical representations of their vocabularies are used. The main goal in our work has been to scale up the SOM algorithm to be able to deal with large amounts of high-dimensional data. In a practical experiment we mapped 6,840,568 patent abstracts onto a 1,002,240-node SOM. As the feature vectors we used 500-dimensional vectors of stochastic figures obtained as random projections of weighted word histograms.", "title": "Self organization of a massive document collection"}, "92a29cfad137a3a9e4f21efe6c4d3fd2c4bcaaec": {"paper_id": "92a29cfad137a3a9e4f21efe6c4d3fd2c4bcaaec", "abstract": "The Self-Organizing Map (SOM) forms a nonlinear projection from a high-dimensional data manifold onto a low-dimensional grid. A representative model of some subset of data is associated with each grid point. The SOM algorithm computes an optimal collection of models that approximates the data in the sense of some error criterion and also takes into account the similarity relations of the models. The models then become ordered on the grid according to their similarity. When the SOM is used for the exploration of statistical data, the data vectors can be approximated by models of the same dimensionality. When mapping documents, one can represent them statistically by their word frequency histograms or some reduced representations of the histograms that can be regarded as data vectors. We have made SOMs of collections of over one million documents. Each document is mapped onto some grid point, with a link from this point to the document database. The documents are ordered on the grid according to their contents and neighboring documents can be browsed readily. Keywords or key texts can be used to search for the most relevant documents rst. New eeective coding and computing schemes of the mapping are described.", "title": "Self-Organization of Very Large Document Collections: State of the Art"}, "38991e1d9cc9286ff3c60e6978276451c1d18e54": {"paper_id": "38991e1d9cc9286ff3c60e6978276451c1d18e54", "abstract": "We derive a family of risk-sensitive reinforcement learning methods for agents, who face sequential decision-making tasks in uncertain environments. By applying a utility function to the temporal difference (TD) error, nonlinear transformations are effectively applied not only to the received rewards but also to the true transition probabilities of the underlying Markov decision process. When appropriate utility functions are chosen, the agents\u2019 behaviors express key features of human behavior as predicted by prospect theory (Kahneman & Tversky, 1979), for example, different risk preferences for gains and losses, as well as the shape of subjective probability curves. We derive a risk-sensitive Q-learning algorithm, which is necessary for modeling human behavior when transition probabilities are unknown, and prove its convergence. As a proof of principle for the applicability of the new framework, we apply it to quantify human behavior in a sequential investment task. We find that the risk-sensitive variant provides a significantly better fit to the behavioral data and that it leads to an interpretation of the subject's responses that is indeed consistent with prospect theory. The analysis of simultaneously measured fMRI signals shows a significant correlation of the risk-sensitive TD error with BOLD signal change in the ventral striatum. In addition we find a significant correlation of the risk-sensitive Q-values with neural activity in the striatum, cingulate cortex, and insula that is not present if standard Q-values are used.", "title": "Risk-Sensitive Reinforcement Learning"}, "688b9f4c9c0374a4b5eb0eb92e491357c46f15b4": {"paper_id": "688b9f4c9c0374a4b5eb0eb92e491357c46f15b4", "abstract": "We develop a new version of prospect theory that employs cumulative rather than separable decision weights and extends the theory in several respects. This version, called cumulative prospect theory, applies to uncertain as well as to risky prospects with any number of outcomes, and it allows different weighting functions for gains and for losses. Two principles, diminishing sensitivity and loss aversion, are invoked to explain the characteristic curvature of the value function and the weighting functions. A review of the experimental evidence and the results of a new experiment confirm a distinctive fourfold pattern of risk attitudes: risk aversion for gains and risk seeking for losses of high probability; risk seeking for gains and risk aversion for losses of low probability. Expected utility theory reigned for several decades as the dominant normative and descriptive model of decision making under uncertainty, but it has come under serious question in recent years. There is now general agreement that the theory does not provide an adequate description of individual choice: a substantial body of evidence shows that decision makers systematically violate its basic tenets. Many alternative models have been proposed in response to this empirical challenge (for reviews, see Camerer, 1989; Fishburn, 1988; Machina, 1987). Some time ago we presented a model of choice, called prospect theory, which explained the major violations of expected utility theory in choices between risky prospects with a small number of outcomes (Kahneman and Tversky, 1979; Tversky and Kahneman, 1986). The key elements of this theory are 1) a value function that is concave for gains, convex for losses, and steeper for losses than for gains, *An earlier version of this article was entitled \"Cumulative Prospect Theory: An Analysis of Decision under Uncertainty.\" This article has benefited from discussions with Colin Camerer, Chew Soo-Hong, David Freedman, and David H. Krantz. We are especially grateful to Peter P. Wakker for his invaluable input and contribution to the axiomatic analysis. We are indebted to Richard Gonzalez and Amy Hayes for running the experiment and analyzing the data. This work was supported by Grants 89-0064 and 88-0206 from the Air Force Office of Scientific Research, by Grant SES-9109535 from the National Science Foundation, and by the Sloan Foundation. 298 AMOS TVERSKY/DANIEL KAHNEMAN and 2) a nonlinear transformation of the probability scale, which overweights small probabilities and underweights moderate and high probabilities. In an important later development, several authors (Quiggin, 1982; Schmeidler, 1989; Yaari, 1987; Weymark, 1981) have advanced a new representation, called the rank-dependent or the cumulative functional, that transforms cumulative rather than individual probabilities. This article presents a new version of prospect theory that incorporates the cumulative functional and extends the theory to uncertain as well to risky prospects with any number of outcomes. The resulting model, called cumulative prospect theory, combines some of the attractive features of both developments (see also Luce and Fishburn, 1991). It gives rise to different evaluations of gains and losses, which are not distinguished in the standard cumulative model, and it provides a unified treatment of both risk and uncertainty. To set the stage for the present development, we first list five major phenomena of choice, which violate the standard model and set a minimal challenge that must be met by any adequate descriptive theory of choice. All these findings have been confirmed in a number of experiments, with both real and hypothetical payoffs. Framing effects. The rational theory of choice assumes description invariance: equivalent formulations of a choice problem should give rise to the same preference order (Arrow, 1982). Contrary to this assumption, there is much evidence that variations in the framing of options (e.g., in terms of gains or losses) yield systematically different preferences (Tversky and Kahneman, 1986). Nonlinear preferences. According to the expectation principle, the utility of a risky prospect is linear in outcome probabilities. Allais's (1953) famous example challenged this principle by showing that the difference between probabilities of .99 and 1.00 has more impact on preferences than the difference between 0.10 and 0.11. More recent studies observed nonlinear preferences in choices that do not involve sure things (Camerer and Ho, 1991). Source dependence. People's willingness to bet on an uncertain event depends not only on the degree of uncertainty but also on its source. Ellsberg (1961) observed that people prefer to bet on an urn containing equal numbers of red and green balls, rather than on an urn that contains red and green balls in unknown proportions. More recent evidence indicates that people often prefer a bet on an event in their area of competence over a bet on a matched chance event, although the former probability is vague and the latter is clear (Heath and Tversky, 1991). Risk seeking. Risk aversion is generally assumed in economic analyses of decision under uncertainty. However, risk-seeking choices are consistently observed in two classes of decision problems. First, people often prefer a small probability of winning a large prize over the expected value of that prospect. Second, risk seeking is prevalent when people must choose between a sure loss and a substantial probability of a larger loss. Loss' aversion. One of the basic phenomena of choice under both risk and uncertainty is that losses loom larger than gains (Kahneman and Tversky, 1984; Tversky and Kahneman, 1991). The observed asymmetry between gains and losses is far too extreme to be explained by income effects or by decreasing risk aversion. ADVANCES IN PROSPECT THEORY 299 The present development explains loss aversion, risk seeking, and nonlinear preferences in terms of the value and the weighting functions. It incorporates a framing process, and it can accommodate source preferences. Additional phenomena that lie beyond the scope of the theory--and of its alternatives--are discussed later. The present article is organized as follows. Section 1.1 introduces the (two-part) cumulative functional; section 1.2 discusses relations to previous work; and section 1.3 describes the qualitative properties of the value and the weighting functions. These properties are tested in an extensive study of individual choice, described in section 2, which also addresses the question of monetary incentives. Implications and limitations of the theory are discussed in section 3. An axiomatic analysis of cumulative prospect theory is presented in the appendix.", "title": "Advances in Prospect Theory : Cumulative Representation of Uncertainty"}, "57b199e1d22752c385c34191c1058bcabb850d9f": {"paper_id": "57b199e1d22752c385c34191c1058bcabb850d9f", "abstract": "Recent neurophysiological studies reveal that neurons in certain brain structures carry specific signals about past and future rewards. Dopamine neurons display a short-latency, phasic reward signal indicating the difference between actual and predicted rewards. The signal is useful for enhancing neuronal processing and learning behavioral reactions. It is distinctly different from dopamine's tonic enabling of numerous behavioral processes. Neurons in the striatum, frontal cortex, and amygdala also process reward information but provide more differentiated information for identifying and anticipating rewards and organizing goal-directed behavior. The different reward signals have complementary functions, and the optimal use of rewards in voluntary behavior would benefit from interactions between the signals. Addictive psychostimulant drugs may exert their action by amplifying the dopamine reward signal.", "title": "Getting Formal with Dopamine and Reward"}, "bd0d47398cfa42c338c3445f4277b33b222b44ed": {"paper_id": "bd0d47398cfa42c338c3445f4277b33b222b44ed", "abstract": "The research and development costs of 106 randomly selected new drugs were obtained from a survey of 10 pharmaceutical firms. These data were used to estimate the average pre-tax cost of new drug and biologics development. The costs of compounds abandoned during testing were linked to the costs of compounds that obtained marketing approval. The estimated average out-of-pocket cost per approved new compound is $1395 million (2013 dollars). Capitalizing out-of-pocket costs to the point of marketing approval at a real discount rate of 10.5% yields a total pre-approval cost estimate of $2558 million (2013 dollars). When compared to the results of the previous study in this series, total capitalized costs were shown to have increased at an annual rate of 8.5% above general price inflation. Adding an estimate of post-approval R&D costs increases the cost estimate to $2870 million (2013 dollars).", "title": "Innovation in the pharmaceutical industry: New estimates of R&D costs."}, "2f8af00a3d8746c0a10c40bd7e11d9bee927cd41": {"paper_id": "2f8af00a3d8746c0a10c40bd7e11d9bee927cd41", "abstract": "where he holds the chair in telecommunications. He has co-authored 10 books on mobile radio communications, published about 400 research papers, organised and chaired conference sessions, presented overview lectures and has been awarded a number of distinctions. Currently he heads an academic research team, working on a range of research projects in the field of wireless multimedia communications sponsored by industry, the Engineering and Physical Sciences Research Council (EPSRC) UK, the European IST Programme and the Mobile Virtual Centre of Excellence (VCE), UK. He is an enthusiastic supporter of industrial and academic liaison and he offers a range of industrial courses. He is also an IEEE Distinguished Lecturer. For further information on research in progress and associated publications please refer to http://www-where he was involved in developing the KoreaSat monitoring system, Digital DBS transmission system and W-CDMA based Wireless Local Loop (WLL) system. He was awarded the PhD degree in Mobile Communications at the University of Southampton, UK, where he was as a postdoctoral research assistant from Sep. 2001 to Aug. 2002. He is a recipient of the British Chevening Scholarship awarded by the British Council, UK. His current research interests are associated with mobile communication systems design with empasis on adaptive modulation aided OFDM, MC-CDMA and W-CDMA. Southampton, where he completed his PhD in mobile communications. His areas of interest include adaptive OFDM transmission, wideband channel estimation, CDMA and error correction coding. He recently joined Ubinetics, Cambridge, UK, where he is involved in the research vii viii and development of third-genertion wireless systems. Dr. Keller co-authored two monographs and about 30 various research papers.", "title": "OFDM and MC-CDMA for Broadband Multi-user Communications"}, "54ac5f0a131086239cce5bc9784e185862b19d77": {"paper_id": "54ac5f0a131086239cce5bc9784e185862b19d77", "abstract": "Orthogonal frequency-division multiplexing (OFDM) modulation is a promising technique for achieving the high bit rates required for a wireless multimedia service. Without channel estimation and tracking, OFDM systems have to use differential phase-shift keying (DPSK), which has a 3-dB signalto-noise ratio (SNR) loss compared with coherent phase-shift keying (PSK). To improve the performance of OFDM systems by using coherent PSK, we investigate robust channel estimation for OFDM systems. We derive a minimum mean-square-error (MMSE) channel estimator, which makes full use of the timeand frequency-domain correlations of the frequency response of time-varying dispersive fading channels. Since the channel statistics are usually unknown, we also analyze the mismatch of the estimator-to-channel statistics and propose a robust channel estimator that is insensitive to the channel statistics. The robust channel estimator can significantly improve the performance of OFDM systems in a rapid dispersive fading channel.", "title": "Robust channel estimation for OFDM systems with rapid dispersive fading channels"}, "3130a53f7fc8167895d8f16ff45f505e69cbb6ff": {"paper_id": "3130a53f7fc8167895d8f16ff45f505e69cbb6ff", "abstract": "This paper discusses the analysis and simulation of a technique for combating the effects of multipath propagation and cochannel interference on a narrow-band digital mobile channel. This system uses the discrete Fourier transform to orthogonally frequency multiplex many narrow subchannels, each signaling at a very low rate, into one high-rate channel. When this technique is used with pilot-based correction, the effects of flat Rayleigh fading can be reduced significantly. An improvement in signal-to-interference ratio of 6 dB can be obtained over the bursty Rayleigh channel. In additim, with each subchannel signaling at a low rate, this technique can provide added protection against delay spread. To enhance the behavior of the technique in a heavily frequency-selective environment, interpolated pilots are used. A frequency offset reference scheme is employed for the pilots to improve protection against cochannel interference.", "title": "Analysis and Simulation of a Digital Mobile Channel Using Orthogonal Frequency Division Multiplexing"}, "87b42af7c89cdf1886881eb8045ec862b37cfc46": {"paper_id": "87b42af7c89cdf1886881eb8045ec862b37cfc46", "abstract": "The use of multi-amplitude signaling schemes in wireless OFDM systems requires the tracking of the fading radio channel. This paper addresses channel estimation based on time-domain channel statistics. Using a general model for a slowly fading channel, we present the MMSE and LS estimators and a method for modifications compromising between complexity and performance. The symbol error rate for a 16-QAM system is presented by means of simulation results. Depending upon estimator complexity, up to 4 dB in SNR can be gained over the LS estimator. I . INTRODUCTION Currently, orthogonal frequency-division multiplexing (OFDM) systems [l] are subject t o significant investigation. Since this technique has been adopted in the European digital audio broadcasting (DAB) system 121, OFDM signaling in fading channel environments has gained a broad interest. For instance, its applicability to digital TV broadcasting is currently being investigated [3]. The use of differential phase-shzji! keying (DPSK) in OFDM systems avoids the tracking of a time varying channel. However, this will limit the number of bits per symbol and results in a 3 dB loss in signal-to-noise ratio (SNR) [4]. If the receiver contains a channel estimator, multiamplitude signaling schemes can be used. In [5] and [6], 16-QAM modulation in an OFDM system has been investigated. A decision-directed channeltracking method, which allows the use of multi-amplitude schemes in a slow Rayleigh-fading environment is analysed in [ 5 ] . In the design of wireless OFDM systems, the channel is usually assumed to have a finite-length impulse response. A cyclic extension, longer than this impulse response, is put between consecutive blocks in order to avoid interblock interference and preserve orthogonality of the tones [7 ] . Generally, the OFDM system is designed so that the cyclic extension is a small percentage of the total symbol length. This paper discusses channel estimation techniques in wireless OFDM systems, that use this property of the channel impulse response. Hoeher [6] and Cioffi [8] have also addressed this property. In Section 11, we describe the system model. Section I11 discusses the minimum mean-square error (MMSE) and least-squares (LS) channel estimators. The MMSE estimator has good performance but high complexity. The LS estimator has low complexity, but its performance is not as good as that of the MMSE estimator. We present modifications to both MMSE and LS estimators that use the assumption of a finite length impulse response. In Section IV we evaluate the estimators by simulating a 16-QAM signaling scheme. The performance is presented both in terms of mean-square error (MSE) and symbol error rate (SER). 11. SYSTEM DESCRIPTION We will consider the system shown in Fig. 1, where zk: are the transmitted symbols, g ( t ) is the channel impulse response, E ( t ) is the white complex Gaussian channel noise and yk are the received symbols. The transmitted symbols x k are taken from a multi-amplitude signal constellation. The D/A and A/D converters contain ideal low-pass filters with bandwidth l/Ts, where T, is the sampling interval. A cyclic extension of time length TG (not shown in Fig. I. for reasons of simplicity) is used to eliminate inter-block interference and preserve the orthogonality of the tones. We treat the channel impulse response g(t) as a timelimited pulse train of the form g( t ) = ams(t TmTs), (1) m where the amplitudes a, are complex valued and 0 5 T,T, 5 TG, i.e., the entire impulse response lies inside Fig. 1: Base-band OFDM system 0-7803-2742-XI95 $4.00", "title": "On channel estimation in OFDM systems"}, "e5e7b1d5b64d53e9830c9d436f96846cc082c2c3": {"paper_id": "e5e7b1d5b64d53e9830c9d436f96846cc082c2c3", "abstract": "BACKGROUND\nIntelligence theory research has illustrated that people hold either \"fixed\" (intelligence is immutable) or \"growth\" (intelligence can be improved) mindsets and that these views may affect how people learn throughout their lifetime. Little is known about the mindsets of physicians, and how mindset may affect their lifetime learning and integration of feedback.\u00a0Our objective was to determine if pediatric physicians are of the \"fixed\" or \"growth\" mindset and whether individual mindset affects perception of medical error reporting.\u00a0\n\n\nMETHODS\nWe sent an anonymous electronic survey to pediatric residents and attending pediatricians at a tertiary care pediatric hospital. Respondents completed the \"Theories of Intelligence Inventory\" which classifies individuals on a 6-point scale ranging from 1 (Fixed Mindset) to 6 (Growth Mindset). Subsequent questions collected data on respondents' recall of medical errors by self or others.\n\n\nRESULTS\nWe received 176/349 responses (50 %). Participants were equally distributed between mindsets with 84 (49 %) classified as \"fixed\" and 86 (51 %) as \"growth\". Residents, fellows and attendings did not differ in terms of mindset. Mindset did not correlate with the small number of reported medical errors.\n\n\nCONCLUSIONS\nThere is no dominant theory of intelligence (mindset) amongst pediatric physicians. The distribution is similar to that seen in the general population. Mindset did not correlate with error reports.", "title": "A survey of mindset theories of intelligence and medical error self-reporting among pediatric housestaff and faculty."}, "9004ebaa9e853746d122dfc51696f2b8057a8927": {"paper_id": "9004ebaa9e853746d122dfc51696f2b8057a8927", "abstract": "Two studies explored the role of implicit theories of intelligence in adolescents' mathematics achievement. In Study 1 with 373 7th graders, the belief that intelligence is malleable (incremental theory) predicted an upward trajectory in grades over the two years of junior high school, while a belief that intelligence is fixed (entity theory) predicted a flat trajectory. A mediational model including learning goals, positive beliefs about effort, and causal attributions and strategies was tested. In Study 2, an intervention teaching an incremental theory to 7th graders (N=48) promoted positive change in classroom motivation, compared with a control group (N=43). Simultaneously, students in the control group displayed a continuing downward trajectory in grades, while this decline was reversed for students in the experimental group.", "title": "Implicit theories of intelligence predict achievement across an adolescent transition: a longitudinal study and an intervention."}, "a55cdf83abeb99c90a781c0d881be8f642274a61": {"paper_id": "a55cdf83abeb99c90a781c0d881be8f642274a61", "abstract": "Nowadays different approaches are coming forth to tutor students using computers. In this paper, a computer based intelligent tutoring system (ITS) is presented. It projects out a new approach dealing with diagnosis in student modeling which emphasizes on Bayesian networks (for decision making) and item response theory (for adaptive question selection). The advantage of such an approach through Bayesian networks (formal framework of uncertainty) is that this structural model allows substantial simplification when specifying parameters (conditional probabilities) which measures student ability at different levels of granularity. In addition, the probabilistic student model is proved to be more quicker, accurate and efficient. Since most of the tutoring systems are static HTML web pages of class textbooks, our intelligent system can help a student navigate through online course materials and recommended learning goals.", "title": "Intelligent Tutoring System-Bayesian Student Model"}, "837ae38d8eba5635fb8a2e0a5cdb4764e8ea348a": {"paper_id": "837ae38d8eba5635fb8a2e0a5cdb4764e8ea348a", "abstract": "Follow up what we will offer in this article about expert systems and probabilistic network models. You know really that this book is coming as the best seller book today. So, when you are really a good reader or you're fans of the author, it does will be funny if you don't have this book. It means that you have to get this book. For you who are starting to learn about something new and feel curious about this book, it's easy then. Just get this book and feel how this book will give you more exciting lessons.", "title": "Expert Systems and Probabilistic Network Models"}, "17d87b9ac0bedad64489022ef415df05829843ad": {"paper_id": "17d87b9ac0bedad64489022ef415df05829843ad", "abstract": "We consider a graph-theoretic elimination process which is related to performing Gaussian elimination on sparse symmetric positive definite systems of linear equations. We give a new linear-time algorithm to calculate the fill-in produced by any elimination ordering, and we give two new related algorithms for finding orderings with special properties. One algorithm, based on breadth-first search, finds a perfect elimination ordering, if any exists, in O(n + e) time, if the problem graph has n vertices and e edges. An extension of this algorithm finds a minimal (but not necessarily minimum) ordering in O(ne) time. We conjecture that the problem of finding a minimum ordering is", "title": "Algorithmic Aspects of Vertex Elimination on Graphs"}, "8dc087c34cdc1721fa37aafc36fb1b71f6f1b757": {"paper_id": "8dc087c34cdc1721fa37aafc36fb1b71f6f1b757", "abstract": "Production rules are a popular representation for encoding heuristic knowledge in programs for scientific and medical problem solving. However, experience with one of these programs, MYCIN, indicates that the representation has serious limitations: people other than the original rule authors find it difflcuit to modify the rule set, and the rules a r e unsuitable for use in other settings, such as for application to teaching. These problems are rooted in fundamental limitations in MYCIN\u2019s original rule representation: the view that expert knowledge can be encoded as a uniform, weakly-structured set of if/then associations is found to be wanting. To illustrate these problems, this paper examines MYCIN\u2019s rules from the perspective Of a teacher trylng to Justify them and to convey a problem-soivlng approach. We discover that individual rules play different roles, have different kinds of justifications, and are constructed using different rationales for the ordering and choice of premise clauses. This design knowledge, consisting of structural and strategic concepts which lie outside the representation, is shown to be procedurally embedded In the rules. Moreover, because the data/hypothesis associations are themselves a proceduralized form of underlying disease models, they can only be supported by appealing to this deeper level of knowledge. Making explicit this structural, strategic and support knowledge enhances the ability to understand and modify the system.", "title": "The Epistemology of a Rule-Based Expert System - A Framework for Explanation"}, "55b5c3b931f86a71aaa0be99eecdcea3b27078d4": {"paper_id": "55b5c3b931f86a71aaa0be99eecdcea3b27078d4", "abstract": "We propose a multi-accent deep neural network acoustic model with an accent-specific top layer and shared bottom hidden layers. The accent-specific top layer is used to model the distinct accent specific patterns. The shared bottom hidden layers allow maximum knowledge sharing between the native and the accent models. This design is particularly attractive when considering deploying such a system to a live speech service due to its computational efficiency. We applied the KL-divergence (KLD) regularized model adaptation to train the accent-specific top layer. On the mobile short message dictation task (SMD), with 1K, 10K, and 100K British or Indian accent adaptation utterances, the proposed approach achieves 18.1%, 26.0%, and 28.5% or 16.1%, 25.4%, and 30.6% word error rate reduction (WERR) for the British and the Indian accent respectively against a baseline cross entropy (CE) model trained from 400 hour data. On the 100K utterance accent adaptation setup, comparable performance gain can be obtained against a baseline CE model trained with 2000 hour data. We observe smaller yet significant WER reduction on a baseline model trained using the MMI sequence-level criterion.", "title": "Multi-accent deep neural network acoustic model with accent-specific top layer using the KLD-regularized model adaptation"}, "4d6e574c76e4a5ebbdb5f6e382d06c058090e4b7": {"paper_id": "4d6e574c76e4a5ebbdb5f6e382d06c058090e4b7", "abstract": "We propose a novel regularized adaptation technique for context dependent deep neural network hidden Markov models (CD-DNN-HMMs). The CD-DNN-HMM has a large output layer and many large hidden layers, each with thousands of neurons. The huge number of parameters in the CD-DNN-HMM makes adaptation a challenging task, esp. when the adaptation set is small. The technique developed in this paper adapts the model conservatively by forcing the senone distribution estimated from the adapted model to be close to that from the unadapted model. This constraint is realized by adding Kullback-Leibler divergence (KLD) regularization to the adaptation criterion. We show that applying this regularization is equivalent to changing the target distribution in the conventional backpropagation algorithm. Experiments on Xbox voice search, short message dictation, and Switchboard and lecture speech transcription tasks demonstrate that the proposed adaptation technique can provide 2%-30% relative error reduction against the already very strong speaker independent CD-DNN-HMM systems using different adaptation sets under both supervised and unsupervised adaptation setups.", "title": "KL-divergence regularized deep neural network adaptation for improved large vocabulary speech recognition"}, "59c51fddb09f899f298d45a32cccb9760b8465c1": {"paper_id": "59c51fddb09f899f298d45a32cccb9760b8465c1", "abstract": "The use of Deep Belief Networks (DBN) to pretrain Neural Networks has recently led to a resurgence in the use of Artificial Neural Network Hidden Markov Model (ANN/HMM) hybrid systems for Automatic Speech Recognition (ASR). In this paper we report results of a DBN-pretrained context-dependent ANN/HMM system trained on two datasets that are much larger than any reported previously with DBN-pretrained ANN/HMM systems 5870 hours of Voice Search and 1400 hours of YouTube data. On the first dataset, the pretrained ANN/HMM system outperforms the best Gaussian Mixture Model Hidden Markov Model (GMM/HMM) baseline, built with a much larger dataset by 3.7% absolute WER, while on the second dataset, it outperforms the GMM/HMM baseline by 4.7% absolute. Maximum Mutual Information (MMI) fine tuning and model combination using Segmental Conditional Random Fields (SCARF) give additional gains of 0.1% and 0.4% on the first dataset and 0.5% and 0.9% absolute on the second dataset.", "title": "Application of Pretrained Deep Neural Networks to Large Vocabulary Speech Recognition"}, "35d7171423f560194854fc5d203059f9d6bb8059": {"paper_id": "35d7171423f560194854fc5d203059f9d6bb8059", "abstract": "It is well known that recognition performance degrades signi cantly when moving from a speakerdependent to a speaker-independent system. Traditional hidden Markov model (HMM) systems have successfully applied speaker-adaptation approaches to reduce this degradation. In this paper we present and evaluate some techniques for speaker-adaptation of a hybrid HMM-arti cial neural network (ANN) continuous speech recognition system. These techniques are applied to a well trained, speaker-independent, hybrid HMM-ANN system and the recognizer parameters are adapted to a new speaker through o -line procedures. The techniques are evaluated on the DARPA RM corpus using varying amounts of adaptation material and different ANN architectures. The results show that speaker-adaptation within the hybrid framework can substantially improve system performance.", "title": "Speaker-adaptation for hybrid HMM-ANN continuous speech recognition system"}, "a1746d4e1535564e02a7a4d5e4cdd1fa7bedc571": {"paper_id": "a1746d4e1535564e02a7a4d5e4cdd1fa7bedc571", "abstract": "We investigate the potential of Context-Dependent Deep-Neural-Network HMMs, or CD-DNN-HMMs, from a feature-engineering perspective. Recently, we had shown that for speaker-independent transcription of phone calls (NIST RT03S Fisher data), CD-DNN-HMMs reduced the word error rate by as much as one third\u2014from 27.4%, obtained by discriminatively trained Gaussian-mixture HMMs with HLDA features, to 18.5%\u2014using 300+ hours of training data (Switchboard), 9000+ tied triphone states, and up to 9 hidden network layers.", "title": "Feature engineering in Context-Dependent Deep Neural Networks for conversational speech transcription"}, "73405528a769fd10c6a6e7e3c1c8c553fe99033f": {"paper_id": "73405528a769fd10c6a6e7e3c1c8c553fe99033f", "abstract": "As speech recognition systems are used in ever more applications, it is crucial for the systems to be able to deal with accented speakers. Various techniques, such as acoustic model adaptation and pronunciation adaptation, have been reported to improve the recognition of non-native or accented speech. In this paper, we propose a new approach that combines accent detection, accent discriminative acoustic features, acoustic adaptation and model selection for accented Chinese speech recognition. Experimental results show that this approach can improve the recognition of accented speech.", "title": "Accent detection and speech recognition for Shanghai-accented Mandarin"}, "4fc00847860579d603f61fb448c0b70b7a4ee0a0": {"paper_id": "4fc00847860579d603f61fb448c0b70b7a4ee0a0", "abstract": "The inherent uncertainty associated with unstructured grasping tasks makes establishing a successful grasp difficult. Traditional approaches to this problem involve hands that are complex, fragile, require elaborate sensor suites, and are difficult to control. In this paper, we demonstrate a novel autonomous grasping system that is both simple and robust. The four-fingered hand is driven by a single actuator, yet can grasp objects spanning a wide range of size, shape, and mass. The hand is constructed using polymer-based shape deposition manufacturing, with joints formed by elastomeric flexures and actuator and sensor components embedded in tough rigid polymers. The hand has superior robustness properties, able to withstand large impacts without damage and capable of grasping objects in the presence of large positioning errors. We present experimental results showing that the hand mounted on a three degree of freedom manipulator arm can reliably grasp 5 cm-scale objects in the presence of positioning error of up to 100% of the object size and 10 cm-scale objects in the presence of positioning error of up to 33% of the object size, while keeping acquisition contact forces low.", "title": "Simple, Robust Autonomous Grasping in Unstructured Environments"}, "41470ff925fd756948ad9611f16e5632795c4658": {"paper_id": "41470ff925fd756948ad9611f16e5632795c4658", "abstract": "This paper presents the artificial neural network approach namely Back propagation network (BPNs) and probabilistic neural network (PNN). It is used to classify the type of tumor in MRI images of different patients with Astrocytoma type of brain tumor. The image processing techniques have been developed for detection of the tumor in the MRI images. Gray Level Co-occurrence Matrix (GLCM) is used to achieve the feature extraction. The whole system worked in two modes firstly Training/Learning mode and secondly Testing/Recognition mode.", "title": "ANN Approach Based On Back Propagation Network and Probabilistic Neural Network to Classify Brain Cancer"}, "2abc4f065cffc09d7136b7671cd4883e836ce402": {"paper_id": "2abc4f065cffc09d7136b7671cd4883e836ce402", "abstract": "In this paper, a new face recognition technique is introduced based on the gray-level co-occurrence matrix (GLCM). GLCM represents the distributions of the intensities and the information about relative positions of neighboring pixels of an image. We proposed two methods to extract feature vectors using GLCM for face classification. The first method extracts the well-known Haralick features from the GLCM, and the second method directly uses GLCM by converting the matrix into a vector that can be used in the classification process. The results demonstrate that the second method, which uses GLCM directly, is superior to the first method that uses the feature vector containing the statistical Haralick features in both nearest neighbor and neural networks classifiers. The proposed GLCM based face recognition system not only outperforms well-known techniques such as principal component analysis and linear discriminant analysis, but also has comparable performance with local binary patterns and Gabor wavelets.", "title": "Co-occurrence matrix and its statistical features as a new approach for face recognition"}, "5140f1dc83e562de0eb409385480b799e9549d54": {"paper_id": "5140f1dc83e562de0eb409385480b799e9549d54", "abstract": "Texture is one of the important characteristics used in identifying objects or regions of interest in an image, whether the image be a photomicrograph, an aerial photograph, or a satellite image. This paper describes some easily computable textural features based on graytone spatial dependancies, and illustrates their application in categoryidentification tasks of three different kinds of image data: photomicrographs of five kinds of sandstones, 1:20 000 panchromatic aerial photographs of eight land-use categories, and Earth Resources Technology Satellite (ERTS) multispecial imagery containing seven land-use categories. We use two kinds of decision rules: one for which the decision regions are convex polyhedra (a piecewise linear decision rule), and one for which the decision regions are rectangular parallelpipeds (a min-max decision rule). In each experiment the data set was divided into two parts, a training set and a test set. Test set identification accuracy is 89 percent for the photomicrographs, 82 percent for the aerial photographic imagery, and 83 percent for the satellite imagery. These results indicate that the easily computable textural features probably have a general applicability for a wide variety of image-classification applications.", "title": "Textural Features for Image Classification"}, "2a62d0cca2fabf1d6f6ee15e4c14cef415b657d1": {"paper_id": "2a62d0cca2fabf1d6f6ee15e4c14cef415b657d1", "abstract": "A method is presented for the representation of (pictures of) faces. Within a specified framework the representation is ideal. This results in the characterization of a face, to within an error bound, by a relatively low-dimensional vector. The method is illustrated in detail by the use of an ensemble of pictures taken for this purpose.", "title": "Low-dimensional procedure for the characterization of human faces."}, "fc2727118e1ebf541de0a46d7cc10e2a9bf1158d": {"paper_id": "fc2727118e1ebf541de0a46d7cc10e2a9bf1158d", "abstract": "In this paper, modified image segmentation techniques were applied on MRI scan images in order to detect brain tumors. Also in this paper, a modified Probabilistic Neural Network (PNN) model that is based on learning vector quantization (LVQ) with image and data analysis and manipulation techniques is proposed to carry out an automated brain tumor classification using MRI-scans. The assessment of the modified PNN classifier performance is measured in terms of the training performance, classification accuracies and computational time. The simulation results showed that the modified PNN gives rapid and accurate classification compared with the image processing and published conventional PNN techniques. Simulation results also showed that the proposed system out performs the corresponding PNN system presented in [30], and successfully handle the process of brain tumor classification in MRI image with 100% accuracy when the spread value is equal to 1. These results also claim that the proposed LVQ-based PNN system decreases the processing time to approximately 79% compared with the conventional PNN which makes it very promising in the field of in-vivo brain tumor detection and identification. Keywords\u2014 Probabilistic Neural Network, Edge detection, image segmentation, brain tumor detection and identification", "title": "Automated Brain Tumor Detection and Identification Using Image Processing and Probabilistic Neural Network Techniques"}}
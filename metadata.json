{"cf6dc6604fcb4651809a8de7a646d5a8965f154c": {"paper_id": "cf6dc6604fcb4651809a8de7a646d5a8965f154c", "abstract": "As an initial assessment, over 480,000 labeled virtual images of normal highway driving were readily generated in Grand Theft Auto V's virtual environment. Using these images, a CNN was trained to detect following distance to cars/objects ahead, lane markings, and driving angle (angular heading relative to lane centerline): all variables necessary for basic autonomous driving. Encouraging results were obtained when tested on over 50,000 labeled virtual images from substantially different GTA-V driving environments. This initial assessment begins to define both the range and scope of the labeled images needed for training as well as the range and scope of labeled images needed for testing the definition of boundaries and limitations of trained networks. It is the efficacy and flexibility of a\"GTA-V\"-like virtual environment that is expected to provide an efficient well-defined foundation for the training and testing of Convolutional Neural Networks for safe driving. Additionally, described is the Princeton Virtual Environment (PVE) for the training, testing and enhancement of safe driving AI, which is being developed using the video-game engine Unity. PVE is being developed to recreate rare but critical corner cases that can be used in re-training and enhancing machine learning models and understanding the limitations of current self driving models. The Florida Tesla crash is being used as an initial reference.", "title": "Beyond Grand Theft Auto V for Training, Testing and Enhancing Deep Learning in Self Driving Cars"}, "e2304cfcff4d7c9ef2a41f1e68dc67f67939aa2d": {"paper_id": "e2304cfcff4d7c9ef2a41f1e68dc67f67939aa2d", "abstract": "Botnets continue to be an active threat against institutions and individuals worldwide. Previous research regarding botnets has unveiled information on how the system and their stakeholders operate, but an insight on the economic structure behind these stakeholders is lacking. The objective of this research is to build the business model and determine the structure of the underground botnet economy. This means determining the botnet life-cycle, revenue streams and overall economic impact on institutions and stakeholders. Compared to other botnet related research, this paper focuses on the financial aspects, breaking down the components of the botnet life-cycle and estimating the money flow to the different actors involved. What can be concluded is that building a full scale cyber army from scratch can only be done by large institutions or governments, as it is too costly. In contrast, by outsourcing different tasks and making use of existing malware packages, costs are reduced to a minimum and reachable for the average person. Applying this method to earlier researched botnets, in every case the botnet resulted in being profitable for the botmaster. Initial setupand monthly costs were minimal compared to total revenue.", "title": "Business Model of Botnets"}, "920871663e6016e59cac6396311355e723dd499f": {"paper_id": "920871663e6016e59cac6396311355e723dd499f", "abstract": "In recent years social media have become indispensable tools for information dissemination, operating in tandem with traditional media outlets such as newspapers, and it has become critical to understand the interaction between the new and old sources of news. Although social media as well as traditional media have attracted attention from several research communities, most of the prior work has been limited to a single medium. In addition temporal analysis of these sources can provide an understanding of how information spreads and evolves. Modeling temporal dynamics while considering multiple sources is a challenging research problem. In this paper we address the problem of modeling text streams from two news sources - Twitter and Yahoo! News. Our analysis addresses both their individual properties (including temporal dynamics) and their inter-relationships. This work extends standard topic models by allowing each text stream to have both local topics and shared topics. For temporal modeling we associate each topic with a time-dependent function that characterizes its popularity over time. By integrating the two models, we effectively model the temporal dynamics of multiple correlated text streams in a unified framework. We evaluate our model on a large-scale dataset, consisting of text streams from both Twitter and news feeds from Yahoo! News. Besides overcoming the limitations of existing models, we show that our work achieves better perplexity on unseen data and identifies more coherent topics. We also provide analysis of finding real-world events from the topics obtained by our model.", "title": "A time-dependent topic model for multiple text streams"}, "4f94202c9300db57bae73af076f8711b286ec3b2": {"paper_id": "4f94202c9300db57bae73af076f8711b286ec3b2", "abstract": "Previous work on text mining has almost exclusively focused on a single stream. However, we often have available multiple text streams indexed by the same set of time points (called coordinated text streams), which offer new opportunities for text mining. For example, when a major event happens, all the news articles published by different agencies in different languages tend to cover the same event for a certain period, exhibiting a correlated bursty topic pattern in all the news article streams. In general, mining correlated bursty topic patterns from coordinated text streams can reveal interesting latent associations or events behind these streams. In this paper, we define and study this novel text mining problem. We propose a general probabilistic algorithm which can effectively discover correlated bursty patterns and their bursty periods across text streams even if the streams have completely different vocabularies (e.g., English vs Chinese). Evaluation of the proposed method on a news data set and a literature data set shows that it can effectively discover quite meaningful topic patterns from both data sets: the patterns discovered from the news data set accurately reveal the major common events covered in the two streams of news articles (in English and Chinese, respectively), while the patterns discovered from two database publication streams match well with the major research paradigm shifts in database research. Since the proposed method is general and does not require the streams to share vocabulary, it can be applied to any coordinated text streams to discover correlated topic patterns that burst in multiple streams in the same period.", "title": "Mining correlated bursty topic patterns from coordinated text streams"}, "1e56ed3d2c855f848ffd91baa90f661772a279e1": {"paper_id": "1e56ed3d2c855f848ffd91baa90f661772a279e1", "abstract": "We propose a generative model for text and other collections of discrete data that generalizes or improves on several previous models including naive Bayes/unigram, mixture of unigrams [6], and Hofmann's aspect model , also known as probabilistic latent semantic indexing (pLSI) [3]. In the context of text modeling, our model posits that each document is generated as a mixture of topics, where the continuous-valued mixture proportions are distributed as a latent Dirichlet random variable. Inference and learning are carried out efficiently via variational algorithms. We present empirical results on applications of this model to problems in text modeling, collaborative filtering, and text classification.", "title": "Latent Dirichlet Allocation"}, "2d2fcc9bd9ba0c9ddcb73544d28fa0d6eeba9af4": {"paper_id": "2d2fcc9bd9ba0c9ddcb73544d28fa0d6eeba9af4", "abstract": "An overwhelming number of news articles are available every day via the internet. Unfortunately, it is impossible for us to peruse more than a handful; furthermore it is difficult to ascertain an article\u2019s social context, i.e., is it popular, what sorts of people are reading it, etc. In this paper, we develop a system to address this problem in the restricted domain of political news by harnessing implicit and explicit contextual information from the blogosphere. Specifically, we track thousands of blogs and the news articles they cite, collapsing news articles that have highly overlapping content. We then tag each article with the number of blogs citing it, the political orientation of those blogs, and the level of emotional charge expressed in the blog posts that link to the news article. We summarize and present the results to the user via a novel visualization which displays this contextual information; the user can then find the most popular articles, the articles most cited by liberals, the articles most emotionally discussed in the political blogosphere, etc.", "title": "BLEWS: Using Blogs to Provide Context for News Articles"}, "7df47b761c4cbd25acf7140e900d099e5fbe152e": {"paper_id": "7df47b761c4cbd25acf7140e900d099e5fbe152e", "abstract": "Automated tracking of events from chronologically ordered document streams is a new challenge for statistical text classification. Existing learning techniques must be adapted or improved in order to effectively handle difficult situations where the number of positive training instances per event is extremely small, the majority of training documents are unlabelled, and most of the events have a short duration in time. We adapted several supervised text categorization methods, specifically several new variants of the k-Nearest Neighbor (kNN) algorithm and a Rocchio approach, to track events. All of these methods showed significant improvement (up to 71% reduction in weighted error rates) over the performance of the original kNN algorithm on TDT benchmark collections, making kNN among the top-performing systems in the recent TDT3 official evaluation. Furthermore, by combining these methods, we significantly reduced the variance in performance of our event tracking system over different data collections, suggesting a robust solution for parameter optimization.", "title": "Improving text categorization methods for event tracking"}, "21fb86020f68bf2dd57cd1b8a0e8adead5d9a9ae": {"paper_id": "21fb86020f68bf2dd57cd1b8a0e8adead5d9a9ae", "abstract": "Association rule mining was first proposed by Agrawal, Imielinski, and Swami [AIS93]. The Apriori algorithm discussed in Section 5.2.1 for frequent itemset mining was presented in Agrawal and Srikant [AS94b]. A variation of the algorithm using a similar pruning heuristic was developed independently by Mannila, Tiovonen, and Verkamo [MTV94]. A joint publication combining these works later appeared in Agrawal, Mannila, Srikant, Toivonen, and Verkamo [AMS96]. A method for generating association rules from frequent itemsets is described in Agrawal and Srikant [AS94a].", "title": "Data Mining : Concepts and Techniques"}, "2b8a80b18cc7a4461c6e532c2f3de7e570d4fcd6": {"paper_id": "2b8a80b18cc7a4461c6e532c2f3de7e570d4fcd6", "abstract": "Mining subtopics from weblogs and analyzing their spatiotemporal patterns have applications in multiple domains. In this paper, we define the novel problem of mining spatiotemporal theme patterns from weblogs and propose a novel probabilistic approach to model the subtopic themes and spatiotemporal theme patterns simultaneously. The proposed model discovers spatiotemporal theme patterns by (1) extracting common themes from weblogs; (2) generating theme life cycles for each given location; and (3) generating theme snapshots for each given time period. Evolution of patterns can be discovered by comparative analysis of theme life cycles and theme snapshots. Experiments on three different data sets show that the proposed approach can discover interesting spatiotemporal theme patterns effectively. The proposed probabilistic model is general and can be used for spatiotemporal text mining on any domain with time and location information.", "title": "A probabilistic approach to spatiotemporal theme pattern mining on weblogs"}, "215aa495b4c860a1e6d87f2c36f34da464376cc4": {"paper_id": "215aa495b4c860a1e6d87f2c36f34da464376cc4", "abstract": "A first step in identifying the content of a document is determining which topics that document addresses. We describe a generative model for documents, introduced by Blei, Ng, and Jordan [Blei, D. M., Ng, A. Y. & Jordan, M. I. (2003) J. Machine Learn. Res. 3, 993-1022], in which each document is generated by choosing a distribution over topics and then choosing each word in the document from a topic selected according to this distribution. We then present a Markov chain Monte Carlo algorithm for inference in this model. We use this algorithm to analyze abstracts from PNAS by using Bayesian model selection to establish the number of topics. We show that the extracted topics capture meaningful structure in the data, consistent with the class designations provided by the authors of the articles, and outline further applications of this analysis, including identifying \"hot topics\" by examining temporal dynamics and tagging abstracts to illustrate semantic content.", "title": "Finding scientific topics."}, "1fd05854fb509e37393e473490e059cac8abba99": {"paper_id": "1fd05854fb509e37393e473490e059cac8abba99", "abstract": "We introduce the author-topic model, a generative model for documents that extends Latent Dirichlet Allocation (LDA; Blei, Ng, & Jordan, 2003) to include authorship information. Each author is associated with a multinomial distribution over topics and each topic is associated with a multinomial distribution over words. A document with multiple authors is modeled as a distribution over topics that is a mixture of the distributions associated with the authors. We apply the model to a collection of 1,700 NIPS conference papers and 160,000 CiteSeer abstracts. Exact inference is intractable for these datasets and we use Gibbs sampling to estimate the topic and author distributions. We compare the performance with two other generative models for documents, which are special cases of the author-topic model: LDA (a topic model) and a simple author model in which each author is associated with a distribution over words rather than a distribution over topics. We show topics recovered by the authortopic model, and demonstrate applications to computing similarity between authors and entropy of author output.", "title": "The Author-Topic Model for Authors and Documents"}, "fb6583a73833a9593a072bb39048298c5b3dd010": {"paper_id": "fb6583a73833a9593a072bb39048298c5b3dd010", "abstract": "Mastocytosis is a rare, heterogeneous disease of complex etiology, characterized by a marked increase in mast cell density in the skin, bone marrow, liver, spleen, gastrointestinal mucosa and lymph nodes. The most frequent site of organ involvement is the skin. Cutaneous lesions include urticaria pigmentosa, mastocytoma, diffuse and erythematous cutaneous mastocytosis, and telangiectasia macularis eruptiva perstans. Human mast cells originate from CD34 progenitors, under the influence of stem cell factor (SCF); a substantial number of patients exhibit activating mutations in c-kit, the receptor for SCF. Mast cells can synthesize a variety of cytokines that could affect the skeletal system, increasing perforating bone resorption and leading to osteoporosis. The coexistence of hematologic disorders, such as myeloproliferative or myelodysplastic syndromes, or of lymphoreticular malignancies, is common. Compared with radiographs, Tc-99m methylenediphosphonate (MDP) scintigraphy is better able to show the widespread skeletal involvement in patients with diffuse disease. T1-weighted MR imaging is a sensitive technique for detecting marrow abnormalities in patients with systemic mastocytosis, showing several different patterns of marrow involvement. We report the imaging findings a 36-year old male with well-documented urticaria pigmentosa. In order to evaluate mastocytic bone marrow involvement, 99mTc-MDP scintigraphy, T1-weighted spin echo and short tau inversion recovery MRI at 1.0 T, were performed. Both scan findings were consistent with marrow hyperactivity. Thus, the combined use of bone scan and MRI may be useful in order to recognize marrow involvement in suspected systemic mastocytosis, perhaps avoiding bone biopsy.", "title": "Systemic mastocytosis: bone marrow involvement assessed by Tc-99m MDP scintigraphy and magnetic resonance imaging."}, "72205d2204426e0172a967a76a1b1c4338d33f63": {"paper_id": "72205d2204426e0172a967a76a1b1c4338d33f63", "abstract": "This paper illustrates our recent work on the analysis of expressive gesture related to the motion of the upper body (the head and the hands) in the context of emotional portrayals performed by professional actors. An experiment is presented which is the result of a multidisciplinary joint work. The experiment aims at (i) developing models and algorithms for analysis of such expressive content (ii) individuating which motion cues are involved in conveying the actorpsilas expressive intentions to portray four emotions (anger, joy, relief, sadness) via a scenario approach. The paper discusses the experiment in detail with reference to related conceptual issues, developed techniques, and the obtained results.", "title": "Technique for automatic emotion recognition by body gesture analysis"}, "3d020c8ef6cf9aa678c495a34f139637d77a75d6": {"paper_id": "3d020c8ef6cf9aa678c495a34f139637d77a75d6", "abstract": "This paper presents research using full body skeletal movements captured using video-based sensor technology developed by Vicon Motion Systems, to train a machine to identify different human emotions. The Vicon system uses a series of 6 cameras to capture lightweight markers placed on various points of the body in 3D space, and digitizes movement into x, y, and z displacement data. Gestural data from five subjects was collected depicting four emotions: sadness, joy, anger, and fear. Experimental results with different machine learning techniques show that automatic classification of this data ranges from 84% to 92% depending on how it is calculated. In order to put these automatic classification results into perspective a user study on the human perception of the same data was conducted with average classification accuracy of 93%.", "title": "Gesture-Based Affective Computing on Motion Capture Data"}, "33328d6b669a550c88ab9db4102be3f00fc1c402": {"paper_id": "33328d6b669a550c88ab9db4102be3f00fc1c402", "abstract": "The problem of indexing time series has attracted much interest. Most algorithms used to index time series utilize the Euclidean distance or some variation thereof. However, it has been forcefully shown that the Euclidean distance is a very brittle distance measure. Dynamic time warping (DTW) is a much more robust distance measure for time series, allowing similar shapes to match even if they are out of phase in the time axis. Because of this flexibility, DTW is widely used in science, medicine, industry and finance. Unfortunately, however, DTW does not obey the triangular inequality and thus has resisted attempts at exact indexing. Instead, many researchers have introduced approximate indexing techniques or abandoned the idea of indexing and concentrated on speeding up sequential searches. In this work, we introduce a novel technique for the exact indexing of DTW. We prove that our method guarantees no false dismissals and we demonstrate its vast superiority over all competing approaches in the largest and most comprehensive set of time series indexing experiments ever undertaken.", "title": "Exact indexing of dynamic time warping"}, "4b1a53d987b20630679510c351f3a554faea5281": {"paper_id": "4b1a53d987b20630679510c351f3a554faea5281", "abstract": "Our purpose is to design a useful tool which can be used in psychology to automatically classify utterances into five emotional states such as anger, happiness, neutral, sadness, and surprise. The major contribution of the paper is to rate the discriminating capability of a set of features for emotional speech recognition. A total of 87 features has been calculated over 500 utterances from the Danish Emotional Speech database. The sequential forward selection method (SFS) has been used in order to discover a set of 5 to 10 features which are able to classify the utterances in the best way. The criterion used in SFS is the cross-validated correct classification score of one of the following classifiers: nearest mean and Bayes classifier where class pdf are approximated via Parzen windows or modelled as Gaussians. After selecting the 5 best features, we reduce the dimensionality to two by applying principal component analysis. The result is a 51.6% /spl plusmn/ 3% correct classification rate at 95% confidence interval for the five aforementioned emotions, whereas a random classification would give a correct classification rate of 20%. Furthermore, we find out those two-class emotion recognition problems whose error rates contribute heavily to the average error and we indicate that a possible reduction of the error rates reported in this paper would be achieved by employing two-class classifiers and combining them.", "title": "Automatic emotional speech classification"}, "188ece11e8152122ad5363622fae04477bcd03cd": {"paper_id": "188ece11e8152122ad5363622fae04477bcd03cd", "abstract": "We describe a computer vision system for observing facial motion by using anoptimal estimationoptical flow method coupled with geometric, physical and motion-based dynamic models describing the facial structure. Our method produces a reliable parametric representation of the face's independent muscle action groups, as well as an accurate estimate of facial motion. Previous efforts at analysis of facial expression have been based on the Facial Action Coding System (FACS), a representation developed in order to allow human psychologists to code expression from static pictures. To avoid use of this heuristic coding scheme, we have used our computer vision system to probabilistically characterize facial motion and muscle activation in an experimental population, thus deriving a new, more accurate representation of human facial expressions that we call FACS +. Finally, we show how this method can be used for coding, analysis, interpretation, and recognition of facial expressions.", "title": "Coding, Analysis, Interpretation, and Recognition of Facial Expressions"}, "14516f32a828ea9429cfe84ad9c7338a702c3e7c": {"paper_id": "14516f32a828ea9429cfe84ad9c7338a702c3e7c", "abstract": "A reciprocal frame (RF) is a self-supported three-dimensional structure made up of three or more sloping rods, which form a closed circuit, namely an RF-unit. Large RF-structures built as complex grillages of one or a few similar RF-units have an intrinsic beauty derived from their inherent self-similar and highly symmetric patterns. Designing RF-structures that span over large domains is an intricate and complex task. In this paper, we present an interactive computational tool for designing RF-structures over a 3D guiding surface, focusing on the aesthetic aspect of the design.\n There are three key contributions in this work. First, we draw an analogy between RF-structures and plane tiling with regular polygons, and develop a computational scheme to generate coherent RF-tessellations from simple grammar rules. Second, we employ a conformal mapping to lift the 2D tessellation over a 3D guiding surface, allowing a real-time preview and efficient exploration of wide ranges of RF design parameters. Third, we devise an optimization method to guarantee the collinearity of contact joints along each rod, while preserving the geometric properties of the RF-structure. Our tool not only supports the design of wide variety of RF pattern classes and their variations, but also allows preview and refinement through interactive controls.", "title": "Reciprocal frame structures made easy"}, "5c9a148d8651d98a664b5c5f00d3afd1c6564790": {"paper_id": "5c9a148d8651d98a664b5c5f00d3afd1c6564790", "abstract": "Extremely high correlations between repeated judgments of visual appeal of homepages shown for 50 milliseconds have been interpreted as evidence for a mere exposure effect [Lindgaard et al. 2006]. Continuing that work, the present research had two objectives. First, it investigated the relationship between judgments differing in cognitive demands. Second, it began to identify specific visual attributes that appear to contribute to different judgments. Three experiments are reported. All used the stimuli and viewing time as before. Using a paradigm known to disrupt processing beyond the stimulus offset, Experiment 1 was designed to ensure that the previous findings could not be attributed to such continued processing. Adopting a within-subject design, Experiment 2 investigated the extent to which judgments differing in cognitive demands (visual appeal, perceived usability, trustworthiness) may be driven by the visual characteristics of a Web page. It also enabled analyses of visual attributes that contributed most to the different judgments. Experiment 3 replicated Experiment 2 but using a between-subject design to ensure that no practice effect could occur. The results suggest that all three types of judgments are largely driven by visual appeal, but that cognitively demanding judgments are processed in a qualitatively different manner than visual appeal, and that they rely on somewhat different visual attributes. A model accounting for the results is provided.", "title": "An exploration of relations between visual appeal, trustworthiness and perceived usability of homepages"}, "1f36a281fbb2bc70c18f97289eaae2b9dd0b02a4": {"paper_id": "1f36a281fbb2bc70c18f97289eaae2b9dd0b02a4", "abstract": "An experiment was conducted to test the relationships between users' perceptions of a computerized system's beauty and usability. The experiment used a computerized application as a surrogate for an Automated Teller Machine (ATM). Perceptions were elicited before and after the participants used the system. Pre-experimental measures indicate strong correlations between system's perceived aesthetics and perceived usability. Post-experimental measures indicated that the strong correlation remained intact. A multivariate analysis of covariance revealed that the degree of system's aesthetics affected the post-use perceptions of both aesthetics and usability, whereas the degree of actual usability had no such effect. The results resemble those found by social psychologists regarding the effect of physical attractiveness on the valuation of other personality attributes. The \u00aendings stress the importance of studying the aesthetic aspect of human\u00b1computer interaction (HCI) design and its relationships to other design dimensions. q 2000 Elsevier Science B.V. All rights reserved.", "title": "What is beautiful is usable"}, "c0afcd5ed55055d895a2a667cbae8e078b8117c6": {"paper_id": "c0afcd5ed55055d895a2a667cbae8e078b8117c6", "abstract": "The notion of \u2018user satisfaction\u2019 plays a prominent role in HCI, yet it remains evasive. This exploratory study reports three experiments from an ongoing research program. In this program we aim to uncover (1) what user satisfaction is, (2) whether it is primarily determined by user expectations or by the interactive experience, (3) how user satisfaction may be related to perceived usability, and (4) the extent to which satisfaction rating scales capture the same interface qualities as uncovered in self-reports of interactive experiences. In all three experiments reported here user satisfaction was found to be a complex construct comprising several concepts, the distribution of which varied with the nature of the experience. Expectations were found to play an important role in the way users approached a browsing task. Satisfaction and perceived usability was assessed using two methods: scores derived from unstructured interviews and from the Web site Analysis MeasureMent Inventory (WAMMI) rating scales. Scores on these two instruments were somewhat similar, but conclusions drawn across all three experiments differed in terms of satisfaction ratings, suggesting that rating scales and interview statements may tap different interface qualities. Recent research suggests that \u2018beauty\u2019, or \u2018appeal\u2019 is linked to perceived usability so that what is \u2018beautiful\u2019 is also perceived to be usable [Interacting with Computers 13 (2000) 127]. This was true in one experiment here using a web site high in perceived usability and appeal. However, using a site with high appeal but very low in perceived usability yielded very high satisfaction, but low perceived usability scores, suggesting that what is \u2018beautiful\u2019 need not also be perceived to be usable. The results suggest that web designers may need to pay attention to both visual appeal and usability. q 2002 Elsevier Science B.V. All rights reserved.", "title": "What is this evasive beast we call user satisfaction?"}, "6db7156defdd83e928a114f335eef17c96217a5b": {"paper_id": "6db7156defdd83e928a114f335eef17c96217a5b", "abstract": "The nature of the problems investigated and the techniques employed in this book necessitate a procedure which in many instances is thoroughly mathematical. The mathematical devices used are elementary in the sense that no advanced algebra, or calculus, etc., occurs. (With two, rather unimportant, exceptions: Part of the discussion of an example in 19.7. et sequ. and a remark in A.3.3. make use of some simple integrals.) Concepts originating in set theory, linear geometry and group theory play an important role, but they are invariably taken from the early chapters of those disciplines and are moreover analyzed and explained in special expository sections. Nevertheless the book is not truly elementary because the mathematical deductions are frequently intricate and the logical possibilities are extensively exploited. Thus no specific knowledge of any particular body of advanced mathematics is required. However, the reader who wants to acquaint himself more thoroughly with the subject expounded here, will have to familiarize himself with the mathematical way of reasoning definitely beyond its routine, primitive phases. The character of the procedures will be mostly that of mathematical logics, set theory and functional analysis. We have attempted to present the subject in such a form that a reader who is moderately versed in mathematics can acquire the necessary practice in the course of this study. We hope that we have not entirely failed in this endeavour. In accordance with this, the presentation is not what it would be in a strictly mathematical treatise. All definitions and deductions are considerably broader than they would be there. Besides, purely verbal discussions and analyses take up a considerable amount of space. We have in particular tried to give, whenever possible, a parallel verbal exposition for every major mathematical deduction. It is hoped that this procedure will elucidate in unmathematical language what the Search all I-Revues This Collection", "title": "Theory of Games and Economic Behavior"}, "bc36badb6606b8162d821a227dda09a94aac537f": {"paper_id": "bc36badb6606b8162d821a227dda09a94aac537f", "abstract": "Human activity categories can be mainly divided into two types: body motion (e.g. \"dancing\" and \"jumping\") and human-object interaction (e.g. \"playing guitar\" and \"riding bike\"). In this paper, we propose a model that uses this insight to combine spatiotemporal features with information on human-object interaction to predict human activity categories in realistic videos. In this paper, we (1) improve on prior work by proposing a model to provide long-term prediction based on spatiotemporal features extracted by a deep 3-dimensional convolutional network and (2) extend the spatiotemporal features by combining with information on human-object interaction generated from an object detection model. Our model achieves 92.5% accuracy on UCF101 dataset that outperforms any other state-of-the-art methods. In addition, our approach has high computing efficiency and achieves real-time processing.", "title": "An Optimization Model for Human Activity Recognition Inspired by Information on Human-Object Interaction"}, "39dba6f22d72853561a4ed684be265e179a39e4f": {"paper_id": "39dba6f22d72853561a4ed684be265e179a39e4f", "abstract": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT\u201914 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM\u2019s BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM\u2019s performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.", "title": "Sequence to Sequence Learning with Neural Networks"}, "061356704ec86334dbbc073985375fe13cd39088": {"paper_id": "061356704ec86334dbbc073985375fe13cd39088", "abstract": "In this work we investigate the effect of the convolutional n etwork depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achi eved by pushing the depth to 16\u201319 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first a nd he second places in the localisation and classification tracks respec tively. We also show that our representations generalise well to other datasets, whe re t y achieve the stateof-the-art results. Importantly, we have made our two bestp rforming ConvNet models publicly available to facilitate further research o n the use of deep visual representations in computer vision.", "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition"}, "652d159bf64a70194127722d19841daa99a69b64": {"paper_id": "652d159bf64a70194127722d19841daa99a69b64", "abstract": "This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.", "title": "Generating Sequences With Recurrent Neural Networks"}, "0eb2e4a205a628ab059cab41d3b772f614ad29f2": {"paper_id": "0eb2e4a205a628ab059cab41d3b772f614ad29f2", "abstract": "To allow the hidden units of a restricted Boltzmann machine to model the transformation between two successive images, Memisevic and Hinton (2007) introduced three-way multiplicative interactions that use the intensity of a pixel in the first image as a multiplicative gain on a learned, symmetric weight between a pixel in the second image and a hidden unit. This creates cubically many parameters, which form a three-dimensional interaction tensor. We describe a low-rank approximation to this interaction tensor that uses a sum of factors, each of which is a three-way outer product. This approximation allows efficient learning of transformations between larger image patches. Since each factor can be viewed as an image filter, the model as a whole learns optimal filter pairs for efficiently representing transformations. We demonstrate the learning of optimal filter pairs from various synthetic and real image sequences. We also show how learning about image transformations allows the model to perform a simple visual analogy task, and we show how a completely unsupervised network trained on transformations perceives multiple motions of transparent dot patterns in the same way as humans.", "title": "Learning to Represent Spatial Transformations with Factored Higher-Order Boltzmann Machines"}, "1a090df137014acab572aa5dc23449b270db64b4": {"paper_id": "1a090df137014acab572aa5dc23449b270db64b4", "abstract": null, "title": "LIBSVM: a library for support vector machines"}, "18c84e6b5f1d6da3c670454db3f0fa61266ab1e3": {"paper_id": "18c84e6b5f1d6da3c670454db3f0fa61266ab1e3", "abstract": "This paper demonstrates theoretically and empirically that a greedy algorithm called orthogonal matching pursuit (OMP) can reliably recover a signal with m nonzero entries in dimension d given O(m ln d) random linear measurements of that signal. This is a massive improvement over previous results, which require O(m2) measurements. The new results for OMP are comparable with recent results for another approach called basis pursuit (BP). In some settings, the OMP algorithm is faster and easier to implement, so it is an attractive alternative to BP for signal recovery problems.", "title": "Signal Recovery From Random Measurements Via Orthogonal Matching Pursuit"}, "5183230b706b72f6f6c19415c423d93c79ddde53": {"paper_id": "5183230b706b72f6f6c19415c423d93c79ddde53", "abstract": "This paper addresses the problem of large-scale image search. Three constraints have to be taken into account: search accuracy, efficiency, and memory usage. We first present and evaluate different ways of aggregating local image descriptors into a vector and show that the Fisher kernel achieves better performance than the reference bag-of-visual words approach for any given vector dimension. We then jointly optimize dimensionality reduction and indexing in order to obtain a precise vector comparison as well as a compact representation. The evaluation shows that the image representation can be reduced to a few dozen bytes while preserving high accuracy. Searching a 100 million image data set takes about 250 ms on one processor core.", "title": "Aggregating Local Image Descriptors into Compact Codes"}, "6090ebb9464986e458f8c37d2e08b042c88bf651": {"paper_id": "6090ebb9464986e458f8c37d2e08b042c88bf651", "abstract": "Recently, methods based on local image features have shown promise for texture and object recognition tasks. This paper presents a large-scale evaluation of an approach that represents images as distributions (signatures or histograms) of features extracted from a sparse set of keypoint locations and learns a Support Vector Machine classifier with kernels based on two effective measures for comparing distributions, the Earth Mover\u2019s Distance and the \u03c72 distance. We first evaluate the performance of our approach with different keypoint detectors and descriptors, as well as different kernels and classifiers. We then conduct a comparative evaluation with several state-of-the-art recognition methods on four texture and five object databases. On most of these databases, our implementation exceeds the best reported results and achieves comparable performance on the rest. Finally, we investigate the influence of background correlations on recognition performance via extensive tests on the PASCAL database, for which ground-truth object localization information is available. Our experiments demonstrate that image representations based on distributions of local features are surprisingly effective for classification of texture and object images under challenging real-world conditions, including significant intra-class variations and substantial background clutter.", "title": "Local Features and Kernels for Classification of Texture and Object Categories: A Comprehensive Study"}, "161ba09d1a0025cf0d7d52f2bd51eb4c12ea7b5b": {"paper_id": "161ba09d1a0025cf0d7d52f2bd51eb4c12ea7b5b", "abstract": "The past decade has witnessed a rapid proliferation of video cameras in all walks of life and has resulted in a tremendous explosion of video content. Several applications such as content-based video annotation and retrieval, highlight extraction and video summarization require recognition of the activities occurring in the video. The analysis of human activities in videos is an area with increasingly important consequences from security and surveillance to entertainment and personal archiving. Several challenges at various levels of processing-robustness against errors in low-level processing, view and rate-invariant representations at midlevel processing and semantic representation of human activities at higher level processing-make this problem hard to solve. In this review paper, we present a comprehensive survey of efforts in the past couple of decades to address the problems of representation, recognition, and learning of human activities from video and related applications. We discuss the problem at two major levels of complexity: 1) \"actions\" and 2) \"activities.\" \"Actions\" are characterized by simple motion patterns typically executed by a single human. \"Activities\" are more complex and involve coordinated actions among a small number of humans. We will discuss several approaches and classify them according to their ability to handle varying degrees of complexity as interpreted above. We begin with a discussion of approaches to model the simplest of action classes known as atomic or primitive actions that do not require sophisticated dynamical modeling. Then, methods to model actions with more complex dynamics are discussed. The discussion then leads naturally to methods for higher level representation of complex activities.", "title": "Machine Recognition of Human Activities: A Survey"}, "22d70d143135af112b7852416e416bbd02d093db": {"paper_id": "22d70d143135af112b7852416e416bbd02d093db", "abstract": "While research on articulated human motion and pose estimation has progressed rapidly in the last few years, there has been no systematic quantitative evaluation of competing methods to establish the current state of the art. Current algorithms make many different choices about how to model the human body, how to exploit image evidence and how to approach the inference problem. We argue that there is a need for common datasets that allow fair comparison between different methods and their design choices. Until recently gathering ground-truth data for evaluation of results (especially in 3D) was challenging. In this report we present a novel dataset obtained using a unique setup for capturing synchronized video and ground-truth 3D motion. Data was captured simultaneously using a calibrated marker-based motion capture system and multiple high-speed video capture systems. The video and motion capture streams were synchronized in software using a direct optimization method. The resulting HumanEvaI dataset contains multiple subjects performing a set of predefined actions with a number of repetitions. On the order of 50,000 frames of synchronized motion capture and video was collected at 60 Hz with an additional 37,000 frames of pure motion capture data. The data is partitioned into training, validation, and testing sub-sets. A standard set of error metrics is defined that can be used for evaluation of both 2D and 3D pose estimation and tracking algorithms. Support software and an on-line evaluation system for quantifying results using the test data is being made available to the community. This report provides an overview of the dataset and evaluation metrics and provides pointers into the dataset for additional details. It is our hope that HumanEva-I will become a standard dataset for the evaluation of articulated human motion and pose estimation.", "title": "HumanEva: Synchronized Video and Motion Capture Dataset for Evaluation of Articulated Human Motion"}, "10d6b12fa07c7c8d6c8c3f42c7f1c061c131d4c5": {"paper_id": "10d6b12fa07c7c8d6c8c3f42c7f1c061c131d4c5", "abstract": "We study the question of feature sets for robust visual object recognition; adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.", "title": "Histograms of oriented gradients for human detection"}, "0cf7da0df64557a4774100f6fde898bc4a3c4840": {"paper_id": "0cf7da0df64557a4774100f6fde898bc4a3c4840", "abstract": "We approach recognition in the framework of deformable shape matching, relying on a new algorithm for finding correspondences between feature points. This algorithm sets up correspondence as an integer quadratic programming problem, where the cost function has terms based on similarity of corresponding geometric blur point descriptors as well as the geometric distortion between pairs of corresponding feature points. The algorithm handles outliers, and thus enables matching of exemplars to query images in the presence of occlusion and clutter. Given the correspondences, we estimate an aligning transform, typically a regularized thin plate spline, resulting in a dense correspondence between the two shapes. Object recognition is then handled in a nearest neighbor framework where the distance between exemplar and query is the matching cost between corresponding points. We show results on two datasets. One is the Caltech 101 dataset (Fei-Fei, Fergus and Perona), an extremely challenging dataset with large intraclass variation. Our approach yields a 48% correct classification rate, compared to Fei-Fei et al 's 16%. We also show results for localizing frontal and profile faces that are comparable to special purpose approaches tuned to faces.", "title": "Shape matching and object recognition using low distortion correspondences"}, "0b3cfbf79d50dae4a16584533227bb728e3522aa": {"paper_id": "0b3cfbf79d50dae4a16584533227bb728e3522aa", "abstract": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.", "title": "Long Short-Term Memory"}, "590999a3b8e6e484260b671682c4f704e90bfdac": {"paper_id": "590999a3b8e6e484260b671682c4f704e90bfdac", "abstract": "Rumelhart, Hinton and Williams [Rumelhart et al. 86] describe a learning procedure for layered networks of deterministic, neuron-like units. This paper describes further research on the learning procedure. We start by describing the units, the way they are connected, the learning procedure, and the extension to iterative nets. We then give an example in which a network learns a set of filters that enable it to discriminate formant-like patterns in the presence of noise. The speed of learning is strongly dependent on the shape of the surface formed by the error measure in \"weight space.\" We give examples of the shape of the error surface for a typical task and illustrate how an acceleration method speeds up descent in weight space. The main drawback of the learning procedure is the way it scales as the size of the task and the network increases. We give some preliminary results on scaling and show how the magnitude of the optimal weight changes depends on the fan-in of the units. Additional results illustrate the effects on learning speed of the amount of interaction between the weights. A variation of the learning procedure that back-propagates desired state information rather than error gradients is developed and compared with the standard procedure. Finally, we discuss the relationship between our iterative networks and the \"analog\" networks described by Hopfield and Tank [Hopfield and Tank 85]. The learning procedure can discover appropriate weights in their kind of network, as well as determine an optimal schedule for varying the nonlinearity of the units during a search.", "title": "Experiments on Learning by Back Propagation."}, "0d98aca44d4e4efc7e0458e6405b9c326137a631": {"paper_id": "0d98aca44d4e4efc7e0458e6405b9c326137a631", "abstract": "This paper shows the existence of a finite neural network, made up of sigmoidal nenrons, which simulates a nniversal Turing machine. It is composed of less then lo5 synchronously evolving processors, interconnected linearly. High-order connections are not required.", "title": "Turing Computability with Neural Nets"}, "8804402bd9bd1013d1a67f0f9fb26a9c678b6c78": {"paper_id": "8804402bd9bd1013d1a67f0f9fb26a9c678b6c78", "abstract": "D3EGF(FIH)J KMLONPEGQSRPETN UCV.WYX(Z R.[ V R6\\M[ X N@]_^O\\`JaNcb V RcQ W d EGKeL(^(QgfhKeLOE?i)^(QSj ETNPfPQkRl[ V R)m\"[ X ^(KeLOEG^ npo qarpo m\"[ X ^(KeLOEG^tsAu EGNPb V ^ v wyx zlwO{(|(}<~O\u007fC}\u0081\u0080(\u0082(xp{a\u0083y\u0084.~A}\u0086\u0085\u0088\u0087_~ \u0089C\u008al\u00833\u0089#|<\u0080Az\u0086w#|l\u00806\u0087 \u008b(| \u008c JpfhL X\u008dV\u008f\u008e EG^O\u0090 QgJ \u0091 ETFOR\u0086\u0092\u0093] ^O\\\u0094J\u0095NPb V RcQ\u0097\u0096 X E)ETR \u00986EGKeLOETNcKMLOE\u009a\u0099 F\u0088\u009b ETN V RcQgJp^(^OE ZgZ E i ^(Qkj EGNPfhQSRO\u009b E \u009cOE2m1Jp^ RcNY\u009b E V\u0095Z sO\u009d\u009f\u009e! \u008d\u00a1 q.n sCD X KGKa\u00928\u009d\u00a2EG^ RPNhE\u00a4\u00a3 \u00a5\u00a6Q ZgZ E\u0095s m\u00a7J\u0095^ RPNO\u009b E V\u0095Z s( \u0308 X \u009b EG\u00a9#E\u0081Kas#\u009d V ^ V \u009c V s(H a \u009d\u00aba\u0095\u00ac3\u00ad \u00ae#|.\u0080Y \u0304y} xa\u00b0O\u007fC}l{\u008dx\u0093\u0087 \u0089 \u0083yxl\u0080Y~3{\u008d| \u0084 \u00b12\u0087Pz \u0084 \u009e V J Z J U N V fhKTJp^(Q \u0091 ETFOR\u0086\u0092 J\u0095\\ D vYf3RPEGb \u0301f V ^(\u009c\u00a7\u009d\u0088Jpb\u008fF X RPETN@D KTQ\u0097EG^(KTE i ^(QSjpEGNPfhQSR4v\u03bcJ\u0095\\ U\u00b6Z JaNPEG^(K\u00b7E jYQ V \u009c(Q \u0327D V ^ R V m V N3R V aOs#1 o \u00a1Ga r U Q\u0097NhE\u0081^OoTE1\u20444\u00bb,] R V\u0095Z vC1\u20442 3\u20444 \u0084 x \u00b1 x \u007f \u008b#\u00bf }\u00c0\u0087 \u00893\u0080t}l\u0082C}2\u0087P}<~ \u00act[ X NP\u0090\u0095E\u0081^\u00a7D KeL(b \u0301Qg\u009c(L X \u00a9yETN ] \u0091 DY]_\u00c1 \u009d\u0088J\u0095NPfhJ\u00c3\u00c2 Z j ETo\u0081Q V a\u0095 rpopo2\u00c4 X \u0090 V ^(J(sCD \u00c5)QSRPoTEGN ZgV ^(\u009c \u00c6 \u0089#|\u0095{3 \u0304\u008d|.\u0080(\u007fC}.\u008bC\u00bfY}p\u0084 \u0087Pz\u0086w", "title": "Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies"}, "89eca547b1a2f6208ae529d45a65a51cf49adbff": {"paper_id": "89eca547b1a2f6208ae529d45a65a51cf49adbff", "abstract": "BLEU is the de facto standard automatic evaluation metric in machine translation. While BLEU is undeniably useful, it has a number of limitations. Although it works well for large documents and multiple references, it is unreliable at the sentence or sub-sentence levels, and with a single reference. In this paper, we propose new variants of BLEU which address these limitations, resulting in a more flexible metric which is not only more reliable, but also allows for more accurate discriminative training. Our best metric has better correlation with human judgements than standard BLEU, despite using a simpler formulation. Moreover, these improvements carry over to a system tuned for our new metric.", "title": "BLEU Deconstructed: Designing a Better MT Evaluation Metric"}, "7b4f3d0e4e2486a8d5d3f8e00549cf9a117bf88f": {"paper_id": "7b4f3d0e4e2486a8d5d3f8e00549cf9a117bf88f", "abstract": "Many machine learning tasks can be expressed as the transformation\u2014or transduction\u2014of input sequences into output sequences: speech recognition, machine translation, protein secondary structure prediction and text-to-speech to name but a few. One of the key challenges in sequence transduction is learning to represent both the input and output sequences in a way that is invariant to sequential distortions such as shrinking, stretching and translating. Recurrent neural networks (RNNs) are a powerful sequence learning architecture that has proven capable of learning such representations. However RNNs traditionally require a pre-defined alignment between the input and output sequences to perform transduction. This is a severe limitation since finding the alignment is the most difficult aspect of many sequence transduction problems. Indeed, even determining the length of the output sequence is often challenging. This paper introduces an end-to-end, probabilistic sequence transduction system, based entirely on RNNs, that returns a distribution over output sequences of all possible lengths and alignments for any input sequence. Experimental results are provided on the TIMIT speech corpus.", "title": "Sequence Transduction with Recurrent Neural Networks"}, "396aabd694da04cdb846cb724ca9f866f345cbd5": {"paper_id": "396aabd694da04cdb846cb724ca9f866f345cbd5", "abstract": "We explore efficient domain adaptation for the task of statistical machine translation based on extracting sentences from a large generaldomain parallel corpus that are most relevant to the target domain. These sentences may be selected with simple cross-entropy based methods, of which we present three. As these sentences are not themselves identical to the in-domain data, we call them pseudo in-domain subcorpora. These subcorpora \u2013 1% the size of the original \u2013 can then used to train small domain-adapted Statistical Machine Translation (SMT) systems which outperform systems trained on the entire corpus. Performance is further improved when we use these domain-adapted models in combination with a true in-domain model. The results show that more training data is not always better, and that best results are attained via proper domain-relevant data selection, as well as combining inand general-domain systems during decoding.", "title": "Domain Adaptation via Pseudo In-Domain Data Selection"}, "8729441d734782c3ed532a7d2d9611b438c0a09a": {"paper_id": "8729441d734782c3ed532a7d2d9611b438c0a09a", "abstract": "We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.", "title": "ADADELTA: An Adaptive Learning Rate Method"}, "4b9b7eed30feee37db3452b74503d0db9f163074": {"paper_id": "4b9b7eed30feee37db3452b74503d0db9f163074", "abstract": "We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units. The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model. Through various experiments, we show first that our models obtain a perplexity with respect to gold translations that is > 43% lower than that of stateof-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and meaning of the source sentence despite lacking alignments. Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations.", "title": "Recurrent Continuous Translation Models"}, "0b544dfe355a5070b60986319a3f51fb45d1348e": {"paper_id": "0b544dfe355a5070b60986319a3f51fb45d1348e", "abstract": "In this paper, we propose a novel neural network model called RNN Encoder\u2013 Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder\u2013Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.", "title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation"}, "2a94c84383ee3de5e6211d43d16e7de387f68878": {"paper_id": "2a94c84383ee3de5e6211d43d16e7de387f68878", "abstract": "Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But pyramid representations have been avoided in recent object detectors that are based on deep convolutional networks, partially because they are slow to compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.", "title": "Feature Pyramid Networks for Object Detection"}, "04fa47f1d3983bacfea1e3c838cf868f9b73dc58": {"paper_id": "04fa47f1d3983bacfea1e3c838cf868f9b73dc58", "abstract": "In this paper, we present a novel face detection approach based on a convolutional neural architecture, designed to robustly detect highly variable face patterns, rotated up to /spl plusmn/20 degrees in image plane and turned up to /spl plusmn/60 degrees, in complex real world images. The proposed system automatically synthesizes simple problem-specific feature extractors from a training set of face and nonface patterns, without making any assumptions or using any hand-made design concerning the features to extract or the areas of the face pattern to analyze. The face detection procedure acts like a pipeline of simple convolution and subsampling modules that treat the raw input image as a whole. We therefore show that an efficient face detection system does not require any costly local preprocessing before classification of image areas. The proposed scheme provides very high detection rate with a particularly low level of false positives, demonstrated on difficult test sets, without requiring the use of multiple networks for handling difficult cases. We present extensive experimental results illustrating the efficiency of the proposed approach on difficult test sets and including an in-depth sensitivity analysis with respect to the degrees of variability of the face patterns.", "title": "Convolutional face finder: a neural architecture for fast and robust face detection"}, "20f0357688876fa4662f806f985779dce6e24f3c": {"paper_id": "20f0357688876fa4662f806f985779dce6e24f3c", "abstract": "The artificial neural networks that are used to recognize shapes typically use one or more layers of learned feature detectors that produce scalar outputs. By contrast, the computer vision community uses complicated, hand-engineered features, like SIFT [6], that produce a whole vector of outputs including an explicit representation of the pose of the feature. We show how neural networks can be used to learn features that output a whole vector of instantiation parameters and we argue that this is a much more promising way of dealing with variations in position, orientation, scale and lighting than the methods currently employed in the neural networks community. It is also more promising than the handengineered features currently used in computer vision because it provides an efficient way of adapting the features to the domain.", "title": "Transforming Auto-Encoders"}, "398c296d0cc7f9d180f84969f8937e6d3a413796": {"paper_id": "398c296d0cc7f9d180f84969f8937e6d3a413796", "abstract": "Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible, wide and deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive MNIST handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks.", "title": "Multi-column deep neural networks for image classification"}, "d22c9727d517750822abaffc6f361202dd6b8f12": {"paper_id": "d22c9727d517750822abaffc6f361202dd6b8f12", "abstract": "A neural network algorithm-based system that reads handwritten ZIP codes appearing on real US mail is described. The system uses a recognition-based segmenter, that is a hybrid of connected-components analysis (CCA), vertical cuts, and a neural network recognizer. Connected components that are single digits are handled by CCA. CCs that are combined or dissected digits are handled by the vertical-cut segmenter. The four main stages of processing are preprocessing, in which noise is removed and the digits are deslanted, CCA segmentation and recognition, vertical-cut-point estimation and segmentation, and directly lookup. The system was trained and tested on approximately 10000 images, five- and nine-digit ZIP code fields taken from real mail.<<ETX>>", "title": "Reading handwritten digits: a ZIP code recognition system"}, "9ab0de951cc9cdf16887b1f841f8da6affc9c0de": {"paper_id": "9ab0de951cc9cdf16887b1f841f8da6affc9c0de", "abstract": "We apply Convolutional Networks (ConvNets) to the task of traffic sign classification as part of the GTSRB competition. ConvNets are biologically-inspired multi-stage architectures that automatically learn hierarchies of invariant features. While many popular vision approaches use hand-crafted features such as HOG or SIFT, ConvNets learn features at every level from data that are tuned to the task at hand. The traditional ConvNet architecture was modified by feeding 1st stage features in addition to 2nd stage features to the classifier. The system yielded the 2nd-best accuracy of 98.97% during phase I of the competition (the best entry obtained 98.98%), above the human performance of 98.81%, using 32\u00d732 color input images. Experiments conducted after phase 1 produced a new record of 99.17% by increasing the network capacity, and by using greyscale images instead of color. Interestingly, random features still yielded competitive results (97.33%).", "title": "Traffic sign recognition with multi-scale Convolutional Networks"}, "18fe8a42cb3b7ce14e29a11faa3d7cc45f1be22f": {"paper_id": "18fe8a42cb3b7ce14e29a11faa3d7cc45f1be22f", "abstract": "We propose a category-independent method to produce a bag of regions and rank them, such that top-ranked regions are likely to be good segmentations of different objects. Our key objectives are completeness and diversity: Every object should have at least one good proposed region, and a diverse set should be top-ranked. Our approach is to generate a set of segmentations by performing graph cuts based on a seed region and a learned affinity function. Then, the regions are ranked using structured learning based on various cues. Our experiments on the Berkeley Segmentation Data Set and Pascal VOC 2011 demonstrate our ability to find most objects within a small bag of proposed regions.", "title": "Category-Independent Object Proposals with Diverse Ranking"}, "28f9cf85ebbff86207e1f6067880bb23daff0878": {"paper_id": "28f9cf85ebbff86207e1f6067880bb23daff0878", "abstract": "Generic object detection is the challenging task of proposing windows that localize all the objects in an image, regardless of their classes. Such detectors have recently been shown to benefit many applications such as speeding-up class-specific object detection, weakly supervised learning of object detectors and object discovery. In this paper, we introduce a novel and very efficient method for generic object detection based on a randomized version of Prim's algorithm. Using the connectivity graph of an image's super pixels, with weights modelling the probability that neighbouring super pixels belong to the same object, the algorithm generates random partial spanning trees with large expected sum of edge weights. Object localizations are proposed as bounding-boxes of those partial trees. Our method has several benefits compared to the state-of-the-art. Thanks to the efficiency of Prim's algorithm, it samples proposals very quickly: 1000 proposals are obtained in about 0.7s. With proposals bound to super pixel boundaries yet diversified by randomization, it yields very high detection rates and windows that tightly fit objects. In extensive experiments on the challenging PASCAL VOC 2007 and 2012 and SUN2012 benchmark datasets, we show that our method improves over state-of-the-art competitors for a wide range of evaluation scenarios.", "title": "Prime Object Proposals with Randomized Prim's Algorithm"}, "84dbf284310d71bd2ab83c975757a41584cbbbbd": {"paper_id": "84dbf284310d71bd2ab83c975757a41584cbbbbd", "abstract": "Data mining aims to extract previously unknown patterns or substructures from large databases. In statistics, this is what methods of robust estimation and outlier detection were constructed for, see e.g. Rousseeuw and Leroy (1987). Here we will focus on least trimmed squares (LTS) regression, which is based on the subset of h cases (out of n) whose least squares fit possesses the smallest sum of squared residuals. The coverage h may be set between n/2 and n. The computation time of existing LTS algorithms grows too much with the size of the data set, precluding their use for data mining. In this paper we develop a new algorithm called FAST-LTS. The basic ideas are an inequality involving order statistics and sums of squared residuals, and techniques which we call \u2018selective iteration\u2019 and \u2018nested extensions\u2019. We also use an intercept adjustment technique to improve the precision. For small data sets FAST-LTS typically finds the exact LTS, whereas for larger data sets it gives more accurate results than existing algorithms for LTS and is faster by orders of magnitude. This allows us to apply FAST-LTS to large databases.", "title": "Computing LTS Regression for Large Data Sets"}, "7fc8100e73591ce8af1d553d4296ec38f939c248": {"paper_id": "7fc8100e73591ce8af1d553d4296ec38f939c248", "abstract": "Conceptual clustering is an important way of summarizing and explaining data. However, the recent formulation of this paradigm has allowed little exploration of conceptual clustering as a means of improving performance. Furthermore, previous work in conceptual clustering has not explicitly dealt with constraints imposed by real world environments. This article presents COBWEB, a conceptual clustering system that organizes data so as to maximize inference ability. Additionally, COBWEB is incremental and computationally economical, and thus can be flexibly applied in a variety of domains.", "title": "Knowledge Acquisition Via Incremental Conceptual Clustering"}, "25ebaeb46b4fb3ee1a9d6769832c97cdece00865": {"paper_id": "25ebaeb46b4fb3ee1a9d6769832c97cdece00865", "abstract": "Description: The Wiley-Interscience Paperback Series consists of selected books that have been made more accessible to consumers in an effort to increase global appeal and general circulation. With these new unabridged softcover volumes, Wiley hopes to extend the lives of these works by making them available to future generations of statisticians, mathematicians, and scientists. \"Cluster analysis is the increasingly important and practical subject of finding groupings in data. The authors set out to write a book for the user who does not necessarily have an extensive background in mathematics. They succeed very well.\" \u2014Mathematical Reviews \"Finding Groups in Data [is] a clear, readable, and interesting presentation of a small number of clustering methods. In addition, the book introduced some interesting innovations of applied value to clustering literature.\" \u2014Journal of Classification \"This is a very good, easy-to-read, and practical book. It has many nice features and is highly recommended for students and practitioners in various fields of study.\" \u2014Technometrics An introduction to the practical application of cluster analysis, this text presents a selection of methods that together can deal with most applications. These methods are chosen for their robustness, consistency, and general applicability. This book discusses various types of data, including interval-scaled and binary variables as well as similarity data, and explains how these can be transformed prior to clustering.", "title": "Finding Groups in Data: An Introduction to Cluster Analysis"}, "64ce05fdd0d2953abf83faf39d68d44014a4bb26": {"paper_id": "64ce05fdd0d2953abf83faf39d68d44014a4bb26", "abstract": "Hierarchical clustering is a common method used to determine clusters of similar data points in multidimensional spaces. O(n*) algorithms are known for this problem [3,4,11,19]. This paper reviews important results for sequential algorithms and describes previous work on parallel algorithms for hierarchical clustering. Parallel algorithms to perform hierarchical clustering using several distance metrics are then described. Optimal PRAM algorithms using n/log n processors are given for the average link, complete link, centroid, median, and minimum variance metrics. Optimal butterfly and tree algorithms using n/log n processors are given for the centroid, median, and minimum variance metrics. Optimal asymptotic speedups are achieved for the best practical algorithm to perform clustering using the single link metric on a n/log n processor PRAh4, butterfly, or tree.", "title": "Parallel Algorithms for Hierarchical Clustering"}, "6d61b6bff88f7263a8e87a8474a3a276a03458b0": {"paper_id": "6d61b6bff88f7263a8e87a8474a3a276a03458b0", "abstract": "We are given a large population database that contains information about population instances. The population is known to comprise of m groups, but the population instances are not labeled with the group identi cation. Also given is a population sample (much smaller than the population but representative of it) in which the group labels of the instances are known. We present an interval classi er (IC) which generates a classi cation function for each group that can be used to e ciently retrieve all instances of the specied group from the population database. To allow IC to be embedded in interactive loops to answer adhoc queries about attributes with missing values, IC has been designed to be e cient in the generation of classi cation functions. Preliminary experimental results indicate that IC not only has retrieval and classi er generation e ciency advantages, but also compares favorably in the classi cation accuracy with current tree classi ers, such as ID3, which were primarily designed for minimizing classi cation errors. We also describe some new applications that arise from encapsulating the classi cation capability in database systems and discuss extensions to IC for it to be used in these new application domains. Current address: Computer Science Department, Rutgers University, NJ 08903 Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Very Large Data Base Endowment. To copy otherwise, or to republish, requires a fee and/or special permission from the Endowment. Proceedings of the 18th VLDB Conference Vancouver, British Columbia, Canada 1992", "title": "An Interval Classifier for Database Mining Applications"}, "c20f4f5268e9ab7197af5319a3608e49ef4e34c1": {"paper_id": "c20f4f5268e9ab7197af5319a3608e49ef4e34c1", "abstract": "In this paper, we develop a new framework for sensing and recovering structured signals. In contrast to compressive sensing (CS) systems that employ linear measurements, sparse representations, and computationally complex convex/greedy algorithms, we introduce a deep learning framework that supports both linear and mildly nonlinear measurements, that learns a structured representation from training data, and that efficiently computes a signal estimate. In particular, we apply a stacked denoising autoencoder (SDA), as an unsupervised feature learner. SDA enables us to capture statistical dependencies between the different elements of certain signals and improve signal recovery performance as compared to the CS approach.", "title": "A deep learning approach to structured signal recovery"}, "4c9cec89a2c9c8173ee53ab4cda2c021421eb7a5": {"paper_id": "4c9cec89a2c9c8173ee53ab4cda2c021421eb7a5", "abstract": "Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a structural similarity index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000. A MATLAB implementation of the proposed algorithm is available online at http://www.cns.nyu.edu//spl sim/lcv/ssim/.", "title": "Image quality assessment: from error visibility to structural similarity"}, "6b7cbaa346b8f99b2d6679f24056aec0e6cca4e0": {"paper_id": "6b7cbaa346b8f99b2d6679f24056aec0e6cca4e0", "abstract": "Recently there have been significant advances in image up scaling or image super-resolution based on a dictionary of low and high resolution exemplars. The running time of the methods is often ignored despite the fact that it is a critical factor for real applications. This paper proposes fast super-resolution methods while making no compromise on quality. First, we support the use of sparse learned dictionaries in combination with neighbor embedding methods. In this case, the nearest neighbors are computed using the correlation with the dictionary atoms rather than the Euclidean distance. Moreover, we show that most of the current approaches reach top performance for the right parameters. Second, we show that using global collaborative coding has considerable speed advantages, reducing the super-resolution mapping to a precomputed projective matrix. Third, we propose the anchored neighborhood regression. That is to anchor the neighborhood embedding of a low resolution patch to the nearest atom in the dictionary and to precompute the corresponding embedding matrix. These proposals are contrasted with current state-of-the-art methods on standard images. We obtain similar or improved quality and one or two orders of magnitude speed improvements.", "title": "Anchored Neighborhood Regression for Fast Example-Based Super-Resolution"}, "6434b7610aa9e7c37fce2258ab459fa99a631748": {"paper_id": "6434b7610aa9e7c37fce2258ab459fa99a631748", "abstract": "In this paper, we propose a novel method for solving single-image super-resolution problems. Given a low-resolution image as input, we recover its high-resolution counterpart using a set of training examples. While this formulation resembles other learning-based methods for super-resolution, our method has been inspired by recent manifold teaming methods, particularly locally linear embedding (LLE). Specifically, small image patches in the lowand high-resolution images form manifolds with similar local geometry in two distinct feature spaces. As in LLE, local geometry is characterized by how a feature vector corresponding to a patch can be reconstructed by its neighbors in the feature space. Besides using the training image pairs to estimate the high-resolution embedding, we also enforce local compatibility and smoothness constraints between patches in the target high-resolution image through overlapping. Experiments show that our method is very flexible and gives good empirical results.", "title": "Super-resolution through neighbor embedding"}, "81d7a3b7a250045cbe65cdd0273e69f8e5bb4763": {"paper_id": "81d7a3b7a250045cbe65cdd0273e69f8e5bb4763", "abstract": "This paper describes a single-image super-resolution (SR) algorithm based on nonnegative neighbor embedding. It belongs to the family of single-image example-based SR algorithms, since it uses a dictionary of low resolution (LR) and high resolution (HR) trained patch pairs to infer the unknown HR details. Each LR feature vector in the input image is expressed as the weighted combination of its K nearest neighbors in the dictionary; the corresponding HR feature vector is reconstructed under the assumption that the local LR embedding is preserved. Three key aspects are introduced in order to build a low-complexity competitive algorithm: (i) a compact but efficient representation of the patches (feature representation) (ii) an accurate estimation of the patches by their nearest neighbors (weight computation) (iii) a compact and already built (therefore external) dictionary, which allows a one-step upscaling. The neighbor embedding SR algorithm so designed is shown to give good visual results, comparable to other state-of-the-art methods, while presenting an appreciable reduction of the computational time.", "title": "Low-Complexity Single-Image Super-Resolution based on Nonnegative Neighbor Embedding"}, "1e3ea7a70c9d8984fe8f0d0988b672e411162d43": {"paper_id": "1e3ea7a70c9d8984fe8f0d0988b672e411162d43", "abstract": "In this paper we use sparse-representation modeling for the single image scale-up problem. The goal is to recover an original image from its blurred and down-scaled noisy version. Since this problem is highly ill-posed, a prior is needed in order to solve it in a robust fashion. The literature offers various ways to address this problem, ranging from simple linear space-invariant interpolation schemes (e.g., bicubic interpolation), to spatially adaptive and non-linear filters of various sorts. In this paper, we embark from a recently-proposed algorithm by Yang et. al. [1, 2], and similarly assume a local Sparse-Land model on image patches, thus stabilizing the problem. We introduce several important modifications to the above-mentioned solution, and show that these lead to improved results. These modifications include a major simplification of the overall process both in terms of the computational complexity and the algorithm architecture, using a different training approach for the dictionary-pair, and operating without a training-set by boot-strapping the scale-up task from the given low-resolution image. We demonstrate the results on true images, showing both visual and PSNR improvements.", "title": "On Single Image Scale-Up Using Sparse-Representations"}, "1354c00d5b2bc4b64c0dc63d1d94e8724d3a31b5": {"paper_id": "1354c00d5b2bc4b64c0dc63d1d94e8724d3a31b5", "abstract": "Finite-state machines have been used in various domains of natural language processing. We consider here the use of a type of transducers that supports very efficient programs: sequential transducers. We recall classical theorems and give new ones characterizing sequential string-tostring transducers. Transducers that output weights also play an important role in language and speech processing. We give a specific study of string-to-weight transducers, including algorithms for determinizing and minimizing these transducers very efficiently, and characterizations of the transducers admitting determinization and the corresponding algorithms. Some applications of these algorithms in speech recognition are described and illustrated.", "title": "Finite-State Transducers in Language and Speech Processing"}, "10e70e16e5a68d52fa2c9d0a452db9ed2f9403aa": {"paper_id": "10e70e16e5a68d52fa2c9d0a452db9ed2f9403aa", "abstract": "An elementary proof of a basic uncertainty principle concerning pairs of representations of RN vectors in different orthonormal bases is provided. The result, slightly stronger than stated before, has a direct impact on the uniqueness property of the sparse representation of such vectors using pairs of orthonormal bases as overcomplete dictionaries. The main contribution in this paper is the improvement of an important result due to Donoho and Huo concerning the replacement of the l0 optimization problem by a linear programming minimization when searching for the unique sparse representation.", "title": "A generalized uncertainty principle and sparse representation in pairs of bases"}, "e477810eb2fef42f297116fe518c8efb4b701348": {"paper_id": "e477810eb2fef42f297116fe518c8efb4b701348", "abstract": "The most obvious method for determining the distortion of telegraph signals is to calculate the transients of the telegraph system. This method has been treated by various writers, and solutions are available for telegraph lines with simple terminal conditions. It is well known that the extension of the same methods to more complicated terminal conditions, which represent the usual terminal apparatus, leads to great difficulties. The present paper attacks the same problem from the alternative standpoint of the steady-state characteristics of the system. This method has the advantage over the method of transients that the complication of the circuit which results from the use of terminal apparatus does not complicate the calculations materially. This method of treatment necessitates expressing the criteria of distortionless transmission in terms of the steady-state characteristics. Accordingly, a considerable portion of the paper describes and illustrates a method for making this translation. A discussion is given of the minimum frequency range required for transmission at a given speed of signaling. In the case of carrier telegraphy, this discussion includes a comparison of single-sideband and double-sideband transmission. A number of incidental topics is also discussed.", "title": "Certain Topics in Telegraph Transmission Theory"}, "0cd2285d00cc1337cc95ab120e558707b197862a": {"paper_id": "0cd2285d00cc1337cc95ab120e558707b197862a", "abstract": "T HE recent development of various methods of modulation such as PCM and PPM which exchange bandwidth for signal-to-noise ratio has intensified the interest in a general theory of communication. A basis for such a theory is contained in the important papers of Nyquist 1 and Hartley2 on this subject. In the present paper we will extend the theory to include a number of new factors, in particular the effect of noise in the channel, and the savings possible due to the statistical structure of the original message and due to the nature of the final destination of the information. The fundamental problem of communication is that of reproducing at one point either exactly or approximately a message selected at another point. Frequently the messages have meaning; that is they refer to or are correlated according to some system with certain physical or conceptual entities. These semantic aspects of communication are irrelevant to the engineering problem. The significant aspect is that the actual message is one selected from a set of possible messages. The system must be designed to operate for each possible selection, not just the one which will actually be chosen since this is unknown at the time of design. If the number of messages in the set is finite then this number or any monotonic function of this number can be regarded as a measure of the information produced when one message is chosen from the set, all choices being equally likely. As was pointed out by Hartley the most natural choice is the logarithmic function. Although this definition must be generalized considerably when we consider the influence of the statistics of the message and when we have a continuous range of messages, we will in all cases use an essentially logarithmic measure. The logarithmic measure is more convenient for various reasons:", "title": "The mathematical theory of communication"}, "4b9c85a32ccebb851b94f6da17307461e50345a2": {"paper_id": "4b9c85a32ccebb851b94f6da17307461e50345a2", "abstract": "Learning good image priors is of utmost importance for the study of vision, computer vision and image processing applications. Learning priors and optimizing over whole images can lead to tremendous computational challenges. In contrast, when we work with small image patches, it is possible to learn priors and perform patch restoration very efficiently. This raises three questions - do priors that give high likelihood to the data also lead to good performance in restoration? Can we use such patch based priors to restore a full image? Can we learn better patch priors? In this work we answer these questions. We compare the likelihood of several patch models and show that priors that give high likelihood to data perform better in patch restoration. Motivated by this result, we propose a generic framework which allows for whole image restoration using any patch based prior for which a MAP (or approximate MAP) estimate can be calculated. We show how to derive an appropriate cost function, how to optimize it and how to use it to restore whole images. Finally, we present a generic, surprisingly simple Gaussian Mixture prior, learned from a set of natural images. When used with the proposed framework, this Gaussian Mixture Model outperforms all other generic prior methods for image denoising, deblurring and inpainting.", "title": "From learning models of natural image patches to whole image restoration"}, "2d8e268ae3d99e519c01a5338dd8974606e2c142": {"paper_id": "2d8e268ae3d99e519c01a5338dd8974606e2c142", "abstract": "Proceedings of the 1998 IEEE International Conference on Computer Vision, Bombay, India Bilateral filtering smooths images while preserving edges, by means of a nonlinear combination of nearby image values. The method is noniterative, local, and simple. It combines gray levels or colors based on both their geometric closeness and their photometric similarity, and prefers near values to distant values in both domain and range. In contrast with filters that operate on the three bands of a color image separately, a bilateral filter can enforce the perceptual metric underlying the CIE-Lab color space, and smooth colors and preserve edges in a way that is tuned to human perception. Also, in contrast with standard filtering, bilateral filtering produces no phantom colors along edges in color images, and reduces phantom colors where they appear in the original image.", "title": "Bilateral Filtering for Gray and Color Images"}, "6dba6e15051ecc42997be8eb6dbc8dc5ad337085": {"paper_id": "6dba6e15051ecc42997be8eb6dbc8dc5ad337085", "abstract": "We present an approach to low-level vision that combines two main ideas: the use of convolutional networks as an image processing architecture and an unsupervised learning procedure that synthesizes training samples from specific noise models. We demonstrate this approach on the challenging problem of natural image denoising. Using a test set with a hundred natural images, we find that convolutional networks provide comparable and in some cases superior performance to state of the art wavelet and Markov random field (MRF) methods. Moreover, we find that a convolutional network offers similar performance in the blind denoising setting as compared to other techniques in the non-blind setting. We also show how convolutional networks are mathematically related to MRF approaches by presenting a mean field theory for an MRF specially designed for image denoising. Although these approaches are related, convolutional networks avoid computational difficulties in MRF approaches that arise from probabilistic learning and inference. This makes it possible to learn image processing architectures that have a high degree of representational power (we train models with over 15,000 parameters), but whose computational expense is significantly less than that associated with inference in MRF approaches with even hundreds of parameters.", "title": "Natural Image Denoising with Convolutional Networks"}, "364bcbe6e3cb439d1ca694b259c2066d3769c860": {"paper_id": "364bcbe6e3cb439d1ca694b259c2066d3769c860", "abstract": "We study the notion of Compressed Sensing (CS) as put forward in [14] and related work [20, 3, 4]. The basic idea behind CS is that a signal or image, unknown but supposed to be compressible by a known transform, (eg. wavelet or Fourier), can be subjected to fewer measurements than the nominal number of pixels, and yet be accurately reconstructed. The samples are nonadaptive and measure \u2018random\u2019 linear combinations of the transform coefficients. Approximate reconstruction is obtained by solving for the transform coefficients consistent with measured data and having the smallest possible ` norm. We perform a series of numerical experiments which validate in general terms the basic idea proposed in [14, 3, 5], in the favorable case where the transform coefficients are sparse in the strong sense that the vast majority are zero. We then consider a range of less-favorable cases, in which the object has all coefficients nonzero, but the coefficients obey an ` bound, for some p \u2208 (0, 1]. These experiments show that the basic inequalities behind the CS method seem to involve reasonable constants. We next consider synthetic examples modelling problems in spectroscopy and image processing, and note that reconstructions from CS are often visually \u201cnoisy\u201d . We post-process using translation-invariant de-noising, and find the visual appearance considerably improved. We also consider a multiscale deployment of compressed sensing, in which various scales are segregated and CS applied separately to each; this gives much better quality reconstructions than a literal deployment of the CS methodology. We also report that several workable families of \u2018random\u2019 linear combinations all behave equivalently, including random spherical, random signs, partial Fourier and partial Hadamard. These results show that, when appropriately deployed in a favorable setting, the CS framework is able to save significantly over traditional sampling, and there are many useful extensions of the basic idea.", "title": "Extensions of compressed sensing"}, "19d9fe2395ebfa5dc185a7abbf7c24cac3de3644": {"paper_id": "19d9fe2395ebfa5dc185a7abbf7c24cac3de3644", "abstract": "This paper proposes an image dehazing model built with a convolutional neural network (CNN), called All-in-One Dehazing Network (AOD-Net). It is designed based on a re-formulated atmospheric scattering model. Instead of estimating the transmission matrix and the atmospheric light separately as most previous models did, AOD-Net directly generates the clean image through a light-weight CNN. Such a novel end-to-end design makes it easy to embed AOD-Net into other deep models, e.g., Faster R-CNN, for improving high-level tasks on hazy images. Experimental results on both synthesized and natural hazy image datasets demonstrate our superior performance than the state-of-the-art in terms of PSNR, SSIM and the subjective visual quality. Furthermore, when concatenating AOD-Net with Faster R-CNN, we witness a large improvement of the object detection performance on hazy images.", "title": "AOD-Net: All-in-One Dehazing Network"}, "5808f5285bc60a73bc240621ad0fce606867ebc1": {"paper_id": "5808f5285bc60a73bc240621ad0fce606867ebc1", "abstract": "Multi-agent predictive modeling is an essential step for understanding physical, social and team-play systems. Recently, Interaction Networks (INs) were proposed for the task of modeling multi-agent physical systems. One of the drawbacks of INs is scaling with the number of interactions in the system (typically quadratic or higher order in the number of agents). In this paper we introduce VAIN, a novel attentional architecture for multi-agent predictive modeling that scales linearly with the number of agents. We show that VAIN is effective for multiagent predictive modeling. Our method is evaluated on tasks from challenging multi-agent prediction domains: chess and soccer, and outperforms competing multi-agent approaches.", "title": "VAIN: Attentional Multi-agent Predictive Modeling"}, "922838dd98d599d1d229cc73896d55e7a769aa7c": {"paper_id": "922838dd98d599d1d229cc73896d55e7a769aa7c", "abstract": "Most modern face recognition systems rely on a feature representation given by a hand-crafted image descriptor, such as Local Binary Patterns (LBP), and achieve improved performance by combining several such representations. In this paper, we propose deep learning as a natural source for obtaining additional, complementary representations. To learn features in high-resolution images, we make use of convolutional deep belief networks. Moreover, to take advantage of global structure in an object class, we develop local convolutional restricted Boltzmann machines, a novel convolutional learning model that exploits the global structure by not assuming stationarity of features across the image, while maintaining scalability and robustness to small misalignments. We also present a novel application of deep learning to descriptors other than pixel intensity values, such as LBP. In addition, we compare performance of networks trained using unsupervised learning against networks with random filters, and empirically show that learning weights not only is necessary for obtaining good multilayer representations, but also provides robustness to the choice of the network architecture parameters. Finally, we show that a recognition system using only representations obtained from deep learning can achieve comparable accuracy with a system using a combination of hand-crafted image descriptors. Moreover, by combining these representations, we achieve state-of-the-art results on a real-world face verification database.", "title": "Learning hierarchical representations for face verification with convolutional deep belief networks"}, "3619a9b46ad4779d0a63b20f7a6a8d3d49530339": {"paper_id": "3619a9b46ad4779d0a63b20f7a6a8d3d49530339", "abstract": "Overview. The face verification and recognition domain has mostly been dominated by carefully designed representations based on features computed around numerous facial landmarks [1, 3]. On the other hand, for image classification, image representations are capable of capturing discriminative image information without any domain-specific knowledge by using densely computed features, coupled with a non-linear encoding. For instance, the high-dimensional Fisher vector encoding [6] of SIFT features [5] achieves state-of-the-art performance on several image classification benchmarks [2]. In this paper, we address this discrepancy and make two contributions. First, we show that Fisher vector encoding of dense SIFT, an off-the-shelf image representation, achieves state-of-the-art face verification performance on the challenging \u201cLabeled Faces in the Wild\u201d (LFW) benchmark. Second, we show that the high-dimensional face representation using Fisher vector encoding is amenable to discriminative dimensionality reduction. The resulting compact descriptor has equal or better recognition accuracy and is very well suited to large-scale face recognition tasks. The overview of our face descriptor computation pipeline is shown in Fig. 2. The learnt dimensionality reduction model is visualised in Fig. 3. It is able to automatically extract the discriminative regions of the face.", "title": "Fisher Vector Faces in the Wild"}, "a0d6390dd28d802152f207940c7716fe5fae8760": {"paper_id": "a0d6390dd28d802152f207940c7716fe5fae8760", "abstract": "In this paper, we revisit the classical Bayesian face recognition method by Baback Moghaddam et al. and propose a new joint formulation. The classical Bayesian method models the appearance difference between two faces. We observe that this \u201cdifference\u201d formulation may reduce the separability between classes. Instead, we model two faces jointly with an appropriate prior on the face representation. Our joint formulation leads to an EM-like model learning at the training time and an efficient, closed-formed computation at the test time. On extensive experimental evaluations, our method is superior to the classical Bayesian face and many other supervised approaches. Our method achieved 92.4% test accuracy on the challenging Labeled Face in Wild (LFW) dataset. Comparing with current best commercial system, we reduced the error rate by 10%.", "title": "Bayesian Face Revisited: A Joint Formulation"}, "370b5757a5379b15e30d619e4d3fb9e8e13f3256": {"paper_id": "370b5757a5379b15e30d619e4d3fb9e8e13f3256", "abstract": "Face recognition has benefitted greatly from the many databases that have been produced to study it. Most of these databases have been created under controlled conditions t o facilitate the study of specific parameters on the face recognition problem. These parameters include such variables as position, pose, lighting, expression, background, camera quality, occlusion , age, and gender. While there are many applications for face recognition technology in which one can control the parameters of image acquisition, there are also many applications in which the practitioner has little or no control over such parameters. This database is provided as an aid in studying the latter, unconstrained, face recognition problem. The database represents an initial attempt to provide a set of labeled face photographs spanning the range of conditions typically encountered by people in their everyday lives. The database exhibits \u201cnatural\u201d variability in pose, lighting, focus, resolution, facial expression, age, gender, race, acces sories, make-up, occlusions, background, and photographic quality. Despite this variability, the images in the database are presented in a simple and consistent format for maximum ease of use. In addition to describing the details of the database and its acquisition, we provide specific experimental paradigms for which the database is suitable. This is done in an effort to make research performed with the database as consistent and comparable as possible.", "title": "Labeled Faces in the Wild: A Database forStudying Face Recognition in Unconstrained Environments"}, "3607afdb204de9a5a9300ae98aa4635d9effcda2": {"paper_id": "3607afdb204de9a5a9300ae98aa4635d9effcda2", "abstract": "This paper presents a novel and efficient facial image representation based on local binary pattern (LBP) texture features. The face image is divided into several regions from which the LBP feature distributions are extracted and concatenated into an enhanced feature vector to be used as a face descriptor. The performance of the proposed method is assessed in the face recognition problem under different challenges. Other applications and several extensions are also discussed", "title": "Face Description with Local Binary Patterns: Application to Face Recognition"}, "83174a52f38c80427e237446ccda79e2a9170742": {"paper_id": "83174a52f38c80427e237446ccda79e2a9170742", "abstract": "Rectifying neurons are more biologically plausible than logistic sigmoid neurons, which are themselves more biologically plausible than hyperbolic tangent neurons. However, the latter work better for training multi-layer neural networks than logistic sigmoid neurons. This paper shows that networks of rectifying neurons yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero and create sparse representations with true zeros which are remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extraunlabeled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised neural networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training.", "title": "Deep Sparse Rectifier Neural Networks"}, "05aba481e8a221df5d8775a3bb749001e7f2525e": {"paper_id": "05aba481e8a221df5d8775a3bb749001e7f2525e", "abstract": "We present a new family of subgradient methods that dynamica lly incorporate knowledge of the geometry of the data observed in earlier iterations to perfo rm more informative gradient-based learning. Metaphorically, the adaptation allows us to find n eedles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems fro m recent advances in stochastic optimization and online learning which employ proximal funct ions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adap tively modifying the proximal function, which significantly simplifies setting a learning rate nd results in regret guarantees that are provably as good as the best proximal function that can be cho sen in hindsight. We give several efficient algorithms for empirical risk minimization probl ems with common and important regularization functions and domain constraints. We experimen tally study our theoretical analysis and show that adaptive subgradient methods outperform state-o f-the-art, yet non-adaptive, subgradient algorithms.", "title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization"}, "408f31b7f1a1b9f4a13ea797b6253585ff361430": {"paper_id": "408f31b7f1a1b9f4a13ea797b6253585ff361430", "abstract": "We explore a network architecture introduced by Elman (1988) for predicting successive elements of a sequence. The network uses the pattern of activation over a set of hidden units from time-step t1, together with element t, to predict element t 1. When the network is trained with strings from a particular finite-state grammar, it can learn to be a perfect finite-state recognizer for the grammar. When the network has a minimal number of hidden units, patterns on the hidden units come to correspond to the nodes of the grammar, although this correspondence is not necessary for the network to act as a perfect finite-state recognizer. We explore the conditions under which the network can carry information about distant sequential contingencies across intervening elements. Such information is maintained with relative ease if it is relevant at each intermediate step; it tends to be lost when intervening elements do not depend on it. At first glance this may suggest that such networks are not relevant to natural language, in which dependencies may span indefinite distances. However, embeddings in natural language are not completely independent of earlier information. The final simulation shows that long distance sequential contingencies can be encoded by the network even if only subtle statistical properties of embedded strings depend on the early information.", "title": "Finite State Automata and Simple Recurrent Networks"}, "2245d7893aeebb06e9bbd990a1d21d4608951f5e": {"paper_id": "2245d7893aeebb06e9bbd990a1d21d4608951f5e", "abstract": null, "title": "Complexity of exact gradient computation algorithms for recurrent neural networks"}, "4eb943bf999ce49e5ebb629d7d0ffee44becff94": {"paper_id": "4eb943bf999ce49e5ebb629d7d0ffee44becff94", "abstract": "Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves; the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands; indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction.", "title": "Finding Structure in Time"}, "981fef7155742608b8b6673f4a9566158b76cd67": {"paper_id": "981fef7155742608b8b6673f4a9566158b76cd67", "abstract": null, "title": "ImageNet Large Scale Visual Recognition Challenge"}, "6cf8ec34a008031b018c8a3a4640a87f476d0925": {"paper_id": "6cf8ec34a008031b018c8a3a4640a87f476d0925", "abstract": "We introduce a new interactive system: a game that is fun and can be used to create valuable output. When people play the game they help determine the contents of images by providing meaningful labels for them. If the game is played as much as popular online games, we estimate that most images on the Web can be labeled in a few months. Having proper labels associated with each image on the Web would allow for more accurate image search, improve the accessibility of sites (by providing descriptions of images to visually impaired individuals), and help users block inappropriate images. Our system makes a significant contribution because of its valuable output and because of the way it addresses the image-labeling problem. Rather than using computer vision techniques, which don't work well enough, we encourage people to do the work by taking advantage of their desire to be entertained.", "title": "Labeling images with a computer game"}, "98c524def61762d9c033a64a75562e274b687763": {"paper_id": "98c524def61762d9c033a64a75562e274b687763", "abstract": "Recommender systems are changing from novelties used by a few E-commerce sites, to serious business tools that are re-shaping the world of E-commerce. Many of the largest commerce Web sites are already using recommender systems to help their customers find products to purchase. A recommender system learns from a customer and recommends products that she will find most valuable from among the available products. In this paper we present an explanation of how recommender systems help E-commerce sites increase sales, and analyze six sites that use recommender systems including several sites that use more than one recommender system. Based on the examples, we create a taxonomy of recommender systems, including the interfaces they present to customers, the technologies used to create the recommendations, and the inputs they need from customers. We conclude with ideas for new applications of recommender systems to E-commerce.", "title": "Recommender systems in e-commerce"}, "14b50dfa7f9c2612de1de99b220a5533b6ca4ba6": {"paper_id": "14b50dfa7f9c2612de1de99b220a5533b6ca4ba6", "abstract": "Multiagent systems are rapidly finding applications in a variety of domains, including robotics, distributed control, telecommunications, and economics. The complexity of many tasks arising in these domains makes them difficult to solve with preprogrammed agent behaviors. The agents must, instead, discover a solution on their own, using learning. A significant part of the research on multiagent learning concerns reinforcement learning techniques. This paper provides a comprehensive survey of multiagent reinforcement learning (MARL). A central issue in the field is the formal statement of the multiagent learning goal. Different viewpoints on this issue have led to the proposal of many different goals, among which two focal points can be distinguished: stability of the agents' learning dynamics, and adaptation to the changing behavior of the other agents. The MARL algorithms described in the literature aim---either explicitly or implicitly---at one of these two goals or at a combination of both, in a fully cooperative, fully competitive, or more general setting. A representative selection of these algorithms is discussed in detail in this paper, together with the specific issues that arise in each category. Additionally, the benefits and challenges of MARL are described along with some of the problem domains where the MARL techniques have been applied. Finally, an outlook for the field is provided.", "title": "A Comprehensive Survey of Multiagent Reinforcement Learning"}, "0ce541b876ef83382d51a189be095dcc8e4c8c1a": {"paper_id": "0ce541b876ef83382d51a189be095dcc8e4c8c1a", "abstract": "Opponent modeling is necessary in multi-agent settings where secondary agents with competing goals also adapt their strategies, yet it remains challenging because strategies interact with each other and change. Most previous work focuses on developing probabilistic models or parameterized strategies for specific applications. Inspired by the recent success of deep reinforcement learning, we present neural-based models that jointly learn a policy and the behavior of opponents. Instead of explicitly predicting the opponent\u2019s action, we encode observation of the opponents into a deep Q-Network (DQN); however, we retain explicit modeling (if desired) using multitasking. By using a Mixture-of-Experts architecture, our model automatically discovers different strategy patterns of opponents without extra supervision. We evaluate our models on a simulated soccer game and a popular trivia game, showing superior performance over DQN and its variants.", "title": "Opponent Modeling in Deep Reinforcement Learning"}, "36c009379f804993de22e8b4bc1d35996b324f24": {"paper_id": "36c009379f804993de22e8b4bc1d35996b324f24", "abstract": "There are two widely known issues with properly training Recurrent Neural Networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.", "title": "On the difficulty of training recurrent neural networks"}, "261a056f8b21918e8616a429b2df6e1d5d33be41": {"paper_id": "261a056f8b21918e8616a429b2df6e1d5d33be41", "abstract": "Many real-world sequence learning tasks require the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their outputs into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label unsegmented sequences directly, thereby solving both problems. An experiment on the TIMIT speech corpus demonstrates its advantages over both a baseline HMM and a hybrid HMM-RNN.", "title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks"}, "2446e8f2012f23176ff602be633c0ed2b956d66c": {"paper_id": "2446e8f2012f23176ff602be633c0ed2b956d66c", "abstract": "The context-independent deep belief network (DBN) hidden Markov model (HMM) hybrid architecture has recently achieved promising results for phone recognition. In this work, we propose a context-dependent DBN-HMM system that dramatically outperforms strong Gaussian mixture model (GMM)-HMM baselines on a challenging, large vocabulary, spontaneous speech recognition dataset from the Bing mobile voice search task. Our system achieves absolute sentence accuracy improvements of 5.8% and 9.2% over GMM-HMMs trained using the minimum phone error rate (MPE) and maximum likelihood (ML) criteria, respectively, which translate to relative error reductions of 16.0% and 23.2%.", "title": "Large vocabulary continuous speech recognition with context-dependent DBN-HMMS"}, "4b8089bc9b49f84de43acc2eb8900035f7d492b2": {"paper_id": "4b8089bc9b49f84de43acc2eb8900035f7d492b2", "abstract": "In the first part of this paper, a regular recurrent neural network (RNN) is extended to a bidirectional recurrent neural network (BRNN). The BRNN can be trained without the limitation of using input information just up to a preset future frame. This is accomplished by training it simultaneously in positive and negative time direction. Structure and training procedure of the proposed network are explained. In regression and classification experiments on artificial data, the proposed structure gives better results than other approaches. For real data, classification experiments for phonemes from the TIMIT database show the same tendency. In the second part of this paper, it is shown how the proposed bidirectional structure can be easily modified to allow efficient estimation of the conditional posterior probability of complete symbol sequences without making any explicit assumption about the shape of the distribution. For this part, experiments on real data are reported.", "title": "Bidirectional recurrent neural networks"}, "13b693924cb2a55927b23c24ed0cc04f767b9683": {"paper_id": "13b693924cb2a55927b23c24ed0cc04f767b9683", "abstract": "Recurrent Neural Networks (RNNs) are powerful models for sequential data that have the potential to learn long-term dependencies. However, they are computationally expensive to train and difficult to parallelize. Recent work has shown that normalizing intermediate representations of neural networks can significantly improve convergence rates in feed-forward neural networks [1]. In particular, batch normalization, which uses mini-batch statistics to standardize features, was shown to significantly reduce training time. In this paper, we investigate how batch normalization can be applied to RNNs. We show for both a speech recognition task and language modeling that the way we apply batch normalization leads to a faster convergence of the training criterion but doesn't seem to improve the generalization performance.", "title": "Batch normalized recurrent neural networks"}, "324fc9c732116fa81624faad07524039f193cede": {"paper_id": "324fc9c732116fa81624faad07524039f193cede", "abstract": "The Recurrent Neural Network (RNN) is an extremely powerful sequence model that is often difficult to train. The Long Short-Term Memory (LSTM) is a specific RNN architecture whose design makes it much easier to train. While wildly successful in practice, the LSTM\u2019s architecture appears to be ad-hoc so it is not clear if it is optimal, and the significance of its individual components is unclear. In this work, we aim to determine whether the LSTM architecture is optimal or whether much better architectures exist. We conducted a thorough architecture search where we evaluated over ten thousand different RNN architectures, and identified an architecture that outperforms both the LSTM and the recently-introduced Gated Recurrent Unit (GRU) on some but not all tasks. We found that adding a bias of 1 to the LSTM\u2019s forget gate closes the gap between the LSTM and the GRU.", "title": "An Empirical Exploration of Recurrent Network Architectures"}, "17aa78bd4331ef490f24bdd4d4cd21d22a18c09c": {"paper_id": "17aa78bd4331ef490f24bdd4d4cd21d22a18c09c", "abstract": "We consider the problem of building high-level, class-specific feature detectors from only unlabeled data. For example, is it possible to learn a face detector using only unlabeled images? To answer this, we train a deep sparse autoencoder on a large dataset of images (the model has 1 billion connections, the dataset has 10 million 200\u00d7200 pixel images downloaded from the Internet). We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three days. Contrary to what appears to be a widely-held intuition, our experimental results reveal that it is possible to train a face detector without having to label images as containing a face or not. Control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation. We also find that the same network is sensitive to other high-level concepts such as cat faces and human bodies. Starting from these learned features, we trained our network to recognize 22,000 object categories from ImageNet and achieve a leap of 70% relative improvement over the previous state-of-the-art.", "title": "Building high-level features using large scale unsupervised learning"}, "4ab5790326a3570825c23a217614922f8f2ac786": {"paper_id": "4ab5790326a3570825c23a217614922f8f2ac786", "abstract": "In this paper we examine the effect of receptive field designs on classification accuracy in the commonly adopted pipeline of image classification. While existing algorithms usually use manually defined spatial regions for pooling, we show that learning more adaptive receptive fields increases performance even with a significantly smaller codebook size at the coding layer. To learn the optimal pooling parameters, we adopt the idea of over-completeness by starting with a large number of receptive field candidates, and train a classifier with structured sparsity to only use a sparse subset of all the features. An efficient algorithm based on incremental feature selection and retraining is proposed for fast learning. With this method, we achieve the best published performance on the CIFAR-10 dataset, using a much lower dimensional feature space than previous methods.", "title": "Beyond spatial pyramids: Receptive field learning for pooled image features"}, "05cc38e249a6f642363b5a5cbd71cda67cea5893": {"paper_id": "05cc38e249a6f642363b5a5cbd71cda67cea5893", "abstract": "[1] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient based learning applied to document recognition. Proceeding of the IEEE, 1998. [2] H. Lee, R. Grosse, R. Ranganath, and A.Y. Ng. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In ICML, 2009. [3] M.A. Ranzato, K. Jarrett, K. Kavukcuoglu and Y. LeCun. What is the best multi-stage architecture for object recognition? In ICCV, 2009. [4] A. Hyvarinen and P. Hoyer. Topographic independent component analysis as a model of V1 organization and receptive fields. Neural Computation, 2001. [5] A. Hyvarinen, J. Hurri, and P. Hoyer. Natural Image Statistics. Springer, 2009. [6] K. Kavukcuoglu, M. Ranzato, R. Fergus, Y. LeCun. Learning invariant features through topographic filter maps . In CVPR, 2009. [7] K. Gregor, Y. LeCun. Emergence of Complex-Like Cells in a Temporal Product Network with Local Receptive Fields. ARXIV, 2010.", "title": "Tiled convolutional neural networks"}, "182015c5edff1956cbafbcb3e7bbe294aa54f9fc": {"paper_id": "182015c5edff1956cbafbcb3e7bbe294aa54f9fc", "abstract": "Recent deep learning and unsupervised feature learning systems that learn from unlabeled data have achieved high performance in benchmarks by using extremely large architectures with many features (hidden units) at each layer. Unfortunately, for such large architectures the number of parameters can grow quadratically in the width of the network, thus necessitating hand-coded \u201clocal receptive fields\u201d that limit the number of connections from lower level features to higher ones (e.g., based on spatial locality). In this paper we propose a fast method to choose these connections that may be incorporated into a wide variety of unsupervised training methods. Specifically, we choose local receptive fields that group together those low-level features that are most similar to each other according to a pairwise similarity metric. This approach allows us to harness the advantages of local receptive fields (such as improved scalability, and reduced data requirements) when we do not know how to specify such receptive fields by hand or where our unsupervised training algorithm has no obvious generalization to a topographic setting. We produce results showing how this method allows us to use even simple unsupervised training algorithms to train successful multi-layered networks that achieve state-of-the-art results on CIFAR and STL datasets: 82.0% and 60.1% accuracy, respectively.", "title": "Selecting Receptive Fields in Deep Networks"}, "31f04f8f83365fabf7ba9c9be1179c0da6815128": {"paper_id": "31f04f8f83365fabf7ba9c9be1179c0da6815128", "abstract": "We introduce a new neural architecture and an unsupervised a lgorithm for learning invariant representations from temporal sequence of images. The system uses two groups of complex cells whose outputs are combined multiplicative ly: one that represents the content of the image, constrained to be constant over severa l consecutive frames, and one that represents the precise location of features, which is allowed to vary over time but constrained to be sparse. The architecture uses an encod er to extract features, and a decoder to reconstruct the input from the features. The meth od was applied to patches extracted from consecutive movie frames and produces orien tat o and frequency selective units analogous to the complex cells in V1. An extension of the method is proposed to train a network composed of units with local receptive fiel d spread over a large image of arbitrary size. A layer of complex cells, subject to spars ity constraints, pool feature units over overlapping local neighborhoods, which causes t h feature units to organize themselves into pinwheel patterns of orientation-selecti v receptive fields, similar to those observed in the mammalian visual cortex. A feed-forwa rd encoder efficiently computes the feature representation of full images.", "title": "Emergence of Complex-Like Cells in a Temporal Product Network with Local Receptive Fields"}, "5b9c21826a213857fbfe91f42ee423e0ad0af32b": {"paper_id": "5b9c21826a213857fbfe91f42ee423e0ad0af32b", "abstract": "Scientific computing often requires the availability of a massive number of computers for performing large scale experiments. Traditionally, these needs have been addressed by using high-performance computing solutions and installed facilities such as clusters and super computers, which are difficult to setup, maintain, and operate. Cloud computing provides scientists with a completely new model of utilizing the computing infrastructure. Compute resources, storage resources, as well as applications, can be dynamically provisioned (and integrated within the existing infrastructure) on a pay per use basis. These resources can be released when they are no more needed. Such services are often offered within the context of a Service Level Agreement (SLA), which ensure the desired Quality of Service (QoS). Aneka, an enterprise Cloud computing solution, harnesses the power of compute resources by relying on private and public Clouds and delivers to users the desired QoS. Its flexible and service based infrastructure supports multiple programming paradigms that make Aneka address a variety of different scenarios: from finance applications to computational science. As examples of scientific computing in the Cloud, we present a preliminary case study on using Aneka for the classification of gene expression data and the execution of fMRI brain imaging workflow.", "title": "High-Performance Cloud Computing: A View of Scientific Applications"}, "691d1298da70574ca2dbf67a9f0e256ba329560d": {"paper_id": "691d1298da70574ca2dbf67a9f0e256ba329560d", "abstract": "The tradeoff between the switching energy and electro-thermal robustness is explored for 1.2-kV SiC MOSFET, silicon power MOSFET, and 900-V CoolMOS body diodes at different temperatures. The maximum forward current for dynamic avalanche breakdown is decreased with increasing supply voltage and temperature for all technologies. The CoolMOS exhibited the largest latch-up current followed by the SiC MOSFET and silicon power MOSFET; however, when expressed as current density, the SiC MOSFET comes first followed by the CoolMOS and silicon power MOSFET. For the CoolMOS, the alternating p and n pillars of the superjunctions in the drift region suppress BJT latch-up during reverse recovery by minimizing lateral currents and providing low-resistance paths for carriers. Hence, the temperature dependence of the latch-up current for CoolMOS was the lowest. The switching energy of the CoolMOS body diode is the largest because of its superjunction architecture which means the drift region have higher doping, hence more reverse charge. In spite of having a higher thermal resistance, the SiC MOSFET has approximately the same latch-up current while exhibiting the lowest switching energy because of the least reverse charge. The silicon power MOSFET exhibits intermediate performance on switching energy with lowest dynamic latching current.", "title": "An Analysis of the Switching Performance and Robustness of Power MOSFETs Body Diodes: A Technology Evaluation"}, "6363ccbd8ac0bdd81b752f10c1c3cdc5b7926ffa": {"paper_id": "6363ccbd8ac0bdd81b752f10c1c3cdc5b7926ffa", "abstract": "The retention of existing IT employees is crucial due to the expected shortage of the IT labor force in the U.S., Canada, and European countries. While much of the extant IT turnover literature implicitly assumes that IT employees are homogeneous, we contend that they are a diverse group and that exploring the group in depth would reveal further insights into why employees turnover. We examined a sample of employees by IT job type in a turnover model of the antecedents and impacts of perceived organizational support (POS), which is another infrequently studied concept in the literature but is a potentially important predictor of turnover. A survey of 302 IT employees at a large U.S.-based company showed that these employees are in fact diverse. The relationships between role ambiguity and POS and work schedule flexibility and POS were found to be significant for managerial employees, but not for technically-oriented employees. The relationship between career accommodations and POS, however, was found to be significant for technically-oriented employees, but not managerial employees. As a whole, this study suggests that by combining all IT employees together in our analyses, we may forego some of the unique insights about these employees that we can otherwise cultivate to strengthen the bond between the organization and its employees and to enhance our existing IT turnover literature. The results of this study provide implications for organizations on how they can better balance the tactics they use to retain their valued IT employees. IT managers can be in a better position to focus on building relationships with their employees based on what is generally important to those employees.", "title": "Heterogeneity of IT employees: an analysis of a model of perceived organizational support by job type"}, "1eefc51ff886acd6f2c317f8f25a3f42fe984ddb": {"paper_id": "1eefc51ff886acd6f2c317f8f25a3f42fe984ddb", "abstract": "Previous research on organizational commitment has typically not focused on the underlying dimensions of psychological attachment to the organization. Results of two studies using university employees (N = 82) and students (N = 162) suggest that psychological attachment may be predicated on compliance, identification, and internalization (e.g., Kelman, 19S8). Identification and internalization are positively related to prosocial behaviors and negatively related to turnover. Internalization is predictive of financial donations to a fund-raising campaign. Overall, the results suggest the importance of clearly specifying the underlying dimensions of commitment using notions of psychological attachment and the various forms such attachment can take.", "title": "Organizational Commitment and Psychological Attachment : The Effects of Compliance , Identification , and Internalization on Prosocial Behavior"}, "76d5a90f26e1270c952eac1fa048a83d63f1dd39": {"paper_id": "76d5a90f26e1270c952eac1fa048a83d63f1dd39", "abstract": "In this article, we attempt to distinguish between the properties of moderator and mediator variables at a number of levels. First, we seek to make theorists and researchers aware of the importance of not using the terms moderator and mediator interchangeably by carefully elaborating, both conceptually and strategically, the many ways in which moderators and mediators differ. We then go beyond this largely pedagogical function and delineate the conceptual and strategic implications of making use of such distinctions with regard to a wide range of phenomena, including control and stress, attitudes, and personality traits. We also provide a specific compendium of analytic procedures appropriate for making the most effective use of the moderator and mediator distinction, both separately and in terms of a broader causal system that includes both moderators and mediators.", "title": "The moderator-mediator variable distinction in social psychological research: conceptual, strategic, and statistical considerations."}, "9b4ae1b16915953fbc8b0e98e02af41d40950183": {"paper_id": "9b4ae1b16915953fbc8b0e98e02af41d40950183", "abstract": "Sleep plays a vital role in human health, both mental and physical. Sleep disorders like sleep apnea are increasing in prevalence, with the rapid increase in factors like obesity. Sleep apnea is most commonly treated with Continuous Positive Air Pressure (CPAP) therapy, which maintains the appropriate pressure to ensure continuous airflow. It is widely accepted that in addition to preventing air passage collapse, increase in deep and REM sleep stages would be good metrics for how well the CPAP therapy is working in improving sleep health. Presently, however, there is no mechanism to easily detect a patient\u2019s sleep stages from CPAP flow data alone. We propose, for the first time, an automated sleep staging model based only on the flow signal. Recently deep neural networks have shown high accuracy on sleep staging by eliminating handcrafted features. However, these methods focus exclusively on extracting informative features from the input signal, without paying much attention to the dynamics of sleep stages in the output sequence. We propose an end-to-end framework that uses a combination of deep convolution and recurrent neural networks to extract high-level features from raw flow signal and then uses a structured output layer based on a conditional random field to model the temporal transition structure of the sleep stages. We improve upon the previous methods by 10% using our model, that can be augmented to the previous sleep staging deep learning methods. We also show that our method can be used to accurately track sleep metrics like sleep efficiency calculated from sleep stages that can be deployed for monitoring the response of CPAP therapy on sleep apnea patients. Apart from the technical contributions, we expect this study to motivate new research questions in sleep science, especially towards the understanding of sleep architecture trajectory among patients under CPAP therapy.", "title": "Sleep Staging by Modeling Sleep Stage Transitions using Deep CRF"}, "9e463eefadbcd336c69270a299666e4104d50159": {"paper_id": "9e463eefadbcd336c69270a299666e4104d50159", "abstract": null, "title": "A COEFFICIENT OF AGREEMENT FOR NOMINAL SCALES 1"}, "42fcaf880933b43c7ef1c0cf1b437e46cd9d0a9c": {"paper_id": "42fcaf880933b43c7ef1c0cf1b437e46cd9d0a9c", "abstract": "We study the problem of learning domain invariant representations for time series data while transferring the complex temporal latent dependencies between domains. Our model termed as Variational Recurrent Adversarial Deep Domain Adaptation (VRADA) is built atop a variational recurrent neural network (VRNN) and trains adversarially to capture complex temporal relationships that are domain-invariant. This is (as far as we know) the first to capture and transfer temporal latent dependencies of multivariate time-series data. Through experiments on real-world multivariate healthcare time-series datasets, we empirically demonstrate that learning temporal dependencies helps our model\u2019s ability to create domain-invariant representations, allowing our model to outperform current state-of-the-art deep domain adaptation approaches.", "title": "VARIATIONAL RECURRENT ADVERSARIAL DEEP DOMAIN ADAPTATION"}, "3842ee1e0fdfeff936b5c49973ff21adfaaf3929": {"paper_id": "3842ee1e0fdfeff936b5c49973ff21adfaaf3929", "abstract": "Adversarial learning methods are a promising approach to training robust deep networks, and can generate complex samples across diverse domains. They can also improve recognition despite the presence of domain shift or dataset bias: recent adversarial approaches to unsupervised domain adaptation reduce the difference between the training and test domain distributions and thus improve generalization performance. However, while generative adversarial networks (GANs) show compelling visualizations, they are not optimal on discriminative tasks and can be limited to smaller shifts. On the other hand, discriminative approaches can handle larger domain shifts, but impose tied weights on the model and do not exploit a GAN-based loss. In this work, we first outline a novel generalized framework for adversarial adaptation, which subsumes recent state-of-the-art approaches as special cases, and use this generalized view to better relate prior approaches. We then propose a previously unexplored instance of our general framework which combines discriminative modeling, untied weight sharing, and a GAN loss, which we call Adversarial Discriminative Domain Adaptation (ADDA). We show that ADDA is more effective yet considerably simpler than competing domain-adversarial methods, and demonstrate the promise of our approach by exceeding state-of-the-art unsupervised adaptation results on standard domain adaptation tasks as well as a difficult cross-modality object classification task.", "title": "Adversarial Discriminative Domain Adaptation"}, "488bb25e0b1777847f04c943e6dbc4f84415b712": {"paper_id": "488bb25e0b1777847f04c943e6dbc4f84415b712", "abstract": "We introduce a method to stabilize Generative Adversarial Networks (GANs) by defining the generator objective with respect to an unrolled optimization of the discriminator. This allows training to be adjusted between using the optimal discriminator in the generator\u2019s objective, which is ideal but infeasible in practice, and using the current value of the discriminator, which is often unstable and leads to poor solutions. We show how this technique solves the common problem of mode collapse, stabilizes training of GANs with complex recurrent generators, and increases diversity and coverage of the data distribution by the generator.", "title": "Unrolled Generative Adversarial Networks"}, "2e68190ebda2db8fb690e378fa213319ca915cf8": {"paper_id": "2e68190ebda2db8fb690e378fa213319ca915cf8", "abstract": "We capitalize on large amounts of unlabeled video in order to learn a model of scene dynamics for both video recognition tasks (e.g. action classification) and video generation tasks (e.g. future prediction). We propose a generative adversarial network for video with a spatio-temporal convolutional architecture that untangles the scene\u2019s foreground from the background. Experiments suggest this model can generate tiny videos up to a second at full frame rate better than simple baselines, and we show its utility at predicting plausible futures of static images. Moreover, experiments and visualizations show the model internally learns useful features for recognizing actions with minimal supervision, suggesting scene dynamics are a promising signal for representation learning. We believe generative video models can impact many applications in video understanding and simulation.", "title": "Generating Videos with Scene Dynamics"}, "007e86cb55f0ba0415a7764a1e9f9566c1e8784b": {"paper_id": "007e86cb55f0ba0415a7764a1e9f9566c1e8784b", "abstract": "The ability of the Generative Adversarial Networks (GANs) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing that the latent space of such generators captures semantic variation in the data distribution. Intuitively, models trained to predict these semantic latent representations given data may serve as useful feature representations for auxiliary problems where semantics are relevant. However, in their existing form, GANs have no means of learning the inverse mapping \u2013 projecting data back into the latent space. We propose Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning.", "title": "Adversarial Feature Learning"}, "ccce1cf96f641b3581fba6f4ce2545f4135a15e3": {"paper_id": "ccce1cf96f641b3581fba6f4ce2545f4135a15e3", "abstract": "In this letter we discuss a least squares version for support vector machine (SVM) classifiers. Due to equality type constraints in the formulation, the solution follows from solving a set of linear equations, instead of quadratic programming for classical SVM's. The approach is illustrated on a two-spiral benchmark classification problem.", "title": "Least Squares Support Vector Machine Classifiers"}, "047655e733a9eed9a500afd916efa566915b9110": {"paper_id": "047655e733a9eed9a500afd916efa566915b9110", "abstract": "The temporal distance between events conveys information essential for numerous sequential tasks such as motor control and rhythm detection. While Hidden Markov Models tend to ignore this information, recurrent neural networks (RNNs) can in principle learn to make use of it. We focus on Long Short-Term Memory (LSTM) because it has been shown to outperform other RNNs on tasks involving long time lags. We find that LSTM augmented by \u201cpeephole connections\u201d from its internal cells to its multiplicative gates can learn the fine distinction between sequences of spikes spaced either 50 or 49 time steps apart without the help of any short training exemplars. Without external resets or teacher forcing, our LSTM variant also learns to generate stable streams of precisely timed spikes and other highly nonlinear periodic patterns. This makes LSTM a promising approach for tasks that require the accurate measurement or generation of time intervals.", "title": "Learning Precise Timing with LSTM Recurrent Networks"}, "64a60b0325d1df9fb41afd2934f836de5b342bd5": {"paper_id": "64a60b0325d1df9fb41afd2934f836de5b342bd5", "abstract": "This paper proposes amulti-level feature learning framework for human action recognition using a single body-worn inertial sensor. The framework consists of three phases, respectively designed to analyze signal-based (low-level), components (mid-level) and semantic (high-level) information. Low-level features capture the time and frequency domain property while mid-level representations learn the composition of the action. The Maxmargin Latent Pattern Learning (MLPL) method is proposed to learn high-level semantic descriptions of latent action patterns as the output of our framework. The proposedmethod achieves the state-of-the-art performances, 88.7%, 98.8% and 72.6% (weighted F1 score) respectively, on Skoda, WISDM and OPP datasets. \u00a9 2017 Elsevier B.V. All rights reserved.", "title": "Learning Multi-level Features For Sensor-based Human Action Recognition"}, "007d73c91a1bf90d72eb59fbdd8791a4b009f363": {"paper_id": "007d73c91a1bf90d72eb59fbdd8791a4b009f363", "abstract": "Many algorithms are available to learn deep hierarchies of features from unlabeled data, especially images. In many cases, these algorithms involve multi-layered networks of features (e.g., neural networks) that are sometimes tricky to train and tune and are difficult to scale up to many machines effectively. Recently, it has been found that K-means clustering can be used as a fast alternative training method. The main advantage of this approach is that it is very fast and easily implemented at large scale. On the other hand, employing this method in practice is not completely trivial: K-means has several limitations, and care must be taken to combine the right ingredients to get the system to work well. This chapter will summarize recent results and technical tricks that are needed to make effective use of K-means clustering for learning large-scale representations of images. We will also connect these results to other well-known algorithms to make clear when K-means can be most useful and convey intuitions about its behavior that are useful for debugging and engineering new systems.", "title": "Learning Feature Representations with K-Means"}, "1bb4ae834efeae5488fd0c027bd9dd2817e76999": {"paper_id": "1bb4ae834efeae5488fd0c027bd9dd2817e76999", "abstract": "Purpose \u2013 The purpose of this study is to conduct a meta-analysis of prior scientometric research of the knowledge management (KM) field. Design/methodology/approach \u2013 A total of 108 scientometric studies of the KM discipline were subjected to meta-analysis techniques. Findings \u2013 The overall volume of scientometric KM works has been growing, reaching up to ten publications per year by 2012, but their key findings are somewhat inconsistent. Most scientometric KM research is published in non-KM-centric journals. The KM discipline has deep historical roots. It suffers from a high degree of over-differentiation and is represented by dissimilar research streams. The top six most productive countries for KM research are the USA, the UK, Canada, Germany, Australia, and Spain. KM exhibits attributes of a healthy academic domain with no apparent anomalies and is progressing towards academic maturity. Practical implications \u2013 Scientometric KM researchers should use advanced empirical methods, become aware of prior scientometric research, rely on multiple databases, develop a KM keyword classification scheme, publish their research in KM-centric outlets, focus on rigorous research of the forums for KM publications, improve their cooperation, conduct a comprehensive study of individual and institutional productivity, and investigate interdisciplinary collaboration. KM-centric journals should encourage authors to employ under-represented empirical methods and conduct meta-analysis studies and should discourage conceptual publications, especially the development of new frameworks. To improve the impact of KM research on the state of practice, knowledge dissemination channels should be developed. Originality/value \u2013 This is the first documented attempt to conduct a meta-analysis of scientometric research of the KM discipline.", "title": "Meta-analysis of scientometric research of knowledge management: discovering the identity of the discipline"}, "6742f2a7515fb2f892f81db85af236326c6e6b7d": {"paper_id": "6742f2a7515fb2f892f81db85af236326c6e6b7d", "abstract": "Metadata of scientific articles such as title, abstract, keywords or", "title": "Rule Based Metadata Extraction Framework from Academic Articles"}, "ee8ebbf2ce0f83bf7b0ff03474c874f27a083bf8": {"paper_id": "ee8ebbf2ce0f83bf7b0ff03474c874f27a083bf8", "abstract": "The purpose of this paper is to study the impact of tourism marketing mix and how it affects tourism in Jordan, and to determine which element of the marketing mix has the strongest impact on Jordanian tourism and how it will be used to better satisfy tourists. The paper will focus on foreign tourists coming to Jordan; a field survey will be used by using questionnaires to collect data. Three hundred questionnaires will be collected from actual tourists who visited Jordan, the data will be collected from selected tourism sites like (Petra, Jarash,.... etc.) and classified from one to five stars hotels in Jordan. The questionnaire will be designed in different languages (English, French and Arabic) to meet all tourists from different countries. The study established that from all the marketing mix elements, the researcher studied, product & promotion had the strongest effect on foreign tourist's satisfaction, where price and distribution were also effective significant factors. The research recommends suitable marketing strategies for all elements especially product & promotion.", "title": "The Impact of Tourism Marketing Mix Elements on the Satisfaction of Inbound Tourists to Jordan"}, "87ac8bd7d31e34c2ed9081ace2888389a3f97879": {"paper_id": "87ac8bd7d31e34c2ed9081ace2888389a3f97879", "abstract": "Taylor & Francis makes every effort to ensure the accuracy of all the information (the \u201cContent\u201d) contained in the publications on our platform. Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Versions of published Taylor & Francis and Routledge Open articles and Taylor & Francis and Routledge Open Select articles posted to institutional or subject repositories or any other third-party website are without warranty from Taylor & Francis of any kind, either expressed or implied, including, but not limited to, warranties of merchantability, fitness for a particular purpose, or non-infringement. Any opinions and views expressed in this article are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor & Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.", "title": "Digital culture clash : \u201c massive \u201d education in the E-learning and Digital Cultures MOOC"}, "2d7562e458341ce5f4c109949c33b02bf857344b": {"paper_id": "2d7562e458341ce5f4c109949c33b02bf857344b", "abstract": "Current systems used in education follow a consistent design pattern, one that is not supportive of lifelong learning or personalization, is asymmetric in terms of user capability, and which is disconnected from the global ecology of Internet services. In this paper we propose an alternative design pattern for educational systems that emphasizes symmetric connections with a range of services both in formal and informal learning, work, and leisure, and identify strategies for implementation and experimentation. Personal Learning Environments: Challenging the dominant design of educational systems Journal of e-Learning and Knowledge Society \u2014 Vol. 3, n. 2, june 2007 (pp. 27-38 ) Je-LKS Methodologies and scenarios Scott Wilson, Prof. Oleg Liber, Mark Johnson, Phil Beauvoir, Paul Sharples & Colin Milligan University of Bolton, UK; Scott.Bradley.Wilson@gmail. com, o.liber@bolton.ac.uk, m.w.Johnson@bolton.ac.uk, p.beauvoir@bolton.ac.uk, p.sharples@bolton.ac.uk, colin. milligan@strath.ac.uk", "title": "Personal Learning Environments: Challenging the Dominant Design of Educational Systems"}, "494c2dbd4c7418cb3f6e43c6e68e91efa96ba981": {"paper_id": "494c2dbd4c7418cb3f6e43c6e68e91efa96ba981", "abstract": "Since practicing engineers are hired, retained, and rewarded for solving problems, engineering students should learn how to solve workplace problems . Therefore, we designed and implemented several problem-solving learning environments (PSLEs) for the junior course entitled Kinetics and Homogeneous Reactor Design at Universidad de las Am\u00e9ricas Puebla. Metacognition has been shown to be important for the solution of more open-ended and wellstructured problems. Flavell 5 distinguished two characteristics of metacognition: knowledge of cognition (KC) and regulation of cognition (RC). In order to support student metacognitive processing while learning to solve kinetics and homogeneous reactor design problems, the instructor created a supportive social environment in the course and inserted a series of question prompts during PSLEs, as a form of coaching where the problem to be solved was represented as a case, and cases were used in various ways (worked examples, case studies, structural analogues, prior experiences, alternative perspectives, and simulations) as instructional supports. The Metacognitive Awareness Inventory (MAI) designed by Schraw and Dennison was utilized as a pre(first day of classes) post(last day of classes) test. MAI is a 52-item inventory to measure adults\u2019 metacognitive awareness. Items are classified into eight subcomponents subsumed under two broader categories, KC and RC. Furthermore, in order to assess metacognitive awareness during problem-solving activities, students had to answer the corresponding problem as well as approximately 2-3 embedded problem-solving prompts (from Jonassen) and 4-6 embedded metacognitive prompts (from MAI). Results for the pre-post MAI exhibited a significant (p<0.05) increase in student metacognitive awareness. This increase was also noticed by means of the embedded MAI prompts while solving different kinds of problems (such as story problems, decision-making problems, troubleshooting, and design problems) throughout the course, in which students also improved the quality of their embedded problem-solving answers and corresponding grades. Promoting metacognitive awareness and skills could be a valuable method for improving learning and student performance during kinetics and homogeneous reactor design problemsolving, as has been previously reported for professional educators and dental hygiene students.", "title": "Assessing Metacognitive Awareness during Problem-Solving in a Kinetics and Homogeneous Reactor Design Course"}, "880d44760cdd878e78d345d50536f28799723493": {"paper_id": "880d44760cdd878e78d345d50536f28799723493", "abstract": "The authors summarize 35 years of empirical research on goal-setting theory. They describe the core findings of the theory, the mechanisms by which goals operate, moderators of goal effects, the relation of goals and satisfaction, and the role of goals as mediators of incentives. The external validity and practical significance of goal-setting theory are explained, and new directions in goal-setting research are discussed. The relationships of goal setting to other theories are described as are the theory's limitations.", "title": "Building a practically useful theory of goal setting and task motivation. A 35-year odyssey."}, "391aa276c1acba82b0d7525468d59b1ac4c143df": {"paper_id": "391aa276c1acba82b0d7525468d59b1ac4c143df", "abstract": "Culture fundamentally shapes how individuals make meaning out of illness, suffering, and dying. With increasing diversity in the United States, encounters between patients and physicians of different backgrounds are becoming more common. Thus the risk for cross-cultural misunderstandings surrounding care at the end of life is also increasing. Studies have shown cultural differences in attitudes toward truth telling, life-prolonging technology, and decision-making styles at the end of life. Using 2 case studies of patients, one of an African American couple in the southern United States and the other of a Chinese-American family in Hawaii, we outline some of the major issues involved in cross-cultural care and indicate how the patient, family, and clinician can navigate among differing cultural beliefs, values, and practices. Skilled use of cross-cultural understanding and communication techniques increases the likelihood that both the process and outcomes of care are satisfactory for all involved.", "title": "Negotiating cross-cultural issues at the end of life: \"You got to go where he lives\"."}, "c9dd5ae24520d8cdddfdf8ef6d5f925445e310d9": {"paper_id": "c9dd5ae24520d8cdddfdf8ef6d5f925445e310d9", "abstract": "Naive Bayes classifier is the simplest among Bayesian Network classifiers. It has shown to be very efficient on a variety of data classification problems. However, the strong assumption that all features are conditionally independent given the class is often violated on many real world applications. Therefore, improvement of the Naive Bayes classifier by alleviating the feature independence assumption has attracted much attention. In this paper, we develop a new version of the Naive Bayes classifier without assuming independence of features. The proposed algorithm approximates the interactions between features by using conditional probabilities. We present results of numerical experiments on several real world data sets, where continuous features are discretized by applying two different methods. These results demonstrate that the proposed algorithm significantly improve the performance of the Naive Bayes classifier, yet at the same time maintains its robustness.", "title": "Improving Naive Bayes Classifier Using Conditional Probabilities"}, "1dc53b91327cab503acc0ca5afb9155882b717a5": {"paper_id": "1dc53b91327cab503acc0ca5afb9155882b717a5", "abstract": "Since most real-world applications of classification learning involve continuous-valued attributes, properly addressing the discretization process is an important problem. This paper addresses the use of the entropy minimization heuristic for discretizing the range of a continuous-valued attribute into multiple intervals. We briefly present theoretical evidence for the appropriateness of this heuristic for use in the binary discretization algorithm used in ID3, C4 , CART, and other learning algorithms. The results serve to justify extending the algorithm to derive multiple intervals. We formally derive a criterion based on the minimum description length principle for deciding the partitioning of intervals. We demonstrate via empirical evaluation on several real-world data sets that better decision trees are obtained using the new multi-interval algorithm. Introduction Classification learning algorithms typically use heuristics to guide their search through the large space of possible relations between combinations of attribute' values and classes. One such heuristic uses the notion of selecting attributes locally minimizing the information entropy of the classes in a data set (d. the ID3 algorithm (13) and its extensions, e.g. GID3 (2), GID3* (5), and C4 (15), CART (1), CN2 (3) and others). See (11; 5; 6) for a general discussion of the attribute selection problem. The attributes in a learning problem may be nominal (categorical), or they may be continuous (numerical). The term continuous\" is used in the literature to refer to attributes taking on numerical values (integer or real); or in general an attribute with a linearly ordered range of values. The above mentioned attribute selection process assumes that all attributes are nominal. Continuous-valued attributes are discretized prior to selection , typically by paritioning the range of the attribute into subranges. In general, a discretization is simply a logical condition , in terms of one or more attributes, that serves to partition the data into at least two subsets. In this paper, we focus only on the discretization of continuous-valued attributes. We first present a result about the information entropy minimization heuristic for binary discretization (two-interval splits). This gives us: . a better understanding of the heuristic and its behavior 1022 Machine Learning :;. . formal evidence that supports the usage of the heuristic ;;' in this context , and . a gain in computational effciency that results in speeding . up the evaluation process for continuous-valued attribute discretization. We then proceed to extend the algorithm to divide the range of a continuous-valued attribute into multiple intervals rather than just two. We first motivate the need for such a capability, then we present the multiple interval generalization, and finally we present the empirical evaluation results confirming that the new capability does indeed result in producing better decision trees. Binary Discretization A continuous-valued attribute is typically discretized during decision tree generation by partitioning its range into two intervals. A threshold value for the continuous-valued attribute is determined, and the test ::", "title": "Multi-Interval Discretization of Continuous-Valued Attributes for Classification Learning"}, "3208ed3d4ff2de382ad6a16431cfe7118c000725": {"paper_id": "3208ed3d4ff2de382ad6a16431cfe7118c000725", "abstract": "Constructive induction is the process of changing the representation of examples by creating new attributes from existing attributes. In classi cation, the goal of constructive induction is to nd a representation that facilitates learning a concept description by a particular learning system. Typically, the new attributes are Boolean or arithmetic combinations of existing attributes and the learning algorithms used are decision trees or rule learners. We describe the construction of new attributes that are the Cartesian product of existing attributes. We consider the e ects of this operator on a Bayesian classi er an a nearest neighbor algorithm.", "title": "Constructive Induction of Cartesian Product Attributes"}, "17d87b9ac0bedad64489022ef415df05829843ad": {"paper_id": "17d87b9ac0bedad64489022ef415df05829843ad", "abstract": "We consider a graph-theoretic elimination process which is related to performing Gaussian elimination on sparse symmetric positive definite systems of linear equations. We give a new linear-time algorithm to calculate the fill-in produced by any elimination ordering, and we give two new related algorithms for finding orderings with special properties. One algorithm, based on breadth-first search, finds a perfect elimination ordering, if any exists, in O(n + e) time, if the problem graph has n vertices and e edges. An extension of this algorithm finds a minimal (but not necessarily minimum) ordering in O(ne) time. We conjecture that the problem of finding a minimum ordering is", "title": "Algorithmic Aspects of Vertex Elimination on Graphs"}, "4cd91c51098783ec972f6a0ab430cacdd634a5b2": {"paper_id": "4cd91c51098783ec972f6a0ab430cacdd634a5b2", "abstract": "This is an aptly titled effort to supplement probability theory as developed for chance/aleatory devices by a parallel, but distinct, epistemically oriented quantitative theory of evidence for, and evidential support of, our opinions, judgements of facts, and beliefs. That probability takes its meaning from and is used to describe such diverse phenomena as propensities for physical behavior, propositional attitudes of belief, logical relations of inductive support, and experimental outcomes under prescribed conditions of unlinked repetitions, has long been the source of much of the controversy and vitality in the development and application of probability theory and its associated concepts. Ian Hacking in his recent book The emergence of probability [1] attempted to trace and explain this intertwining of belief/knowledge and physical (objective) behavior in terms of a conceptual transformation of the categories of knowledge and opinion that was mainly completed by the early 18th century. Hacking's historical/philosophical analysis aims to explain what he holds to be our present dualistic conception of probability as being jointly epistemic (oriented towards assessment of knowledge/belief) and aleatory (oriented towards the objective description of the outcomes of 'random' experiments) with most of the present-day emphasis on the latter. Historically, however, the epistemic component was initially dominant in conceptions of probability. Probability through the Renaissance applied only to opinions/beliefs and was based upon authoritative testimony in support of these opinions/beliefs. The 19 year-old Leibniz writing in 1665 wished to formalize the evidential support for beliefs by a numerical assignment on a scale of [0, 1] of what he referred to as 'degrees of proof'. The object of this exercise was to be a rationalized jurisprudence. Key to such assignments was an analysis into equally possible (likely) cases. The growth of an aleatory notion of probability concerning inductive relations between physical signs and physical phenomena starts in the Renaissance. The extent to which the aleatory notion was dependent upon the epistemic notion (there was also a strong converse dependence) is apparent in the posthumously published (1713) A rs conjectandi of J. Bernoulli. In Part IV of the Ars [2] we find the first statement and proof of a law of large numbers, the first firm step on the road to the frequentist/aleatory concepts dominant today. Significantly though, J. Bernoulli was not a frequentist. For Bernoulli, frequency of occurrence was only a clue to the enumeration of the equally possible cases that was the basis of quantitative epistemic probability. Much", "title": "A Mathematical Theory of Evidence"}, "8dc087c34cdc1721fa37aafc36fb1b71f6f1b757": {"paper_id": "8dc087c34cdc1721fa37aafc36fb1b71f6f1b757", "abstract": "Production rules are a popular representation for encoding heuristic knowledge in programs for scientific and medical problem solving. However, experience with one of these programs, MYCIN, indicates that the representation has serious limitations: people other than the original rule authors find it difflcuit to modify the rule set, and the rules a r e unsuitable for use in other settings, such as for application to teaching. These problems are rooted in fundamental limitations in MYCIN\u2019s original rule representation: the view that expert knowledge can be encoded as a uniform, weakly-structured set of if/then associations is found to be wanting. To illustrate these problems, this paper examines MYCIN\u2019s rules from the perspective Of a teacher trylng to Justify them and to convey a problem-soivlng approach. We discover that individual rules play different roles, have different kinds of justifications, and are constructed using different rationales for the ordering and choice of premise clauses. This design knowledge, consisting of structural and strategic concepts which lie outside the representation, is shown to be procedurally embedded In the rules. Moreover, because the data/hypothesis associations are themselves a proceduralized form of underlying disease models, they can only be supported by appealing to this deeper level of knowledge. Making explicit this structural, strategic and support knowledge enhances the ability to understand and modify the system.", "title": "The Epistemology of a Rule-Based Expert System - A Framework for Explanation"}, "7783fd2984ac139194d21c10bd83b4c9764826a3": {"paper_id": "7783fd2984ac139194d21c10bd83b4c9764826a3", "abstract": "Probabilistic methods to create the areas, of computational tools. But I needed to get canned, bayesian networks worked recently strongly. Recently I tossed this book was published. In intelligent systems is researchers in, ai operations research excellence award for graduate. Too concerned about how it i've been. Apparently daphne koller and learning structures evidential reasoning. Pearl is a language for i've. Despite its early publication date it, is not great give the best references.", "title": "Probabilistic reasoning in intelligent systems - networks of plausible inference"}, "2dec5b671af983b1e57418434932f0320f51e9ca": {"paper_id": "2dec5b671af983b1e57418434932f0320f51e9ca", "abstract": "Naive Bayes induction algorithms were previously shown to be surprisingly accurate on many classi cation tasks even when the conditional independence assumption on which they are based is violated How ever most studies were done on small databases We show that in some larger databases the accuracy of Naive Bayes does not scale up as well as decision trees We then propose a new algorithm NBTree which in duces a hybrid of decision tree classi ers and Naive Bayes classi ers the decision tree nodes contain uni variate splits as regular decision trees but the leaves contain Naive Bayesian classi ers The approach re tains the interpretability of Naive Bayes and decision trees while resulting in classi ers that frequently out perform both constituents especially in the larger databases tested", "title": "Scaling Up the Accuracy of Naive-Bayes Classifiers: A Decision-Tree Hybrid"}, "1e3b61f29e5317ef59d367e1a53ba407912d240e": {"paper_id": "1e3b61f29e5317ef59d367e1a53ba407912d240e", "abstract": "After the files from my ipod just to say is intelligent enough room. Thanks I use your playlists or cmd opt. Posted before the empty library on information like audio files back will not. This feature for my music folder and copying but they do not sure you. I was originally trying the ipod to know my tunes versiondo you even. To work fine but majority of transferring. Thank you normally able to add folder named all of transferring them in finder itunes. Im out itunes music and would solve. But the entire music folder that, are not method because file access? D I just accepted files to enable disk mode. Thanks the scope of ipod itself all. Probably take too much obliged but never full. I tuneaid went to add this tutorial have you. My uncle will do the itunes. Thanks further later in fact the use. On your advice by other information contained in my ipod. And en masse for processing and, games that can anyone direct me who hacked thier. Help youll have an external hard drives. My new synchronization from the major advantage. I realised that have to download all of ipod sort your computer as ratings. Thank you can I wouldnt read, a bunch. Software listed procedure it did and uncheck. My quest for a problem if I didnt know just. Itunes and itunes can not want. Some songs from basic copying files now I can anyone advise can. I knew that you not recognize the add songs to recover.", "title": "Computers and Intractability: A Guide to the Theory of NP-Completeness"}, "1880f0b36a38662ab25b5cf01b2c1688f1789900": {"paper_id": "1880f0b36a38662ab25b5cf01b2c1688f1789900", "abstract": "We present an actor language which is an extension of a simple functional language, and provide an operational semantics for this extension. Actor configurations represent open distributed systems, by which we mean that the specification of an actor system explicitly takes into account the interface with external components. We study the composability of such systems. We define and study various notions of testing equivalence on actor expressions and configurations. The model we develop provides fairness. An important result is that the three forms of equivalence, namely, convex, must, and may equivalences, collapse to two in the presence of fairness. We further develop methods for proving laws of equivalence and provide example proofs to illustrate our methodology.", "title": "A Foundation for Actor Computation"}, "c25ad7b478aabaa32c92d37c4aebde7ad9ac2e5c": {"paper_id": "c25ad7b478aabaa32c92d37c4aebde7ad9ac2e5c", "abstract": null, "title": "Towards unified biomedical modeling with subgraph mining and factorization algorithms"}, "6723dda58e5e09089ec78ba42827b65859f030e2": {"paper_id": "6723dda58e5e09089ec78ba42827b65859f030e2", "abstract": "We have recently completed the sixth in a series of \"Message Understanding Conferences\" which are designed to promote and evaluate research in information extraction. MUC-6 introduced several innovations over prior MUCs, most notably in the range of different tasks for which evaluations were conducted. We describe some of the motivations for the new format and briefly discuss some of the results of the evaluations. 1 The M U C Evaluations We have just completed the sixth in a series of Message Understanding Conferences, which have been organized by NRAD, the RDT&E division of the Naval Command, Control and Ocean Surveillance Center (formerly NOSC, the Naval Ocean Systems Center) with the support of DARPA, the Defense Advanced Research Projects Agency. This paper looks briefly at the history of these Conferences and then examines the considerations which led to the structure of MUC-6} The Message Understanding Conferences were initiated by NOSC to assess and to foster research on the automated analysis of mili tary messages containing textual information. Although called \"conferences\", the distinguishing characteristic of the MUCs are not the conferences themselves, but the evaluations to which participants must submit in order to be permit ted to at tend the conference. For each MUC, participating groups have been given sample messages and instructions on the type of information to be extracted, and have developed a system to process such messages. Then, shortly before the conference, participants are given a set of test messages to be run through their system (without making any changes to the system); the output of each part icipant 's system 1The full proceedings of the conference are to be distributed by Morgan Kaufmann Publishers, San Mateo, California; earlier MUC proeeedings~ for MUC-3, 4, and 5, are also available from Morgan Kaufmann. Beth Sundhe im Naval Command, Control and Ocean Surveillance Center Research, Development, Test and Evaluation Division (NRaD) Code 44208 53140 Gatchell Road San Diego, CMifornia 92152-7420 s u n d h e i m @ p o j k e . n o s c . m i l is then evaluated against a manual ly-prepared answer key. The MUCs are remarkable in part because of the degree to which these evaluations have defined a prograin of research and development. DARPA has a number of information science and technology programs which are driven in large part, by regular evaluations. The MUCs are notable, however, in that they in large par t have shaped the research program in information extraction and brought it to its current s ta te}", "title": "Message Understanding Conference- 6: A Brief History"}, "669a1735c515cf1ae2108d1823e624bd0854a3a0": {"paper_id": "669a1735c515cf1ae2108d1823e624bd0854a3a0", "abstract": "As the wealth of biomedical knowledge in the form of literature increases, there is a rising need for effective natural language processing tools to assist in organizing, curating, and retrieving this information. To that end, named entity recognition (the task of identifying words and phrases in free text that belong to certain classes of interest) is an important first step for many of these larger information management goals. In recent years, much attention has been focused on the problem of recognizing gene and protein mentions in biomedical abstracts. This paper presents a framework for simultaneously recognizing occurrences of PROTEIN, DNA, RNA, CELL-LINE, and CELL-TYPE entity classes using Conditional Random Fields with a variety of traditional and novel features. I show that this approach can achieve an overall F1 measure around 70, which seems to be the current state of the art. The system described here was developed as part of the BioNLP/NLPBA 2004 shared task. Experiments were conducted on a training and evaluation set provided by the task organizers.", "title": "Biomedical Named Entity Recognition using Conditional Random Fields and Rich Feature Sets"}, "1406f6b5ed4034b72ed2dccc3fcfa4c5c0810924": {"paper_id": "1406f6b5ed4034b72ed2dccc3fcfa4c5c0810924", "abstract": "The UMLS Metathesaurus, the largest thesaurus in the biomedical domain, provides a representation of biomedical knowledge consisting of concepts classified by semantic type and both hierarchical and non-hierarchical relationships among the concepts. This knowledge has proved useful for many applications including decision support systems, management of patient records, information retrieval (IR) and data mining. Gaining effective access to the knowledge is critical to the success of these applications. This paper describes MetaMap, a program developed at the National Library of Medicine (NLM) to map biomedical text to the Metathesaurus or, equivalently, to discover Metathesaurus concepts referred to in text. MetaMap uses a knowledge intensive approach based on symbolic, natural language processing (NLP) and computational linguistic techniques. Besides being applied for both IR and data mining applications, MetaMap is one of the foundations of NLM's Indexing Initiative System which is being applied to both semi-automatic and fully automatic indexing of the biomedical literature at the library.", "title": "Effective mapping of biomedical text to the UMLS Metathesaurus: the MetaMap program"}, "653309364bb5b36057ca7c3392aa6e48c37f2125": {"paper_id": "653309364bb5b36057ca7c3392aa6e48c37f2125", "abstract": "The matching of two-dimensional shapes is an important problem with applications in domains as diverse as biometrics, industry, medicine and anthropology. The distance measure used must be invariant to many distortions, including scale, offset, noise, partial occlusion, etc. Most of these distortions are relatively easy to handle, either in the representation of the data or in the similarity measure used. However rotation invariance seems to be uniquely difficult. Current approaches typically try to achieve rotation invariance in the representation of the data, at the expense of discrimination ability, or in the distance measure, at the expense of efficiency. In this work we show that we can take the slow but accurate approaches and dramatically speed them up. On real world problems our technique can take current approaches and make them four orders of magnitude faster, without false dismissals. Moreover, our technique can be used with any of the dozens of existing shape representations and with all the most popular distance measures including Euclidean distance, Dynamic Time Warping and Longest Common Subsequence.", "title": "LB_Keogh Supports Exact Indexing of Shapes under Rotation Invariance with Arbitrary Representations and Distance Measures"}, "03a00248b7d5e2d89f5337e62c39fad277c66102": {"paper_id": "03a00248b7d5e2d89f5337e62c39fad277c66102", "abstract": "problems To understand the class of polynomial-time solvable proble ms, we must first have a formal notion of what a \u201cproblem\u201d is. We define anbstract problemQ to be a binary relation on a set I of probleminstancesand a setS of problemsolutions. For example, an instance for SHORTEST-PATH is a triple consi sting of a graph and two vertices. A solution is a sequence of vertices in the g raph, with perhaps the empty sequence denoting that no path exists. The problem SHORTEST-PATH itself is the relation that associates each instance of a gra ph and two vertices with a shortest path in the graph that connects the two vertices. S ince shortest paths are not necessarily unique, a given problem instance may have mo r than one solution. This formulation of an abstract problem is more general than is required for our purposes. As we saw above, the theory of NP-completeness res tricts attention to decision problems : those having a yes/no solution. In this case, we can view an abstract decision problem as a function that maps the instan ce setI to the solution set {0, 1}. For example, a decision problem related to SHORTEST-PATH i s the problem PATH that we saw earlier. If i = \u3008G,u, v,k\u3009 is an instance of the decision problem PATH, then PATH(i ) = 1 (yes) if a shortest path fromu to v has at mostk edges, and PATH (i ) = 0 (no) otherwise. Many abstract problems are not decision problems, but rather optimization problems , in which some value must be minimized or maximized. As we saw above, however, it is usual ly a simple matter to recast an optimization problem as a decision problem that is no harder. 1See Hopcroft and Ullman [156] or Lewis and Papadimitriou [20 4] for a thorough treatment of the Turing-machine model. 34.1 Polynomial time 973", "title": "Introduction to Algorithms"}, "5b24ec5dc67169c3ddbd671d09fc62b6d15f9bd0": {"paper_id": "5b24ec5dc67169c3ddbd671d09fc62b6d15f9bd0", "abstract": "OBJECTIVE\nTo evaluate the performance of a natural language processing system in extracting pneumonia-related concepts from chest x-ray reports.\n\n\nMETHODS\n\n\n\nDESIGN\nFour physicians, three lay persons, a natural language processing system, and two keyword searches (designated AAKS and KS) detected the presence or absence of three pneumonia-related concepts and inferred the presence or absence of acute bacterial pneumonia from 292 chest x-ray reports. Gold standard: Majority vote of three independent physicians. Reliability of the gold standard was measured.\n\n\nOUTCOME MEASURES\nRecall, precision, specificity, and agreement (using Finn's R: statistic) with respect to the gold standard. Differences between the physicians and the other subjects were tested using the McNemar test for each pneumonia concept and for the disease inference of acute bacterial pneumonia.\n\n\nRESULTS\nReliability of the reference standard ranged from 0.86 to 0.96. Recall, precision, specificity, and agreement (Finn R:) for the inference on acute bacterial pneumonia were, respectively, 0.94, 0.87, 0.91, and 0.84 for physicians; 0.95, 0.78, 0.85, and 0.75 for natural language processing system; 0.46, 0.89, 0.95, and 0.54 for lay persons; 0.79, 0.63, 0.71, and 0.49 for AAKS; and 0.87, 0.70, 0.77, and 0.62 for KS. The McNemar pairwise comparisons showed differences between one physician and the natural language processing system for the infiltrate concept and between another physician and the natural language processing system for the inference on acute bacterial pneumonia. The comparisons also showed that most physicians were significantly different from the other subjects in all pneumonia concepts and the disease inference.\n\n\nCONCLUSION\nIn extracting pneumonia related concepts from chest x-ray reports, the performance of the natural language processing system was similar to that of physicians and better than that of lay persons and keyword searches. The encoded pneumonia information has the potential to support several pneumonia-related applications used in our institution. The applications include a decision support system called the antibiotic assistant, a computerized clinical protocol for pneumonia, and a quality assurance application in the radiology department.", "title": "Research Paper: Automatic Detection of Acute Bacterial Pneumonia from Chest X-ray Reports"}, "bb13eb440c38db3afacad04ae60ec37525890239": {"paper_id": "bb13eb440c38db3afacad04ae60ec37525890239", "abstract": "Linguistically annotated corpus based on texts in biomedical domain has been constructed to tune natural language processing (NLP) tools for biotextmining. As the focus of information extraction is shifting from \"nominal\" information such as named entity to \"verbal\" information such as function and interaction of substances, application of parsers has become one of the key technologies and thus the corpus annotated for syntactic structure of sentences is in demand. A subset of the GENIA corpus consisting of 500 MEDLINE abstracts has been annotated for syntactic structure in an XMLbased format based on Penn Treebank II (PTB) scheme. Inter-annotator agreement test indicated that the writing style rather than the contents of the research abstracts is the source of the difficulty in tree annotation, and that annotation can be stably done by linguists without much knowledge of biology with appropriate guidelines regarding to linguistic phenomena particular to scientific texts.", "title": "Syntax Annotation for the GENIA Corpus"}, "bad06d5347ba93db8f13edc0973a9034fc757557": {"paper_id": "bad06d5347ba93db8f13edc0973a9034fc757557", "abstract": "Very dense deployments of small cells are one of the key enablers to tackle the ever-growing demand on mobile bandwidth. In such deployments, centralization of RAN functions on cloud resources is envisioned to overcome severe inter-cell interference and to keep costs acceptable. However, RAN back-haul constraints need to be considered when designing the functional split between RAN front-ends and centralized equipment. In this paper we analyse constraints and outline applications of flexible RAN centralization.", "title": "Towards a flexible functional split for cloud-RAN networks"}, "4975817dbb73865cd588b505634bd0d4edf1c269": {"paper_id": "4975817dbb73865cd588b505634bd0d4edf1c269", "abstract": "This paper presents a novel multi-label classification framework for domains with large numbers of labels. Automatic image annotation is such a domain, as the available semantic concepts are typically hundreds. The proposed framework comprises an initial clustering phase that breaks the original training set into several disjoint clusters of data. It then trains a multi-label classifier from the data of each cluster. Given a new test instance, the framework first finds the nearest cluster and then applies the corresponding model. Empirical results using two clustering algorithms, four multi-label classification algorithms and three image annotation data sets suggest that the proposed approach can improve the performance and reduce the training time of standard multi-label classification algorithms, particularly in the case of large number of labels.", "title": "Clustering based multi-label classification for image annotation and retrieval"}, "86dc975f9cbd9a205f8e82fb1db3b61c6b738fa5": {"paper_id": "86dc975f9cbd9a205f8e82fb1db3b61c6b738fa5", "abstract": "As increasingly powerful techniques emerge for machine tagging multimedia content, it becomes ever more important to standardize the underlying vocabularies. Doing so provides interoperability and lets the multimedia community focus ongoing research on a well-defined set of semantics. This paper describes a collaborative effort of multimedia researchers, library scientists, and end users to develop a large standardized taxonomy for describing broadcast news video. The large-scale concept ontology for multimedia (LSCOM) is the first of its kind designed to simultaneously optimize utility to facilitate end-user access, cover a large semantic space, make automated extraction feasible, and increase observability in diverse broadcast news video data sets", "title": "Large-scale concept ontology for multimedia"}, "b94c7ff9532ab26c3aedbee3988ec4c7a237c173": {"paper_id": "b94c7ff9532ab26c3aedbee3988ec4c7a237c173", "abstract": "w e propose Q novel approach for solving the perceptual grouping problem in vision. Rather than focusing on local features and their consistencies in the amage data, our approach aims a t extracting the global impression of an image. We treat image segmentation QS (I graph partitioning problem and propose Q novel global criterion, the normalized cut, for segmenting the graph. The normalized cut craterion measures both the total dissimilarity between the different groups QS well as the total similarity within the groups. We show that an eficient computational technique based on a generaked eigenvalue problem can be used to optimize this criterion. w e have applied this approach to segmenting static images and found results very enco u raging.", "title": "Normalized Cuts and Image Segmentation"}, "9703efad5e36e1ef3ab2292144c1a796515e5f6a": {"paper_id": "9703efad5e36e1ef3ab2292144c1a796515e5f6a", "abstract": "We describe a series o,f five statistical models o,f the translation process and give algorithms,for estimating the parameters o,f these models given a set o,f pairs o,f sentences that are translations o,f one another. We define a concept o,f word-by-word alignment between such pairs o,f sentences. For any given pair of such sentences each o,f our models assigns a probability to each of the possible word-by-word alignments. We give an algorithm for seeking the most probable o,f these alignments. Although the algorithm is suboptimal, the alignment thus obtained accounts well for the word-by-word relationships in the pair o,f sentences. We have a great deal o,f data in French and English from the proceedings o,f the Canadian Parliament. Accordingly, we have restricted our work to these two languages; but we,feel that because our algorithms have minimal linguistic content they would work well on other pairs o,f languages. We also ,feel, again because of the minimal linguistic content o,f our algorithms, that it is reasonable to argue that word-by-word alignments are inherent in any sufficiently large bilingual corpus.", "title": "The Mathematics of Statistical Machine Translation: Parameter Estimation"}, "ab3e1ab27cf19b656d695f3e9d2ac647e597effd": {"paper_id": "ab3e1ab27cf19b656d695f3e9d2ac647e597effd", "abstract": "The accessible presentation of this book gives both a general view of the entire computer vision enterprise and also offers sufficient detail to be able to build useful applications. Users learn techniques that have proven to be useful by first-hand experience and a wide range of mathematical methods. A CD-ROM with every copy of the text contains source code for programming practice, color images, and illustrative movies. Comprehensive and up-to-date, this book includes essential topics that either reflect practical significance or are of theoretical importance. Topics are discussed in substantial and increasing depth. Application surveys describe numerous important application areas such as image based rendering and digital libraries. Many important algorithms broken down and illustrated in pseudo code. Appropriate for use by engineers as a comprehensive reference to the computer vision enterprise.", "title": "Computer Vision: A Modern Approach"}, "57d774b8592b4b3f83f1304be43701ad8517e79a": {"paper_id": "57d774b8592b4b3f83f1304be43701ad8517e79a", "abstract": "In multilabel learning, each instance in the training set is associated with a set of labels and the task is to output a label set whose size is unknown a priori for each unseen instance. In this paper, this problem is addressed in the way that a neural network algorithm named BP-MLL, i.e., backpropagation for multilabel learning, is proposed. It is derived from the popular backpropagation algorithm through employing a novel error function capturing the characteristics of multilabel learning, i.e., the labels belonging to an instance should be ranked higher than those not belonging to that instance. Applications to two real-world multilabel learning problems, i.e., functional genomics and text categorization, show that the performance of BP-MLL is superior to that of some well-established multilabel learning algorithms", "title": "Multilabel Neural Networks with Applications to Functional Genomics and Text Categorization"}, "26f4f07696a3828f5eeb0d8bb8944da80228b77d": {"paper_id": "26f4f07696a3828f5eeb0d8bb8944da80228b77d", "abstract": "The application of boosting procedures to decision tree algorithms has been shown to produce very accurate classi ers. These classiers are in the form of a majority vote over a number of decision trees. Unfortunately, these classi ers are often large, complex and di\u00c6cult to interpret. This paper describes a new type of classi cation rule, the alternating decision tree, which is a generalization of decision trees, voted decision trees and voted decision stumps. At the same time classi ers of this type are relatively easy to interpret. We present a learning algorithm for alternating decision trees that is based on boosting. Experimental results show it is competitive with boosted decision tree algorithms such as C5.0, and generates rules that are usually smaller in size and thus easier to interpret. In addition these rules yield a natural measure of classi cation con dence which can be used to improve the accuracy at the cost of abstaining from predicting examples that are hard to classify.", "title": "The Alternating Decision Tree Learning Algorithm"}, "9ca182ba46b2c60b3b9ffaf8234312a933a8cc6f": {"paper_id": "9ca182ba46b2c60b3b9ffaf8234312a933a8cc6f", "abstract": "1. ABSTRACT Text categorization \u2013 the assignment of natural language texts to one or more predefined categories based on their content \u2013 is an important component in many information organization and management tasks. We compare the effectiveness of five different automatic learning algorithms for text categorization in terms of learning speed, realtime classification speed, and classification accuracy. We also examine training set size, and alternative document representations. Very accurate text classifiers can be learned automatically from training examples. Linear Support Vector Machines (SVMs) are particularly promising because they are very accurate, quick to train, and quick to evaluate. 1.1", "title": "Inductive Learning Algorithms and Representations for Text Categorization"}, "896b9c1551b7ffa347baed144582ec3b5d88f703": {"paper_id": "896b9c1551b7ffa347baed144582ec3b5d88f703", "abstract": "In classic pattern recognition problems, classes are mutually exclusive by de\"nition. Classi\"cation errors occur when the classes overlap in the feature space. We examine a di5erent situation, occurring when the classes are, by de\"nition, not mutually exclusive. Such problems arise in semantic scene and document classi\"cation and in medical diagnosis. We present a framework to handle such problems and apply it to the problem of semantic scene classi\"cation, where a natural scene may contain multiple objects such that the scene can be described by multiple class labels (e.g., a \"eld scene with a mountain in the background). Such a problem poses challenges to the classic pattern recognition paradigm and demands a di5erent treatment. We discuss approaches for training and testing in this scenario and introduce new metrics for evaluating individual examples, class recall and precision, and overall accuracy. Experiments show that our methods are suitable for scene classi\"cation; furthermore, our work appears to generalize to other classi\"cation problems of the same nature. ? 2004 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.", "title": "Learning multi-label scene classification"}, "aae6bf5f8ef9d4431146f0ced8c3922dbf36c935": {"paper_id": "aae6bf5f8ef9d4431146f0ced8c3922dbf36c935", "abstract": "As drivers back out of the driving task, when transported automatically by an intelligent car for a longer time, they are not always able to react properly, if a driver take over request occurs. This paper presents two ways, how to deal with this problem within the scope of a functional safety concept. Thereto, the difference between fully automatic and autonomous driving assistance systems is explained. Afterwards two different strategies to reach a safe state in consequence of a system boundary crossing are proposed. In the first case the fall back state is reached by a driver take over, in the second case by an automatic, active fail-safe mechanism. Subsequently the necessary components for monitoring and reaching a safe state and their embedment in a basic, functional architecture of a driving assistance system are described. In this context, special regard is paid to aspects of redundancy as well. In the end it is concluded, that the safety concept proposed here is crucial for guaranteeing enduring safety in an automatically driving car and in consequence for making automatic driving functions commercially ready for serial production.", "title": "Strategy and architecture of a safety concept for fully automatic and autonomous driving assistance systems"}, "21dca2af71cf9a704a16b44a102d1527635da4a8": {"paper_id": "21dca2af71cf9a704a16b44a102d1527635da4a8", "abstract": "Augmenting a processor with special hardware that is able to apply a Single Instruction to Multiple Data(SIMD) at the same time is a cost effective way of improving processor performance. It also offers a means of improving the ratio of processor performance to power usage due to reduced and more effective data movement and intrinsically lower instruction counts. This paper considers and compares the NEON SIMD instruction set used on the ARM Cortex-A series of RISC processors with the SSE2 SIMD instruction set found on Intel platforms within the context of the Open Computer Vision (OpenCV) library. The performance obtained using compiler auto-vectorization is compared with that achieved using hand-tuning across a range of five different benchmarks and ten different hardware platforms. On the ARM platforms the hand-tuned NEON benchmarks were between 1.05\u00d7 and 13.88\u00d7 faster than the auto-vectorized code, while for the Intel platforms the hand-tuned SSE benchmarks were between 1.34\u00d7 and 5.54\u00d7 faster.", "title": "Use of SIMD Vector Operations to Accelerate Application Code Performance on Low-Powered ARM and Intel Platforms"}, "719de8345edd73a78566e1267aeca60c9545e3df": {"paper_id": "719de8345edd73a78566e1267aeca60c9545e3df", "abstract": "This tutorial reviews image alignment and image stitching algorithms. Image alignment algorithms can discover the correspondence relationships among images with varying degrees of overlap. They are ideally suited for applications such as video stabilization, summarization, and the creation of panoramic mosaics. Image stitching algorithms take the alignment estimates produced by such registration algorithms and blend the images in a seamless manner, taking care to deal with potential problems such as blurring or ghosting caused by parallax and scene movement as well as varying image exposures. This tutorial reviews the basic motion models underlying alignment and stitching algorithms, describes effective direct (pixel-based) and feature-based alignment algorithms, and describes blending algorithms used to produce seamless mosaics. It closes with a discussion of open research problems in the area.", "title": "Image Alignment and Stitching : A Tutorial 1"}, "e1e2f5c00c9584deef1601181b7491bb71220d9b": {"paper_id": "e1e2f5c00c9584deef1601181b7491bb71220d9b", "abstract": "Smart home has become a visible concept that attracts the collaboration of various areas of science and engineering. Increasing the power efficiency and power management has become an important research topic since a decade. In this research article, the focus is to ease the life of elder and handicapped people. In point of fact as compare to the healthy people, elderly and disabled people experiences difficulties to perform their everyday activities. Elderly and disabled people can be supported by using smart homes by providing them secure, safe, and controlled environments. The developed system allows the users to be able to control the appliances with least physical efforts. More particularly, the designed system allow users to switch the home appliances ON and OFF just by sending message command by using Android application or SMS with the help of a cell phone. Not only the far remote monitoring, but the local management of appliances is made possible by using the Bluetooth technology. The experimental results demonstrate that the user can control the home appliances with the designed Android application by using the voice, Bluetooth, and SMS inputs.", "title": "A Smart Home Appliances Power Management System for Handicapped, Elder and Blind People"}, "37aa8184c0c7445d88a3c6d152e13181f0fd0a7a": {"paper_id": "37aa8184c0c7445d88a3c6d152e13181f0fd0a7a", "abstract": "We describe a framework for building abstraction hierarchies whereby an agent alternates skill- and representation-construction phases to construct a sequence of increasingly abstract Markov decision processes. Our formulation builds on recent results showing that the appropriate abstract representation of a problem is specified by the agent's skills. We describe how such a hierarchy can be used for fast planning, and illustrate the construction of an appropriate hierarchy for the Taxi domain.", "title": "Constructing Abstraction Hierarchies Using a Skill-Symbol Loop"}, "4670fdc1f338e425187aea6ea5e0f8379c1367f4": {"paper_id": "4670fdc1f338e425187aea6ea5e0f8379c1367f4", "abstract": "A formalism is presented for computing and organizing actions for autonomous agents in dynamic environments. We introduce the notion of teleo-reactive (T-R) programs whose execution entails the construction of circuitry for the continuous computation of the parameters and conditions on which agent action is based. In addition to continuous feedback, T-R programs support parameter binding and recursion. A primary di erence between T-R programs and many other circuit-based systems is that the circuitry of T-R programs is more compact; it is constructed at run time and thus does not have to anticipate all the contingencies that might arise over all possible runs. In addition, T-R programs are intuitive and easy to write and are written in a form that is compatible with automatic planning and learning methods. We brie y describe some experimental applications of T-R programs in the control of simulated and actual mobile robots.", "title": "Teleo-Reactive Programs for Agent Control"}, "2966ae949d1bc255bad11045fd0ff8eb5848cf5a": {"paper_id": "2966ae949d1bc255bad11045fd0ff8eb5848cf5a", "abstract": "This dissertation investigates the use of hierarchy and problem decomposition as a means of solving large, stochastic, sequential decision problems. These problems are framed as Markov decision problems (MDPs). The new technical content of this dissertation begins with a discussion of the concept of temporal abstraction. Temporal abstraction is shown to be equivalent to the transformation of a policy deened over a region of an MDP to an action in a semi-Markov decision problem (SMDP). Several algorithms are presented for performing this transformation eeciently. This dissertation introduces the HAM method for generating hierarchical, temporally abstract actions. This method permits the partial speciication of abstract actions in a way that corresponds to an abstract plan or strategy. Abstract actions speciied as HAMs can be optimally reened for new tasks by solving a reduced SMDP. The formal results show that traditional MDP algorithms can be used to optimally reene HAMs for new tasks. This can be achieved in much less time than it would take to learn a new policy for the task from scratch. HAMs complement some novel decomposition algorithms that are presented in this dissertation. These algorithms work by constructing a cache of policies for diierent regions of the MDP and then optimally combining the cached solution to produce a global solution that is within provable bounds of the optimal solution. Together, the methods developed in this dissertation provide important tools for 2 producing good policies for large MDPs. Unlike some ad-hoc methods, these methods provide strong formal guarantees. They use prior knowledge in a principled way, and they reduce larger MDPs into smaller ones while maintaining a well-deened relationship between the smaller problem and the larger problem.", "title": "Hierarchical control and learning for markov decision processes"}, "7eca3acd1a4239d8a299478885c7c0548f3900a8": {"paper_id": "7eca3acd1a4239d8a299478885c7c0548f3900a8", "abstract": "This paper presents a method by which a reinforcement learning agent can automatically discover certain types of subgoals online. By creating useful new subgoals while learning, the agent is able to accelerate learning on the current task and to transfer its expertise to other, related tasks through the reuse of its ability to attain subgoals. The agent discovers subgoals based on commonalities across multiple paths to a solution. We cast the task of finding these commonalities as a multiple-instance learning problem and use the concept of diverse density to find solutions. We illustrate this approach using several gridworld tasks.", "title": "Automatic Discovery of Subgoals in Reinforcement Learning using Diverse Density"}, "0b036ba37045e3258a5446ff1380074d9cd1b679": {"paper_id": "0b036ba37045e3258a5446ff1380074d9cd1b679", "abstract": "This paper presents a new approach to hierarchical reinforcement learning based on decomposing the target Markov decision process (MDP) into a hierarchy of smaller MDPs and decomposing the value function of the target MDP into an additive combination of the value functions of the smaller MDPs. The decomposition, known as the MAXQ decomposition, has both a procedural semantics|as a subroutine hierarchy|and a declarative semantics|as a representation of the value function of a hierarchical policy. MAXQ uni es and extends previous work on hierarchical reinforcement learning by Singh, Kaelbling, and Dayan and Hinton. It is based on the assumption that the programmer can identify useful subgoals and de ne subtasks that achieve these subgoals. By de ning such subgoals, the programmer constrains the set of policies that need to be considered during reinforcement learning. The MAXQ value function decomposition can represent the value function of any policy that is consistent with the given hierarchy. The decomposition also creates opportunities to exploit state abstractions, so that individual MDPs within the hierarchy can ignore large parts of the state space. This is important for the practical application of the method. This paper de nes the MAXQ hierarchy, proves formal results on its representational power, and establishes ve conditions for the safe use of state abstractions. The paper presents an online model-free learning algorithm, MAXQ-Q, and proves that it converges with probability 1 to a kind of locally-optimal policy known as a recursively optimal policy, even in the presence of the ve kinds of state abstraction. The paper evaluates the MAXQ representation and MAXQ-Q through a series of experiments in three domains and shows experimentally that MAXQ-Q (with state abstractions) converges to a recursively optimal policy much faster than at Q learning. The fact that MAXQ learns a representation of the value function has an important bene t: it makes it possible to compute and execute an improved, non-hierarchical policy via a procedure similar to the policy improvement step of policy iteration. The paper demonstrates the e ectiveness of this non-hierarchical execution experimentally. Finally, the paper concludes with a comparison to related work and a discussion of the design tradeo s in hierarchical reinforcement learning. c 2000 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.", "title": "Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition"}, "737c2076eb52dc62850fd78e905c7299340079a2": {"paper_id": "737c2076eb52dc62850fd78e905c7299340079a2", "abstract": "Anton Schwartz Dept. of Computer Science Stanford University Stanford, CA 94305 Email: schwartz@cs.stanford.edu Reinforcement learning addresses the problem of learning to select actions in order to maximize one's performance in unknown environments. To scale reinforcement learning to complex real-world tasks, such as typically studied in AI, one must ultimately be able to discover the structure in the world, in order to abstract away the myriad of details and to operate in more tractable problem spaces. This paper presents the SKILLS algorithm. SKILLS discovers skills, which are partially defined action policies that arise in the context of multiple, related tasks. Skills collapse whole action sequences into single operators. They are learned by minimizing the compactness of action policies, using a description length argument on their representation. Empirical results in simple grid navigation tasks illustrate the successful discovery of structure in reinforcement learning.", "title": "Finding Structure in Reinforcement Learning"}, "19059f39effd43aa1501c9b84fcbddfa1c925de4": {"paper_id": "19059f39effd43aa1501c9b84fcbddfa1c925de4", "abstract": "An open problem in reinforcement learning is discovering hierarchical structure. HEXQ, an algorithm which automatically attempts to decompose and solve a model-free factored MDP hierarchically is described. By searching for aliased Markov sub-space regions based on the state variables the algorithm uses temporal and state abstraction to construct a hierarchy of interlinked smaller MDPs.", "title": "Discovering Hierarchy in Reinforcement Learning with HEXQ"}, "11a9594b53c45680f0c6d3dd274f764b7ab7c6ea": {"paper_id": "11a9594b53c45680f0c6d3dd274f764b7ab7c6ea", "abstract": "R-max is a very simple model-based reinforcement learning algorithm which can attain near-optimal average reward in polynomial time. In R-max, the agent always maintains a complete, but possibly inaccurate model of its environment and acts based on the optimal policy derived from this model. The model is initialized in an optimistic fashion: all actions in all states return the maximal possible reward (hence the name). During execution, it is updated based on the agent\u2019s observations. R-max improves upon several previous algorithms: (1) It is simpler and more general than Kearns and Singh\u2019s E algorithm, covering zero-sum stochastic games. (2) It has a built-in mechanism for resolving the exploration vs. exploitation dilemma. (3) It formally justifies the \u201coptimism under uncertainty\u201d bias used in many RL algorithms. (4) It is simpler, more general, and more efficient than Brafman and Tennenholtz\u2019s LSG algorithm for learning in single controller stochastic games. (5) It generalizes the algorithm by Monderer and Tennenholtz for learning in repeated games. (6) It is the only algorithm for learning in repeated games, to date, which is provably efficient, considerably improving and simplifying previous algorithms by Banos and by Megiddo.", "title": "R-MAX - A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning"}, "749c81cc86c3a946424332f3866474b4a1e8014f": {"paper_id": "749c81cc86c3a946424332f3866474b4a1e8014f", "abstract": "Locally weighted projection regression is a new algorithm that achieves nonlinear function approximation in high dimensional spaces with redundant and irrelevant input dimensions. At its core, it uses locally linear models, spanned by a small number of univariate regressions in selected directions in input space. This paper evaluates different methods of projection regression and derives a nonlinear function approximator based on them. This nonparametric local learning system i) learns rapidly with second order learning methods based on incremental training, ii) uses statistically sound stochastic cross validation to learn iii) adjusts its weighting kernels based on local information only, iv) has a computational complexity that is linear in the number of inputs, and v) can deal with a large number of possibly redundant inputs, as shown in evaluations with up to 50 dimensional data sets. To our knowledge, this is the first truly incremental spatially localized learning method to combine all these properties.", "title": "Locally Weighted Projection Regression : An O(n) Algorithm for Incremental Real Time Learning in High Dimensional Space"}, "b85017cd90762af4ae85e678dcb6f1cf838ac086": {"paper_id": "b85017cd90762af4ae85e678dcb6f1cf838ac086", "abstract": "We present Variable Influence Structure Analysis, or VISA, an algorithm that performs hierarchical decomposition of factored Markov decision processes. VISA uses a dynamic Bayesian network model of actions, and constructs a causal graph that captures relationships between state variables. In tasks with sparse causal graphs VISA exploits structure by introducing activities that cause the values of state variables to change. The result is a hierarchy of activities that together represent a solution to the original task. VISA performs state abstraction for each activity by ignoring irrelevant state variables and lower-level activities. In addition, we describe an algorithm for constructing compact models of the activities introduced. State abstraction and compact activity models enable VISA to apply efficient algorithms to solve the stand-alone subtask associated with each activity. Experimental results show that the decomposition introduced by VISA can significantly accelerate construction of an optimal, or near-optimal, policy.", "title": "Causal Graph Based Decomposition of Factored MDPs"}, "4cc4dc9d7d51b342510e8cc90b51b189ccf57dd4": {"paper_id": "4cc4dc9d7d51b342510e8cc90b51b189ccf57dd4", "abstract": "Within the field of Neuro Robotics we are driven primarily by the desire to understand how humans and animals live and grow and solve every day\u2019s problems. To this aim we adopted a \u201clearn by doing\u201d approach by building artificial systems, e.g. robots that not only look like human beings but also represent a model of some brain process. They should, ideally, behave and interact like human beings (being situated). The main emphasis in robotics has been on systems that act as a reaction to an external stimulus (e.g. tracking, reaching), rather than as a result of an internal drive to explore or \u201cunderstand\u201d the environment. We think it is now appropriate to try to move from acting, in the sense explained above, to \u201cunderstanding\u201d. As a starting point we addressed the problem of learning about the effects and consequences of self-generated actions. How does the robot learn how to pull an object toward itself or to push it away? How does the robot learn that spherical objects roll while a cube only slides if pushed? Interacting with objects is important because it implicitly explores object representation, event understanding, and can provide definition of objecthood that could not be grasped with a mere passive observation of the world. Further, learning to understand what one\u2019s own body can do is an essential step toward learning by imitation. In this view two actions are similar not only if their kinematics and dynamics are similar but rather if the effects on the external world are the same. Along this line of research we discuss some recent experiments performed at the AILab at MIT and at the LIRA-Lab at the University of Genova on COG and Babybot respectively. We show how the humanoid robots can learn how to poke and prod objects to obtain a consistently repeatable effect (e.g. sliding in a given direction), to help visual segmentation, and to interpret a poking action performed by a human manipulator.", "title": "Learning about objects through action -initial steps towards artificial cognition"}, "384bb3944abe9441dcd2cede5e7cd7353e9ee5f7": {"paper_id": "384bb3944abe9441dcd2cede5e7cd7353e9ee5f7", "abstract": null, "title": "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods"}, "2ebeb153fddca8f3dca8665566cbe85dae21f641": {"paper_id": "2ebeb153fddca8f3dca8665566cbe85dae21f641", "abstract": "Agents learning to act autonomously in realworld domains must acquire a model of the dynamics of the domain in which they operate. Learning domain dynamics can be challenging, especially where an agent only has partial access to the world state, and/or noisy external sensors. Even in standard STRIPS domains, existing approaches cannot learn from noisy, incomplete observations typical of real-world domains. We propose a method which learns STRIPS action models in such domains, by decomposing the problem into first learning a transition function between states in the form of a set of classifiers, and then deriving explicit STRIPS rules from the classifiers\u2019 parameters. We evaluate our approach on simulated standard planning domains from the International Planning Competition, and show that it learns useful domain descriptions from noisy, incomplete observations.", "title": "Learning STRIPS Operators from Noisy and Incomplete Observations"}, "24d462ab793395cf629620952fc455d3f6a86314": {"paper_id": "24d462ab793395cf629620952fc455d3f6a86314", "abstract": "A fundamental problem in reinforcement learning is balancing exploration and exploitation. We address this problem in the context of model-based reinforcement learning in large stochastic relational domains by developing relational extensions of the concepts of the E and R-max algorithms. Efficient exploration in exponentially large state spaces needs to exploit the generalization of the learned model: what in a propositional setting would be considered a novel situation and worth exploration may in the relational setting be a wellknown context in which exploitation is promising. To address this we introduce relational count functions which generalize the classical notion of state and action visitation counts. We provide guarantees on the exploration efficiency of our framework using count functions under the assumption that we had a relational KWIK learner and a near-optimal planner. We propose a concrete exploration algorithm which integrates a practically efficient probabilistic rule learner and a relational planner (for which there are no guarantees, however) and employs the contexts of learned relational rules as features to model the novelty of states and actions. Our results in noisy 3D simulated robot manipulation problems and in domains of the international planning competition demonstrate that our approach is more effective than existing propositional and factored exploration techniques.", "title": "Exploration in relational domains for model-based reinforcement learning"}, "c08549d77b291b720e66392378ff9917d5a1a498": {"paper_id": "c08549d77b291b720e66392378ff9917d5a1a498", "abstract": "Some of the most frequently used online applications are Facebook, WhatsApp, and Twitter. These applications allow individuals to communicate with other users, to share information or pictures, and to stay in contact with friends all over the world. However, a growing number of users suffer from negative consequences due to their excessive use of these applications, which can be referred to as Internet-communication disorder. The frequent use and easy access of these applications may also trigger the individual's fear of missing out on content when not accessing these applications. Using a sample of 270 participants, a structural equation model was analyzed to investigate the role of psychopathological symptoms and the fear of missing out on expectancies towards Internet-communication applications in the development of symptoms of an Internet-communication disorder. The results suggest that psychopathological symptoms predict higher fear of missing out on the individual's Internet-communication applications and higher expectancies to use these applications as a helpful tool to escape from negative feelings. These specific cognitions mediate the effect of psychopathological symptoms on Internet-communication disorder. Our results are in line with the theoretical model by Brand et al. (2016) as they show how Internet-related cognitive bias mediates the relationship between a person's core characteristics (e.g., psychopathological symptoms) and Internet-communication disorder. However, further studies should investigate the role of the fear of missing out as a specific predisposition, as well as specific cognition in the online context.", "title": "Online-specific fear of missing out and Internet-use expectancies contribute to symptoms of Internet-communication disorder"}, "19cac228f0b1882d97d0c0a09eade1d1fc62a274": {"paper_id": "19cac228f0b1882d97d0c0a09eade1d1fc62a274", "abstract": "Although social networking sites (SNSs) have attracted increased attention and members in recent years, there has been little research on it: particularly on how a users\u2019 extroversion or introversion can affect their intention to pay for these services and what other factors might influence them. We therefore proposed and tested a model that measured the users\u2019 value and satisfaction perspectives by examining the influence of these factors in an empirical survey of 288 SNS members. At the same time, the differences due to their psychological state were explored. The causal model was validated using PLSGraph 3.0; six out of eight study hypotheses were supported. The results indicated that perceived value significantly influenced the intention to pay SNS subscription fees while satisfaction did not. Moreover, extroverts thought more highly of the social value of the SNS, while introverts placed more importance on its emotional and price value. The implications of these findings are discussed. Crown Copyright 2010 Published by Elsevier B.V. All rights reserved.", "title": "The influence of extro/introversion on the intention to pay for social networking sites"}, "d39fe688a41ff3b105e39166a11edd454cf33d37": {"paper_id": "d39fe688a41ff3b105e39166a11edd454cf33d37", "abstract": "There is growing concern about excessive Internet use and whether this can amount to an addiction. In researching this topic, a valid and reliable assessment instrument is essential. In her survey of Internet addiction, Young designed the Internet Addiction Test (IAT), which provides a basis for developments. The IAT has high face validity, but it has not been subjected to systematic psychometric testing. This study sought to replicate and expand Young's survey, and to examine the IAT more systematically. A questionnaire that existed as a Web page was devised, consisting of the IAT and 15 other questions regarding the respondents' demographic information and Internet usage. Participants were recruited through the Internet, yielding 86 valid responses (29 males and 57 females). Factor analysis of the IAT revealed six factors--salience, excessive use, neglecting work, anticipation, lack of control, and neglecting social life. These factors showed good internal consistency and concurrent validity, with salience being the most reliable. Younger and more recent users reported more problems, mainly concerning the neglect of work and social life. We expected interactive Internet functions to be more addictive; however, this was not found to be so. Overall, the IAT is a valid and reliable instrument that may be used in further research on Internet addiction.", "title": "The Psychometric Properties of the Internet Addiction Test"}, "158e131aac4ed0751a0e6752f8618b4528da879b": {"paper_id": "158e131aac4ed0751a0e6752f8618b4528da879b", "abstract": "Electronic networks of practice are computer mediated discussion forums focused on problems of practice that enable individuals to exchange advice and ideas with others based on common interests. However, why individuals help strangers in these electronic networks is not well under stood: there is no immediate benefit to the contri 1V. Sambamurthy and Mani Subramani were the accepting senior editors for this paper. butor, and free-riders are able to acquire the same knowledge as everyone else. To understand this paradox, we apply theories of collective action to examine how individual motivations and social capital influence knowledge contribution in elect ronic networks. This study reports on the activities of one electronic network supporting a professional legal association. Using archival, network, survey, and content analysis data, we empirically test a model of knowledge contribution. We find that people contribute their knowledge when they per ceive that it enhances their professional repu tations, when they have the experience to share, and when they are structurally embedded in the network. Surprisingly, contributions occur without regard to expectations of reciprocity from others or high levels of commitment to the network.", "title": "Why Should I Share? Examining Social Capital and Knowledge Contribution in Electronic Networks of Practice"}, "7752e0835506a6629c1b06e67f2afb1e5d2bb714": {"paper_id": "7752e0835506a6629c1b06e67f2afb1e5d2bb714", "abstract": "Content Memory (Learning Ability) As Comprehension 82 Vocabulary Cs .30 ( ) .23 .31 ( ) .31 .31 .35 ( ) .29 .48 .35 .38 ( ) .30 .40 .47 .58 .48 ( ) As judged against these latter values, comprehension (.48) and vocabulary (.47), but not memory (.31), show some specific validity. This transmutability of the validation matrix argues for the comparisons within the heteromethod block as the most generally relevant validation data, and illustrates the potential interchangeability of trait and method components. Some of the correlations in Chi's (1937) prodigious study of halo effect in ratings are appropriate to a multitrait-multimethod matrix in which each rater might be regarded as representing a different method. While the published report does not make these available in detail because it employs averaged values, it is apparent from a comparison of his Tables IV and VIII that the ratings generally failed to meet the requirement that ratings of the same trait by different raters should correlate higher than ratings of different traits by the same rater. Validity is shown to the extent that of the correlations in the heteromethod block, those in the validity diagonal are higher than the average heteromethod-heterotrait values. A conspicuously unsuccessful multitrait-multimethod matrix is provided by Campbell (1953, 1956) for rating of the leadership behavior of officers by themselves and by their subordinates. Only one of 11 variables (Recognition Behavior) met the requirement of providing a validity diagonal value higher than any of the heterotrait-heteromethod values, that validity being .29. For none of the variables were the validities higher than heterotrait-monomethod values. A study of attitudes toward authority and nonauthority figures by Burwen and Campbell (1957) contains a complex multitrait-multimethod matrix, one symmetrical excerpt from which is shown in Table 6. Method variance was strong for most of the procedures in this study. Where validity was found, it was primarily at the level of validity diagonal values higher than heterotrait-heteromethod values. As illustrated in Table 6, attitude toward father showed this kind of validity, as did attitude toward peers to a lesser degree. Attitude toward boss showed no validity. There was no evidence of a generalized attitude toward authority which would include father and boss, although such values as the VALIDATION BY THE MULTITRAIT-MULTIMETHOD MATRIX", "title": "Convergent and discriminant validation by the multitrait-multimethod matrix."}, "4308a36ae0f69f236de4a62215c20856650bd269": {"paper_id": "4308a36ae0f69f236de4a62215c20856650bd269", "abstract": "In the increasingly user-generated Web, users\u2019 personality traits may be crucial factors leading them to engage in this participatory media. The literature suggests factors such as extraversion, emotional stability and openness to experience are related to uses of social applications on the Internet. Using a national sample of US adults, this study investigated the relationship between these three dimensions of the BigFive model and social media use (defined as use of social networking sites and instant messages). It also examined whether gender and age played a role in that dynamic. Results revealed that while extraversion and openness to experiences were positively related to social media use, emotional stability was a negative predictor, controlling for socio-demographics and life satisfaction. These findings differed by gender and age. While extraverted men and women were both likely to be more frequent users of social media tools, only the men with greater degrees of emotional instability were more regular users. The relationship between extraversion and social media use was particularly important among the young adult cohort. Conversely, being open to new experiences emerged as an important personality predictor of social media use for the more mature segment of the sample. 2009 Elsevier Ltd. All rights reserved.", "title": "Who interacts on the Web?: The intersection of users' personality and social media use"}, "12fabf5b64b8bbbb12493bbd86f375f803eb1be6": {"paper_id": "12fabf5b64b8bbbb12493bbd86f375f803eb1be6", "abstract": "When time is limited, researchers may be faced with the choice of using an extremely brief measure of the Big-Five personality dimensions or using no measure at all. To meet the need for a very brief measure, 5 and 10-item inventories were developed and evaluated. Although somewhat inferior to standard multi-item instruments, the instruments reached adequate levels in terms of: (a) convergence with widely used Big-Five measures in self, observer, and peer reports, (b) test\u2013retest reliability, (c) patterns of predicted external correlates, and (d) convergence between self and observer ratings. On the basis of these tests, a 10-item measure of the Big-Five dimensions is offered for situations where very short measures are needed, personality is not the primary topic of interest, or researchers can tolerate the somewhat diminished psychometric properties associated with very brief measures. ! 2003 Elsevier Science (USA). All rights reserved.", "title": "A very brief measure of the Big-Five personality domains q"}, "bfd040e969ac790cb113cd37bea7f284fe9e230c": {"paper_id": "bfd040e969ac790cb113cd37bea7f284fe9e230c", "abstract": "Are there systematic differences between people who use social network sites and those who stay away, despite a familiarity with them? Based on data from a survey administered to a diverse group of young adults, this article looks at the predictors of SNS usage, with particular focus on Facebook, MySpace, Xanga, and Friendster. Findings suggest that use of such sites is not randomly distributed across a group of highly wired users. A person's gender, race and ethnicity, and parental educational background are all associated with use, but in most cases only when the aggregate concept of social network sites is disaggregated by service. Additionally, people with more experience and autonomy of use are more likely to be users of such sites. Unequal participation based on user background suggests that differential adoption of such services may be contributing to digital inequality.", "title": "Whose Space? Differences Among Users and Non-Users of Social Network Sites"}, "61ebbc3458002805accc332f9cde1177cf98439a": {"paper_id": "61ebbc3458002805accc332f9cde1177cf98439a", "abstract": "Using data from a popular online social network site, this paper explores the relationship between profile structure (namely, which fields are completed) and number of friends, giving designers insight into the importance of the profile and how it works to encourage connections and articulated relationships between users. We describe a theoretical framework that draws on aspects of signaling theory, common ground theory, and transaction costs theory to generate an understanding of why certain profile fields may be more predictive of friendship articulation on the site. Using a dataset consisting of 30,773 Facebook profiles, we determine which profile elements are most likely to predict friendship links and discuss the theoretical and design implications of our findings.", "title": "A familiar face(book): profile elements as signals in an online social network"}, "807ee6cc7f803fb1fbac7efc0dde57439a6968bb": {"paper_id": "807ee6cc7f803fb1fbac7efc0dde57439a6968bb", "abstract": "Hypotheses involving mediation are common in the behavioral sciences. Mediation exists when a predictor affects a dependent variable indirectly through at least one intervening variable, or mediator. Methods to assess mediation involving multiple simultaneous mediators have received little attention in the methodological literature despite a clear need. We provide an overview of simple and multiple mediation and explore three approaches that can be used to investigate indirect processes, as well as methods for contrasting two or more mediators within a single model. We present an illustrative example, assessing and contrasting potential mediators of the relationship between the helpfulness of socialization agents and job satisfaction. We also provide SAS and SPSS macros, as well as Mplus and LISREL syntax, to facilitate the use of these methods in applications.", "title": "Asymptotic and resampling strategies for assessing and comparing indirect effects in multiple mediator models."}, "67739eab6289ca65f05a0747433c25b892962cb9": {"paper_id": "67739eab6289ca65f05a0747433c25b892962cb9", "abstract": "Previous research has shown a relationship between use of social networking sites and feelings of social capital. However, most studies have relied on self-reports by college students. The goals of the current study are to (1) validate the common self-report scale using empirical data from Facebook, (2) test whether previous findings generalize to older and international populations, and (3) delve into the specific activities linked to feelings of social capital and loneliness. In particular, we investigate the role of directed interaction between pairs---such as wall posts, comments, and \"likes\" --- and consumption of friends' content, including status updates, photos, and friends' conversations with other friends. We find that directed communication is associated with greater feelings of bonding social capital and lower loneliness, but has only a modest relationship with bridging social capital, which is primarily related to overall friend network size. Surprisingly, users who consume greater levels of content report reduced bridging and bonding social capital and increased loneliness. Implications for designs to support well-being are discussed.", "title": "Social network activity and social well-being"}, "216e92be827a9774130263754f2036aab0808484": {"paper_id": "216e92be827a9774130263754f2036aab0808484", "abstract": "This study examines the relationship between use of Facebook, a popular online social network site, and the formation and maintenance of social capital. In addition to assessing bonding and bridging social capital, we explore a dimension of social capital that assesses one's ability to stay connected with members of a previously inhabited community, which we call maintained social capital. Regression analyses conducted on results from a survey of undergraduate students (N=286) suggest a strong association between use of Facebook and the three types of social capital, with the strongest relationship being to bridging social capital. In addition, Facebook usage was found to interact with measures of psychological well-being, suggesting that it might provide greater benefits for users experiencing low self-esteem and low life satisfaction.", "title": "The Benefits of Facebook \"Friends: \" Social Capital and College Students' Use of Online Social Network Sites"}, "200c10cbb2eaf70727af4751f0d8e0a5a6661e07": {"paper_id": "200c10cbb2eaf70727af4751f0d8e0a5a6661e07", "abstract": "Human beings can be proactive and engaged or, alternatively, passive and alienated, largely as a function of the social conditions in which they develop and function. Accordingly, research guided by self-determination theory has focused on the social-contextual conditions that facilitate versus forestall the natural processes of self-motivation and healthy psychological development. Specifically, factors have been examined that enhance versus undermine intrinsic motivation, self-regulation, and well-being. The findings have led to the postulate of three innate psychological needs--competence, autonomy, and relatedness--which when satisfied yield enhanced self-motivation and mental health and when thwarted lead to diminished motivation and well-being. Also considered is the significance of these psychological needs and processes within domains such as health care, education, work, sport, religion, and psychotherapy.", "title": "Self-determination theory and the facilitation of intrinsic motivation, social development, and well-being."}, "7bb966dee8b010ae1987f4f8de5156ea3cc7cfa1": {"paper_id": "7bb966dee8b010ae1987f4f8de5156ea3cc7cfa1", "abstract": "Discussions with traditional developers and managers concerning agile software development practices nearly always contain two somewhat contradictory ideas. They find that on small, stand-alone projects, agile practices are less burdensome and more in tune with the software industry's increasing needs for rapid development and coping with continuous change. Managers face several barriers, real and perceived, when they try to bring agile approaches into traditional organizations. They categorized the barriers either as problems only in terms of scope or scale, or as significant general issues needing resolution. From these two categories, we've identified three areas - development process conflicts, business process conflicts, and people conflicts - that we believe are the critical challenges to software managers of large organizations in bringing agile approaches to bear in their projects.", "title": "Management challenges to implementing agile processes in traditional development organizations"}, "ee573602c129b147e1d9274345e034d4cd0d30a2": {"paper_id": "ee573602c129b147e1d9274345e034d4cd0d30a2", "abstract": "This paper presents \"BetterAuth\", an authentication protocol for Web applications. Its design is based on the experiences of two decades with the Web. BetterAuth addresses existing attacks on Web authentication, ranging from network attacks to Cross-site Request Forgery up to Phishing. Furthermore, the protocol can be realized completely in standard JavaScript. This allows Web applications an early adoption, even in a situation with limited browser support.", "title": "BetterAuth: web authentication revisited"}, "389f55c5c376db4ce1c88161dca98c329614faa8": {"paper_id": "389f55c5c376db4ce1c88161dca98c329614faa8", "abstract": "User errors cause or contribute to most computer security failures, yet user interfaces for security still tend to be clumsy, confusing, or near-nonexistent. Is this simply due to a failure to apply standard user interface design techniques to security? We argue that, on the contrary, effective security requires a different usability standard, and that it will not be achieved through the user interface design techniques appropriate to other types of consumer software. To test this hypothesis, we performed a case study of a security program which does have a good user interface by general standards: PGP 5.0. Our case study used a cognitive walkthrough analysis together with a laboratory user test to evaluate whether PGP 5.0 can be successfully used by cryptography novices to achieve effective electronic mail security. The analysis found a number of user interface design flaws that may contribute to security failures, and the user test demonstrated that when our test participants were given 90 minutes in which to sign and encrypt a message using PGP 5.0, the majority of them were unable to do so successfully. We conclude that PGP 5.0 is not usable enough to provide effective security for most computer users, despite its attractive graphical user interface, supporting our hypothesis that user interface design for effective security remains an open problem. We close with a brief description of our continuing work on the development and application of user interface design principles and techniques for security.", "title": "Why Johnny Can't Encrypt: A Usability Evaluation of PGP 5.0"}, "57e58e630ed2b8ca2029aa970b862aa59d53a3dc": {"paper_id": "57e58e630ed2b8ca2029aa970b862aa59d53a3dc", "abstract": "Current security systems su er from the fact that they fail to account for human factors. This paper considers two human limitations: First, people are slow and unreliable when comparing meaningless strings; and second, people have di culties in remembering strong passwords or PINs. We identify two applications where these human factors negatively a ect security: Validation of root keys in public-key infrastructures, and user authentication. Our approach to improve the security of these systems is to use hash visualization, a technique which replaces meaningless strings with structured images. We examine the requirements of such a system and propose the prototypical solution Random Art . We also show how to apply hash visualization to improve the real-world security of root key validation and user authentication.", "title": "Hash Visualization : a New Technique to improve Real-World Security"}, "67f5f4cd77c63f6c196f5ced2047ae80145d49cb": {"paper_id": "67f5f4cd77c63f6c196f5ced2047ae80145d49cb", "abstract": "This paper presents a new password authentication and key-exchange protocol suitable for authenticating users and exchanging keys over an untrusted network. The new protocol resists dictionary attacks mounted by either passive or active network intruders, allowing, in principle, even weak passphrases to be used safely. It also o ers perfect forward secrecy, which protects past sessions and passwords against future compromises. Finally, user passwords are stored in a form that is not plaintext-equivalent to the password itself, so an attacker who captures the password database cannot use it directly to compromise security and gain immediate access to the host. This new protocol combines techniques of zero-knowledge proofs with asymmetric key exchange protocols and o ers signi cantly improved performance over comparably strong extended methods that resist stolen-veri er attacks such as Augmented EKE or B-SPEKE.", "title": "The Secure Remote Password Protocol"}, "4a80cadc310768f1dc2da605495c9e18eecc8e48": {"paper_id": "4a80cadc310768f1dc2da605495c9e18eecc8e48", "abstract": "The idea that chemotherapy can be used in combination with immunotherapy may seem somewhat counterproductive, as it can theoretically eliminate the immune cells needed for antitumour immunity. However, much preclinical work has now demonstrated that in addition to direct cytotoxic effects on cancer cells, a proportion of DNA damaging agents may actually promote immunogenic cell death, alter the inflammatory milieu of the tumour microenvironment and/or stimulate neoantigen production, thereby activating an antitumour immune response. Some notable combinations have now moved forward into the clinic, showing promise in phase I\u2013III trials, whereas others have proven toxic, and challenging to deliver. In this review, we discuss the emerging data of how DNA damaging agents can enhance the immunogenic properties of malignant cells, focussing especially on immunogenic cell death, and the expansion of neoantigen repertoires. We discuss how best to strategically combine DNA damaging therapeutics with immunotherapy, and the challenges of successfully delivering these combination regimens to patients. With an overwhelming number of chemotherapy/immunotherapy combination trials in process, clear hypothesis-driven trials are needed to refine the choice of combinations, and determine the timing and sequencing of agents in order to stimulate antitumour immunological memory and improve maintained durable response rates, with minimal toxicity.", "title": "Combining DNA damaging therapeutics with immunotherapy: more haste, less speed"}, "59c5eecef1f35bd331f7e9ebe1a1a784bcdb76a5": {"paper_id": "59c5eecef1f35bd331f7e9ebe1a1a784bcdb76a5", "abstract": "In this paper, a new decoupling control scheme, in terms of an internal model control (IMC) structure, is proposed for two-input -two-output (TITO) processes with time delays. Noteworthy improvement of the decoupling regulation can be achieved for the nominal system output responses, and moreover, either of the system output responses can be quantitatively regulated by virtue of the analytical relationship between the adjustable parameters of the decoupling controller matrix and the nominal system transfer matrix. The ideally optimal controller matrix is analytically derived by proposing the practically desired diagonal system transfer matrix, in terms of the robust H 2 optimal performance objective. Because of the difficulty of physical implementation, its practical form is carefully configured according to whether there exist any complex righthalf-plane (RHP) zeros in the process transfer matrix determinant. At the same time, tuning constraints for the proposed decoupling controller matrix to hold the control system robust stability are analyzed in the presence of the process additive and multiplicative uncertainties, and, accordingly, the on-line tuning rule is provided to cope with the process unmodeled dynamics in practice. Finally, illustrative simulation examples are included to demonstrate the remarkable superiority of the proposed method.", "title": "Analytical Design of Decoupling Internal Model Control ( IMC ) Scheme for Two-Input-Two-Output ( TITO ) Processes with Time Delays"}, "9f6199cb2cf831401c0e33199abe9fc3aa911a33": {"paper_id": "9f6199cb2cf831401c0e33199abe9fc3aa911a33", "abstract": "Finding methods to represent multiple types of nodes in heterogeneous networks is both challenging and rewarding, as there is much less work in this area compared with that of homogeneous networks. In this paper, we propose a novel approach to learn node embedding for heterogeneous networks through a joint learning framework of both network links and text associated with nodes. A novel attention mechanism is also used to make good use of text extended through links to obtain much larger network context. Link embedding is first learned through a random-walk-based method to process multiple types of links. Text embedding is separately learned at both sentence level and document level to capture salient semantic information more comprehensively. Then, both types of embeddings are jointly fed into a hierarchical neural network model to learn node representation through mutual enhancement. The attention mechanism follows linked edges to obtain context of adjacent nodes to extend context for node representation. The evaluation on a link prediction task in a heterogeneous network data set shows that our method outperforms the current state-of-the-art method by 2.5%\u20135.0% in AUC values with p-value less than 10\u22129, indicating very significant improvement.", "title": "Learning Heterogeneous Network Embedding From Text and Links"}, "b294b61f0b755383072ab332061f45305e0c12a1": {"paper_id": "b294b61f0b755383072ab332061f45305e0c12a1", "abstract": "We present a fast method for re-purposing existing semantic word vectors to improve performance in a supervised task. Recently, with an increase in computing resources, it became possible to learn rich word embeddings from massive amounts of unlabeled data. However, some methods take days or weeks to learn good embeddings, and some are notoriously difficult to train. We propose a method that takes as input an existing embedding, some labeled data, and produces an embedding in the same space, but with a better predictive performance in the supervised task. We show improvement on the task of sentiment classification with respect to several baselines, and observe that the approach is most useful when the training set is sufficiently small.", "title": "Re-embedding words"}, "1c95596284319b192958825776a2a9455b9078ed": {"paper_id": "1c95596284319b192958825776a2a9455b9078ed", "abstract": "This paper addresses how a recursive neural network model can automatically leave out useless information and emphasize important evidence, in other words, to perform \u201cweight tuning\u201d for higher-level representation acquisition. We propose two models, Weighted Neural Network (WNN) and Binary-Expectation Neural Network (BENN), which automatically control how much one specific unit contributes to the higher-level representation. The proposed model can be viewed as incorporating a more powerful compositional function for embedding acquisition in recursive neural networks. Experimental results demonstrate the significant improvement over standard neural models.", "title": "Feature Weight Tuning for Recursive Neural Networks"}, "459745250d453dacf8373b7a34a353bb1fa9e147": {"paper_id": "459745250d453dacf8373b7a34a353bb1fa9e147", "abstract": "We propose a mathematical framework for a unification of the distributional theory of meaning in terms of vector space models, and a compositional theory for grammatical types, namely Lambek\u2019s pregroup semantics. A key observation is that the monoidal category of (finite dimensional) vector spaces, linear maps and the tensor product, as well as any pregroup, are examples of compact closed categories. Since, by definition, a pregroup is a compact closed category with trivial morphisms, its compositional content is reflected within the compositional structure of any non-degenerate compact closed category. The (slightly refined) category of vector spaces enables us to compute the meaning of a compound well-typed sentence from the meaning of its constituents, by \u2018lifting\u2019 the type reduction mechanisms of pregroup semantics to the whole category. These sentence meanings live in a single space, independent of the grammatical structure of the sentence. Hence we can use the inner-product to compare meanings of arbitrary sentences. A variation of this procedure which involves constraining the scalars of the vector spaces to the semiring of Booleans results in the well-known Montague semantics.", "title": "A Compositional Distributional Model of Meaning"}, "1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba": {"paper_id": "1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba", "abstract": "We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.", "title": "Convolutional Neural Networks for Sentence Classification"}, "6d81305e1a3680455ee66418e306defeb5969a68": {"paper_id": "6d81305e1a3680455ee66418e306defeb5969a68", "abstract": "We present a graph-based semi-supervised learning algorithm to address the sentiment analysis task of rating inference. Given a set of documents (e.g., movie reviews) and accompanying ratings (e.g., \u201c4 stars\u201d), the task calls for inferring numerical ratings for unlabeled documents based on the perceived sentiment expressed by their text. In particular, we are interested in the situation where labeled data is scarce. We place this task in the semi-supervised setting and demonstrate that considering unlabeled reviews in the learning process can improve ratinginference performance. We do so by creating a graph on both labeled and unlabeled data to encode certain assumptions for this task. We then solve an optimization problem to obtain a smooth rating function over the whole graph. When only limited labeled data is available, this method achieves significantly better predictive accuracy over other methods that ignore the unlabeled examples during training.", "title": "Seeing Stars When There Aren\u2019t Many Stars: Graph-Based Semi-Supervised Learning For Sentiment Categorization"}, "862a1b44938fd280d730c6fb1224f27303e05607": {"paper_id": "862a1b44938fd280d730c6fb1224f27303e05607", "abstract": "The limitations of diagnostic \"accuracy\" as a measure of decision performance require introduction of the concepts of the \"sensitivity\" and \"specificity\" of a diagnostic test. These measures and the related indices, \"true positive fraction\" and \"false positive fraction,\" are more meaningful than \"accuracy,\" yet do not provide a unique description of diagnostic performance because they depend on the arbitrary selection of a decision threshold. The receiver operating characteristic (ROC) curve is shown to be a simple yet complete empirical description of this decision threshold effect, indicating all possible combinations of the relative frequencies of the various kinds of correct and incorrect decisions. Practical experimental techniques for measuring ROC curves are described, and the issues of case selection and curve-fitting are discussed briefly. Possible generalizations of conventional ROC analysis to account for decision performance in complex diagnostic tasks are indicated. ROC analysis is shown to be related in a direct and natural way to cost/benefit analysis of diagnostic decision making. The concepts of \"average diagnostic cost\" and \"average net benefit\" are developed and used to identify the optimal compromise among various kinds of diagnostic error. Finally, the way in which ROC analysis can be employed to optimize diagnostic strategies is suggested.", "title": "Basic principles of ROC analysis."}, "9be428c9383d47b86570b1b9fc20faf006346c5d": {"paper_id": "9be428c9383d47b86570b1b9fc20faf006346c5d", "abstract": "The modern science of networks has brought significant advances to our understanding of complex systems. One of the most relevant features of graphs representing real systems is community structure, or clustering, i. e. the organization of vertices in clusters, with many edges joining vertices of the same cluster and comparatively few edges joining vertices of different clusters. Such clusters, or communities, can be considered as fairly independent compartments of a graph, playing a similar role like, e. g., the tissues or the organs in the human body. Detecting communities is of great importance in sociology, biology and computer science, disciplines where systems are often represented as graphs. This problem is very hard and not yet satisfactorily solved, despite the huge effort of a large interdisciplinary community of scientists working on it over the past few years. We will attempt a thorough exposition of the topic, from the definition of the main elements of the problem, to the presentation of most methods developed, with a special focus on techniques designed by statistical physicists, from the discussion of crucial issues like the significance of clustering and how methods should be tested and compared against each other, to the description of applications to real networks.", "title": "Community detection in graphs"}, "312a2edbec5fae34beaf33faa059d37d04cb7235": {"paper_id": "312a2edbec5fae34beaf33faa059d37d04cb7235", "abstract": "Uncovering the community structure exhibited by real networks is a crucial step toward an understanding of complex systems that goes beyond the local organization of their constituents. Many algorithms have been proposed so far, but none of them has been subjected to strict tests to evaluate their performance. Most of the sporadic tests performed so far involved small networks with known community structure and/or artificial graphs with a simplified structure, which is very uncommon in real systems. Here we test several methods against a recently introduced class of benchmark graphs, with heterogeneous distributions of degree and community size. The methods are also tested against the benchmark by Girvan and Newman [Proc. Natl. Acad. Sci. U.S.A. 99, 7821 (2002)] and on random graphs. As a result of our analysis, three recent algorithms introduced by Rosvall and Bergstrom [Proc. Natl. Acad. Sci. U.S.A. 104, 7327 (2007); Proc. Natl. Acad. Sci. U.S.A. 105, 1118 (2008)], Blondel [J. Stat. Mech.: Theory Exp. (2008), P10008], and Ronhovde and Nussinov [Phys. Rev. E 80, 016109 (2009)] have an excellent performance, with the additional advantage of low computational complexity, which enables one to analyze large systems.", "title": "Community detection algorithms: a comparative analysis."}, "6b6ae4ff053bcee2834b5e7718810cb5bc15c36c": {"paper_id": "6b6ae4ff053bcee2834b5e7718810cb5bc15c36c", "abstract": "We describe some new exactly solvable models of the structure of social networks, based on random graphs with arbitrary degree distributions. We give models both for simple unipartite networks, such as acquaintance networks, and bipartite networks, such as affiliation networks. We compare the predictions of our models to data for a number of real-world social networks and find that in some cases, the models are in remarkable agreement with the data, whereas in others the agreement is poorer, perhaps indicating the presence of additional social structure in the network that is not captured by the random graph.", "title": "Random graph models of social networks."}, "013cd20c0eaffb9cab80875a43086e0c3224fe20": {"paper_id": "013cd20c0eaffb9cab80875a43086e0c3224fe20", "abstract": "The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.", "title": "Representation Learning: A Review and New Perspectives"}, "1a67622ca58aa851afe36ad6c6e78f9fb9d691d2": {"paper_id": "1a67622ca58aa851afe36ad6c6e78f9fb9d691d2", "abstract": "We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models. DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs.\n DeepWalk uses local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences. We demonstrate DeepWalk's latent representations on several multi-label network classification tasks for social networks such as BlogCatalog, Flickr, and YouTube. Our results show that DeepWalk outperforms challenging baselines which are allowed a global view of the network, especially in the presence of missing information. DeepWalk's representations can provide F1 scores up to 10% higher than competing methods when labeled data is sparse. In some experiments, DeepWalk's representations are able to outperform all baseline methods while using 60% less training data.\n DeepWalk is also scalable. It is an online learning algorithm which builds useful incremental results, and is trivially parallelizable. These qualities make it suitable for a broad class of real world applications such as network classification, and anomaly detection.", "title": "DeepWalk: online learning of social representations"}, "44044556dae0e21cab058c18f704b15d33bd17c5": {"paper_id": "44044556dae0e21cab058c18f704b15d33bd17c5", "abstract": "Attributed network embedding aims to seek low-dimensional vector representations for nodes in a network, such that original network topological structure and node attribute proximity can be preserved in the vectors. These learned representations have been demonstrated to be helpful in many learning tasks such as network clustering and link prediction. While existing algorithms follow an unsupervised manner, nodes in many real-world attributed networks are often associated with abundant label information, which is potentially valuable in seeking more effective joint vector representations. In this paper, we investigate how labels can be modeled and incorporated to improve attributed network embedding. This is a challenging task since label information could be noisy and incomplete. In addition, labels are completely distinct with the geometrical structure and node attributes. The bewildering combination of heterogeneous information makes the joint vector representation learning more difficult. To address these issues, we propose a novel Label informed Attributed Network Embedding (LANE) framework. It can smoothly incorporate label information into the attributed network embedding while preserving their correlations. Experiments on real-world datasets demonstrate that the proposed framework achieves significantly better performance compared with the state-of-the-art embedding algorithms.", "title": "Label Informed Attributed Network Embedding"}, "4148d9c252a560ebba2b37b70a39d27ae3b9f2db": {"paper_id": "4148d9c252a560ebba2b37b70a39d27ae3b9f2db", "abstract": "Natural language understanding and dialogue policy learning are both essential in conversational systems that predict the next system actions in response to a current user utterance. Conventional approaches aggregate separate models of natural language understanding (NLU) and system action prediction (SAP) as a pipeline that is sensitive to noisy outputs of error-prone NLU. To address the issues, we propose an end-to-end deep recurrent neural network with limited contextual dialogue memory by jointly training NLU and SAP on DSTC4 multi-domain human-human dialogues. Experiments show that our proposed model significantly outperforms the state-of-the-art pipeline models for both NLU and SAP, which indicates that our joint model is capable of mitigating the affects of noisy NLU outputs, and NLU model can be refined by error flows backpropagating from the extra supervised signals of system actions.", "title": "End-to-end joint learning of natural language understanding and dialogue manager"}, "0a8221f744e8ba86d69a07295b35b05531523e49": {"paper_id": "0a8221f744e8ba86d69a07295b35b05531523e49", "abstract": "Online social networks play a major role in the spread of information at very large scale. A lot of effort have been made in order to understand this phenomenon, ranging from popular topic detection to information diffusion modeling, including influential spreaders identification. In this article, we present a survey of representative methods dealing with these issues and propose a taxonomy that summarizes the state-of-the-art. The objective is to provide a comprehensive analysis and guide of existing efforts around information diffusion in social networks. This survey is intended to help researchers in quickly understanding existing works and possible improvements to bring.", "title": "Information diffusion in online social networks: a survey"}, "b7b9196227cdf2f55bd5b6fac373c9127ccb7b57": {"paper_id": "b7b9196227cdf2f55bd5b6fac373c9127ccb7b57", "abstract": "Data embedding is used in many machine learning applications to create low-dimensional feature representations, which preserves the structure of data points in their original space. In this paper, we examine the scenario of a heterogeneous network with nodes and content of various types. Such networks are notoriously difficult to mine because of the bewildering combination of heterogeneous contents and structures. The creation of a multidimensional embedding of such data opens the door to the use of a wide variety of off-the-shelf mining techniques for multidimensional data. Despite the importance of this problem, limited efforts have been made on embedding a network of scalable, dynamic and heterogeneous data. In such cases, both the content and linkage structure provide important cues for creating a unified feature representation of the underlying network. In this paper, we design a deep embedding algorithm for networked data. A highly nonlinear multi-layered embedding function is used to capture the complex interactions between the heterogeneous data in a network. Our goal is to create a multi-resolution deep embedding function, that reflects both the local and global network structures, and makes the resulting embedding useful for a variety of data mining tasks. In particular, we demonstrate that the rich content and linkage information in a heterogeneous network can be captured by such an approach, so that similarities among cross-modal data can be measured directly in a common embedding space. Once this goal has been achieved, a wide variety of data mining problems can be solved by applying off-the-shelf algorithms designed for handling vector representations. Our experiments on real-world network datasets show the effectiveness and scalability of the proposed algorithm as compared to the state-of-the-art embedding methods.", "title": "Heterogeneous Network Embedding via Deep Architectures"}, "63cd2482cdc8dfab89c9f98f893a6647cdfa08e0": {"paper_id": "63cd2482cdc8dfab89c9f98f893a6647cdfa08e0", "abstract": "MOTIVATION\nMost existing methods for predicting causal disease genes rely on specific type of evidence, and are therefore limited in terms of applicability. More often than not, the type of evidence available for diseases varies-for example, we may know linked genes, keywords associated with the disease obtained by mining text, or co-occurrence of disease symptoms in patients. Similarly, the type of evidence available for genes varies-for example, specific microarray probes convey information only for certain sets of genes. In this article, we apply a novel matrix-completion method called Inductive Matrix Completion to the problem of predicting gene-disease associations; it combines multiple types of evidence (features) for diseases and genes to learn latent factors that explain the observed gene-disease associations. We construct features from different biological sources such as microarray expression data and disease-related textual data. A crucial advantage of the method is that it is inductive; it can be applied to diseases not seen at training time, unlike traditional matrix-completion approaches and network-based inference methods that are transductive.\n\n\nRESULTS\nComparison with state-of-the-art methods on diseases from the Online Mendelian Inheritance in Man (OMIM) database shows that the proposed approach is substantially better-it has close to one-in-four chance of recovering a true association in the top 100 predictions, compared to the recently proposed Catapult method (second best) that has <15% chance. We demonstrate that the inductive method is particularly effective for a query disease with no previously known gene associations, and for predicting novel genes, i.e. genes that are previously not linked to diseases. Thus the method is capable of predicting novel genes even for well-characterized diseases. We also validate the novelty of predictions by evaluating the method on recently reported OMIM associations and on associations recently reported in the literature.\n\n\nAVAILABILITY\nSource code and datasets can be downloaded from http://bigdata.ices.utexas.edu/project/gene-disease.", "title": "Inductive matrix completion for predicting gene\u2013disease associations"}, "10eb7bfa7687f498268bdf74b2f60020a151bdc6": {"paper_id": "10eb7bfa7687f498268bdf74b2f60020a151bdc6", "abstract": "We present a new technique called \u201ct-SNE\u201d that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large datasets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of datasets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on almost all of the datasets.", "title": "Visualizing Data using t-SNE"}, "d0b7c8828f0fca4dd901674e8fb5bd464a187664": {"paper_id": "d0b7c8828f0fca4dd901674e8fb5bd464a187664", "abstract": "Network embedding is an important method to learn low-dimensional representations of vertexes in networks, aiming to capture and preserve the network structure. Almost all the existing network embedding methods adopt shallow models. However, since the underlying network structure is complex, shallow models cannot capture the highly non-linear network structure, resulting in sub-optimal network representations. Therefore, how to find a method that is able to effectively capture the highly non-linear network structure and preserve the global and local structure is an open yet important problem. To solve this problem, in this paper we propose a Structural Deep Network Embedding method, namely SDNE. More specifically, we first propose a semi-supervised deep model, which has multiple layers of non-linear functions, thereby being able to capture the highly non-linear network structure. Then we propose to exploit the first-order and second-order proximity jointly to preserve the network structure. The second-order proximity is used by the unsupervised component to capture the global network structure. While the first-order proximity is used as the supervised information in the supervised component to preserve the local network structure. By jointly optimizing them in the semi-supervised deep model, our method can preserve both the local and global network structure and is robust to sparse networks. Empirically, we conduct the experiments on five real-world networks, including a language network, a citation network and three social networks. The results show that compared to the baselines, our method can reconstruct the original network significantly better and achieves substantial gains in three applications, i.e. multi-label classification, link prediction and visualization.", "title": "Structural Deep Network Embedding"}, "1df176123ada3c3aebba3d9fcec55386091b0d13": {"paper_id": "1df176123ada3c3aebba3d9fcec55386091b0d13", "abstract": "We consider the task of estimating, from observed data, a pro babilistic model that is parameterized by a finite number of parameters. In particular, we are consid ering the situation where the model probability density function is unnormalized. That is, the model is only specified up to the partition function. The partition function normalizes a model so that it integrates to one for any choice of the parameters. However, it is often impossible to obtain it closed form. Gibbs distributions, Markov and multi-layer networks are examples of models wher e analytical normalization is often impossible. Maximum likelihood estimation can then not be u s d without resorting to numerical approximations which are often computationally expensive . W propose here a new objective function for the estimation of both normalized and unnormalized models. The basic idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially generated noise. With this approach, the normalizing partition f u ction can be estimated like any other parameter. We prove that the new estimation method leads to a consistent (convergent) estimator of the parameters. For large noise sample sizes, the new esti mator is furthermore shown to behave like the maximum likelihood estimator. In the estimati on of unnormalized models, there is a trade-off between statistical and computational performa nce. We show that the new method strikes a competitive trade-off in comparison to other estimation m ethods for unnormalized models. As an application to real data, we estimate novel two-layer model s of natural image statistics with spline nonlinearities.", "title": "Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics"}, "1005645c05585c2042e3410daeed638b55e2474d": {"paper_id": "1005645c05585c2042e3410daeed638b55e2474d", "abstract": "Neural probabilistic language models (NPLMs) have been shown to be competitive with and occasionally superior to the widely-used n-gram language models. The main drawback of NPLMs is their extremely long training and testing times. Morin and Bengio have proposed a hierarchical language model built around a binary tree of words, which was two orders of magnitude faster than the nonhierarchical model it was based on. However, it performed considerably worse than its non-hierarchical counterpart in spite of using a word tree created using expert knowledge. We introduce a fast hierarchical language model along with a simple feature-based algorithm for automatic construction of word trees from the data. We then show that the resulting models can outperform non-hierarchical neural models as well as the best n-gram models.", "title": "A Scalable Hierarchical Distributed Language Model"}, "23694a80bf1b9b38215be3e23068dd75296bc90f": {"paper_id": "23694a80bf1b9b38215be3e23068dd75296bc90f", "abstract": "A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.", "title": "A Neural Probabilistic Language Model"}, "052b1d8ce63b07fec3de9dbb583772d860b7c769": {"paper_id": "052b1d8ce63b07fec3de9dbb583772d860b7c769", "abstract": "We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal \u2018hidden\u2019 units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.", "title": "Learning representations by back-propagating errors"}, "4c15b129a8da55127e4e2fe47f54799d0a313367": {"paper_id": "4c15b129a8da55127e4e2fe47f54799d0a313367", "abstract": "If we take an existing supervised NLP system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih & Hinton, 2009) embeddings of words on both NER and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines. We find further improvements by combining different word representations. You can download our word features, for off-the-shelf use in existing NLP systems, as well as our code, here: http://metaoptimize. com/projects/wordreprs/", "title": "Word Representations: A Simple and General Method for Semi-Supervised Learning"}, "1a07186bc10592f0330655519ad91652125cd907": {"paper_id": "1a07186bc10592f0330655519ad91652125cd907", "abstract": "We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in state-of-the-art-performance.", "title": "A unified architecture for natural language processing: deep neural networks with multitask learning"}, "3a91c3eb9a05ef5c8b875ab112448cc3f44a1268": {"paper_id": "3a91c3eb9a05ef5c8b875ab112448cc3f44a1268", "abstract": "We present several modifications of the original recurrent neural network language model (RNN LM).While this model has been shown to significantly outperform many competitive language modeling techniques in terms of accuracy, the remaining problem is the computational complexity. In this work, we show approaches that lead to more than 15 times speedup for both training and testing phases. Next, we show importance of using a backpropagation through time algorithm. An empirical comparison with feedforward networks is also provided. In the end, we discuss possibilities how to reduce the amount of parameters in the model. The resulting RNN model can thus be smaller, faster both during training and testing, and more accurate than the basic one.", "title": "Extensions of recurrent neural network language model"}, "b13813b49f160e1a2010c44bd4fb3d09a28446e3": {"paper_id": "b13813b49f160e1a2010c44bd4fb3d09a28446e3", "abstract": "We have already shown that extracting long-term dependencies from sequential data is difficult, both for determimstic dynamical systems such as recurrent networks, and probabilistic models such as hidden Markov models (HMMs) or input/output hidden Markov models (IOHMMs). In practice, to avoid this problem, researchers have used domain specific a-priori knowledge to give meaning to the hidden or state variables representing past context. In this paper, we propose to use a more general type of a-priori knowledge, namely that the temporal dependencIes are structured hierarchically. This implies that long-term dependencies are represented by variables with a long time scale. This principle is applied to a recurrent network which includes delays and multiple time scales. Experiments confirm the advantages of such structures. A similar approach is proposed for HMMs and IOHMMs.", "title": "Hierarchical Recurrent Neural Networks for Long-Term Dependencies"}, "2006f8d01395dab714bdcdcfd1cebbe6d6276e35": {"paper_id": "2006f8d01395dab714bdcdcfd1cebbe6d6276e35", "abstract": "One of the key problems in spoken language understanding (SLU) is the task of slot filling. In light of the recent success of applying deep neural network technologies in domain detection and intent identification, we carried out an in-depth investigation on the use of recurrent neural networks for the more difficult task of slot filling involving sequence discrimination. In this work, we implemented and compared several important recurrent-neural-network architectures, including the Elman-type and Jordan-type recurrent networks and their variants. To make the results easy to reproduce and compare, we implemented these networks on the common Theano neural network toolkit, and evaluated them on the ATIS benchmark. We also compared our results to a conditional random fields (CRF) baseline. Our results show that on this task, both types of recurrent networks outperform the CRF baseline substantially, and a bi-directional Jordantype network that takes into account both past and future dependencies among slots works best, outperforming a CRFbased baseline by 14% in relative error reduction.", "title": "Investigation of recurrent-neural-network architectures and learning methods for spoken language understanding"}, "29cc0a8802126d4e97f28109763df26ab91c6531": {"paper_id": "29cc0a8802126d4e97f28109763df26ab91c6531", "abstract": "How do real graphs evolve over time? What are normal growth patterns in social, technological, and information networks? Many studies have discovered patterns in static graphs, identifying properties in a single snapshot of a large network or in a very small number of snapshots; these include heavy tails for in- and out-degree distributions, communities, small-world phenomena, and others. However, given the lack of information about network evolution over long periods, it has been hard to convert these findings into statements about trends over time.\n Here we study a wide range of real graphs, and we observe some surprising phenomena. First, most of these graphs densify over time with the number of edges growing superlinearly in the number of nodes. Second, the average distance between nodes often shrinks over time in contrast to the conventional wisdom that such distance parameters should increase slowly as a function of the number of nodes (like O(log n) or O(log(log n)).\n Existing graph generation models do not exhibit these types of behavior even at a qualitative level. We provide a new graph generator, based on a forest fire spreading process that has a simple, intuitive justification, requires very few parameters (like the flammability of nodes), and produces graphs exhibiting the full range of properties observed both in prior work and in the present study.\n We also notice that the forest fire model exhibits a sharp transition between sparse graphs and graphs that are densifying. Graphs with decreasing distance between the nodes are generated around this transition point.\n Last, we analyze the connection between the temporal evolution of the degree distribution and densification of a graph. We find that the two are fundamentally related. We also observe that real networks exhibit this type of relation between densification and the degree distribution.", "title": "Graph evolution: Densification and shrinking diameters"}, "0834e74304b547c9354b6d7da6fa78ef47a48fa8": {"paper_id": "0834e74304b547c9354b6d7da6fa78ef47a48fa8", "abstract": "This paper studies the problem of embedding very large information networks into low-dimensional vector spaces, which is useful in many tasks such as visualization, node classification, and link prediction. Most existing graph embedding methods do not scale for real world information networks which usually contain millions of nodes. In this paper, we propose a novel network embedding method called the ``LINE,'' which is suitable for arbitrary types of information networks: undirected, directed, and/or weighted. The method optimizes a carefully designed objective function that preserves both the local and global network structures. An edge-sampling algorithm is proposed that addresses the limitation of the classical stochastic gradient descent and improves both the effectiveness and the efficiency of the inference. Empirical experiments prove the effectiveness of the LINE on a variety of real-world information networks, including language networks, social networks, and citation networks. The algorithm is very efficient, which is able to learn the embedding of a network with millions of vertices and billions of edges in a few hours on a typical single machine. The source code of the LINE is available online\\footnote{\\url{https://github.com/tangjianpku/LINE}}.", "title": "LINE: Large-scale Information Network Embedding"}, "50e983fd06143cad9d4ac75bffc2ef67024584f2": {"paper_id": "50e983fd06143cad9d4ac75bffc2ef67024584f2", "abstract": "LIBLINEAR is an open source library for large-scale linear classification. It supports logistic regression and linear support vector machines. We provide easy-to-use command-line tools and library calls for users and developers. Comprehensive documents are available for both beginners and advanced users. Experiments demonstrate that LIBLINEAR is very efficient on large sparse data sets.", "title": "LIBLINEAR: A Library for Large Linear Classification"}, "ee742cdcec6fb80fda256c7202ffc3e7e2b34f4f": {"paper_id": "ee742cdcec6fb80fda256c7202ffc3e7e2b34f4f", "abstract": null, "title": "The link-prediction problem for social networks"}, "18ca2837d280a6b2250024b6b0e59345601064a7": {"paper_id": "18ca2837d280a6b2250024b6b0e59345601064a7", "abstract": "Many areas of science depend on exploratory data analysis and visualization. The need to analyze large amounts of multivariate data raises the fundamental problem of dimensionality reduction: how to discover compact representations of high-dimensional data. Here, we introduce locally linear embedding (LLE), an unsupervised learning algorithm that computes low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs. Unlike clustering methods for local dimensionality reduction, LLE maps its inputs into a single global coordinate system of lower dimensionality, and its optimizations do not involve local minima. By exploiting the local symmetries of linear reconstructions, LLE is able to learn the global structure of nonlinear manifolds, such as those generated by images of faces or documents of text.", "title": "Nonlinear dimensionality reduction by locally linear embedding."}, "0ce8879ea7fc0e96fd0e4e242a46002010f86e18": {"paper_id": "0ce8879ea7fc0e96fd0e4e242a46002010f86e18", "abstract": "One of the central problems in machine learning and pattern recognition is to develop appropriate representations for complex data. We consider the problem of constructing a representation for data lying on a low-dimensional manifold embedded in a high-dimensional space. Drawing on the correspondence between the graph Laplacian, the Laplace Beltrami operator on the manifold, and the connections to the heat equation, we propose a geometrically motivated algorithm for representing the high-dimensional data. The algorithm provides a computationally efficient approach to nonlinear dimensionality reduction that has locality-preserving properties and a natural connection to clustering. Some potential applications and illustrative examples are discussed.", "title": "Laplacian Eigenmaps for Dimensionality Reduction and Data Representation"}, "373f76633cc1f6c7a421e31c989842021a52fca4": {"paper_id": "373f76633cc1f6c7a421e31c989842021a52fca4", "abstract": "We show how to use complementary priors to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.", "title": "A Fast Learning Algorithm for Deep Belief Nets"}, "2fac6c403631eaa1658e0405aed7980786a001ab": {"paper_id": "2fac6c403631eaa1658e0405aed7980786a001ab", "abstract": "We propose a framework for collaborative filtering based on Restricted Boltzmann Machines (RBM), which extends previous RBMbased approaches in several important directions. First, while previous RBM research has focused on modeling the correlation between item ratings, we model both user-user and item-item correlations in a unified hybrid non-IID framework. We further use real values in the visible layer as opposed to multinomial variables, thus taking advantage of the natural order between user-item ratings. Finally, we explore the potential of combining the original training data with data generated by the RBM-based model itself in a bootstrapping fashion. The evaluation on two MovieLens datasets (with 100K and 1M user-item ratings, respectively), shows that our RBM model rivals the best previouslyproposed approaches.", "title": "A non-IID Framework for Collaborative Filtering with Restricted Boltzmann Machines"}, "1efdad6f91e830fd64306e4625f74191b05ef9c4": {"paper_id": "1efdad6f91e830fd64306e4625f74191b05ef9c4", "abstract": "Numerous important problems can be framed as learning from graph data. We propose a framework for learning convolutional neural networks for arbitrary graphs. These graphs may be undirected, directed, and with both discrete and continuous node and edge attributes. Analogous to image-based convolutional networks that operate on locally connected regions of the input, we present a general approach to extracting locally connected regions from graphs. Using established benchmark data sets, we demonstrate that the learned feature representations are competitive with state of the art graph kernels and that their computation is highly efficient.", "title": "Learning Convolutional Neural Networks for Graphs"}, "c2fd72cb2a77941e655b5d949d0d59b01e173c3b": {"paper_id": "c2fd72cb2a77941e655b5d949d0d59b01e173c3b", "abstract": "In this paper, we present {GraRep}, a novel model for learning vertex representations of weighted graphs. This model learns low dimensional vectors to represent vertices appearing in a graph and, unlike existing work, integrates global structural information of the graph into the learning process. We also formally analyze the connections between our work and several previous research efforts, including the DeepWalk model of Perozzi et al. as well as the skip-gram model with negative sampling of Mikolov et al.\n We conduct experiments on a language network, a social network as well as a citation network and show that our learned global representations can be effectively used as features in tasks such as clustering, classification and visualization. Empirical results demonstrate that our representation significantly outperforms other state-of-the-art methods in such tasks.", "title": "GraRep: Learning Graph Representations with Global Structural Information"}, "8b79a3cf6499757a477683643eadf5f4597f4f2c": {"paper_id": "8b79a3cf6499757a477683643eadf5f4597f4f2c", "abstract": "Recommender systems o\u0089en use latent features to explain the behaviors of users and capture the properties of items. As users interact with di\u0082erent items over time, user and item features can in\u0083uence each other, evolve and co-evolve over time. \u008ce compatibility of user and item\u2019s feature further in\u0083uence the future interaction between users and items. Recently, point process based models have been proposed in the literature aiming to capture the temporally evolving nature of these latent features. However, these models o\u0089en make strong parametric assumptions about the evolution process of the user and item latent features, which may not re\u0083ect the reality, and has limited power in expressing the complex and nonlinear dynamics underlying these processes. To address these limitations, we propose a novel deep coevolutionary network model (DeepCoevolve), for learning user and item features based on their interaction graph. DeepCoevolve use recurrent neural network (RNN) over evolving networks to de\u0080ne the intensity function in point processes, which allows the model to capture complex mutual in\u0083uence between users and items, and the feature evolution over time. We also develop an e\u0081cient procedure for training the model parameters, and show that the learned models lead to signi\u0080cant improvements in recommendation and activity prediction compared to previous state-of-the-arts parametric models.", "title": "Deep Coevolutionary Network: Embedding User and Item Features for Recommendation"}, "6f6de78d1804b13ab3d2879390db5a2adebcf3af": {"paper_id": "6f6de78d1804b13ab3d2879390db5a2adebcf3af", "abstract": "The goal of subspace learning is to find a k-dimensional subspace of R, such that the expected squared distance between instance vectors and the subspace is as small as possible. In this paper we study subspace learning in a partial information setting, in which the learner can only observe r \u2264 d attributes from each instance vector. We propose several efficient algorithms for this task, and analyze their sample complexity.", "title": "Subspace Learning with Partial Information"}, "ecc7607622c202f5a4e83a9a233930d1ba2f6648": {"paper_id": "ecc7607622c202f5a4e83a9a233930d1ba2f6648", "abstract": "The correct determination of driver's level of fatigue has been of vital importance for the safety of driving. There are various methods, such as analyzing facial expression, eyelid activity, and head movements to assess the fatigue level of drivers. This paper describes the design and prototype implementation of a driver fatigue level determination system based on detection of saccadic eye movements. Driver's eye movement speed is used to assess driver's fatigue level. The information about eyes is obtained via infrared led camera device. Movements of pupils were recorded in two driving scenarios with different traffic density. In the first scenario, the traffic density was set to low while the second scenario was based on high density and aggressive traffic. Based on the movements of pupils, the data on saccadic eye movement was analyzed to determine fatigue level of the driver. Acceleration, speed, and size of pupils at both traffic scenarios were compared with data mining techniques, such as segmentation adaptive peak, entropy, and data distribution analyses. Significantly different levels of fatigue were found between the tired and vigorous driver for the different types of scenarios.", "title": "Driver fatigue detection based on saccadic eye movements"}, "f125bf582310fe1b952536701008665f48abbf2c": {"paper_id": "f125bf582310fe1b952536701008665f48abbf2c", "abstract": "Preventing accidents caused by drowsiness has become a major focus of active safety driving in recent years. It requires an optimal technique to continuously detect drivers' cognitive state related to abilities in perception, recognition, and vehicle control in (near-) real-time. The major challenges in developing such a system include: 1) the lack of significant index for detecting drowsiness and 2) complicated and pervasive noise interferences in a realistic and dynamic driving environment. In this paper, we develop a drowsiness-estimation system based on electroencephalogram (EEG) by combining independent component analysis (ICA), power-spectrum analysis, correlation evaluations, and linear regression model to estimate a driver's cognitive state when he/she drives a car in a virtual reality (VR)-based dynamic simulator. The driving error is defined as deviations between the center of the vehicle and the center of the cruising lane in the lane-keeping driving task. Experimental results demonstrate the feasibility of quantitatively estimating drowsiness level using ICA-based multistream EEG spectra. The proposed ICA-based method applied to power spectrum of ICA components can successfully (1) remove most of EEG artifacts, (2) suggest an optimal montage to place EEG electrodes, and estimate the driver's drowsiness fluctuation indexed by the driving performance measure. Finally, we present a benchmark study in which the accuracy of ICA-component-based alertness estimates compares favorably to scalp-EEG based.", "title": "EEG-based drowsiness estimation for safety driving using independent component analysis"}, "958ea430ebece9b14671428cd030a68a6ba612b8": {"paper_id": "958ea430ebece9b14671428cd030a68a6ba612b8", "abstract": "Edges characterize boundaries and are therefore a problem of fundamental importance in image processing. Image Edge detection significantly reduces the amount of data and filters out useless information, while preserving the important structural properties in an image. Since edge detection is in the forefront of image processing for object detection, it is crucial to have a good understanding of edge detection algorithms. In this paper the comparative analysis of various Image Edge Detection techniques is presented. The software is developed using MATLAB 7.0. It has been shown that the Canny s edge detection algorithm performs better than all these operators under almost all scenarios. Evaluation of the images showed that under noisy conditions Canny, LoG( Laplacian of Gaussian), Robert, Prewitt, Sobel exhibit better performance, respectively. . It has been observed that Canny s edge detection algorithm is computationally more expensive compared to LoG( Laplacian of Gaussian), Sobel, Prewitt and Robert s operator CITED BY (354) 1 Gra a, R. F. P. S. O. (2012). Segmenta o de imagens tor cicas de Raio-X (Doctoral dissertation, UNIVERSIDADE DA BEIRA INTERIOR). 2 ZENDI, M., & YILMAZ, A. (2013). DEGISIK BAKIS A ILARINDAN ELDE EDILEN G R NT GRUPLARININ SINIFLANDIRILMASI. Journal of Aeronautics & Space Technolog ies/Havacilik ve Uzay Teknolojileri Derg is i, 6(1). 3 TROFINO, A. F. N. (2014). TRABALHO DE CONCLUS O DE CURSO. 4 Juan Albarrac n, J. (2011). Dise o, an lis is y optimizaci n de un s istema de reconocimiento de im genes basadas en contenido para imagen publicitaria (Doctoral dissertation). 5 Bergues, G., Ames, G., Canali, L., Schurrer, C., & Fles ia, A. G. (2014, June). Detecci n de l neas en im genes con ruido en un entorno de medici n de alta precis i n. In Biennial Congress of Argentina (ARGENCON), 2014 IEEE (pp. 582-587). IEEE. 6 Andrianto, D. S. (2013). Analisa Statistik terhadap perubahan beberapa faktor deteksi kemacetan melalui pemrosesan video beserta peng iriman notifikas i kemacetan. Jurnal Sarjana ITB bidang Teknik Elektro dan Informatika, 2(1). 7 Pier g , M., & Jaskowiec, J. Identyfikacja twarzy z wykorzystaniem Sztucznych Sieci Neuronowych oraz PCA. 8 Nugraha, K. A., Santoso, A. J., & Suselo, T. (2015, July). ALGORITMA BACKPROPAGATION PADA JARINGAN SARAF TIRUAN UNTUK PENGENALAN POLA WAYANG KULIT. In Seminar Nasional Informatika 2008 (Vol. 1, No. 4). 9 Cornet, T. (2012). Formation et D veloppement des Lacs de Titan: Interpr tation G omorpholog ique d'Ontario Lacus et Analogues Terrestres (Doctoral dissertation, Ecole Centrale de Nantes (ECN)(ECN)(ECN)(ECN)). 10 Li, L., Sun, L., Ning , G., & Tan, S. (2014). Automatic Pavement Crack Recognition Based on BP Neural Network. PROMET-Traffic&Transportation, 26(1), 11-22. 11 Quang Hong , N., Khanh Quoc, D., Viet Anh, N., Chien Van, T., ???, & ???. (2015). Rate Allocation for Block-based Compressive Sensing . Journal of Broadcast Eng ineering , 20(3), 398-407. 12 Swillo, S. (2013). Zastosowanie techniki wizyjnej w automatyzacji pomiar w geometrii i podnoszeniu jakosci wyrob w wytwarzanych w przemysle motoryzacyjnym. Prace Naukowe Politechniki Warszawskiej. Mechanika, (257), 3-128. 13 V zina, M. (2014). D veloppement de log iciels de thermographie infrarouge visant am liorer le contr le de la qualit de la pose de l enrob bitumineux. 14 Decourselle, T. (2014). Etude et mod lisation du comportement des gouttelettes de produits phytosanitaires sur les feuilles de vigne par imagerie ultra-rapide et analyse de texture (Doctoral dissertation, Univers it de Bourgogne). 15 Reja, I. D., & Santoso, A. J. (2013). Pengenalan Motif Sarung (Utan Maumere) Menggunakan Deteksi Tepi. Semantik, 3(1). 16 Feng , Y., & Chen, F. (2013). Fast volume measurement algorithm based on image edge detection. Journal of Computer Applications, 6, 064. 17 Krawczuk, A., & Dominczuk, J. (2014). The use of computer image analys is in determining adhesion properties . Applied Computer Science, 10(3), 68-77. 18 Hui, L., Park, M. W., & Brilakis , I. (2014). Automated Brick Counting for Fa ade Construction Progress Estimation. Journal of Computing in Civil Eng ineering , 04014091. 19 Mahmud, S., Mohammed, J., & Muaidi, H. (2014). A Survey of Dig ital Image Processing Techniques in Character Recognition. IJCSNS, 14(3), 65. 20 Yazdanparast, E., Dos Anjos , A., Garcia, D., Loeuillet, C., Shahbazkia, H. R., & Vergnes, B. (2014). INsPECT, an Open-Source and Versatile Software for Automated Quantification of (Leishmania) Intracellular Parasites . 21 Furtado, L. F. F., Trabasso, L. G., Villani, E., & Francisco, A. (2012, December). Temporal filter applied to image sequences acquired by an industrial robot to detect defects in large aluminum surfaces areas. In MECHATRONIKA, 2012 15th International Symposium (pp. 1-6). IEEE. 22 Zhang , X. H., Li, G., Li, C. L., Zhang , H., Zhao, J., & Hou, Z. X. (2015). Stereo Matching Algorithm Based on 2D Delaunay Triangulation. Mathematical Problems in Eng ineering , 501, 137193. 23 Hasan, H. M. Image Based Vehicle Traffic Measurement. 24 Taneja, N. PERFORMANCE EVALUATION OF IMAGE SEGMENTATION TECHNIQUES USED FOR QUALITATIVE ANALYSIS OF MEMBRANE FILTER. 25 Mathur, A., & Mathur, R. (2013). Content Based Image Retrieval by Multi Features us ing Image Blocks. International Journal of Advanced Computer Research, 3(4), 251. 26 Pandey, A., Pant, D., & Gupta, K. K. (2013). A Novel Approach on Color Image Refocusing and Defocusing . International Journal of Computer Applications, 73(3), 13-17. 27 S le, I. (2014). The determination of the twist level of the Chenille yarn using novel image processing methods: Extraction of axial grey-level characteristic and multi-step gradient based thresholding . Dig ital Signal Processing , 29, 78-99. 28 Azzabi, T., Amor, S. B., & Nejim, S. (2014, November). Obstacle detection for Unmanned Surface Vehicle. In Electrical Sciences and Technolog ies in Maghreb (CISTEM), 2014 International Conference on (pp. 1-7). IEEE. 29 Zacharia, K., Elias , E. P., & Varghese, S. M. (2012). Personalised product design using virtual interactive techniques. arXiv preprint arXiv:1202.1808. 30 Kim, J. H., & Lattimer, B. Y. (2015). Real-time probabilis tic class ification of fire and smoke using thermal imagery for intelligent firefighting robot. Fire Safety Journal, 72, 40-49. 31 N ez, J. M. Edge detection for Very High Resolution Satellite Imagery based on Cellular Neural Network. Advances in Pattern Recognition, 55. 32 Capobianco, J., Pallone, G., & Daudet, L. (2012, October). Low Complexity Transient Detection in Audio Coding Using an Image Edge Detection Approach. In Audio Eng ineering Society Convention 133. Audio Eng ineering Society. 33 zt rk, S., & Akdemir, B. (2015). Comparison of Edge Detection Algorithms for Texture Analys is on Glass Production. Procedia-Social and Behavioral Sciences, 195, 2675-2682. 34 Ahmed, A. M., & Elramly, S. Hyperspectral Data Compression Based On Weighted Prediction. 35 Jayas, D. S. A. Manickavasagan, HN Al-Shekaili, G. Thomas, MS Rahman, N. Guizani &. 36 Khashu, S., Vijayanagar, S., Manikantan, K., & Ramachandran, S. (2014, February). Face Recognition using Dual Wavelet Transform and Filter-Transformed Flipping . In Electronics and Communication Systems (ICECS), 2014 International Conference on (pp. 1-7). IEEE. 37 Brown, R. C. (2014). IRIS: Intelligent Roadway Image Segmentation using an Adaptive Reg ion of Interest (Doctoral dissertation, Virg inia Polytechnic Institute and State Univers ity). 38 Huang , L., Zuo, X., Fang , Y., & Yu, X. A Segmentation Algorithm for Remote Sensing Imag ing Based on Edge and Heterogeneity of Objects . 39 Park, J., Kim, Y., & Kim, S. (2015). Landing Site Searching and Selection Algorithm Development Using Vis ion System and Its Application to Quadrotor. Control Systems Technology, IEEE Transactions on, 23(2), 488-503. 40 Sikchi, P., Beknalkar, N., & Rane, S. Real-Time Cartoonization Using Raspberry Pi. 41 Bachmakov, E., Molina, C., & Wynne, R. (2014, March). Image-based spectroscopy for environmental monitoring . In SPIE Smart Structures and Materials+ Nondestructive Evaluation and Health Monitoring (pp. 90620B-90620B). International Society for Optics and Photonics . 42 Kulyukin, V., & Zaman, T. (2014). Vis ion-Based Localization and Scanning of 1D UPC and EAN Barcodes with Relaxed Pitch, Roll, and Yaw Camera Alignment Constraints . International Journal of Image Processing (IJIP), 8(5), 355. 43 Sandhu, E. M. S., Mutneja, E. V., & Nishi, E. Image Edge Detection by Using Rule Based Fuzzy Class ifier. 44 Tarwani, K. M., & Bhoyar, K. K. Approaches to Gender Class ification using Facial Images. 45 Kuppili, S. K., & Prasad, P. M. K. (2015). Design of Area Optimized Sobel Edge Detection. In Computational Intelligence in Data Mining-Volume 2 (pp. 647-655). Springer India. 46 Singh, R. K., Shaw, D. K., & Alam, M. J. (2015). Experimental Studies of LSB Watermarking with Different Noise. Procedia Computer Science, 54, 612-620. 47 Xu, Y., Da-qiao, Z., Da-wei, D., Bo, W., & Chao-nan, T. (2014, July). A speed monitoring method in steel pipe of 3PE-coating process based on industrial Charge-coupled Device. In Control Conference (CCC), 2014 33rd Chinese (pp. 2908-2912). IEEE. 48 Yasiran, S. S., Jumaat, A. K., Malek, A. A., Hashim, F. H., Nasrir, N., Hassan, S. N. A. S., ... & Mahmud, R. (1987). Microcalcifications Segmentation using Three Edge Detection Techniques on Mammogram Images. 49 Roslan, N., Reba, M. N. M., Askari, M., & Halim, M. K. A. (2014, February). Linear and non-linear enhancement for sun g lint reduction in advanced very high resolution radiometer (AVHRR) image. In IOP Conference Series : Earth and Environmental Science (Vol. 18, No. 1, p. 012041). IOP Publishing . 50 Gupta, P. K. D., Pattnaik, S., & Nayak, M. (2014). Inter-level Spatial Cloud Compression Algorithm. Defence Science Journal, 64(6), 536-541. 51 Foster, R. (2015). A comparison of machine learning techniques for hand shape recogn", "title": "Study and comparison of various image edge detection techniques"}, "f62fbec04ed3a4625a1bc4e83e6766e9b74673cc": {"paper_id": "f62fbec04ed3a4625a1bc4e83e6766e9b74673cc", "abstract": "A theory of edge detection is presented. The analysis proceeds in two parts. (1) Intensity changes, which occur in a natural image over a wide range of scales, are detected separately at different scales. An appropriate filter for this purpose at a given scale is found to be the second derivative of a Gaussian, and it is shown that, provided some simple conditions are satisfied, these primary filters need not be orientation-dependent. Thus, intensity changes at a given scale are best detected by finding the zero values of delta 2G(x,y)*I(x,y) for image I, where G(x,y) is a two-dimensional Gaussian distribution and delta 2 is the Laplacian. The intensity changes thus discovered in each of the channels are then represented by oriented primitives called zero-crossing segments, and evidence is given that this representation is complete. (2) Intensity changes in images arise from surface discontinuities or from reflectance or illumination boundaries, and these all have the property that they are spatially. Because of this, the zero-crossing segments from the different channels are not independent, and rules are deduced for combining them into a description of the image. This description is called the raw primal sketch. The theory explains several basic psychophysical findings, and the operation of forming oriented zero-crossing segments from the output of centre-surround delta 2G filters acting on the image forms the basis for a physiological model of simple cells (see Marr & Ullman 1979).", "title": "Theory of edge detection."}, "6b4fe4aa4d66fecc7b2869569002714d91d0b3f7": {"paper_id": "6b4fe4aa4d66fecc7b2869569002714d91d0b3f7", "abstract": "What chiefly distinguishes cerebral cortex from other parts of the central nervous system is the great diversity of its cell types and inter-connexions. It would be astonishing if such a structure did not profoundly modify the response patterns of fibres coming into it. In the cat's visual cortex, the receptive field arrangements of single cells suggest that there is indeed a degree of complexity far exceeding anything yet seen at lower levels in the visual system. In a previous paper we described receptive fields of single cortical cells, observing responses to spots of light shone on one or both retinas (Hubel & Wiesel, 1959). In the present work this method is used to examine receptive fields of a more complex type (Part I) and to make additional observations on binocular interaction (Part II). This approach is necessary in order to understand the behaviour of individual cells, but it fails to deal with the problem of the relationship of one cell to its neighbours. In the past, the technique of recording evoked slow waves has been used with great success in studies of functional anatomy. It was employed by Talbot & Marshall (1941) and by Thompson, Woolsey & Talbot (1950) for mapping out the visual cortex in the rabbit, cat, and monkey. Daniel & Whitteiidge (1959) have recently extended this work in the primate. Most of our present knowledge of retinotopic projections, binocular overlap, and the second visual area is based on these investigations. Yet the method of evoked potentials is valuable mainly for detecting behaviour common to large populations of neighbouring cells; it cannot differentiate functionally between areas of cortex smaller than about 1 mm2. To overcome this difficulty a method has in recent years been developed for studying cells separately or in small groups during long micro-electrode penetrations through nervous tissue. Responses are correlated with cell location by reconstructing the electrode tracks from histological material. These techniques have been applied to CAT VISUAL CORTEX 107 the somatic sensory cortex of the cat and monkey in a remarkable series of studies by Mountcastle (1957) and Powell & Mountcastle (1959). Their results show that the approach is a powerful one, capable of revealing systems of organization not hinted at by the known morphology. In Part III of the present paper we use this method in studying the functional architecture of the visual cortex. It helped us attempt to explain on anatomical \u2026", "title": "Receptive fields, binocular interaction and functional architecture in the cat's visual cortex."}, "1116467d67f475d1bd2539862f10f792032fe13b": {"paper_id": "1116467d67f475d1bd2539862f10f792032fe13b", "abstract": "1. The striate cortex was studied in lightly anaesthetized macaque and spider monkeys by recording extracellularly from single units and stimulating the retinas with spots or patterns of light. Most cells can be categorized as simple, complex, or hypercomplex, with response properties very similar to those previously described in the cat. On the average, however, receptive fields are smaller, and there is a greater sensitivity to changes in stimulus orientation. A small proportion of the cells are colour coded.2. Evidence is presented for at least two independent systems of columns extending vertically from surface to white matter. Columns of the first type contain cells with common receptive-field orientations. They are similar to the orientation columns described in the cat, but are probably smaller in cross-sectional area. In the second system cells are aggregated into columns according to eye preference. The ocular dominance columns are larger than the orientation columns, and the two sets of boundaries seem to be independent.3. There is a tendency for cells to be grouped according to symmetry of responses to movement; in some regions the cells respond equally well to the two opposite directions of movement of a line, but other regions contain a mixture of cells favouring one direction and cells favouring the other.4. A horizontal organization corresponding to the cortical layering can also be discerned. The upper layers (II and the upper two-thirds of III) contain complex and hypercomplex cells, but simple cells are virtually absent. The cells are mostly binocularly driven. Simple cells are found deep in layer III, and in IV A and IV B. In layer IV B they form a large proportion of the population, whereas complex cells are rare. In layers IV A and IV B one finds units lacking orientation specificity; it is not clear whether these are cell bodies or axons of geniculate cells. In layer IV most cells are driven by one eye only; this layer consists of a mosaic with cells of some regions responding to one eye only, those of other regions responding to the other eye. Layers V and VI contain mostly complex and hypercomplex cells, binocularly driven.5. The cortex is seen as a system organized vertically and horizontally in entirely different ways. In the vertical system (in which cells lying along a vertical line in the cortex have common features) stimulus dimensions such as retinal position, line orientation, ocular dominance, and perhaps directionality of movement, are mapped in sets of superimposed but independent mosaics. The horizontal system segregates cells in layers by hierarchical orders, the lowest orders (simple cells monocularly driven) located in and near layer IV, the higher orders in the upper and lower layers.", "title": "Receptive fields and functional architecture of monkey striate cortex."}, "63c44c93478624fc6f39015c0a35906542cb9129": {"paper_id": "63c44c93478624fc6f39015c0a35906542cb9129", "abstract": "But practical implementation of the definition is not feasible, so pragmatic approach would be to make the algorithm as strong as possible. Steganography is most widely formulated in terms of the prisoner\u2019s problem where Alice and Bob are two inmates who wish to communicate in order to hatch an escape plan. However, all communication between them is examined by the warden, Wendy, who will put them in solitary confinement at the slightest suspicion of trouble. Specifically, in the general model for steganography, we have Alice wishing to send a secret message M to Bob. In order to do so she \u201dembeds\u201d M into a cover-object C, to obtain the stego-object S. The stego-object S is then sent through a public channel. The warden Wendy who is free to examine all messages exchanged between Alice and Bob can be passive or active. A passive warden simply examines the message and tries to determine if it potentially contains a hidden message. If it appears that it does, then she takes appropriate action else she lets the message through without alteration. An active warden on the other hand can alter messages deliberately, even though she does not see any trace of a hidden message, in order to foil any secret communication that can nevertheless be occurring between Alice and Bob.", "title": "Analysis of LSB based image steganography techniques"}, "874b9e74fb000c6554c6941f73ddd4c66c6de38d": {"paper_id": "874b9e74fb000c6554c6941f73ddd4c66c6de38d", "abstract": "Steganography is the art of hiding information in ways that prevent the detection of hidden messages. It includes a vast array of secret communications methods that conceal the message's very existence. These methods include invisible inks, microdots, character arrangement, digital signatures, covert channels, and spread spectrum communications. Steganography and cryptography are cousins in the spycraft family: cryptography scrambles a message so it cannot be understood while steganography hides the message so it cannot be seen. In this article the authors discuss image files and how to hide information in them, and discuss results obtained from evaluating available steganographic software. They argue that steganography by itself does not ensure secrecy, but neither does simple encryption. If these methods are combined, however, stronger encryption methods result. If an encrypted message is intercepted, the interceptor knows the text is an encrypted message. But with steganography, the interceptor may not know that a hidden message even exists. For a brief look at how steganography evolved, there is included a sidebar titled \"Steganography: Some History.\"", "title": "Exploring steganography: Seeing the unseen"}, "884de90bc9e3fb0d3a65c07794e7d2089b62f104": {"paper_id": "884de90bc9e3fb0d3a65c07794e7d2089b62f104", "abstract": "Interpretation of image contents is one of the objectives in computer vision specifically in image processing. In this era it has received much awareness of researchers. In image interpretation the partition of the image into object and background is a severe step. Segmentation separates an image into its component regions or objects. Image segmentation t needs to segment the object from the background to read the image properly and identify the content of the image carefully. In this context, edge detection is a fundamental tool for image segmentation. In this paper an attempt is made to study the performance of most commonly used edge detection techniques for image segmentation and also the comparison of these techniques is carried out with an experiment by using MATLAB software.", "title": "EDGE DETECTION TECHNIQUES FOR IMAGE SEGMENTATION"}, "f336e2e73423f0ef847d982b888e4bdfa575a910": {"paper_id": "f336e2e73423f0ef847d982b888e4bdfa575a910", "abstract": "This paper introduces a two-switch flyback-forward pulse-width modulated (PWM) DC-DC converter along with the steady-state analysis, simplified design procedure, and experimental verification. The proposed converter topology is the result of integrating the secondary sides of the two-switch flyback and the two-switch forward converters in an anti-parallel connection, while retaining the two-main switches and the clamping diodes on a single winding primary side. The hybrid two-switch flyback-forward converter shares the semiconductor devices on the primary side and the magnetic component on the secondary side resulting in a low volume DC-DC converter with reduced switch voltage stress. Simulation and experimental results are given for a 10-V/30-W, 100 kHz laboratory prototype to verify the theoretical analysis.", "title": "Two-switch flyback-forward PWM DC-DC converter with reduced switch voltage stress"}, "a545e5214086d048624b6176c0017db57652163e": {"paper_id": "a545e5214086d048624b6176c0017db57652163e", "abstract": "Decompilation is important for many security applications; it facilitates the tedious task of manual malware reverse engineering and enables the use of source-based security tools on binary code. This includes tools to find vulnerabilities, discover bugs, and perform taint tracking. Recovering high-level control constructs is essential for decompilation in order to produce structured code that is suitable for human analysts and sourcebased program analysis techniques. State-of-the-art decompilers rely on structural analysis, a pattern-matching approach over the control flow graph, to recover control constructs from binary code. Whenever no match is found, they generate goto statements and thus produce unstructured decompiled output. Those statements are problematic because they make decompiled code harder to understand and less suitable for program analysis. In this paper, we present DREAM, the first decompiler to offer a goto-free output. DREAM uses a novel patternindependent control-flow structuring algorithm that can recover all control constructs in binary programs and produce structured decompiled code without any goto statement. We also present semantics-preserving transformations that can transform unstructured control flow graphs into structured graphs. We demonstrate the correctness of our algorithms and show that we outperform both the leading industry and academic decompilers: Hex-Rays and Phoenix. We use the GNU coreutils suite of utilities as a benchmark. Apart from reducing the number of goto statements to zero, DREAM also produced more compact code (less lines of code) for 72.7% of decompiled functions compared to Hex-Rays and 98.8% compared to Phoenix. We also present a comparison of Hex-Rays and DREAM when decompiling three samples from Cridex, ZeusP2P, and SpyEye malware families.", "title": "No More Gotos: Decompilation Using Pattern-Independent Control-Flow Structuring and Semantic-Preserving Transformations"}, "0f4d2d219919bac65fa624719749133277d5ef2c": {"paper_id": "0f4d2d219919bac65fa624719749133277d5ef2c", "abstract": "Policy-based confinement, employed in SELinux and specification-based intrusion detection systems, is a popular approach for defending against exploitation of vulnerabilities in benign software. Conventional access control policies employed in these approaches are effective in detecting privilege escalation attacks. However, they are unable to detect attacks that \u201chijack\u201d legitimate access privileges granted to a program, e.g., an attack that subverts an FTP server to download the password file. (Note that an FTP server would normally need to access the password file for performing user authentication.) Some of the common attack types reported today, such as SQL injection and cross-site scripting, involve such subversion of legitimate access privileges. In this paper, we present a new approach to strengthen policy enforcement by augmenting security policies with information about the trustworthiness of data used in securitysensitive operations. We evaluated this technique using 9 available exploits involving several popular software packages containing the above types of vulnerabilities. Our technique sucessfully defeated these exploits.", "title": "Taint-Enhanced Policy Enforcement: A Practical Approach to Defeat a Wide Range of Attacks"}, "0c515587e546ea2bdf9ac77eaf0d8bc578954443": {"paper_id": "0c515587e546ea2bdf9ac77eaf0d8bc578954443", "abstract": "ertification mechanism for verifying the secure flow of information through a program. Because it exploits the properties of a lattice structure among security classes, the procedure is sufficiently simple that it can easily be included in the analysis phase of most existing compilers. Appropriate semantics are presented and proved correct. An important application is the confinement problem: The mechanism can prove that a program cannot cause supposedly nonconfidential results to depend on confidential input data.", "title": "Certification of Programs for Secure Information Flow"}, "480b73f8a55a868374176375f45918d6f0570715": {"paper_id": "480b73f8a55a868374176375f45918d6f0570715", "abstract": "Dynamic taint analysis and forward symbolic execution are quickly becoming staple techniques in security analyses. Example applications of dynamic taint analysis and forward symbolic execution include malware analysis, input filter generation, test case generation, and vulnerability discovery. Despite the widespread usage of these two techniques, there has been little effort to formally define the algorithms and summarize the critical issues that arise when these techniques are used in typical security contexts. The contributions of this paper are two-fold. First, we precisely describe the algorithms for dynamic taint analysis and forward symbolic execution as extensions to the run-time semantics of a general language. Second, we highlight important implementation choices, common pitfalls, and considerations when using these techniques in a security context.", "title": "All You Ever Wanted to Know about Dynamic Taint Analysis and Forward Symbolic Execution (but Might Have Been Afraid to Ask)"}, "1e4e04a06d1a340d8ffb3227feb4b466a15439f8": {"paper_id": "1e4e04a06d1a340d8ffb3227feb4b466a15439f8", "abstract": "Malware authors have recently begun using emulation technology to obfuscate their code. They convert native malware binaries into bytecode programs written in a randomly generated instruction set and paired with a native binary emulator that interprets the bytecode. No existing malware analysis can reliably reverse this obfuscation technique. In this paper, we present the first work in automatic reverse engineering of malware emulators. Our algorithms are based on dynamic analysis. We execute the emulated malware in a protected environment and record the entire x86 instruction trace generated by the emulator. We then use dynamic data-flow and taint analysis over the trace to identify data buffers containing the bytecode program and extract the syntactic and semantic information about the bytecode instruction set. With these analysis outputs, we are able to generate data structures, such as control-flow graphs, that provide the foundation for subsequent malware analysis. We implemented a proof-of-concept system called Rotalume and evaluated it using both legitimate programs and malware emulated by VMProtect and Code Virtualizer. The results show that Rotalume accurately reveals the syntax and semantics of emulated instruction sets and reconstructs execution paths of original programs from their bytecode representations.", "title": "Automatic Reverse Engineering of Malware Emulators"}, "7c605dffcac6860a1ffb252b911060eb007f7171": {"paper_id": "7c605dffcac6860a1ffb252b911060eb007f7171", "abstract": "In order to match shoppers with desired products and provide personalized promotions, whether in online or offline shopping worlds, it is critical to model both consumer preferences and price sensitivities simultaneously. Personalized preferences have been thoroughly studied in the field of recommender systems, though price (and price sensitivity) has received relatively little attention. At the same time, price sensitivity has been richly explored in the area of economics, though typically not in the context of developing scalable, working systems to generate recommendations. In this study, we seek to bridge the gap between large-scale recommender systems and established consumer theories from economics, and propose a nested feature-based matrix factorization framework to model both preferences and price sensitivities. Quantitative and qualitative results indicate the proposed personalized, interpretable and scalable framework is capable of providing satisfying recommendations (on two datasets of grocery transactions) and can be applied to obtain economic insights into consumer behavior.", "title": "Modeling Consumer Preferences and Price Sensitivities from Large-Scale Grocery Shopping Transaction Logs"}, "b8593122c6b1dd2432125a4eeb1a908c280b1d1d": {"paper_id": "b8593122c6b1dd2432125a4eeb1a908c280b1d1d", "abstract": "Bayesian approach remained rather unsuccessful in treating nonparametric problems. This is primarily due to the difficulty in finding workable prior distribution on the parameter space , which in nonparametric problems is taken to be a set of probability distributions on a given sample space. Two Desirable Properties of a Prior 1. The support of the prior should be large. 2. Posterior distribution given a sample of observation should be manageable analytically. These properties are antagonistic : One may be obtained at the expense of other.", "title": "A Bayesian Analysis of Some Nonparametric Problems"}, "abfb3352a188c7f3e7ad518dcf0dbcff480bcec4": {"paper_id": "abfb3352a188c7f3e7ad518dcf0dbcff480bcec4", "abstract": "We present a probabilistic model for generating personalised recommendations of items to users of a web service. The Matchbox system makes use of content information in the form of user and item meta data in combination with collaborative filtering information from previous user behavior in order to predict the value of an item for a user. Users and items are represented by feature vectors which are mapped into a low-dimensional `trait space' in which similarity is measured in terms of inner products. The model can be trained from different types of feedback in order to learn user-item preferences. Here we present three alternatives: direct observation of an absolute rating each user gives to some items, observation of a binary preference (like/ don't like) and observation of a set of ordinal ratings on a user-specific scale. Efficient inference is achieved by approximate message passing involving a combination of Expectation Propagation (EP) and Variational Message Passing. We also include a dynamics model which allows an item's popularity, a user's taste or a user's personal rating scale to drift over time. By using Assumed-Density Filtering (ADF) for training, the model requires only a single pass through the training data. This is an on-line learning algorithm capable of incrementally taking account of new data so the system can immediately reflect the latest user preferences. We evaluate the performance of the algorithm on the MovieLens and Netflix data sets consisting of approximately 1,000,000 and 100,000,000 ratings respectively. This demonstrates that training the model using the on-line ADF approach yields state-of-the-art performance with the option of improving performance further if computational resources are available by performing multiple EP passes over the training data.", "title": "Matchbox: large scale online bayesian recommendations"}, "c6ef3955c8c4bf826b2c33e5c1bccc84720c8f3b": {"paper_id": "c6ef3955c8c4bf826b2c33e5c1bccc84720c8f3b", "abstract": "Your use of the JSTOR archive indicates your acceptance of JSTOR's Terms and Conditions of Use, available at http://www.jstor.org/about/terms.html. JSTOR's Terms and Conditions of Use provides, in part, that unless you have obtained prior permission, you may not download an entire issue of a journal or multiple copies of articles, and you may use content in the JSTOR archive only for your personal, non-commercial use.", "title": "Markov Chain Sampling Methods for Dirichlet Process Mixture Models"}, "2275762a28582716db92df6d525ed2481c7d7f14": {"paper_id": "2275762a28582716db92df6d525ed2481c7d7f14", "abstract": "Recommender systems provide users with personalized suggestions for products or services. These systems often rely on Collaborating Filtering (CF), where past transactions are analyzed in order to establish connections between users and products. The two more successful approaches to CF are latent factor models, which directly profile both users and products, and neighborhood models, which analyze similarities between products or users. In this work we introduce some innovations to both approaches. The factor and neighborhood models can now be smoothly merged, thereby building a more accurate combined model. Further accuracy improvements are achieved by extending the models to exploit both explicit and implicit feedback by the users. The methods are tested on the Netflix data. Results are better than those previously published on that dataset. In addition, we suggest a new evaluation metric, which highlights the differences among methods, based on their performance at a top-K recommendation task.", "title": "Factorization meets the neighborhood: a multifaceted collaborative filtering model"}, "29bae9472203546847ec1352a604566d0f602728": {"paper_id": "29bae9472203546847ec1352a604566d0f602728", "abstract": "Is perception of the whole based on perception of its parts? There is psychological and physiological evidence for parts-based representations in the brain, and certain computational theories of object recognition rely on such representations. But little is known about how brains or computers might learn the parts of objects. Here we demonstrate an algorithm for non-negative matrix factorization that is able to learn parts of faces and semantic features of text. This is in contrast to other methods, such as principal components analysis and vector quantization, that learn holistic, not parts-based, representations. Non-negative matrix factorization is distinguished from the other methods by its use of non-negativity constraints. These constraints lead to a parts-based representation because they allow only additive, not subtractive, combinations. When non-negative matrix factorization is implemented as a neural network, parts-based representations emerge by virtue of two properties: the firing rates of neurons are never negative and synaptic strengths do not change sign.", "title": "Learning the parts of objects by non-negative matrix factorization"}, "55830e7ad66bad06de9684ed44a48e50ae145c5a": {"paper_id": "55830e7ad66bad06de9684ed44a48e50ae145c5a", "abstract": "The collaborative filtering (CF) using known user ratings of items has proved to be effective for predicting user preferences in item selection. This thrivi ng subfield of machine learning became popular in the late 1990s with the spread of online services t hat use recommender systems, such as Amazon, Yahoo! Music, and Netflix. CF approaches are usually designed to work on very large data sets. Therefore the scalability of the methods is cruci al. In this work, we propose various scalable solutions that are validated against the Netflix Pr ize data set, currently the largest publicly available collection. First, we propose various matrix fac torization (MF) based techniques. Second, a neighbor correction method for MF is outlined, which alloy s the global perspective of MF and the localized property of neighbor based approaches efficie ntly. In the experimentation section, we first report on some implementation issues, and we suggest on how parameter optimization can be performed efficiently for MFs. We then show that the propos ed calable approaches compare favorably with existing ones in terms of prediction accurac y nd/or required training time. Finally, we report on some experiments performed on MovieLens and Jes ter data sets.", "title": "Scalable Collaborative Filtering Approaches for Large Recommender Systems"}, "807c1f19047f96083e13614f7ce20f2ac98c239a": {"paper_id": "807c1f19047f96083e13614f7ce20f2ac98c239a", "abstract": null, "title": "C4.5: Programs for Machine Learning"}, "21922f216002ed2bc44b886b5e8e3042da45c40c": {"paper_id": "21922f216002ed2bc44b886b5e8e3042da45c40c", "abstract": "Topic Detection and Tracking (TDT) is a DARPA-sponsored initiative to investigate the state of the art in finding and following new events in a stream of broadcast news stories. The TDT problem consists of three major tasks: (1) segmenting a stream of data, especially recognized speech, into distinct stories; (2) identifying those news stories that are the first to discuss a new event occurring in the news; and (3) given a small number of sample news stories about an event, finding all following stories in the stream. The TDT Pilot Study ran from September 1996 through October 1997. The primary participants were DARPA, Carnegie Mellon University, Dragon Systems, and the University of Massachusetts at Amherst. This report summarizes the findings of the pilot study. The TDT work continues in a new project involving larger training and test corpora, more active participants, and a more broadly defined notion of \u201ctopic\u201d than was used in the pilot study. The following individuals participated in the research reported.", "title": "Topic Detection and Tracking Pilot Study Final Report"}, "4e77fb934237e164ec090617a66de381ef0661a0": {"paper_id": "4e77fb934237e164ec090617a66de381ef0661a0", "abstract": "We consider two algorithms for on-line prediction based on a linear model. The algorithms are the well-known gradient descent (GD) algorithm and a new algorithm, which we call EG. They both maintain a weight vector using simple updates. For the GD algorithm, the update is based on subtracting the gradient of the squared error made on a prediction. The EG algorithm uses the components of the gradient in the exponents of factors that are used in updating the weight vector multiplicatively. We present worst-case loss bounds for EG and compare them to previously known bounds for the GD algorithm. The bounds suggest that the losses of the algorithms are in general incomparable, but EG has a much smaller loss if only few components of the input are relevant for the predictions. We have performed experiments which show that our worst-case upper bounds are quite tight already on simple artificial data. ] 1997 Academic Press", "title": "Exponentiated Gradient Versus Gradient Descent for Linear Predictors"}, "04ce064505b1635583fa0d9cc07cac7e9ea993cc": {"paper_id": "04ce064505b1635583fa0d9cc07cac7e9ea993cc", "abstract": "Recent approaches to text classification have used two different first-order probabilistic models for classification, both of which make the naive Bayes assumption. Some use a multi-variate Bernoulli model, that is, a Bayesian Network with no dependencies between words and binary word features (e.g. Larkey and Croft 1996; Koller and Sahami 1997). Others use a multinomial model, that is, a uni-gram language model with integer word counts (e.g. Lewis and Gale 1994; Mitchell 1997). This paper aims to clarify the confusion by describing the differences and details of these two models, and by empirically comparing their classification performance on five text corpora. We find that the multi-variate Bernoulli performs well with small vocabulary sizes, but that the multinomial performs usually performs even better at larger vocabulary sizes\u2014providing on average a 27% reduction in error over the multi-variate Bernoulli model at any vocabulary size.", "title": "A Comparison of Event Models for Naive Bayes Text Classification"}, "0b98fd7bf704876e6d49eb5c9310b37b78989112": {"paper_id": "0b98fd7bf704876e6d49eb5c9310b37b78989112", "abstract": "When documents are organized in a large number of topic categories, the categories are often arranged in a hierarchy. The U.S. patent database and Yahoo are two examples. This paper shows that the accuracy of a naive Bayes text classi er can be signi cantly improved by taking advantage of a hierarchy of classes. We adopt an established statistical technique called shrinkage that smoothes parameter estimates of a data-sparse child with its parent in order to obtain more robust parameter estimates. The approach is also employed in deleted interpolation, a technique for smoothing n-grams in language modeling for speech recognition. Our method scales well to large data sets, with numerous categories in large hierarchies. Experimental results on three real-world data sets from UseNet, Yahoo, and corporate web pages show improved performance, with a reduction in error up to 29% over the traditional at classi er.", "title": "Improving Text Classification by Shrinkage in a Hierarchy of Classes"}, "9931c6b050e723f5b2a189dd38c81322ac0511de": {"paper_id": "9931c6b050e723f5b2a189dd38c81322ac0511de", "abstract": "We present a review on the current state of publicly available datasets within the human action recognition community; highlighting the revival of pose based methods and recent progress of understanding person-person interaction modeling. We categorize datasets regarding several key properties for usage as a benchmark dataset; including the number of class labels, ground truths provided, and application domain they occupy. We also consider the level of abstraction of each dataset; grouping those that present actions, interactions and higher level semantic activities. The survey identifies key appearance and pose based datasets, noting a tendency for simplistic, emphasized, or scripted action classes that are often readily definable by a stable collection of subaction gestures. There is a clear lack of datasets that provide closely related actions, those that are not implicitly identified via a series of poses and gestures, but rather a dynamic set of interactions. We therefore propose a novel dataset that represents complex conversational interactions between two individuals via 3D pose. 8 pairwise interactions describing 7 separate conversation based scenarios were collected using two Kinect depth sensors. The intention is to provide events that are constructed from numerous primitive actions, interactions and motions, over a period of time; providing a set of subtle action classes that are more representative of the real world, and a challenge to currently developed recognition methodologies. We believe this is among one of the first datasets devoted to conversational interaction classification using 3D pose Preprint submitted to Elsevier October 27, 2015 features and the attributed papers show this task is indeed possible. The full dataset is made publicly available to the research community at [1].", "title": "From Pose to Activity: Surveying Datasets and Introducing CONVERSE"}, "09f2af091f6bf5dfe25700c5a8c82f220fac5631": {"paper_id": "09f2af091f6bf5dfe25700c5a8c82f220fac5631", "abstract": "This paper presents a method for recognizing scene categories based on approximate global geometric correspondence. This technique works by partitioning the image into increasingly fine sub-regions and computing histograms of local features found inside each sub-region. The resulting \"spatial pyramid\" is a simple and computationally efficient extension of an orderless bag-of-features image representation, and it shows significantly improved performance on challenging scene categorization tasks. Specifically, our proposed method exceeds the state of the art on the Caltech-101 database and achieves high accuracy on a large database of fifteen natural scene categories. The spatial pyramid framework also offers insights into the success of several recently proposed image descriptions, including Torralba\u2019s \"gist\" and Lowe\u2019s SIFT descriptors.", "title": "Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories"}, "b91180d8853d00e8f2df7ee3532e07d3d0cce2af": {"paper_id": "b91180d8853d00e8f2df7ee3532e07d3d0cce2af", "abstract": "We present a novel method for generic visual categorization: the problem of identifying the object content of natural images while generalizing across variations inherent to the object class. This bag of keypoints method is based on vector quantization of affine invariant descriptors of image patches. We propose and compare two alternative implementations using different classifiers: Na\u00efve Bayes and SVM. The main advantages of the method are that it is simple, computationally efficient and intrinsically invariant. We present results for simultaneously classifying seven semantic visual categories. These results clearly demonstrate that the method is robust to background clutter and produces good categorization accuracy even without exploiting geometric information.", "title": "Visual Categorization with Bags of Keypoints"}, "27f2c4969e424242200eb1901143aeda7d0a4030": {"paper_id": "27f2c4969e424242200eb1901143aeda7d0a4030", "abstract": "This paper studies the use of volumetric features as an alternative to popular local descriptor approaches for event detection in video sequences. Motivated by the recent success of similar ideas in object detection on static images, we generalize the notion of 2D box features to 3D spatio-temporal volumetric features. This general framework enables us to do real-time video analysis. We construct a realtime event detector for each action of interest by learning a cascade of filters based on volumetric features that efficiently scans video sequences in space and time. This event detector recognizes actions that are traditionally problematic for interest point methods - such as smooth motions where insufficient space-time interest points are available. Our experiments demonstrate that the technique accurately detects actions on real-world sequences and is robust to changes in viewpoint, scale and action speed. We also adapt our technique to the related task of human action classification and confirm that it achieves performance comparable to a current interest point based human activity recognizer on a standard database of human activities.", "title": "Efficient visual event detection using volumetric features"}, "0674c1e2fd78925a1baa6a28216ee05ed7b48ba0": {"paper_id": "0674c1e2fd78925a1baa6a28216ee05ed7b48ba0", "abstract": "Proc. of the International Conference on Computer Vision, Corfu (Sept. 1999) An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest-neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low-residual least-squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially-occluded images with a computation time of under 2 seconds.", "title": "Object Recognition from Local Scale-Invariant Features"}, "5010f30b0e6a71a16b49cbc2134450bb9e3a2659": {"paper_id": "5010f30b0e6a71a16b49cbc2134450bb9e3a2659", "abstract": "The ability to recognize humans and their activities by vision is key for a machine to interact intelligently and effortlessly with a human-inhabited environment. Because of many potentially important applications, \u201clooking at people\u201d is currently one of the most active application domains in computer vision. This survey identifies a number of promising applications and provides an overview of recent developments in this domain. The scope of this survey is limited to work on whole-body or hand motion; it does not include work on human faces. The emphasis is on discussing the various methodologies; they are grouped in 2-D approaches with or without explicit shape models and 3-D approaches. Where appropriate, systems are reviewed. We conclude with some thoughts about future directions. c \u00a9 1999 Academic Press", "title": "The Visual Analysis of Human Movement: A Survey"}, "6b2e3c9b32e92dbbdd094d2bd88eb60a80c3083d": {"paper_id": "6b2e3c9b32e92dbbdd094d2bd88eb60a80c3083d", "abstract": "The problem we are addressing in Alvey Project MMI149 is that of using computer vision to understand the unconstrained 3D world, in which the viewed scenes will in general contain too wide a diversity of objects for topdown recognition techniques to work. For example, we desire to obtain an understanding of natural scenes, containing roads, buildings, trees, bushes, etc., as typified by the two frames from a sequence illustrated in Figure 1. The solution to this problem that we are pursuing is to use a computer vision system based upon motion analysis of a monocular image sequence from a mobile camera. By extraction and tracking of image features, representations of the 3D analogues of these features can be constructed.", "title": "A Combined Corner and Edge Detector"}, "3e66bfc9bf3068636bb2cd52bfb614004fb81b81": {"paper_id": "3e66bfc9bf3068636bb2cd52bfb614004fb81b81", "abstract": "Recognition of three dimensional (3D) objects in noisy and cluttered scenes is a challenging problem in 3D computer vision. One approach that has been successful in past research is the regional shape descriptor. In this paper, we introduce two new regional shape descriptors: 3D shape contexts and harmonic shape contexts. We evaluate the performance of these descriptors on the task of recognizing vehicles in range scans of scenes using a database of 56 cars. We compare the two novel descriptors to an existing descriptor, the spin image, showing that the shape context based descriptors have a higher recognition rate on noisy scenes and that 3D shape contexts outperform the others on cluttered scenes.", "title": "Recognizing Objects in Range Data Using Regional Point Descriptors"}, "4e3a22ed94c260b9143eee9fdf6d5d6e892ecd8f": {"paper_id": "4e3a22ed94c260b9143eee9fdf6d5d6e892ecd8f", "abstract": null, "title": "A Performance Evaluation of Local Descriptors"}, "103ac34236fb995809ec55b38c1d16c6df8b9f94": {"paper_id": "103ac34236fb995809ec55b38c1d16c6df8b9f94", "abstract": "In this paper, we present a novel approach for human action recognition with histograms of 3D joint locations (HOJ3D) as a compact representation of postures. We extract the 3D skeletal joint locations from Kinect depth maps using Shotton et al.'s method [6]. The HOJ3D computed from the action depth sequences are reprojected using LDA and then clustered into k posture visual words, which represent the prototypical poses of actions. The temporal evolutions of those visual words are modeled by discrete hidden Markov models (HMMs). In addition, due to the design of our spherical coordinate system and the robust 3D skeleton estimation from Kinect, our method demonstrates significant view invariance on our 3D action dataset. Our dataset is composed of 200 3D sequences of 10 indoor activities performed by 10 individuals in varied views. Our method is real-time and achieves superior results on the challenging 3D action dataset. We also tested our algorithm on the MSR Action3D dataset and our algorithm outperforms Li et al. [25] on most of the cases.", "title": "Invariant Human Action Recognition Using Histograms of 3 D Joints"}, "208d2d6ae61955d93b13a5a497c10ba8c7086e87": {"paper_id": "208d2d6ae61955d93b13a5a497c10ba8c7086e87", "abstract": "CENsus TRansform hISTogram (CENTRIST), a new visual descriptor for recognizing topological places or scene categories, is introduced in this paper. We show that place and scene recognition, especially for indoor environments, require its visual descriptor to possess properties that are different from other vision domains (e.g., object recognition). CENTRIST satisfies these properties and suits the place and scene recognition task. It is a holistic representation and has strong generalizability for category recognition. CENTRIST mainly encodes the structural properties within an image and suppresses detailed textural information. Our experiments demonstrate that CENTRIST outperforms the current state of the art in several place and scene recognition data sets, compared with other descriptors such as SIFT and Gist. Besides, it is easy to implement and evaluates extremely fast.", "title": "CENTRIST: A Visual Descriptor for Scene Categorization"}, "54805299ef8baabd0ae6e9996f52e08d08f98abf": {"paper_id": "54805299ef8baabd0ae6e9996f52e08d08f98abf", "abstract": "This paper describes our effort for a planning-based computational model of narrative generation that is designed to elicit surprise in the reader's mind, making use of two temporal narrative devices: flashback and foreshadowing. In our computational model, flashback provides a backstory to explain what causes a surprising outcome, while foreshadowing gives hints about the surprise before it occurs. Here, we present Prevoyant, a planning-based computational model of surprise arousal in narrative generation, and analyze the effectiveness of Prevoyant. The work here also presents a methodology to evaluate surprise in narrative generation using a planning-based approach based on the cognitive model of surprise causes. The results of the experiments that we conducted show strong support that Prevoyant effectively generates a discourse structure for surprise arousal in narrative.", "title": "A Computational Model of Narrative Generation for Surprise Arousal"}, "0f73f4ebc58782c03cc78aa7d6a391d23101ba09": {"paper_id": "0f73f4ebc58782c03cc78aa7d6a391d23101ba09", "abstract": "In monkeys, the rostral part of ventral premotor cortex (area F5) contains neurons that discharge, both when the monkey grasps or manipulates objects and when it observes the experimenter making similar actions. These neurons (mirror neurons) appear to represent a system that matches observed events to similar, internally generated actions, and in this way forms a link between the observer and the actor. Transcranial magnetic stimulation and positron emission tomography (PET) experiments suggest that a mirror system for gesture recognition also exists in humans and includes Broca's area. We propose here that such an observation/execution matching system provides a necessary bridge from'doing' to'communicating',as the link between actor and observer becomes a link between the sender and the receiver of each message.", "title": "Language within our grasp"}, "0f52a233d2e20e7b270a4eed9e06aff1840a46d6": {"paper_id": "0f52a233d2e20e7b270a4eed9e06aff1840a46d6", "abstract": "A category of stimuli of great importance for primates, humans in particular, is that formed by actions done by other individuals. If we want to survive, we must understand the actions of others. Furthermore, without action understanding, social organization is impossible. In the case of humans, there is another faculty that depends on the observation of others' actions: imitation learning. Unlike most species, we are able to learn by imitation, and this faculty is at the basis of human culture. In this review we present data on a neurophysiological mechanism--the mirror-neuron mechanism--that appears to play a fundamental role in both action understanding and imitation. We describe first the functional properties of mirror neurons in monkeys. We review next the characteristics of the mirror-neuron system in humans. We stress, in particular, those properties specific to the human mirror-neuron system that might explain the human capacity to learn by imitation. We conclude by discussing the relationship between the mirror-neuron system and language.", "title": "The mirror-neuron system."}, "94194bc400a8aa54bceae63e9a1e311251cd2479": {"paper_id": "94194bc400a8aa54bceae63e9a1e311251cd2479", "abstract": "Because of its high modularity and carry-free addition, a redundant binary (RB) representation can be used when designing high performance multipliers. The conventional RB multiplier needs for an additional RB partial product (RBPP) row, because an error-correcting word (ECW) is created by both the radix-8 and radix-4 Modified Booth encodings (MBE). This becomes subject in an additional RBPP accumulation stage for the MBE multiplier. A new RB modified partial product generator (RBMPPG) was proposed in this paper; it takes off the extra ECW and hence, it rescues one RBPP accumulation stage. Therefore, than a conventional RB MBE multiplier, the proposed RBMPPG produces fewer partial product rows. Simulation results show that the proposed RBMPPG based designs sufficiently make better the area and power consumption when the word length of each operand in the multiplier is at least 32 bits; these decreases over previous NB multiplier designs need in a small delay increase (approximately 5%). The power-delay product can be making smaller by up to 59% using the proposed RB multipliers when estimated with existing RB multipliers.", "title": "Modified Partial Product Generator for Redundant Binary Multiplier with High Modularity and Carry-Free Addition"}, "ce6f22b572d69112591a4a319517a679ab984e55": {"paper_id": "ce6f22b572d69112591a4a319517a679ab984e55", "abstract": "Using an ostomy appliance can affect many aspects of a person's health-related quality of life (HRQL). A 2-part, descrip- tive study was designed to develop and validate an instrument to assess quality-of-life outcomes related to ostomy ap- pliance use. Study inclusion/exclusion criteria stipulated participants should be 18 to 85 years of age, have an ileostomy or colostomy, used an appliance for a minimum of 3 months without assistance, and able to complete an online survey. All participants provided sociodemographic and clinical information. In phase 1, a literature search was conducted and existing instruments used to measure HRQL in persons with an ostomy were assessed. Subsequently, the Ostomy-Q, a 23-item, Likert-response type questionnaire, divided into 4 domains (Discreetness, Comfort, Confidence, and Social Life), was developed based on published evidence and existing ostomy-related HRQL tools. Seven (7) participants re- cruited from a manufacturer user panel took part in exploratory/cognitive qualitative interviews to refine the new quality- of-life questionnaire. In phase 2, the instrument was tested to assess item variability and conceptual structure, item-total correlation, internal consistency, test-retest reliability, sensitivity, and minimal important difference (MID) in an online validation study among 200 participants from the manufacturer's user panel (equally divided by gender, 125 [62.5%] >50 years old, 128 [64%] with an ileostomy). This exercise also included completion of the Stoma Quality of Life Question- naire and 2 domains from the Ostomy Adjustment Inventory-23 to assess convergent validity. Eighty-two (82) participants recompleted these study instruments 2 weeks later to assess test-retest reliability. Sociodemographic and clinical data were assessed using descriptive statistics; Cronbach's alpha was used for internal consistency (minimum 0.70), principle component analysis for item variability/conceptual structure, and item-total correlation; intraclass correlation coefficient was used for test-retest reliability; and standard error of measurement was applied to MID. All domains demonstrated good internal consistency (between 0.69 and 0.78). All scales showed stability, with a minimum intraclass correlation coefficient of 0.743 (P <.001). The Ostomy-Q showed good convergent validity with other instruments to which it was compared (P <.01). In this study, the Ostomy-Q was found to be a reliable and valid outcome measure that can enhance understanding of the impact of ostomy appliances on users. Some items for social relationships and discreetness may need more exploring in the future with other patient groups.", "title": "The Ostomy-Q: Development and Psychometric Validation of an Instrument to Evaluate Outcomes Associated with Ostomy Appliances."}, "603561e877ceee2213a72b90a9248d684f83e6ba": {"paper_id": "603561e877ceee2213a72b90a9248d684f83e6ba", "abstract": "We consider the problem of learning to summarize images by text and visualize text utilizing images, which we call Mutual-Summarization. We divide the web image-text data space into three subspaces, namely pure image space (PIS), pure text space (PTS) and image-text joint space (ITJS). Naturally, we treat the ITJS as a knowledge base.\n For summarizing images by sentence issue, we map images from PIS to ITJS via image classification models and use text summarization on the corresponding texts in ITJS to summarize images. For text visualization problem, we map texts from PTS to ITJS via text categorization models and generate the visualization by choosing the semantic related images from ITJS, where the selected images are ranked by their confidence. In above approaches images are represented by color histograms, dense visual words and feature descriptors at different levels of spatial pyramid; and the texts are generated according to the Latent Dirichlet Allocation (LDA) topic model. Multiple Kernel (MK) methodologies are used to learn classifiers for image and text respectively. We show the Mutual-Summarization results on our newly collected dataset of six big events (\"Gulf Oil Spill\", \"Haiti Earthquake\", etc.) as well as demonstrate improved cross-media retrieval performance over existing methods in terms of MAP, Precision and Recall.", "title": "Learning to summarize web image and text mutually"}, "c411f93539714f512e437c45a7a9d0a6d5a7675e": {"paper_id": "c411f93539714f512e437c45a7a9d0a6d5a7675e", "abstract": "Grouping images into semantically meaningful categories using low-level visual features is a challenging and important problem in content-based image retrieval. Based on these groupings, eeective indices can be built for an image database. In this paper, we show how a speciic high-level classiication problem (city images vs. landscapes) can be solved from relatively simple low-level features geared for the particular classes. We have developed a procedure to qualitatively measure the saliency of a feature towards a classiication problem based on the plot of the intra-class and inter-class distance distributions. We use this approach to determine the discriminative power of the following features: color histogram, color coherence vector, DCT coeecient, edge direction histogram, and edge direction coherence vector. We determine that the edge direction-based features have the most discriminative power for the classiication problem of interest here. A weighted k-NN classiier is used for the classiication which results in an accuracy of 93:9% when evaluated on an image database of 2; 716 images using the leave-one-out method. This approach has been extended to further classify 528 landscape images into forests, mountains, and sunset/sunrise classes. First, the input images are classiied as sunset/sunrise images vs. forest & mountain images (94:5% accuracy) and then the forest & mountain images are classiied as forest images or mountain images (91:7% accuracy). We are currently identifying further semantic classes to assign to images as well as extracting low level features which are salient for these classes. Our nal goal is to combine multiple 2-class classiiers into a single hierarchical classiier.", "title": "On image classification: city images vs. landscapes"}, "661b5b5eded3ff47c4c5d8d30f3c963a06a39e44": {"paper_id": "661b5b5eded3ff47c4c5d8d30f3c963a06a39e44", "abstract": "THE receptive fields of simple cells in mammalian primary visual cortex can be characterized as being spatially localized, oriented1\u20134 and bandpass (selective to structure at different spatial scales), comparable to the basis functions of wavelet transforms5,6. One approach to understanding such response properties of visual neurons has been to consider their relationship to the statistical structure of natural images in terms of efficient coding7\u201312. Along these lines, a number of studies have attempted to train unsupervised learning algorithms on natural images in the hope of developing receptive fields with similar properties13\u201318, but none has succeeded in producing a full set that spans the image space and contains all three of the above properties. Here we investigate the proposal8,12 that a coding strategy that maximizes sparseness is sufficient to account for these properties. We show that a learning algorithm that attempts to find sparse linear codes for natural scenes will develop a complete family of localized, oriented, bandpass receptive fields, similar to those found in the primary visual cortex. The resulting sparse image code provides a more efficient representation for later stages of processing because it possesses a higher degree of statistical independence among its outputs.", "title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images"}, "bf12813b219ce0730f123bbd3e9c9227d0934f0b": {"paper_id": "bf12813b219ce0730f123bbd3e9c9227d0934f0b", "abstract": "We see the world in scenes, where visual objects occur in rich surroundings, often embedded in a typical context with other related objects. How does the human brain analyse and use these common associations? This article reviews the knowledge that is available, proposes specific mechanisms for the contextual facilitation of object recognition, and highlights important open questions. Although much has already been revealed about the cognitive and cortical mechanisms that subserve recognition of individual objects, surprisingly little is known about the neural underpinnings of contextual analysis and scene perception. Building on previous findings, we now have the means to address the question of how the brain integrates individual elements to construct the visual experience.", "title": "Visual objects in context"}, "02a962051d5a1a3b12241df2c8edcbace756a1a2": {"paper_id": "02a962051d5a1a3b12241df2c8edcbace756a1a2", "abstract": "\u00d0Retrieving images from large and varied collections using image content as a key is a challenging and important problem. We present a new image representation that provides a transformation from the raw pixel data to a small set of image regions that are coherent in color and texture. This aBlobworldo representation is created by clustering pixels in a joint color-texture-position feature space. The segmentation algorithm is fully automatic and has been run on a collection of 10,000 natural images. We describe a system that uses the Blobworld representation to retrieve images from this collection. An important aspect of the system is that the user is allowed to view the internal representation of the submitted image and the query results. Similar systems do not offer the user this view into the workings of the system; consequently, query results from these systems can be inexplicable, despite the availability of knobs for adjusting the similarity metrics. By finding image regions that roughly correspond to objects, we allow querying at the level of objects rather than global image properties. We present results indicating that querying for images using Blobworld produces higher precision than does querying using color and texture histograms of the entire image in cases where the image contains distinctive objects. Index Terms\u00d0Segmentation and grouping, image retrieval, image querying, clustering, Expectation-Maximization.", "title": "Blobworld: Image Segmentation Using Expectation-Maximization and Its Application to Image Querying"}, "a2c2999b134ba376c5ba3b610900a8d07722ccb3": {"paper_id": "a2c2999b134ba376c5ba3b610900a8d07722ccb3", "abstract": null, "title": "Bleu: a Method for Automatic Evaluation of Machine Translation"}, "7910e30380895294678089ff818c320f6de1f39e": {"paper_id": "7910e30380895294678089ff818c320f6de1f39e", "abstract": "The work presented here is driven by two observations. First, heterogeneous architectures that integrate a CPU and a GPU on the same chip are emerging, and hold much promise for supporting power-efficient and scalable high performance computing. Second, MapReduce has emerged as a suitable framework for simplified parallel application development for many classes of applications, including data mining and machine learning applications that benefit from accelerators.\n This paper focuses on the challenge of scaling a MapReduce application using the CPU and GPU together in an integrated architecture. We develop different methods for dividing the work, which are the map-dividing scheme, where map tasks are divided between both devices, and the pipelining scheme, which pipelines the map and the reduce stages on different devices. We develop dynamic work distribution schemes for both the approaches. To achieve high load balance while keeping scheduling costs low, we use a runtime tuning method to adjust task block sizes for the map-dividing scheme. Our implementation of MapReduce is based on a continuous reduction method, which avoids the memory overheads of storing key-value pairs.\n We have evaluated the different design decisions using 5 popular MapReduce applications. For 4 of the applications, our system achieves 1.21 to 2.1 speedup over the better of the CPU-only and GPU-only versions. The speedups over a single CPU core execution range from 3.25 to 28.68. The runtime tuning method we have developed achieves very low load imbalance, while keeping scheduling overheads low. Though our current work is specific to MapReduce, many underlying ideas are also applicable towards intra-node acceleration of other applications on integrated CPU-GPU nodes.", "title": "Accelerating MapReduce on a coupled CPU-GPU architecture"}, "ea4bdeb0ba42ed22c85290528372941678680755": {"paper_id": "ea4bdeb0ba42ed22c85290528372941678680755", "abstract": "Many inductive knowledge acquisition algorithms generate classifiers in the form of decision trees. This paper describes a technique for transforming such trees to small sets of production rules, a common formalism for expressing knowledge in expert systems. The method makes use of the training set of cases from which the decision tree was generated, first to generalize and assess the reliability of individual rules extracted from the tree, and subsequently to refine the collection of rules as a whole. The final set of production rules is usually both simpler than the decision tree from which it was obtained, and more accurate when classifying unseen cases. Transformation to production rules also provides a way of combining different decision trees for the same classification domain.", "title": "Generating Production Rules from Decision Trees"}, "29026f502e747508d2251df104caebfda0a1da12": {"paper_id": "29026f502e747508d2251df104caebfda0a1da12", "abstract": "A simple linear neuron model with constrained Hebbian-type synaptic modification is analyzed and a new class of unconstrained learning rules is derived. It is shown that the model neuron tends to extract the principal component from a stationary input vector sequence.", "title": "A simplified neuron model as a principal component analyzer."}, "2f0b25f9d93bbc1651cdb4ea8d9a0e32cdedb62f": {"paper_id": "2f0b25f9d93bbc1651cdb4ea8d9a0e32cdedb62f", "abstract": "Private query processing on encrypted databases allows users to obtain data from encrypted databases in such a way that the user\u2019s sensitive data will be protected from exposure. Given an encrypted database, the users typically submit queries similar to the following examples: \u2013 How many employees in an organization make over $100,000? \u2013 What is the average age of factory workers suffering from leukemia? Answering the above questions requires one to search and then compute over the encrypted databases in sequence. In the case of privately processing queries with only one of these operations, many efficient solutions have been developed using a special-purpose encryption scheme (e.g., searchable encryption). In this paper, we are interested in efficiently processing queries that need to perform both operations on fully encrypted databases. One immediate solution is to use several special-purpose encryption schemes at the same time, but this approach is associated with a high computational cost for maintaining multiple encryption contexts. The other solution is to use a privacy homomorphism (or fully homomorphic encryption) scheme. However, no secure solutions have been developed that meet the efficiency requirements. In this work, we construct a unified framework so as to efficiently and privately process queries with \u201csearch\u201d and \u201ccompute\u201d operations. To this end, the first part of our work involves devising some underlying circuits as primitives for queries on encrypted data. Second, we apply two optimization techniques to improve the efficiency of the circuit primitives. One technique is to exploit SIMD techniques to accelerate their basic operations. In contrast to general SIMD approaches, our SIMD implementation can be applied even when one basic operation is executed. The other technique is to take a large integer ring (e.g., Z2t) as a message space instead of a binary field. Even for an integer of k bits with k \u0105 t, addition can be performed with degree 1 circuits with lazy carry operations. As a result, search queries including a conjunctive or disjunctive query on encrypted databases of N tuples with \u03bc-bit attributes require OpN log\u03bcq homomorphic operations with depth Oplog\u03bcq circuits. Search-and-compute queries, such as a conjunctive query with aggregate functions in the same conditions, are processed using Op\u03bcNq homomorphic operations with at most depth Oplog\u03bc logNq circuits. Further, we can process search-and-compute queries using only OpN log\u03bcq homomorphic operations with depth Oplog\u03bcq circuits, even in the large domain. Finally, we present various experiments by varying the parameters, such as the query type and the number of tuples.", "title": "Search-and-Compute on Encrypted Data"}, "19de1229db1c2e62367a3d1459e24848064dfd02": {"paper_id": "19de1229db1c2e62367a3d1459e24848064dfd02", "abstract": "We present a simple and efficient compiler for transforming secure multi-party computation (MPC) protocols that enjoy security only with an honest majority into MPC protocols that guarantee security with no honest majority, in the oblivious-transfer (OT) hybrid model. Our technique works by combining a secure protocol in the honest majority setting with a protocol achieving only security against semi-honest parties in the setting of no honest majority. Applying our compiler to variants of protocols from the literature, we get several applications for secure two-party computation and for MPC with no honest majority. These include: \u2022 Constant-rate two-party computation in the OT-hybrid model. We obtain a statistically UC-secure two-party protocol in the OT-hybrid model that can evaluate a general circuit C of size s and depth d with a total communication complexity of O(s)+ poly(k, d, log s) and O(d) rounds. The above result generalizes to a constant number of parties. \u2022 Extending OTs in the malicious model. We obtain a computationally efficient protocol for generating many string OTs from few string OTs with only a constant amortized communication overhead compared to the total length of the string OTs. \u2022 Black-box constructions for constant-round MPC with no honest majority. We obtain general computationally UC-secure MPC protocols in the OT-hybrid model that use only a constant number of rounds, and only make a black-box access to a pseudorandom generator. This gives the first constant-round protocols for three or more parties that only make a black-box use of cryptographic primitives (and avoid expensive zero-knowledge proofs).", "title": "Founding Cryptography on Oblivious Transfer - Efficiently"}, "04948723dec0e6724777ee56f0d10168cce44921": {"paper_id": "04948723dec0e6724777ee56f0d10168cce44921", "abstract": "We propose a general multiparty computation protocol secure against an active adversary corrupting up to n\u22121 of the n players. The protocol may be used to compute securely arithmetic circuits over any finite field Fpk . Our protocol consists of a preprocessing phase that is both independent of the function to be computed and of the inputs, and a much more efficient online phase where the actual computation takes place. The online phase is unconditionally secure and has total computational (and communication) complexity linear in n, the number of players, where earlier work was quadratic in n. Moreover, the work done by each player is only a small constant factor larger than what one would need to compute the circuit in the clear. We show this is optimal for computation in large fields. In practice, for 3 players, a secure 64-bit multiplication can be done in 0.05 ms. Our preprocessing is based on a somewhat homomorphic cryptosystem. We extend a scheme by Brakerski et al., so that we can perform distributed decryption and handle many values in parallel in one ciphertext. The computational complexity of our preprocessing phase is dominated by the public-key operations, we need O(n/s) operations per secure multiplication where s is a parameter that increases with the security parameter of the cryptosystem. Earlier work in this model needed \u03a9(n) operations. In practice, the preprocessing prepares a secure 64-bit multiplication for 3 players in about 13 ms.", "title": "Multiparty Computation from Somewhat Homomorphic Encryption"}, "3cfc0b1e3c19ffb422f0c98754c382a9d8fbbc0b": {"paper_id": "3cfc0b1e3c19ffb422f0c98754c382a9d8fbbc0b", "abstract": "At PKC 2010 Smart and Vercauteren presented a variant of Gentry\u2019s fully homomorphic public key encryption scheme and mentioned that the scheme could support SIMD style operations. The slow key generation process of the Smart\u2013Vercauteren system was then addressed in a paper by Gentry and Halevi, but their key generation method appears to exclude the SIMD style operation alluded to by Smart and Vercauteren. In this paper, we show how to select parameters to enable such SIMD operations, whilst still maintaining practicality of the key generation technique of Gentry and Halevi. As such, we obtain a somewhat homomorphic scheme supporting both SIMD operations and operations on large finite fields of characteristic two. This somewhat homomorphic scheme can be made fully homomorphic in a naive way by recrypting all data elements seperately. However, we show that the SIMD operations can be used to perform the recrypt procedure in parallel, resulting in a substantial speed-up. Finally, we demonstrate how such SIMD operations can be used to perform various tasks by studying two use cases: implementing AES homomorphically and encrypted database lookup.", "title": "Fully homomorphic SIMD operations"}, "65886ebfe5efb3b962e170995951c7ae63bb89d8": {"paper_id": "65886ebfe5efb3b962e170995951c7ae63bb89d8", "abstract": "Small ducted fan autonomous vehicles have potential for several applications, especially for missions in urban environments. This paper discusses the use of dynamic inversion with neural network adaptation to provide an adaptive controller for the GTSpy, a small ducted fan autonomous vehicle based on the Micro Autonomous Systems\u2019 Helispy. This approach allows utilization of the entire low speed flight envelope with a relatively poorly understood vehicle. A simulator model is constructed from a force and moment analysis of the vehicle, allowing for a validation of the controller in preparation for flight testing. Data from flight testing of the system is provided.", "title": "1 Modeling , Control , and Flight Testing of a Small Ducted Fan Aircraft"}, "2375f6d71ce85a9ff457825e192c36045e994bdd": {"paper_id": "2375f6d71ce85a9ff457825e192c36045e994bdd", "abstract": null, "title": "Multilayer feedforward networks are universal approximators"}, "f06da9eebebb7a5a75708de524d23013def45987": {"paper_id": "f06da9eebebb7a5a75708de524d23013def45987", "abstract": "This paper proposes a voice conversion technique based on WaveNet to directly generate target audio waveforms from acoustic features of a source speaker. In voice conversion based on statistical models, the relation between acoustic features, such as spectral parameters, extracted from source and target audio waveforms is generally modeled using statistical models, such as Gaussian mixture models and neural networks. Although modeling the relation between acoustic features is reasonable and efficient, these models are not optimized for predicting target audio waveforms because the vocoder parameters are used as intermediate representations. To overcome this problem, we developed a voice conversion method to model the relation between target audio waveforms and acoustic features extracted from source audio waveforms using WaveNet, which is a generative model for audio waveforms. The proposed model can directly generate converted audio waveforms without vocoders. Experimental results indicate that the proposed method can generate a more naturally sounding converted speech than that using a conventional DNN method.", "title": "Statistical Voice Conversion Based on Wavenet"}, "c06331e930460da453052e0a99d145f1dd15eda5": {"paper_id": "c06331e930460da453052e0a99d145f1dd15eda5", "abstract": "In this paper, we describe a novel spectral conversion method for voice conversion (VC). A Gaussian mixture model (GMM) of the joint probability density of source and target features is employed for performing spectral conversion between speakers. The conventional method converts spectral parameters frame by frame based on the minimum mean square error. Although it is reasonably effective, the deterioration of speech quality is caused by some problems: 1) appropriate spectral movements are not always caused by the frame-based conversion process, and 2) the converted spectra are excessively smoothed by statistical modeling. In order to address those problems, we propose a conversion method based on the maximum-likelihood estimation of a spectral parameter trajectory. Not only static but also dynamic feature statistics are used for realizing the appropriate converted spectrum sequence. Moreover, the oversmoothing effect is alleviated by considering a global variance feature of the converted spectra. Experimental results indicate that the performance of VC can be dramatically improved by the proposed method in view of both speech quality and conversion accuracy for speaker individuality.", "title": "Voice Conversion Based on Maximum-Likelihood Estimation of Spectral Parameter Trajectory"}, "ad8c23e268e97c0e513af57a72198eb878de5627": {"paper_id": "ad8c23e268e97c0e513af57a72198eb878de5627", "abstract": "This paper derives a speech parameter generation algorithm for HMM-based speech synthesis, in which speech parameter sequence is generated from HMMs whose observation vector consists of spectral parameter vector and its dynamic feature vectors. In the algorithm, we assume that the state sequence (state and mixture sequence for the multi-mixture case) or a part of the state sequence is unobservable (i.e., hidden or latent). As a result, the algorithm iterates the forward-backward algorithm and the parameter generation algorithm for the case where state sequence is given. Experimental results show that by using the algorithm, we can reproduce clear formant structure from multi-mixture HMMs as compared with that produced from single-mixture HMMs.", "title": "Speech parameter generation algorithms for HMM-based speech synthesis"}, "857369bf1ffe79dbd24941b64022a2c652a45610": {"paper_id": "857369bf1ffe79dbd24941b64022a2c652a45610", "abstract": "A simple new method for estimating temporally stable power spectra is introduced to provide a unified basis for computing an interference-free spectrum, the fundamental frequency (F0), as well as aperiodicity estimation. F0 adaptive spectral smoothing and cepstral liftering based on consistent sampling theory are employed for interference-free spectral estimation. A perturbation spectrum, calculated from temporally stable power and interference-free spectra, provides the basis for both F0 and aperiodicity estimation. The proposed approach eliminates ad-hoc parameter tuning and the heavy demand on computational power, from which STRAIGHT has suffered in the past.", "title": "Tandem-STRAIGHT: A temporally stable power spectral representation for periodic signals and applications to interference-free spectrum, F0, and aperiodicity estimation"}, "16fea85194803a3980bf0381fa5ec997d3de178c": {"paper_id": "16fea85194803a3980bf0381fa5ec997d3de178c", "abstract": "Acoustic models based on long short-term memory recurrent neural networks (LSTM-RNNs) were applied to statistical parametric speech synthesis (SPSS) and showed significant improvements in naturalness and latency over those based on hidden Markov models (HMMs). This paper describes further optimizations of LSTM-RNN-based SPSS for deployment on mobile devices; weight quantization, multi-frame inference, and robust inference using an -contaminated Gaussian loss function. Experimental results in subjective listening tests show that these optimizations can make LSTM-RNN-based SPSS comparable to HMM-based SPSS in runtime speed while maintaining naturalness. Evaluations between LSTM-RNNbased SPSS and HMM-driven unit selection speech synthesis are also presented.", "title": "Fast, Compact, and High Quality LSTM-RNN Based Statistical Parametric Speech Synthesizers for Mobile Devices"}, "ba91dabec842d507a647aab97ad224b4abdc1635": {"paper_id": "ba91dabec842d507a647aab97ad224b4abdc1635", "abstract": "A vocoder-based speech synthesis system, named WORLD, was developed in an effort to improve the sound quality of realtime applications using speech. Speech analysis, manipulation, and synthesis on the basis of vocoders are used in various kinds of speech research. Although several high-quality speech synthesis systems have been developed, real-time processing has been difficult with them because of their high computational costs. This new speech synthesis system has not only sound quality but also quick processing. It consists of three analysis algorithms and one synthesis algorithm proposed in our previous research. The effectiveness of the system was evaluated by comparing its output with against natural speech including consonants. Its processing speed was also compared with those of conventional systems. The results showed that WORLD was superior to the other systems in terms of both sound quality and processing speed. In particular, it was over ten times faster than the conventional systems, and the real time factor (RTF) indicated that it was fast enough for real-time processing. key words: speech analysis, speech synthesis, vocoder, sound quality, realtime processing", "title": "WORLD: A Vocoder-Based High-Quality Speech Synthesis System for Real-Time Applications"}, "272216c1f097706721096669d85b2843c23fa77d": {"paper_id": "272216c1f097706721096669d85b2843c23fa77d", "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.", "title": "Adam: A Method for Stochastic Optimization"}, "02b3d1d162080d9aefd3fc30a0bcc9a843073b5d": {"paper_id": "02b3d1d162080d9aefd3fc30a0bcc9a843073b5d", "abstract": "In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling, a task central to language understanding. We extend current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language. We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark. Our best single model significantly improves state-of-the-art perplexity from 51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20), while an ensemble of models sets a new record by improving perplexity from 41.0 down to 24.2. We also release these models for the NLP and ML community to study and improve upon.", "title": "Exploring the Limits of Language Modeling"}, "1ea6b2f67a3a7f044209aae0d0fd1cb14a1e9e06": {"paper_id": "1ea6b2f67a3a7f044209aae0d0fd1cb14a1e9e06", "abstract": "Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast twodimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent.", "title": "Pixel Recurrent Neural Networks"}, "735d4220d5579cc6afe956d9f6ea501a96ae99e2": {"paper_id": "735d4220d5579cc6afe956d9f6ea501a96ae99e2", "abstract": "A momentum term is usually included in the simulations of connectionist learning algorithms. Although it is well known that such a term greatly improves the speed of learning, there have been few rigorous studies of its mechanisms. In this paper, I show that in the limit of continuous time, the momentum parameter is analogous to the mass of Newtonian particles that move through a viscous medium in a conservative force field. The behavior of the system near a local minimum is equivalent to a set of coupled and damped harmonic oscillators. The momentum term improves the speed of convergence by bringing some eigen components of the system closer to critical damping. Similar results can be obtained for the discrete time case used in computer simulations. In particular, I derive the bounds for convergence on learning-rate and momentum parameters, and demonstrate that the momentum term can increase the range of learning rate over which the system converges. The optimal condition for convergence is also analyzed.", "title": "On the momentum term in gradient descent learning algorithms"}, "bcdce6325b61255c545b100ef51ec7efa4cced68": {"paper_id": "bcdce6325b61255c545b100ef51ec7efa4cced68", "abstract": "Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.", "title": "An overview of gradient descent optimization algorithms"}, "4856e7719e566f2466369ae2031afb07c934d4d3": {"paper_id": "4856e7719e566f2466369ae2031afb07c934d4d3", "abstract": "A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new approach to second-order optimization, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep or recurrent neural network training, and provide numerical evidence for its superior optimization performance.", "title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization"}, "b056f9f90252b906b83111e4c3494283ddb1e4ca": {"paper_id": "b056f9f90252b906b83111e4c3494283ddb1e4ca", "abstract": "Cloud-assisted video streaming has emerged as a new paradigm to optimize multimedia content distribution over the Internet. This article investigates the problem of streaming cloud-assisted real-time video to multiple destinations (e.g., cloud video conferencing, multi-player cloud gaming, etc.) over lossy communication networks. The user diversity and network dynamics result in the delay differences among multiple destinations. This research proposes <underline>D</underline>ifferentiated cloud-<underline>A</underline>ssisted <underline>VI</underline>deo <underline>S</underline>treaming (DAVIS) framework, which proactively leverages such delay differences in video coding and transmission optimization. First, we analytically formulate the optimization problem of joint coding and transmission to maximize received video quality. Second, we develop a quality optimization framework that integrates the video representation selection and FEC (Forward Error Correction) packet interleaving. The proposed DAVIS is able to effectively perform differentiated quality optimization for multiple destinations by taking advantage of the delay differences in cloud-assisted video streaming system. We conduct the performance evaluation through extensive experiments with the Amazon EC2 instances and Exata emulation platform. Evaluation results show that DAVIS outperforms the reference cloud-assisted streaming solutions in video quality and delay performance.", "title": "Delay-Aware Quality Optimization in Cloud-Assisted Video Streaming System"}, "da5b66fae86c2a79d4787cadd0104eb8e4c07bd6": {"paper_id": "da5b66fae86c2a79d4787cadd0104eb8e4c07bd6", "abstract": "This review reports on the effects of hypoxia on human skeletal muscle tissue. It was hypothesized in early reports that chronic hypoxia, as the main physiological stress during exposure to altitude, per se might positively affect muscle oxidative capacity and capillarity. However, it is now established that sustained exposure to severe hypoxia has detrimental effects on muscle structure. Short-term effects on skeletal muscle structure can readily be observed after 2 months of acute exposure of lowlanders to severe hypoxia, e.g. during typical mountaineering expeditions to the Himalayas. The full range of phenotypic malleability of muscle tissue is demonstrated in people living permanently at high altitude (e.g. at La Paz, 3600-4000 m). In addition, there is some evidence for genetic adaptations to hypoxia in high-altitude populations such as Tibetans and Quechuas, who have been exposed to altitudes in excess of 3500 m for thousands of generations. The hallmark of muscle adaptation to hypoxia in all these cases is a decrease in muscle oxidative capacity concomitant with a decrease in aerobic work capacity. It is thought that local tissue hypoxia is an important adaptive stress for muscle tissue in exercise training, so these results seem contra-intuitive. Studies have therefore been conducted in which subjects were exposed to hypoxia only during exercise sessions. In this situation, the potentially negative effects of permanent hypoxic exposure and other confounding variables related to exposure to high altitude could be avoided. Training in hypoxia results, at the molecular level, in an upregulation of the regulatory subunit of hypoxia-inducible factor-1 (HIF-1). Possibly as a consequence of this upregulation of HIF-1, the levels mRNAs for myoglobin, for vascular endothelial growth factor and for glycolytic enzymes, such as phosphofructokinase, together with mitochondrial and capillary densities, increased in a hypoxia-dependent manner. Functional analyses revealed positive effects on V(O(2)max) (when measured at altitude) on maximal power output and on lean body mass. In addition to the positive effects of hypoxia training on athletic performance, there is some recent indication that hypoxia training has a positive effect on the risk factors for cardiovascular disease.", "title": "Muscle tissue adaptations to hypoxia."}, "bb16aeb12a98cdabf6f6054dd16302645a474d88": {"paper_id": "bb16aeb12a98cdabf6f6054dd16302645a474d88", "abstract": "Question answering (QA) systems often consist of several components such as Named Entity Disambiguation (NED), Relation Extraction (RE), and Query Generation (QG). In this paper, we focus on the QG process of a QA pipeline on a large-scale Knowledge Base (KB), with noisy annotations and complex sentence structures. We therefore propose SQG, a SPARQL Query Generator with modular architecture, enabling easy integration with other components for the construction of a fully functional QA pipeline. SQG can be used on large open-domain KBs and handle noisy inputs by discovering a minimal subgraph based on uncertain inputs, that it receives from the NED and RE components. This ability allows SQG to consider a set of candidate entities/relations, as opposed to the most probable ones, which leads to a significant boost in the performance of the QG component. The captured subgraph covers multiple candidate walks, which correspond to SPARQL queries. To enhance the accuracy, we present a ranking model based on Tree-LSTM that takes into account the syntactical structure of the question and the tree representation of the candidate queries to find the one representing the correct intention behind the question. SQG outperforms the baseline systems and achieves a macro F1-measure of 75% on the LC-QuAD dataset.", "title": "Formal Query Generation for Question Answering over Knowledge Bases"}, "434d7ca6ae0e824ad27867b9ccfa3af6daeae05a": {"paper_id": "434d7ca6ae0e824ad27867b9ccfa3af6daeae05a", "abstract": "The third instalment of the open challenge on Question Answering over Linked Data (QALD-3) has been conducted as a half-day lab at CLEF 2013. Differently from previous editions of the challenge, QALD-3 put a strong emphasis on multilinguality, offering two tasks: one on multilingual question answering and one on ontology lexicalization. While no submissions were received for the latter, the former attracted six teams who submitted their systems\u2019 results on the provided datasets. This paper provides an overview of QALD-3, discussing the approaches experimented by the participating systems as well as the obtained results.", "title": "Multilingual Question Answering over Linked Data (QALD-3): Lab Overview"}, "2b2c30dfd3968c5d9418bb2c14b2382d3ccc64b2": {"paper_id": "2b2c30dfd3968c5d9418bb2c14b2382d3ccc64b2", "abstract": "DBpedia is a community effort to extract structured information from Wikipedia and to make this information available on the Web. DBpedia allows you to ask sophisticated queries against datasets derived from Wikipedia and to link other datasets on the Web to Wikipedia data. We describe the extraction of the DBpedia datasets, and how the resulting information is published on the Web for humanand machineconsumption. We describe some emerging applications from the DBpedia community and show how website authors can facilitate DBpedia content within their sites. Finally, we present the current status of interlinking DBpedia with other open datasets on the Web and outline how DBpedia could serve as a nucleus for an emerging Web of open data.", "title": "DBpedia: A Nucleus for a Web of Open Data"}, "cb4d277a51da6894fe5143013978567ef5f805c8": {"paper_id": "cb4d277a51da6894fe5143013978567ef5f805c8", "abstract": "RDF question/answering (Q/A) allows users to ask questions in natural languages over a knowledge base represented by RDF. To answer a national language question, the existing work takes a two-stage approach: question understanding and query evaluation. Their focus is on question understanding to deal with the disambiguation of the natural language phrases. The most common technique is the joint disambiguation, which has the exponential search space. In this paper, we propose a systematic framework to answer natural language questions over RDF repository (RDF Q/A) from a graph data-driven perspective. We propose a semantic query graph to model the query intention in the natural language question in a structural way, based on which, RDF Q/A is reduced to subgraph matching problem. More importantly, we resolve the ambiguity of natural language questions at the time when matches of query are found. The cost of disambiguation is saved if there are no matching found. We compare our method with some state-of-the-art RDF Q/A systems in the benchmark dataset. Extensive experiments confirm that our method not only improves the precision but also speeds up query performance greatly.", "title": "Natural language question answering over RDF: a graph data driven approach"}, "b52c1b64ba302460aad5dbb2e0ba239be55f5e38": {"paper_id": "b52c1b64ba302460aad5dbb2e0ba239be55f5e38", "abstract": "As an increasing amount of RDF data is published as Linked Data, intuitive ways of accessing this data become more and more important. Question answering approaches have been proposed as a good compromise between intuitiveness and expressivity. Most question answering systems translate questions into triples which are matched against the RDF data to retrieve an answer, typically relying on some similarity metric. However, in many cases, triples do not represent a faithful representation of the semantic structure of the natural language question, with the result that more expressive queries can not be answered. To circumvent this problem, we present a novel approach that relies on a parse of the question to produce a SPARQL template that directly mirrors the internal structure of the question. This template is then instantiated using statistical entity identification and predicate detection. We show that this approach is competitive and discuss cases of questions that can be answered with our approach but not with competing approaches.", "title": "Template-based question answering over RDF data"}, "26d3469d96abb7bcfa802df4380fe7862798d6b2": {"paper_id": "26d3469d96abb7bcfa802df4380fe7862798d6b2", "abstract": "Natural language interfaces offer end-users a familiar and convenient option for querying ontology-based knowledge bases. Several studies have shown that they can achieve high retrieval performance as well as domain independence. This paper focuses on usability and investigates if NLIs are useful from an end-user\u2019s point of view. To that end, we introduce four interfaces each allowing a different query language and present a usability study benchmarking these interfaces. The results of the study reveal a clear preference for full sentences as query language and confirm that NLIs are useful for querying Semantic Web data.", "title": "How Useful Are Natural Language Interfaces to the Semantic Web for Casual End-Users?"}, "233f5e420642b33ee4fea3af9db846324ccba469": {"paper_id": "233f5e420642b33ee4fea3af9db846324ccba469", "abstract": "The availability of large amounts of open, distributed and structured semantic data on the web has no precedent in the history of computer science. In recent years, there have been important advances in semantic search and question answering over RDF data. In particular, natural language interfaces to online semantic data have the advantage that they can exploit the expressive power of Semantic Web data models and query languages, while at the same time hiding their complexity from the user. However, despite the increasing interest in this area, there are no evaluations so far that systematically evaluate this kind of systems, in contrast to traditional question answering and search interfaces to document spaces. To address this gap, we have set up a series of evaluation challenges for question answering over linked data. The main goal of the challenge was to get insight into the strengths, capabilities and current shortcomings of question answering systems as interfaces to query linked data sources, as well as benchmarking how these interaction paradigms can deal with the fact that the amount of RDF data available on the web is very large and heterogeneous with respect to the vocabularies and schemas used. Here we report on the results from the first and second of such evaluation campaigns. We also discuss how the second evaluation addressed some of the issues and limitations which arose from the first one, as well as the open issues to be addressed in future competitions.", "title": "Evaluating question answering over linked data"}, "1379019778ff40ebeaf62795edf17d973047c29e": {"paper_id": "1379019778ff40ebeaf62795edf17d973047c29e", "abstract": "Overview \u2022 Learning flexible word representations is the first step towards learning semantics. \u2022The best current approach to learning word embeddings involves training a neural language model to predict each word in a sentence from its neighbours. \u2013 Need to use a lot of data and high-dimensional embeddings to achieve competitive performance. \u2013 More scalable methods translate to better results. \u2022We propose a simple and scalable approach to learning word embeddings based on training lightweight models with noise-contrastive estimation. \u2013 It is simpler, faster, and produces better results than the current state-of-the art method.", "title": "Learning word embeddings efficiently with noise-contrastive estimation"}, "7ef3ac14cdb484aaa2b039850093febd5cf73a21": {"paper_id": "7ef3ac14cdb484aaa2b039850093febd5cf73a21", "abstract": "Experimentol corroboration was obtained for the hypothesis that the proportion of words common to the contexts of word A and to the contexts of word B is a function of the degree to which A and B are similar in meaning. The tests were carried out for variously defined contexts. The shapes of the functions, however, indicate that similarity of context is reliable as criterion only for detecting pairs of words that are very similar in meaning.", "title": "Contextual correlates of synonymy"}, "13082af1fd6bb9bfe63e73cf007de1655b7f9ae0": {"paper_id": "13082af1fd6bb9bfe63e73cf007de1655b7f9ae0", "abstract": "The automated categorization (or classification) of texts into predefined categories has witnessed a booming interest in the last 10 years, due to the increased availability of documents in digital form and the ensuing need to organize them. In the research community the dominant approach to this problem is based on machine learning techniques: a general inductive process automatically builds a classifier by learning, from a set of preclassified documents, the characteristics of the categories. The advantages of this approach over the knowledge engineering approach (consisting in the manual definition of a classifier by domain experts) are a very good effectiveness, considerable savings in terms of expert labor power, and straightforward portability to different domains. This survey discusses the main approaches to text categorization that fall within the machine learning paradigm. We will discuss in detail issues pertaining to three different problems, namely, document representation, classifier construction, and classifier evaluation.", "title": "Machine learning in automated text categorization"}, "09193e19b59fc8f05bee9d6efbfb1607ca5b6501": {"paper_id": "09193e19b59fc8f05bee9d6efbfb1607ca5b6501", "abstract": "We address a central problem of neuroanatomy, namely, the automatic segmentation of neuronal structures depicted in stacks of electron microscopy (EM) images. This is necessary to efficiently map 3D brain structure and connectivity. To segment biological neuron membranes, we use a special type of deep artificial neural network as a pixel classifier. The label of each pixel (membrane or nonmembrane) is predicted from raw pixel values in a square window centered on it. The input layer maps each window pixel to a neuron. It is followed by a succession of convolutional and max-pooling layers which preserve 2D information and extract features with increasing levels of abstraction. The output layer produces a calibrated probability for each class. The classifier is trained by plain gradient descent on a 512 \u00d7 512 \u00d7 30 stack with known ground truth, and tested on a stack of the same size (ground truth unknown to the authors) by the organizers of the ISBI 2012 EM Segmentation Challenge. Even without problem-specific postprocessing, our approach outperforms competing techniques by a large margin in all three considered metrics, i.e. rand error, warping error and pixel error. For pixel error, our approach is the only one outperforming a second human observer.", "title": "Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images"}, "22eff4a1cd15d76b4e89ff3111713607a348816a": {"paper_id": "22eff4a1cd15d76b4e89ff3111713607a348816a", "abstract": "Recent work has shown that neuralembedded word representations capture many relational similarities, which can be recovered by means of vector arithmetic in the embedded space. We show that Mikolov et al.\u2019s method of first adding and subtracting word vectors, and then searching for a word similar to the result, is equivalent to searching for a word that maximizes a linear combination of three pairwise word similarities. Based on this observation, we suggest an improved method of recovering relational similarities, improving the state-of-the-art results on two recent word-analogy datasets. Moreover, we demonstrate that analogy recovery is not restricted to neural word embeddings, and that a similar amount of relational similarities can be recovered from traditional distributional word representations.", "title": "Linguistic Regularities in Sparse and Explicit Word Representations"}, "73e6351a8fb61afc810a8bb3feaa44c41e5c5d7b": {"paper_id": "73e6351a8fb61afc810a8bb3feaa44c41e5c5d7b", "abstract": "The lexical semantic system is an important component of human language and cognitive processing. One approach to modeling semantic knowledge makes use of hand-constructed networks or trees of interconnected word senses (Miller, Beckwith, Fellbaum, Gross, & Miller, 1990; Jarmasz & Szpakowicz, 2003). An alternative approach seeks to model word meanings as high-dimensional vectors, which are derived from the cooccurrence of words in unlabeled text corpora (Landauer & Dumais, 1997; Burgess & Lund, 1997a). This paper introduces a new vector-space method for deriving word-meanings from large corpora that was inspired by the HAL and LSA models, but which achieves better and more consistent results in predicting human similarity judgments. We explain the new model, known as COALS, and how it relates to prior methods, and then evaluate the various models on a range of tasks, including a novel set of semantic similarity ratings involving both semantically and morphologically related terms.", "title": "An Improved Model of Semantic Similarity Based on Lexical Co-Occurrence"}, "3cfbb77e5a0e24772cfdb2eb3d4f35dead54b118": {"paper_id": "3cfbb77e5a0e24772cfdb2eb3d4f35dead54b118", "abstract": "Context-predicting models (more commonly known as embeddings or neural language models) are the new kids on the distributional semantics block. Despite the buzz surrounding these models, the literature is still lacking a systematic comparison of the predictive models with classic, count-vector-based distributional semantic approaches. In this paper, we perform such an extensive evaluation, on a wide range of lexical semantics tasks and across many parameter settings. The results, to our own surprise, show that the buzz is fully justified, as the context-predicting models obtain a thorough and resounding victory against their count-based counterparts.", "title": "Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors"}, "0a10d64beb0931efdc24a28edaa91d539194b2e2": {"paper_id": "0a10d64beb0931efdc24a28edaa91d539194b2e2", "abstract": "We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.", "title": "Efficient Estimation of Word Representations in Vector Space"}, "0826c98d1b1513aa2f45e6654bb5075a58b64649": {"paper_id": "0826c98d1b1513aa2f45e6654bb5075a58b64649", "abstract": "\u2022 Neural network language model and distributed representation for words (Vector representation) \u2022 Capture syntactic and remantic regularities in language \u2022 Outperform state-of-the-art", "title": "Linguistic Regularities in Continuous Space Word Representations"}, "29c6a8fd6201200bc83b54bad6a10d0c023b9928": {"paper_id": "29c6a8fd6201200bc83b54bad6a10d0c023b9928", "abstract": "We present a question answering system (CASIA@V2) over Linked Data (DBpedia), which translates natural language questions into structured queries automatically. Existing systems usually adopt a pipeline framework, which contains four major steps: 1) Decomposing the question and detecting candidate phrases; 2) mapping the detected phrases into semantic items of Linked Data; 3) grouping the mapped semantic items into semantic triples; and 4) generating the rightful SPARQL query. We present a jointly learning framework using Markov Logic Network(MLN) for phrase detection, phrases mapping to semantic items and semantic items grouping. We formulate the knowledge for resolving the ambiguities in three steps of QALD as first-order logic clauses in a MLN. We evaluate our approach on QALD-4 test dataset and achieve an F-measure score of 0.36, an average precision of 0.32 and an average recall of 0.40 over 50 questions.", "title": "CASIA@V2: A MLN-based Question Answering System over Linked Data"}, "06d51deada6e771a2571807a47dd991120c8dd1a": {"paper_id": "06d51deada6e771a2571807a47dd991120c8dd1a", "abstract": "Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder\u2013Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.", "title": "On the Properties of Neural Machine Translation: Encoder-Decoder Approaches"}, "289e3e6b84982eb65aea8e3a64f2f6916c98e87e": {"paper_id": "289e3e6b84982eb65aea8e3a64f2f6916c98e87e", "abstract": "Syntactic constituency parsing is a fundamental problem in natural language processing and has been the subject of intensive research and engineering for decades. As a result, the most accurate parsers are domain specific, complex, and inefficient. In this paper we show that the domain agnostic attention-enhanced sequence-to-sequence model achieves state-of-the-art results on the most widely used syntactic constituency parsing dataset, when trained on a large synthetic corpus that was annotated using existing parsers. It also matches the performance of standard parsers when trained only on a small human-annotated dataset, which shows that this model is highly data-efficient, in contrast to sequence-to-sequence models without the attention mechanism. Our parser is also fast, processing over a hundred sentences per second with an unoptimized CPU implementation.", "title": "Grammar as a Foreign Language"}, "2d9e3f53fcdb548b0b3c4d4efb197f164fe0c381": {"paper_id": "2d9e3f53fcdb548b0b3c4d4efb197f164fe0c381", "abstract": "In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.", "title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling"}, "62a6cf246c9bec56babab9424fa36bfc9d4a47e8": {"paper_id": "62a6cf246c9bec56babab9424fa36bfc9d4a47e8", "abstract": "How can we enable computers to automatically answer questions like \u201cWho created the character Harry Potter\u201d? Carefully built knowledge bases provide rich sources of facts. However, it remains a challenge to answer factoid questions raised in natural language due to numerous expressions of one question. In particular, we focus on the most common questions \u2014 ones that can be answered with a single fact in the knowledge base. We propose CFO, a Conditional Focused neuralnetwork-based approach to answering factoid questions with knowledge bases. Our approach first zooms in a question to find more probable candidate subject mentions, and infers the final answers with a unified conditional probabilistic framework. Powered by deep recurrent neural networks and neural embeddings, our proposed CFO achieves an accuracy of 75.7% on a dataset of 108k questions \u2013 the largest public one to date. It outperforms the current state of the art by an absolute margin of 11.8%.", "title": "CFO: Conditional Focused Neural Question Answering with Large-scale Knowledge Bases"}, "4c65c806358cdb55ad5a763405db1489e58a2a0e": {"paper_id": "4c65c806358cdb55ad5a763405db1489e58a2a0e", "abstract": "In this paper, we evaluate the performance of Multicarrier-Low Density Spreading Multiple Access (MC-LDSMA) as a multiple access technique for mobile communication systems. The MC-LDSMA technique is compared with current multiple access techniques, OFDMA and SC-FDMA. The performance is evaluated in terms of cubic metric, block error rate, spectral efficiency and fairness. The aim is to investigate the expected gains of using MC-LDSMA in the uplink for next generation cellular systems. The simulation results of the link and system-level performance evaluation show that MC-LDSMA has significant performance improvements over SC-FDMA and OFDMA. It is shown that using MC-LDSMA can considerably reduce the required transmission power and increase the spectral efficiency and fairness among the users.", "title": "Performance evaluation of Low Density Spreading Multiple Access"}, "3130a53f7fc8167895d8f16ff45f505e69cbb6ff": {"paper_id": "3130a53f7fc8167895d8f16ff45f505e69cbb6ff", "abstract": "This paper discusses the analysis and simulation of a technique for combating the effects of multipath propagation and cochannel interference on a narrow-band digital mobile channel. This system uses the discrete Fourier transform to orthogonally frequency multiplex many narrow subchannels, each signaling at a very low rate, into one high-rate channel. When this technique is used with pilot-based correction, the effects of flat Rayleigh fading can be reduced significantly. An improvement in signal-to-interference ratio of 6 dB can be obtained over the bursty Rayleigh channel. In additim, with each subchannel signaling at a low rate, this technique can provide added protection against delay spread. To enhance the behavior of the technique in a heavily frequency-selective environment, interpolated pilots are used. A frequency offset reference scheme is employed for the pilots to improve protection against cochannel interference.", "title": "Analysis and Simulation of a Digital Mobile Channel Using Orthogonal Frequency Division Multiplexing"}, "89479c450707968f73f6ac992525a0e172586be8": {"paper_id": "89479c450707968f73f6ac992525a0e172586be8", "abstract": null, "title": "Frequency domain equalization for single-carrier broadband wireless systems"}, "c2fe958ccd2d5b087aedeccf42615736d3ebcee7": {"paper_id": "c2fe958ccd2d5b087aedeccf42615736d3ebcee7", "abstract": null, "title": "Wireless communications - principles and practice"}, "1ae04a7c66b946fd2d231690a2b44ca1f03984c8": {"paper_id": "1ae04a7c66b946fd2d231690a2b44ca1f03984c8", "abstract": "Latent fingerprints are routinely found at crime scenes due to the inadvertent contact of the criminals' finger tips with various objects. As such, they have been used as crucial evidence for identifying and convicting criminals by law enforcement agencies. However, compared to plain and rolled prints, latent fingerprints usually have poor quality of ridge impressions with small fingerprint area, and contain large overlap between the foreground area (friction ridge pattern) and structured or random noise in the background. Accordingly, latent fingerprint segmentation is a difficult problem. In this paper, we propose a latent fingerprint segmentation algorithm whose goal is to separate the fingerprint region (region of interest) from background. Our algorithm utilizes both ridge orientation and frequency features. The orientation tensor is used to obtain the symmetric patterns of fingerprint ridge orientation, and local Fourier analysis method is used to estimate the local ridge frequency of the latent fingerprint. Candidate fingerprint (foreground) regions are obtained for each feature type; an intersection of regions from orientation and frequency features localizes the true latent fingerprint regions. To verify the viability of the proposed segmentation algorithm, we evaluated the segmentation results in two aspects: a comparison with the ground truth foreground and matching performance based on segmented region.", "title": "Automatic segmentation of latent fingerprints"}, "2a8a474a6b613105b672bb4d58a59d6eafd035ef": {"paper_id": "2a8a474a6b613105b672bb4d58a59d6eafd035ef", "abstract": "Latent fingerprint identification is of critical importance to law enforcement agencies in forensics application. While tremendous progress has been made in the field of automatic fingerprint matching, latent fingerprint matching continues to be a difficult problem because the challenges involved in latent print matching are quite different from plain or rolled fingerprint matching. Poor quality of friction ridge impressions, small finger area and large non-linear distortion are some of the main difficulties in latent fingerprint matching. We propose a system for matching latent images to rolled fingerprints that takes into account the specific characteristics of the latent matching problem. In addition to minutiae, additional features like orientation field and quality map are also used in our system. Experimental results on the NIST SD27 latent database indicate that the introduction of orientation field and quality map to minutiae-based matching leads to good recognition performance despite the inherently difficult nature of the problem. We achieve the rank-20 accuracy of 93.4% in retrieving 258 latents from a background database of 2,258 rolled fingerprints.", "title": "On matching latent fingerprints"}, "6a649276903f2d59891e95009b7426dc95eb84cf": {"paper_id": "6a649276903f2d59891e95009b7426dc95eb84cf", "abstract": "A novel minutiae-based fingerprint matching algorithm is proposed. A minutiae matching algorithm has to solve two problems: correspondence and similarity computation. For the correspondence problem, we assign each minutia two descriptors: texture-based and minutiae-based descriptors, and use an alignment-based greedy matching algorithm to establish the correspondences between minutiae. For the similarity computation, we extract a 17-D feature vector from the matching result, and convert the feature vector into a matching score using support vector classifier. The proposed algorithm is tested on FVC2002 databases and compared to all participators in FVC2002. According to equal error rate, the proposed algorithm ranks 1st on DB3, the most difficult database in FVC2002, and on the average ranks 2nd on all 4 databases. 2007 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.", "title": "Combining minutiae descriptors for fingerprint matching"}, "9ca44d39f4f6c55001fa110850a66221b49219e4": {"paper_id": "9ca44d39f4f6c55001fa110850a66221b49219e4", "abstract": "In the era of information burst, optimization of processes for Information Retrieval, Text Summarization, Text and Data Analytic systems becomes utmost important. Therefore in order to achieve accuracy, redundant words with low or no semantic meaning must be filtered out. Such words are known as Stopwords. Stopwords list has been developed for languages like English, Chinese, Arabic, Hindi, etc but standard stopword list is still missing for Sanskrit language. Identifying stop words manually from Sanskrit text is a herculean task hence this paper reflects an automated stop word generator algorithm based on frequency of word and its implementation to ease the task. To fine-tune the generated list still manual intervention by language expert is required thus following a hybrid approach. The paper presents the first of its kind, a list of seventy-five generic stopwords of Sanskrit language extracted from a data amounting to nearly seventy-six thousand words.", "title": "Generating Stopword List for Sanskrit Language"}, "d181f2d6332d4ef92d72435bd81900d2c6543415": {"paper_id": "d181f2d6332d4ef92d72435bd81900d2c6543415", "abstract": "Summary form only given. We have designed and implemented an efficient stop-word removal algorithm for Arabic language based on a finite state machine (FSM). An efficient stop-word removal technique is needed in many natural language processing application such as: spelling normalization, stemming and stem weighting, Question answering systems and in information retrieval systems (IR). Most of the existing stop-word removal techniques bases on a dictionary that contains a list of stop-word, it is very expensive, it takes too much time for searching process and required too much space to store these stop-words. The new Arabic removal stop-word technique has been tested using a set of 242 Arabic abstracts chosen from the Proceedings of the Saudi Arabian National Computer conferences, and another set of data chosen from the holy Q'uran, and it gives impressive results that reached approximately to 98%.", "title": "Stop-word removal algorithm for Arabic language"}, "4feea0c50c9e841b0c2022814e2a12d72596da5a": {"paper_id": "4feea0c50c9e841b0c2022814e2a12d72596da5a", "abstract": "Due to the high precision and non-contact characteristics, infrared thermography has been widely used in equipment inspection to ensure the safety of electric power systems. A fundamental step toward automatic inspection and diagnosis is the detection of equipment in thermal images. Therefore, this paper presents a deep learning approach to detect equipment parts in real-time. Specifically, we propose a deep convolutional neural network that predicts the coordinates, orientation angle, and class type of each equipment part. A prior concerning orientation consistency between parts is also incorporated into our model to improve the prediction results. For evaluation, we construct a large image set containing various kinds of scenarios. Experiments on the data set show that our method is robust to noise, achieving 93.7% mean average precision when the intersection over union threshold is 0.5, and running at 20 fps on GPU. We believe that our high accurate detection results can benefit the subsequent diagnosis.", "title": "A Deep Learning Approach for Oriented Electrical Equipment Detection in Thermal Images"}, "33da83b54410af11d0cd18fd07c74e1a99f67e84": {"paper_id": "33da83b54410af11d0cd18fd07c74e1a99f67e84", "abstract": "We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be repurposed to novel generic tasks. Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks. We investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges. We compare the efficacy of relying on various network levels to define a fixed feature, and report novel results that significantly outperform the state-of-the-art on several important vision challenges. We are releasing DeCAF, an open-source implementation of these deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms.", "title": "DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition"}, "0585b80713848a5b54b82265a79f031a4fbd3332": {"paper_id": "0585b80713848a5b54b82265a79f031a4fbd3332", "abstract": "In this paper we are interested in how semantic segmentation can help object detection. Towards this goal, we propose a novel deformable part-based model which exploits region-based segmentation algorithms that compute candidate object regions by bottom-up clustering followed by ranking of those regions. Our approach allows every detection hypothesis to select a segment (including void), and scores each box in the image using both the traditional HOG filters as well as a set of novel segmentation features. Thus our model ``blends'' between the detector and segmentation models. Since our features can be computed very efficiently given the segments, we maintain the same complexity as the original DPM. We demonstrate the effectiveness of our approach in PASCAL VOC 2010, and show that when employing only a root filter our approach outperforms Dalal & Triggs detector on all classes, achieving 13% higher average AP. When employing the parts, we outperform the original DPM in $19$ out of $20$ classes, achieving an improvement of 8% AP. Furthermore, we outperform the previous state-of-the-art on VOC 2010 test by 4%.", "title": "Bottom-Up Segmentation for Top-Down Detection"}, "cbcd9f32b526397f88d18163875d04255e72137f": {"paper_id": "cbcd9f32b526397f88d18163875d04255e72137f", "abstract": null, "title": "Gradient-based learning applied to document recognition"}, "7fb8d9c36c23f274f2dd84945dd32ec2cc143de1": {"paper_id": "7fb8d9c36c23f274f2dd84945dd32ec2cc143de1", "abstract": "Feature extraction, coding and pooling, are important components on many contemporary object recognition paradigms. In this paper we explore novel pooling techniques that encode the second-order statistics of local descriptors inside a region. To achieve this effect, we introduce multiplicative second-order analogues of average and maxpooling that together with appropriate non-linearities lead to state-ofthe-art performance on free-form region recognition, without any type of feature coding. Instead of coding, we found that enriching local descriptors with additional image information leads to large performance gains, especially in conjunction with the proposed pooling methodology. We show that second-order pooling over free-form regions produces results superior to those of the winning systems in the Pascal VOC 2011 semantic segmentation challenge, with models that are 20,000 times faster.", "title": "Semantic Segmentation with Second-Order Pooling"}, "76f02eca773414828271da315b379efa9e1f57fa": {"paper_id": "76f02eca773414828271da315b379efa9e1f57fa", "abstract": "We present a novel framework to generate and rank plausible hypotheses for the spatial extent of objects in images using bottom-up computational processes and mid-level selection cues. The object hypotheses are represented as figure-ground segmentations, and are extracted automatically, without prior knowledge of the properties of individual object classes, by solving a sequence of Constrained Parametric Min-Cut problems (CPMC) on a regular image grid. In a subsequent step, we learn to rank the corresponding segments by training a continuous model to predict how likely they are to exhibit real-world regularities (expressed as putative overlap with ground truth) based on their mid-level region properties, then diversify the estimated overlap score using maximum marginal relevance measures. We show that this algorithm significantly outperforms the state of the art for low-level segmentation in the VOC 2009 and 2010 data sets. In our companion papers [1], [2], we show that the algorithm can be used, successfully, in a segmentation-based visual object category recognition pipeline. This architecture ranked first in the VOC2009 and VOC2010 image segmentation and labeling challenges.", "title": "CPMC: Automatic Object Segmentation Using Constrained Parametric Min-Cuts"}, "1395f0561db13cad21a519e18be111cbe1e6d818": {"paper_id": "1395f0561db13cad21a519e18be111cbe1e6d818", "abstract": "We address the problem of segmenting and recognizing objects in real world images, focusing on challenging articulated categories such as humans and other animals. For this purpose, we propose a novel design for region-based object detectors that integrates efficiently top-down information from scanning-windows part models and global appearance cues. Our detectors produce class-specific scores for bottom-up regions, and then aggregate the votes of multiple overlapping candidates through pixel classification. We evaluate our approach on the PASCAL segmentation challenge, and report competitive performance with respect to current leading techniques. On VOC2010, our method obtains the best results in 6/20 categories and the highest performance on articulated objects.", "title": "Semantic segmentation using regions and parts"}, "6074c1108997e0c1f97dc3c199323a162ffe978d": {"paper_id": "6074c1108997e0c1f97dc3c199323a162ffe978d", "abstract": "Torch7 is a versatile numeric computing framework and machine learning library that extends Lua. Its goal is to provide a flexible environment to design and train learning machines. Flexibility is obtained via Lua, an extremely lightweight scripting language. High performance is obtained via efficient OpenMP/SSE and CUDA implementations of low-level numeric routines. Torch7 can easily be interfaced to third-party software thanks to Lua\u2019s light interface.", "title": "Torch7: A Matlab-like Environment for Machine Learning"}, "2f7ad26514bce4df6c8ebc42c90383ef3a974df4": {"paper_id": "2f7ad26514bce4df6c8ebc42c90383ef3a974df4", "abstract": "Pylearn2 is a machine learning research library. This does n t just mean that it is a collection of machine learning algorithms that share a comm n API; it means that it has been designed for flexibility and extensibility in ord e to facilitate research projects that involve new or unusual use cases. In this paper we give a brief history of the library, an overview of its basic philosophy, a summar y of the library\u2019s architecture, and a description of how the Pylearn2 communi ty functions socially.", "title": "Pylearn2: a machine learning research library"}, "1109b663453e78a59e4f66446d71720ac58cec25": {"paper_id": "1109b663453e78a59e4f66446d71720ac58cec25", "abstract": "We present an integrated framework for using Convolutional Networks for classification , localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.", "title": "OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks"}, "4af785bf8a5959d7e8eb37ca87c45db2ac6a544c": {"paper_id": "4af785bf8a5959d7e8eb37ca87c45db2ac6a544c", "abstract": "The style of an image plays a significant role in how it is viewed, but style has received little attention in computer vision research. We describe an approach to predicting style of images, and perform a thorough evaluation of different image features for these tasks. We find that features learned in a multi-layer network generally perform best \u2013 even when trained with object class (not style) labels. Our large-scale learning methods results in the best published performance on an existing dataset of aesthetic ratings and photographic style annotations. We present two novel datasets: 80K Flickr photographs annotated with 20 curated style labels, and 85K paintings annotated with 25 style/genre labels. Our approach shows excellent classification performance on both datasets. We use the learned classifiers to extend traditional tag-based image search to consider stylistic constraints, and demonstrate cross-dataset understanding of style.", "title": "Recognizing Image Style"}, "24984bb1848e04ce2d668a9a79cab7ab391271f9": {"paper_id": "24984bb1848e04ce2d668a9a79cab7ab391271f9", "abstract": "Many biological monitoring projects rely on acoustic detection of birds. Despite increasingly large datasets, this detection is often manual or semi-automatic, requiring manual tuning/postprocessing. We review the state of the art in automatic bird sound detection, and identify a widespread need for tuning-free and species-agnostic approaches. We introduce new datasets and an IEEE research challenge to address this need, to make possible the development of fully automatic algorithms for bird sound detection.", "title": "Bird detection in audio: A survey and a challenge"}, "385b401bf75771b02b5721641ae04ace43d2bbcd": {"paper_id": "385b401bf75771b02b5721641ae04ace43d2bbcd", "abstract": "Due to its low storage cost and fast query speed, hashing has been widely adopted for similarity search in multimedia data. In particular, more and more attentions have been payed to multimodal hashing for search in multimedia data with multiple modalities, such as images with tags. Typically, supervised information of semantic labels is also available for the data points in many real applications. Hence, many supervised multimodal hashing (SMH) methods have been proposed to utilize such semantic labels to further improve the search accuracy. However, the training time complexity of most existing SMH methods is too high, which makes them unscalable to large-scale datasets. In this paper, a novel SMH method, called semantic correlation maximization (SCM), is proposed to seamlessly integrate semantic labels into the hashing learning procedure for large-scale data modeling. Experimental results on two real-world datasets show that SCM can significantly outperform the state-of-the-art SMH methods, in terms of both accuracy and scalability.", "title": "Large-Scale Supervised Multimodal Hashing with Semantic Correlation Maximization"}, "ed9db7b20e019cdb1c7db8b7921221ee2d9f36e2": {"paper_id": "ed9db7b20e019cdb1c7db8b7921221ee2d9f36e2", "abstract": "Current computational approaches to learning visual object categories require thousands of training images, are slow, cannot learn in an incremental manner and cannot incorporate prior information into the learning process. In addition, no algorithm presented in the literature has been tested on more than a handful of object categories. We present an method for learning object categories from just a few training images. It is quick and it uses prior information in a principled way. We test it on a dataset composed of images of objects belonging to 101 widely varied categories. Our proposed method is based on making use of prior information, assembled from (unrelated) object categories which were previously learnt. A generative probabilistic model is used, which represents the shape and appearance of a constellation of features belonging to the object. The parameters of the model are learnt incrementally in a a c Purchase Export Previous article Next article Check if you have access through your login credentials or your institution.", "title": "Learning Generative Visual Models from Few Training Examples: An Incremental Bayesian Approach Tested on 101 Object Categories"}, "1379ad7fe27fa07419b7f6956af754bdb6d49558": {"paper_id": "1379ad7fe27fa07419b7f6956af754bdb6d49558", "abstract": "Semantic hashing[1] seeks compact binary codes of data-points so that the Hamming distance between codewords correlates with semantic similarity. In this paper, we show that the problem of finding a best code for a given dataset is closely related to the problem of graph partitioning and can be shown to be NP hard. By relaxing the original problem, we obtain a spectral method whose solutions are simply a subset of thresholded eigenvectors of the graph Laplacian. By utilizing recent results on convergence of graph Laplacian eigenvectors to the Laplace-Beltrami eigenfunctions of manifolds, we show how to efficiently calculate the code of a novel datapoint. Taken together, both learning the code and applying it to a novel point are extremely simple. Our experiments show that our codes outperform the state-of-the art.", "title": "Spectral Hashing"}, "5ff8c048a042d37a99a9f9c5acb0d82972a45ad0": {"paper_id": "5ff8c048a042d37a99a9f9c5acb0d82972a45ad0", "abstract": "We propose a method for learning similaritypreserving hash functions that map highdimensional data onto binary codes. The formulation is based on structured prediction with latent variables and a hinge-like loss function. It is efficient to train for large datasets, scales well to large code lengths, and outperforms state-of-the-art methods.", "title": "Minimal Loss Hashing for Compact Binary Codes"}, "7346545684d4554d11ac5997f89419bce8ccccfc": {"paper_id": "7346545684d4554d11ac5997f89419bce8ccccfc", "abstract": "Hashing based Approximate Nearest Neighbor (ANN) search has attracted much attention due to its fast query time and drastically reduced storage. However, most of the hashing methods either use random projections or extract principal directions from the data to derive hash functions. The resulting embedding suffers from poor discrimination when compact codes are used. In this paper, we propose a novel data-dependent projection learning method such that each hash function is designed to correct the errors made by the previous one sequentially. The proposed method easily adapts to both unsupervised and semi-supervised scenarios and shows significant performance gains over the state-ofthe-art methods on two large datasets containing up to 1 million points.", "title": "Sequential Projection Learning for Hashing with Compact Codes"}, "1d32f29a9880998264286633e66ea92915cc557a": {"paper_id": "1d32f29a9880998264286633e66ea92915cc557a", "abstract": "Similarity indices for high-dimensional data are very desirable for building content-based search systems for featurerich data such as audio, images, videos, and other sensor data. Recently, locality sensitive hashing (LSH) and its variations have been proposed as indexing techniques for approximate similarity search. A significant drawback of these approaches is the requirement for a large number of hash tables in order to achieve good search quality. This paper proposes a new indexing scheme called multi-probe LSH that overcomes this drawback. Multi-probe LSH is built on the well-known LSH technique, but it intelligently probes multiple buckets that are likely to contain query results in a hash table. Our method is inspired by and improves upon recent theoretical work on entropy-based LSH designed to reduce the space requirement of the basic LSH method. We have implemented the multi-probe LSH method and evaluated the implementation with two different high-dimensional datasets. Our evaluation shows that the multi-probe LSH method substantially improves upon previously proposed methods in both space and time efficiency. To achieve the same search quality, multi-probe LSH has a similar timeefficiency as the basic LSH method while reducing the number of hash tables by an order of magnitude. In comparison with the entropy-based LSH method, to achieve the same search quality, multi-probe LSH uses less query time and 5 to 8 times fewer number of hash tables.", "title": "Multi-Probe LSH: Efficient Indexing for High-Dimensional Similarity Search"}, "3d35fcd86ff0248fdcf6ff522e72e7ec3f733673": {"paper_id": "3d35fcd86ff0248fdcf6ff522e72e7ec3f733673", "abstract": "The Internet contains billions of images, freely available online. Methods for efficiently searching this incredibly rich resource are vital for a large number of applications. These include object recognition, computer graphics, personal photo collections, online image search tools. In this paper, our goal is to develop efficient image search and scene matching techniques that are not only fast, but also require very little memory, enabling their use on standard hardware or even on handheld devices. Our approach uses recently developed machine learning techniques to convert the Gist descriptor (a real valued vector that describes orientation energies at different scales and orientations within an image) to a compact binary code, with a few hundred bits per image. Using our scheme, it is possible to perform real-time searches with millions from the Internet using a single large PC and obtain recognition results comparable to the full descriptor. Using our codes on high quality labeled images from the LabelMe database gives surprisingly powerful recognition results using simple nearest neighbor techniques.", "title": "Small codes and large image databases for recognition"}, "a31e9d09d90261fc68acffe097df592cfdcb7706": {"paper_id": "a31e9d09d90261fc68acffe097df592cfdcb7706", "abstract": "We present a system for interactively browsing and exploring a large unstructured collection of photographs of a scene using a novel 3D interface. Our system consists of an image-based modeling front end, which automatically computes the viewpoint of each photograph as well as a sparse 3D model of the scene and image to model correspondences. Our photo navigation tool uses imagebased rendering techniques to smoothly transition between photographs, while also enabling full 3D navigation and exploration of the set of images and world geometry, along with auxiliary information such as overhead maps. Our system also makes it easy to construct photo tours of scenic or historic locations, as well as to annotate image details, which are automatically transferred to other relevant images in the collection. We demonstrate our system on several large personal photo collections as well as images gathered from photo sharing Web sites on the Internet.", "title": "Photo tourism: exploring photo collections in 3D"}, "3b8e7c8220d3883d54960d896a73045f3c70ac17": {"paper_id": "3b8e7c8220d3883d54960d896a73045f3c70ac17", "abstract": "We describe new algorithms for training tagging models, as an alternative to maximum-entropy models or conditional random elds (CRFs). The algorithms rely on Viterbi decoding of training examples, combined with simple additive updates. We describe theory justifying the algorithms through a modi cation of the proof of convergence of the perceptron algorithm for classi cation problems. We give experimental results on part-of-speech tagging and base noun phrase chunking, in both cases showing improvements over results for a maximum-entropy tagger.", "title": "Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms"}, "0224e11e8582dd35b32203e9da064d4a3935a792": {"paper_id": "0224e11e8582dd35b32203e9da064d4a3935a792", "abstract": "Example-basedmethodsareeffectivefor parameterestimationproblemswhentheunderlyingsystemis simpleor thedimensionalityof the input is low. For complex andhigh-dimensional problemssuch asposeestimation,thenumberof required examplesand the computationalcomplexity rapidly becmeprohibitivelyhigh. We introducea new algorithm that learnsa setof hashingfunctionsthat efficiently index examplesrelevant to a particular estimationtask. Our algorithm extendsa recentlydevelopedmethodfor locality-sensitivehashing, which findsapproximateneighborsin timesublinearin thenumber of examples.Thismethoddependscritically on thechoiceof hashfunctions;weshowhowto find thesetof hashfunctions thatare optimallyrelevantto a particular estimationproblem.Experimentsdemonstr atethat theresultingalgorithm,which wecall Parameter -SensitiveHashing, canrapidlyandaccuratelyestimatethearticulatedposeof humanfiguresfroma large databaseof exampleimages. 0Part of thiswork wasdonewhenG.S.andP.V. werewith MitsubishiElectricResearchLabs,Cambridge,MA.", "title": "Fast Pose Estimation with Parameter-Sensitive Hashing"}, "219101fe724232acc330ff0910152931538f85c7": {"paper_id": "219101fe724232acc330ff0910152931538f85c7", "abstract": "Consider a set of <italic>S</italic> of <italic>n</italic> data points  in real <italic>d</italic>-dimensional space, R<supscrpt>d</supscrpt>, where distances are measured using any Minkowski metric. In nearest neighbor searching, we preprocess <italic>S</italic> into a data structure, so that given any query point <italic>q</italic><inline-equation> <f>\u2208</f></inline-equation> R<supscrpt>d</supscrpt>, is the closest point of S to <italic>q</italic> can be reported quickly. Given any positive real \u03b5, data point <italic>p</italic> is a (1 +\u03b5)-<italic>approximate nearest neighbor</italic> of <italic>q</italic> if its distance from <italic>q</italic> is within a factor of (1 + \u03b5) of the distance to the true nearest neighbor. We show that it is possible to preprocess a    set of <italic>n</italic> points in     R<supscrpt>d</supscrpt> in <italic>O(dn</italic> log <italic>n</italic>) time and <italic>O(dn)</italic> space, so that given a query point <italic> q</italic> <inline-equation> <f>\u2208</f></inline-equation> R<supscrpt>d</supscrpt>, and \u03b5 > 0, a (1 + \u03b5)-approximate nearest neighbor of <italic>q</italic> can be computed in <italic>O</italic>(<italic>c</italic><subscrpt><italic>d</italic>, \u03b5</subscrpt> log <italic>n</italic>) time, where <italic>c<subscrpt>d,\u03b5</subscrpt></italic>\u2264<italic>d</italic> <inline-equation> <f><fen lp=\"ceil\">1 + 6d/<g>e</g><rp post=\"ceil\"></fen></f></inline-equation>;<supscrpt>d</supscrpt> is a factor depending only on dimension and \u03b5. In general, we show that given an integer <italic>k</italic> \u2265 1, (1 + \u03b5)-approximations  to the  <italic>k</italic> nearest neighbors of <italic>q</italic> can  be computed in additional <italic>O(kd</italic> log <italic>n</italic>) time.", "title": "An Optimal Algorithm for Approximate Nearest Neighbor Searching Fixed Dimensions"}, "27ae23bb8d284a1fa8c8ab24e23a72e1836ff5cc": {"paper_id": "27ae23bb8d284a1fa8c8ab24e23a72e1836ff5cc", "abstract": "(MATH) A locality sensitive hashing scheme is a distribution on a family $\\F$ of hash functions operating on a collection of objects, such that for two objects <i>x,y</i>, <b>Pr</b><sub><i>h</i></sub>\u03b5F[<i>h</i>(<i>x</i>) = <i>h</i>(<i>y</i>)] = sim(<i>x,y</i>), where <i>sim</i>(<i>x,y</i>) \u03b5 [0,1] is some similarity function defined on the collection of objects. Such a scheme leads to a compact representation of objects so that similarity of objects can be estimated from their compact sketches, and also leads to efficient algorithms for approximate nearest neighbor search and clustering. Min-wise independent permutations provide an elegant construction of such a locality sensitive hashing scheme for a collection of subsets with the set similarity measure <i>sim</i>(<i>A,B</i>) = \\frac{|A &Pgr; B|}{|A &Pgr B|}.(MATH) We show that rounding algorithms for LPs and SDPs used in the context of approximation algorithms can be viewed as locality sensitive hashing schemes for several interesting collections of objects. Based on this insight, we construct new locality sensitive hashing schemes for:<ol><li>A collection of vectors with the distance between \u2192 \\over <i>u</i> and \u2192 \\over <i>v</i> measured by \u00d8(\u2192 \\over <i>u</i>, \u2192 \\over <i>v</i>)/\u03c0, where \u00d8(\u2192 \\over <i>u</i>, \u2192 \\over <i>v</i>) is the angle between \u2192 \\over <i>u</i>) and \u2192 \\over <i>v</i>). This yields a sketching scheme for estimating the cosine similarity measure between two vectors, as well as a simple alternative to minwise independent permutations for estimating set similarity.</li><li>A collection of distributions on <i>n</i> points in a metric space, with distance between distributions measured by the Earth Mover Distance (<b>EMD</b>), (a popular distance measure in graphics and vision). Our hash functions map distributions to points in the metric space such that, for distributions <i>P</i> and <i>Q</i>, <b>EMD</b>(<i>P,Q</i>) &xie; <b>E</b><sub>h\u03b5\\F</sub> [<i>d</i>(<i>h</i>(<i>P</i>),<i>h</i>(<i>Q</i>))] &xie; <i>O</i>(log <i>n</i> log log <i>n</i>). <b>EMD</b>(<i>P, Q</i>).</li></ol>.", "title": "Similarity estimation techniques from rounding algorithms"}, "1955266a8a58d94e41ad0efe20d707c92a069e95": {"paper_id": "1955266a8a58d94e41ad0efe20d707c92a069e95", "abstract": "The nearest neighbor problem is the follolving: Given a set of n points P = (PI, . . . ,p,} in some metric space X, preprocess P so as to efficiently answer queries which require finding bhe point in P closest to a query point q E X. We focus on the particularly interesting case of the d-dimensional Euclidean space where X = Wd under some Zp norm. Despite decades of effort, t,he current solutions are far from saabisfactory; in fact, for large d, in theory or in practice, they provide litt,le improvement over the brute-force algorithm which compares the query point to each data point. Of late, t,here has been some interest in the approximate newest neighbors problem, which is: Find a point p E P that is an c-approximate nearest neighbor of the query q in t,hat for all p\u2019 E P, d(p, q) < (1 + e)d(p\u2019, q). We present two algorithmic results for the approximate version t,hat significantly improve the known bounds: (a) preprocessing cost polynomial in n and d, and a truly sublinear query t.ime (for 6 > 1); and, (b) query time polynomial in log-n and d, and only a mildly exponential preprocessing cost* O(n) x 0(1/~)~. Furt.her, applying a classical geometric lemma on random projections (for which we give a simpler proof), we obtain t.he first known algorithm with polynomial preprocessing and query t.ime polynomial in d and log n. Unfortunately, for small E, the latter is a purely theoretical result since bhe e?rponent depends on l/e. Experimental resuits indicate that our tit algori&m offers orders of magnitude improvement on running times over real data sets. Its key ingredient is the notion of locality-sensitive hashing which may be of independent interest; here, we give applications to information ret,rieval, pattern recognition, dynamic closest-pairs, and fast clustering algorithms. *Supported by a Stanford Graduate Fellowship and NSF", "title": "Approximate Nearest Neighbors: Towards Removing the Curse of Dimensionality"}, "b391878646123f5490ef2e2103de09a0947e4dc9": {"paper_id": "b391878646123f5490ef2e2103de09a0947e4dc9", "abstract": "A recognition scheme that scales efficiently to a large number of objects is presented. The efficiency and quality is exhibited in a live demonstration that recognizes CD-covers from a database of 40000 images of popular music CD\u2019s. The scheme builds upon popular techniques of indexing descriptors extracted from local regions, and is robust to background clutter and occlusion. The local region descriptors are hierarchically quantized in a vocabulary tree. The vocabulary tree allows a larger and more discriminatory vocabulary to be used efficiently, which we show experimentally leads to a dramatic improvement in retrieval quality. The most significant property of the scheme is that the tree directly defines the quantization. The quantization and the indexing are therefore fully integrated, essentially being one and the same. The recognition quality is evaluated through retrieval on a database with ground truth, showing the power of the vocabulary tree approach, going as high as 1 million images.", "title": "Scalable Recognition with a Vocabulary Tree"}, "4fa2b00f78b2a73b63ad014f3951ec902b8b24ae": {"paper_id": "4fa2b00f78b2a73b63ad014f3951ec902b8b24ae", "abstract": "Large scale image search has recently attracted considerable attention due to easy availability of huge amounts of data. Several hashing methods have been proposed to allow approximate but highly efficient search. Unsupervised hashing methods show good performance with metric distances but, in image search, semantic similarity is usually given in terms of labeled pairs of images. There exist supervised hashing methods that can handle such semantic similarity but they are prone to overfitting when labeled data is small or noisy. Moreover, these methods are usually very slow to train. In this work, we propose a semi-supervised hashing method that is formulated as minimizing empirical error on the labeled data while maximizing variance and independence of hash bits over the labeled and unlabeled data. The proposed method can handle both metric as well as semantic similarity. The experimental results on two large datasets (up to one million samples) demonstrate its superior performance over state-of-the-art supervised and unsupervised methods.", "title": "Semi-supervised hashing for scalable image retrieval"}, "4561bcb30237972dbcf808b6e30c30ba56fb2e70": {"paper_id": "4561bcb30237972dbcf808b6e30c30ba56fb2e70", "abstract": "The mobile robots often perform the dangerous missions such as planetary exploration, reconnaissance, anti-terrorism, rescue, and so on. So it is required that the robots should be able to move in the complex and unpredictable environment where the ground might be soft and hard, even and uneven. To access to such terrains, a novel robot (NEZA-I) with the self-adaptive mobile mechanism is proposed and developed. It consists of a control system unit and two symmetric transformable wheel-track (TWT) units. Each TWT unit is driven only by one servo motor, and can efficiently move over rough terrain by changing the locomotion mode and transforming the track configuration. It means that the mobile mechanism of NEZA-I has self-adaptability to the irregular environment. The paper proposes the design concept of NEZA-I, presents the structure and the drive system of NEZA-I, and describes the self-adaptive principle of the mobile mechanism to the rough terrains. The locomotion mode and posture of the mobile mechanism is analyzed by the means of simulation. Finally, basic experiments verify the mobility of NEZA-I.", "title": "Design and basic experiments of a transformable wheel-track robot with self-adaptive mobile mechanism"}, "da4ec25bd12cd18caa3cf36308d22c7f309ac1a7": {"paper_id": "da4ec25bd12cd18caa3cf36308d22c7f309ac1a7", "abstract": "Autonomous mobile robots have become a key technology for unmanned planetary missions. To cope with the rough terrain encountered on most of the planets of interest, new locomotion concepts for rovers and micro-rovers have to be developed and investigated. The most advanced locomotion concepts are based on wheels or caterpillars (e.g. Sojourner (NASA) or Nanokhod (ESA)). These rovers have clear advantages regarding power efficiency and complexity if compared with walking robots. However, they still have quite limited climbing abilities. Typically they can only overcome obstacle of their wheel size. In our paper we present an innovative rover concept with 6 motorized wheels. Using a rhombus configuration, the rover has a steering wheel in the front and the rear, and two wheels arranged on a bogie on each side. The front wheel has a spring suspension to guarantee optimal ground contact of all wheels at any time. The steering of the rover is realized by synchronizing the steering of the front and rear wheels and the speed difference of the bogie wheels. This allows for high precision maneuvers and even turning on the spot with minimum slip. The use of parallel articulations for the front wheel and the bogies enables to set a virtual center of rotation at the level of the wheel axis. This insures maximum stability and climbing abilities even for very low friction coefficients between the wheel and the ground. A well functioning prototype has been designed and manufactured in our lab. It shows excellent performance surpassing our expectations. This rover is able to passively overcome unstructured obstacles of up to two times its wheel diameter and can climb stairs with steps of over 20 cm.", "title": "An Innovative Space Rover with Extended Climbing Abilities"}, "d086c4c610521dc8fddab7b03a90d4cff4de5264": {"paper_id": "d086c4c610521dc8fddab7b03a90d4cff4de5264", "abstract": "Unmanned aerial vehicles (UAVs) have shown promise in recent years for autonomous sensing. UAVs systems have been proposed for a wide range of applications such as mapping, surveillance, search, and tracking operations. The recent availability of low-cost UAVs suggests the use of teams of vehicles to perform sensing tasks. To leverage the capabilities of a team of vehicles, efficient methods of decentralized sensing and cooperative path planning are necessary. The goal of this work is to examine practical control strategies for a team of fixed-wing vehicles performing cooperative sensing. We seek to develop decentralized, autonomous control strategies that can account for a wide variety of sensing missions. Sensing goals are posed from an information theoretic standpoint to design strategies that explicitly minimize uncertainty. This work proposes a tightly coupled approach, in which sensor models and estimation objectives are used online for path planning.", "title": "Autonomous UAV path planning and estimation"}, "a2eb1abd58e1554dc6bac5a8ea2f9876e9f4d36b": {"paper_id": "a2eb1abd58e1554dc6bac5a8ea2f9876e9f4d36b", "abstract": "OBJECTIVES\nIntergenerational contact has been linked to a range of health outcomes, including greater engagement and lower depression. Measures of contact are limited. Informed by Allport's contact theory, the Queen's University Scale consists of items rating contact with elders. We administered the survey to a young adult sample (N = 606) to identify factors that may optimize intervention programming and enhance young persons' health as they age.\n\n\nMETHODS\nWe conducted exploratory factor analysis (EFA) in the structural equation modeling framework and then confirmatory factor analysis with items pertaining to the general elder population.\n\n\nRESULTS\nEFAs did not yield an adequate factor structure. We tested two alternative confirmatory models based on findings from the EFA. Neither a second-order model nor a first-order model allowing double loadings and correlated errors proved adequate.\n\n\nCONCLUSION\nDifficulty finding an adequate factor solution reflects challenges to measuring intergenerational contact with this scale. Items reflect relevant topics but subscale models are limited in interpretability. Knox and colleagues' analyses led them to recommend a brief, global scale, but we did not find empirical support for such a measure. Next steps include development and testing of a reliable, valid scale measuring dimensions of contact as perceived by both youth and elders.", "title": "Measuring dimensions of intergenerational contact: factor analysis of the Queen's University Scale."}, "1f1940fa524e2a762fbd67fe09ea1acec05470eb": {"paper_id": "1f1940fa524e2a762fbd67fe09ea1acec05470eb", "abstract": "While deep learning models have achieved state-of-the-art accuracies for many prediction tasks, understanding these models remains a challenge. Despite the recent interest in developing visual tools to help users interpret deep learning models, the complexity and wide variety of models deployed in industry, and the large-scale datasets that they used, pose unique design challenges that are inadequately addressed by existing work. Through participatory design sessions with over 15 researchers and engineers at Facebook, we have developed, deployed, and iteratively improved ActiVis, an interactive visualization system for interpreting large-scale deep learning models and results. By tightly integrating multiple coordinated views, such as a computation graph overview of the model architecture, and a neuron activation view for pattern discovery and comparison, users can explore complex deep neural network models at both the instance-and subset-level. ActiVis has been deployed on Facebook's machine learning platform. We present case studies with Facebook researchers and engineers, and usage scenarios of how ActiVis may work with different models.", "title": "ActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models"}, "9843bc3c8e5b97c3011704792c86ba94a3ed7c94": {"paper_id": "9843bc3c8e5b97c3011704792c86ba94a3ed7c94", "abstract": "Serial periodic data exhibit both serial and periodic properties. For example, time continues forward serially, but weeks, months, and years are periods that recur. While there are extensive visualization techniques for exploring serial data, and a few for exploring periodic data, no existing technique simultaneously displays serial and periodic attributes of a data set. We introduce a spiral visualization technique, which displays data along a spiral to highlight serial attributes along the spiral axis and periodic ones along the radii. We show several applications of the spiral visualization to data exploration tasks, present our implementation, discuss the capacity for data analysis, and present findings of our informal study with users in data-rich scientific domains.", "title": "Interactive Visualization of Serial Periodic Data"}, "162e0c4df8bbd9381fcd3279492ebdb75a42de98": {"paper_id": "162e0c4df8bbd9381fcd3279492ebdb75a42de98", "abstract": "Finding patterns of events over time is important in searching patient histories, Web logs, news stories, and criminal activities. This paper presents PatternFinder, an integrated interface for query and result-set visualization for search and discovery of temporal patterns within multivariate and categorical data sets. We define temporal patterns as sequences of events with inter-event time spans. PatternFinder allows users to specify the attributes of events and time spans to produce powerful pattern queries that are difficult to express with other formalisms. We characterize the range of queries PatternFinder supports as users vary the specificity at which events and time spans are defined. Pattern Finder's query capabilities together with coupled ball-and-chain and tabular visualizations enable users to effectively query, explore and analyze event patterns both within and across data entities (e.g. patient histories, terrorist groups, Web logs, etc.)", "title": "A Visual Interface for Multivariate Temporal Data: Finding Patterns of Events across Multiple Histories"}, "31c99f6ffb90ea9b5d5c9d0c9ba1b2c10ec01b5b": {"paper_id": "31c99f6ffb90ea9b5d5c9d0c9ba1b2c10ec01b5b", "abstract": "Extracting insights from temporal event sequences is an important challenge. In particular, mining frequent patterns from event sequences is a desired capability for many domains. However, most techniques for mining frequent patterns are ineffective for real-world data that may be low-resolution, concurrent, or feature many types of events, or the algorithms may produce results too complex to interpret. To address these challenges, we propose Frequence, an intelligent user interface that integrates data mining and visualization in an interactive hierarchical information exploration system for finding frequent patterns from longitudinal event sequences. Frequence features a novel frequent sequence mining algorithm to handle multiple levels-of-detail, temporal context, concurrency, and outcome analysis. Frequence also features a visual interface designed to support insights, and support exploration of patterns of the level-of-detail relevant to users. Frequence's effectiveness is demonstrated with two use cases: medical research mining event sequences from clinical records to understand the progression of a disease, and social network research using frequent sequences from Foursquare to understand the mobility of people in an urban environment.", "title": "Frequence: interactive mining and visualization of temporal frequent event sequences"}, "188587e5d1cbf57ba5115cf0f2fe560f129c8007": {"paper_id": "188587e5d1cbf57ba5115cf0f2fe560f129c8007", "abstract": "Event sequence analysis is an important task in many domains: medical researchers may study the patterns of transfers within the hospital for quality control; transportation experts may study accident response logs to identify best practices. In many cases they deal with thousands of records. While previous research has focused on searching and browsing, overview tasks are often overlooked. We introduce a novel interactive visual overview of event sequences called \\emph{LifeFlow}. LifeFlow is scalable, can summarize all possible sequences, and represents the temporal spacing of the events within sequences. Two case studies with healthcare and transportation domain experts are presented to illustrate the usefulness of LifeFlow. A user study with ten participants confirmed that after 15 minutes of training novice users were able to rapidly answer questions about the prevalence and temporal characteristics of sequences, find anomalies, and gain significant insight from the data.", "title": "LifeFlow: visualizing an overview of event sequences"}, "0e7441a8138bdcfcc15dfb0e0f809ab280871bce": {"paper_id": "0e7441a8138bdcfcc15dfb0e0f809ab280871bce", "abstract": "Location sharing services (LSS) like Foursquare, Gowalla, and Facebook Places support hundreds of millions of userdriven footprints (i.e., \u201ccheckins\u201d). Those global-scale footprints provide a unique opportunity to study the social and temporal characteristics of how people use these services and to model patterns of human mobility, which are significant factors for the design of future mobile+location-based services, traffic forecasting, urban planning, as well as epidemiological models of disease spread. In this paper, we investigate 22 million checkins across 220,000 users and report a quantitative assessment of human mobility patterns by analyzing the spatial, temporal, social, and textual aspects associated with these footprints. We find that: (i) LSS users follow the \u201cL\u00e8vy Flight\u201d mobility pattern and adopt periodic behaviors; (ii) While geographic and economic constraints affect mobility patterns, so does individual social status; and (iii) Content and sentiment-based analysis of posts associated with checkins can provide a rich source of context for better understanding how users engage with these services.", "title": "Exploring Millions of Footprints in Location Sharing Services"}, "8478c0f46dd30ef7f4052145983d6d315c2e1f17": {"paper_id": "8478c0f46dd30ef7f4052145983d6d315c2e1f17", "abstract": "This paper presents a series of new latent semantic models based on a convolutional neural network (CNN) to learn low-dimensional semantic vectors for search queries and Web documents. By using the convolution-max pooling operation, local contextual information at the word n-gram level is modeled first. Then, salient local fea-tures in a word sequence are combined to form a global feature vector. Finally, the high-level semantic information of the word sequence is extracted to form a global vector representation. The proposed models are trained on clickthrough data by maximizing the conditional likelihood of clicked documents given a query, us-ing stochastic gradient ascent. The new models are evaluated on a Web document ranking task using a large-scale, real-world data set. Results show that our model significantly outperforms other se-mantic models, which were state-of-the-art in retrieval performance prior to this work.", "title": "Learning semantic representations using convolutional neural networks for web search"}, "127316fbe268c78c519ceb23d41100e86639418a": {"paper_id": "127316fbe268c78c519ceb23d41100e86639418a", "abstract": "Recent results indicate that the generic descriptors extracted from the convolutional neural networks are very powerful. This paper adds to the mounting evidence that this is indeed the case. We report on a series of experiments conducted for different recognition tasks using the publicly available code and model of the OverFeat network which was trained to perform object classification on ILSVRC13. We use features extracted from the OverFeat network as a generic image representation to tackle the diverse range of recognition tasks of object image classification, scene recognition, fine grained recognition, attribute detection and image retrieval applied to a diverse set of datasets. We selected these tasks and datasets as they gradually move further away from the original task and data the OverFeat network was trained to solve. Astonishingly, we report consistent superior results compared to the highly tuned state-of-the-art systems in all the visual classification tasks on various datasets. For instance retrieval it consistently outperforms low memory footprint methods except for sculptures dataset. The results are achieved using a linear SVM classifier (or L2 distance in case of retrieval) applied to a feature representation of size 4096 extracted from a layer in the net. The representations are further modified using simple augmentation techniques e.g. jittering. The results strongly suggest that features obtained from deep learning with convolutional nets should be the primary candidate in most visual recognition tasks.", "title": "CNN Features Off-the-Shelf: An Astounding Baseline for Recognition"}, "bdc6acc8d11b9ef1e8f0fe2f0f41ce7b6f6a100a": {"paper_id": "bdc6acc8d11b9ef1e8f0fe2f0f41ce7b6f6a100a", "abstract": "Traditional text similarity measures consider each term similar only to itself and do not model semantic relatedness of terms. We propose a novel discriminative training method that projects the raw term vectors into a common, low-dimensional vector space. Our approach operates by finding the optimal matrix to minimize the loss of the pre-selected similarity function (e.g., cosine) of the projected vectors, and is able to efficiently handle a large number of training examples in the highdimensional space. Evaluated on two very different tasks, cross-lingual document retrieval and ad relevance measure, our method not only outperforms existing state-of-the-art approaches, but also achieves high accuracy at low dimensions and is thus more efficient.", "title": "Learning Discriminative Projections for Text Similarity Measures"}, "c829b63a3ae72a47e1953e1295826c7b2f93bf50": {"paper_id": "c829b63a3ae72a47e1953e1295826c7b2f93bf50", "abstract": "The recently introduced continuous Skip-gram model is an ef fici nt method for learning high-quality distributed vector representation s that capture a large number of precise syntactic and semantic word relationships. I n this paper we present several extensions that improve both the quality of the vect ors and the training speed. By subsampling of the frequent words we obtain signifi ca t speedup and also learn more regular word representations. We also descr ib a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their ind ifference to word order and their inability to represent idiomatic phrases. For exa mple, the meanings of \u201cCanada\u201d and \u201cAir\u201d cannot be easily combined to obtain \u201cAir C anada\u201d. Motivated by this example, we present a simple method for finding phrase s in t xt, and show that learning good vector representations for millions of p hrases is possible.", "title": "Distributed Representations of Words and Phrases and their Compositionality"}, "2116b2eaaece4af9c28c32af2728f3d49b792cf9": {"paper_id": "2116b2eaaece4af9c28c32af2728f3d49b792cf9", "abstract": "When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This overfitting is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random \u201cdropout\u201d gives big improvements on many benchmark tasks and sets new records for speech and object recognition. A feedforward, artificial neural network uses layers of non-linear hidden units between its inputs and its outputs. By adapting the weights on the incoming connections of these hidden units it learns feature detectors that enable it to predict the correct output when given an input vector [15]. If the relationship between the input and the correct output is complicated and the network has enough hidden units to model it accurately, there will typically be many different settings of the weights that can model the training set almost perfectly, especially if there is only a limited amount of labeled training data. Each of these weight vectors will make different predictions on held-out test data and almost all of them will do worse on the test data than on the training data because the feature detectors have been tuned to work well together on the training data but not on the test data. Overfitting can be reduced by using \u201cdropout\u201d to prevent complex co-adaptations on the training data. On each presentation of each training case, each hidden unit is randomly omitted from the network with a probability of 0.5, so a hidden unit cannot rely on other hidden units being present. Another way to view the dropout procedure is as a very efficient way of performing model averaging with neural networks. A good way to reduce the error on the test set is to average the predictions produced by a very large number of different networks. The standard way to do this is to train many separate networks and then to apply each of these networks to the test data, but this is computationally expensive during both training and testing. Random dropout makes it possible to train a huge number of different networks in a reasonable time. There is almost certainly a different network for each presentation of each training case but all of these networks share the same weights for the hidden units that are present. We use the standard, stochastic gradient descent procedure for training the dropout neural networks on mini-batches of training cases, but we modify the penalty term that is normally used to prevent the weights from growing too large. Instead of penalizing the squared length (L2 norm) of the whole weight vector, we set an upper bound on the L2 norm of the incoming weight vector for each individual hidden unit. If a weight-update violates this constraint, we renormalize the weights of the hidden unit by division. Using a constraint rather than a penalty prevents weights from growing very large no matter how large the proposed weight-update is. This makes it possible to start with a", "title": "Improving neural networks by preventing co-adaptation of feature detectors"}, "13e5f0c40c85ca8e01b3756963d5352358de7c29": {"paper_id": "13e5f0c40c85ca8e01b3756963d5352358de7c29", "abstract": "Merchants selling products on the Web often ask their customers to review the products that they have purchased and the associated services. As e-commerce is becoming more and more popular, the number of customer reviews that a product receives grows rapidly. For a popular product, the number of reviews can be in hundreds or even thousands. This makes it difficult for a potential customer to read them to make an informed decision on whether to purchase the product. It also makes it difficult for the manufacturer of the product to keep track and to manage customer opinions. For the manufacturer, there are additional difficulties because many merchant sites may sell the same product and the manufacturer normally produces many kinds of products. In this research, we aim to mine and to summarize all the customer reviews of a product. This summarization task is different from traditional text summarization because we only mine the features of the product on which the customers have expressed their opinions and whether the opinions are positive or negative. We do not summarize the reviews by selecting a subset or rewrite some of the original sentences from the reviews to capture the main points as in the classic text summarization. Our task is performed in three steps: (1) mining product features that have been commented on by customers; (2) identifying opinion sentences in each review and deciding whether each opinion sentence is positive or negative; (3) summarizing the results. This paper proposes several novel techniques to perform these tasks. Our experimental results using reviews of a number of products sold online demonstrate the effectiveness of the techniques.", "title": "Mining and summarizing customer reviews"}, "1510cf4b8abea80b9f352325ca4c132887de21a0": {"paper_id": "1510cf4b8abea80b9f352325ca4c132887de21a0", "abstract": "Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, \u201cpowerful,\u201d \u201cstrong\u201d and \u201cParis\u201d are equally distant. In this paper, we proposeParagraph Vector , an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-ofwords models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.", "title": "Distributed Representations of Sentences and Documents"}, "126df9f24e29feee6e49e135da102fbbd9154a48": {"paper_id": "126df9f24e29feee6e49e135da102fbbd9154a48", "abstract": "In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning & evolutionary computation, and indirect search for short programs encoding deep and large networks.", "title": "Deep learning in neural networks: An overview"}, "2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc": {"paper_id": "2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc", "abstract": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.", "title": "ImageNet Classification with Deep Convolutional Neural Networks"}, "38f35dd624cd1cf827416e31ac5e0e0454028eca": {"paper_id": "38f35dd624cd1cf827416e31ac5e0e0454028eca", "abstract": "We introduce DropConnect, a generalization of Dropout (Hinton et al., 2012), for regularizing large fully-connected layers within neural networks. When training with Dropout, a randomly selected subset of activations are set to zero within each layer. DropConnect instead sets a randomly selected subset of weights within the network to zero. Each unit thus receives input from a random subset of units in the previous layer. We derive a bound on the generalization performance of both Dropout and DropConnect. We then evaluate DropConnect on a range of datasets, comparing to Dropout, and show state-of-the-art results on several image recognition benchmarks by aggregating multiple DropConnect-trained models.", "title": "Regularization of Neural Networks using DropConnect"}, "5d90f06bb70a0a3dced62413346235c02b1aa086": {"paper_id": "5d90f06bb70a0a3dced62413346235c02b1aa086", "abstract": "Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it dicult to learn a good set of lters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels. The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of each of 100 non-overlapping classes. Using these labels, we show that object recognition is signicantly improved by pre-training a layer of features on a large set of unlabeled tiny images.", "title": "Learning Multiple Layers of Features from Tiny Images"}, "1314e6ea34a8d749ca6190a0d2dd00b3a1879cc6": {"paper_id": "1314e6ea34a8d749ca6190a0d2dd00b3a1879cc6", "abstract": "A general nonparametric technique is proposed for the analysis of a complex multimodal feature space and to delineate arbitrarily shaped clusters in it. The basic computational module of the technique is an old pattern recognition procedure, the mean shift. We prove for discrete data the convergence of a recursive mean shift procedure to the nearest stationary point of the underlying density function and thus its utility in detecting the modes of the density. The equivalence of the mean shift procedure to the Nadaraya\u2013Watson estimator from kernel regression and the robust M-estimators of location is also established. Algorithms for two low-level vision tasks, discontinuity preserving smoothing and image segmentation are described as applications. In these algorithms the only user set parameter is the resolution of the analysis, and either gray level or color images are accepted as input. Extensive experimental results illustrate their excellent performance.", "title": "Mean Shift: A Robust Approach Toward Feature Space Analysis"}, "15cf63f8d44179423b4100531db4bb84245aa6f1": {"paper_id": "15cf63f8d44179423b4100531db4bb84245aa6f1", "abstract": "Machine-learning technology powers many aspects of modern society: from web searches to content filtering on social networks to recommendations on e-commerce websites, and it is increasingly present in consumer products such as cameras and smartphones. Machine-learning systems are used to identify objects in images, transcribe speech into text, match news items, posts or products with users\u2019 interests, and select relevant results of search. Increasingly, these applications make use of a class of techniques called deep learning. Conventional machine-learning techniques were limited in their ability to process natural data in their raw form. For decades, constructing a pattern-recognition or machine-learning system required careful engineering and considerable domain expertise to design a feature extractor that transformed the raw data (such as the pixel values of an image) into a suitable internal representation or feature vector from which the learning subsystem, often a classifier, could detect or classify patterns in the input. Representation learning is a set of methods that allows a machine to be fed with raw data and to automatically discover the representations needed for detection or classification. Deep-learning methods are representation-learning methods with multiple levels of representation, obtained by composing simple but non-linear modules that each transform the representation at one level (starting with the raw input) into a representation at a higher, slightly more abstract level. With the composition of enough such transformations, very complex functions can be learned. For classification tasks, higher layers of representation amplify aspects of the input that are important for discrimination and suppress irrelevant variations. An image, for example, comes in the form of an array of pixel values, and the learned features in the first layer of representation typically represent the presence or absence of edges at particular orientations and locations in the image. The second layer typically detects motifs by spotting particular arrangements of edges, regardless of small variations in the edge positions. The third layer may assemble motifs into larger combinations that correspond to parts of familiar objects, and subsequent layers would detect objects as combinations of these parts. The key aspect of deep learning is that these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure. Deep learning is making major advances in solving problems that have resisted the best attempts of the artificial intelligence community for many years. It has turned out to be very good at discovering intricate structures in high-dimensional data and is therefore applicable to many domains of science, business and government. In addition to beating records in image recognition and speech recognition, it has beaten other machine-learning techniques at predicting the activity of potential drug molecules, analysing particle accelerator data, reconstructing brain circuits, and predicting the effects of mutations in non-coding DNA on gene expression and disease. Perhaps more surprisingly, deep learning has produced extremely promising results for various tasks in natural language understanding, particularly topic classification, sentiment analysis, question answering and language translation. We think that deep learning will have many more successes in the near future because it requires very little engineering by hand, so it can easily take advantage of increases in the amount of available computation and data. New learning algorithms and architectures that are currently being developed for deep neural networks will only accelerate this progress.", "title": "Deep Learning"}, "340f48901f72278f6bf78a04ee5b01df208cc508": {"paper_id": "340f48901f72278f6bf78a04ee5b01df208cc508", "abstract": "The theory of reinforcement learning provides a normative account, deeply rooted in psychological and neuroscientific perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms. While reinforcement learning agents have achieved some successes in a variety of domains, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.", "title": "Human-level control through deep reinforcement learning"}, "5288d14f6a3937df5e10109d4e23d79b7ddf080f": {"paper_id": "5288d14f6a3937df5e10109d4e23d79b7ddf080f", "abstract": null, "title": "Fast Algorithms for Mining Association Rules in Large Databases"}, "573edc20ae85e708f3570113723ab443e98d3c56": {"paper_id": "573edc20ae85e708f3570113723ab443e98d3c56", "abstract": "A novel method is proposed for performing multilabel, interactive image segmentation. Given a small number of pixels with user-defined (or predefined) labels, one can analytically and quickly determine the probability that a random walker starting at each unlabeled pixel will first reach one of the prelabeled pixels. By assigning each pixel to the label for which the greatest probability is calculated, a high-quality image segmentation may be obtained. Theoretical properties of this algorithm are developed along with the corresponding connections to discrete potential theory and electrical circuits. This algorithm is formulated in discrete space (i.e., on a graph) using combinatorial analogues of standard operators and principles from continuous potential theory, allowing it to be applied in arbitrary dimension on arbitrary graphs", "title": "Random Walks for Image Segmentation"}, "ca08de0b39976f5ef1ff0fbc29efdf2763667b22": {"paper_id": "ca08de0b39976f5ef1ff0fbc29efdf2763667b22", "abstract": "In recent years, interest in nonlinear dimensionality reduction has grown. This has led to the proposal of various new nonlinear techniques that are claimed to be capable of dealing with complex low-dimensional data. These techniques have been shown to outperform traditional linear techniques on artificial tasks such as the Swiss roll task. Hitherto, the dimensionality reduction techniques have not been compared in a systematic way. In this paper, we discuss and compare ten nonlinear techniques for dimensionality reduction. We investigate the performance of the nonlinear techniques for dimensionality reduction on artificial and natural tasks, and compare their performances to those of the two principal linear techniques: (1) Principal Components Analysis, and (2) Linear Discriminant Analysis. The experiments reveal that nonlinear techniques for dimensionality reduction perform well on selected artificial tasks, but do not outperform the linear techniques on many real-world tasks. The paper concludes that the results suggest that despite their theoretical capability to find complex low-dimensional embeddings, nonlinear techniques for dimensionality reduction are not yet capable of outperforming traditional linear techniques on real-world tasks. We foresee that the performance of nonlinear techniques for dimensionality reduction will be improved by the development of new techniques that represent the global structure of manifolds by a number of separate linear models.", "title": "Dimensionality Reduction: A Comparative Review"}, "3eaca435527a765572ed04acd06ba512faac89eb": {"paper_id": "3eaca435527a765572ed04acd06ba512faac89eb", "abstract": "Drawing on the correspondence between the graph Laplacian the Laplace Beltrami operator on a manifold and the connections to the heat equation we propose a geometrically motivated algorithm for constructing a representation for data sampled from a low di mensional manifold embedded in a higher dimensional space The algorithm provides a computationally e cient approach to non linear dimensionality reduction that has locality preserving prop erties and a natural connection to clustering Several applications are considered In many areas of arti cial intelligence information retrieval and data mining one is often confronted with intrinsically low dimensional data lying in a very high di mensional space For example gray scale n n images of a xed object taken with a moving camera yield data points in R However the intrinsic dimensionality of the space of all images of the same object is the number of degrees of freedom of the camera in fact the space has the natural structure of a manifold embedded in R n While there is a large body of work on dimensionality reduction in general most existing approaches do not explicitly take into account the structure of the manifold on which the data may possibly reside Recently there has been some interest Tenenbaum et al Roweis and Saul in the problem of devel oping low dimensional representations of data in this particular context In this paper we present a new algorithm and an accompanying framework of analysis for geometrically motivated dimensionality reduction The core algorithm is very simple has a few local computations and one sparse eigenvalue problem The solution re ects the intrinsic geometric structure of the manifold The justi cation comes from the role of the Laplacian operator in pro viding an optimal embedding The Laplacian of the graph obtained from the data points may be viewed as an approximation to the Laplace Beltrami operator de ned on the manifold The embedding maps for the data come from approximations to a natural map that is de ned on the entire manifold The framework of analysis presented here makes this connection explicit While this connection is known to geometers and specialists in spectral graph theory for example see to the best of our knowledge we do not know of any application to data representation yet The connection of the Laplacian to the heat kernel enables us to choose the weights of the graph in a principled manner The locality preserving character of the Laplacian Eigenmap algorithmmakes it rel atively insensitive to outliers and noise A byproduct of this is that the algorithm implicitly emphasizes the natural clusters in the data Connections to spectral clus tering algorithms developed in learning and computer vision see Shi and Malik become very clear Following the discussion of Roweis and Saul and Tenenbaum et al we note that the biological perceptual apparatus is con fronted with high dimensional stimuli from which it must recover low dimensional structure One might argue that if the approach to recovering such low dimensional structure is inherently local then a natural clustering will emerge and thus might serve as the basis for the development of categories in biological perception The Algorithm Given k points x xk in R we construct a weighted graph with k nodes one for each point and the set of edges connecting neighboring points to each other Step Constructing the Graph We put an edge between nodes i and j if xi and xj are close There are two variations a neighborhoods parameter R Nodes i and j are connected by an edge if jjxi xj jj Advantages geometrically motivated the relationship is naturally symmetric Disadvantages often leads to graphs with several connected compo nents di cult to choose b n nearest neighbors parameter n N Nodes i and j are connected by an edge if i is among n nearest neighbors of j or j is among n nearest neighbors of i Advantages simpler to choose tends to lead to connected graphs Disadvantages less geometrically intuitive Step Choosing the weights Here as well we have two variations for weighting the edges a Heat kernel parameter t R If nodes i and j are connected put Wij e jjxi j jj t The justi cation for this choice of weights will be provided later b Simple minded No parameters Wij if and only if vertices i and j are connected by an edge A simpli cation which avoids the necessity of choosing t Step Eigenmaps Assume the graph G constructed above is connected otherwise proceed with Step for each connected component 0 20 40 0 10 20 30 40 \u22125 0 5 x 10 \u22123 \u22128 \u22126 \u22124 \u22122 0 2 4 6 8 x 10 \u22123", "title": "Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering"}, "53ea5aa438e041820d2fd9413d2b3aaf87a95212": {"paper_id": "53ea5aa438e041820d2fd9413d2b3aaf87a95212", "abstract": "In this work, we propose a novel decoding approach for neural machine translation (NMT) based on continuous optimisation. The resulting optimisation problem can then be tackled using a whole range of continuous optimisation algorithms which have been developed and used in the literature mainly for training. Our approach is general and can be applied to other sequence-to-sequence neural models as well. We make use of this powerful decoding approach to intersect an underlying NMT with a language model, to intersect left-to-right and right-to-left NMT models, and to decode with soft constraints involving coverage and fertility of the source sentence words. The experimental results show the promise of the proposed framework.", "title": "Decoding as Continuous Optimization in Neural Machine Translation"}, "197a7fc2f8d57d93727b348851b59b34ce990afd": {"paper_id": "197a7fc2f8d57d93727b348851b59b34ce990afd", "abstract": "SRILM is a collection of C++ libraries, executable programs, and helper scripts designed to allow both production of and experimentation with statistical language models for speech recognition and other applications. SRILM is freely available for noncommercial purposes. The toolkit supports creation and evaluation of a variety of language model types based on N-gram statistics, as well as several related tasks, such as statistical tagging and manipulation of N-best lists and word lattices. This paper summarizes the functionality of the toolkit and discusses its design and implementation, highlighting ease of rapid prototyping, reusability, and combinability of tools.", "title": "SRILM - an extensible language modeling toolkit"}, "4b7a0ba426690b08489a86038db161846ffcfaa9": {"paper_id": "4b7a0ba426690b08489a86038db161846ffcfaa9", "abstract": "This paper provides guidance to some of the concepts surrounding recurrent neural networks. Contrary to feedforward networks, recurrent networks can be sensitive, and be adapted to past inputs. Backpropagation learning is described for feedforward networks, adapted to suit our (probabilistic) modeling needs, and extended to cover recurrent networks. The aim of this brief paper is to set the scene for applying and understanding recurrent neural networks.", "title": "A guide to recurrent neural networks and backpropagation"}, "07c43a3ff15f2104022f2b1ca8ec4128a930b414": {"paper_id": "07c43a3ff15f2104022f2b1ca8ec4128a930b414", "abstract": "We investigate the problem of modeling symbolic sequences of polyphonic music in a completely general piano-roll representation. We introduce a probabilistic model based on distribution estimators conditioned on a recurrent neural network that is able to discover temporal dependencies in high-dimensional sequences. Our approach outperforms many traditional models of polyphonic music on a variety of realistic datasets. We show how our musical language model can serve as a symbolic prior to improve the accuracy of polyphonic transcription.", "title": "Modeling Temporal Dependencies in High-Dimensional Sequences: Application to Polyphonic Music Generation and Transcription"}, "0b44fcbeea9415d400c5f5789d6b892b6f98daff": {"paper_id": "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "abstract": "In this paper, we review our experience with constructing one such large annotated corpus--the Penn Treebank, a corpus consisting of over 4.5 million words of American English. During the first three-year phase of the Penn Treebank Project (1989-1992), this corpus has been annotated for part-of-speech (POS) information. In addition, over half of it has been annotated for skeletal syntactic structure. Comments University of Pennsylvania Department of Computer and Information Science Technical Report No. MSCIS-93-87. This technical report is available at ScholarlyCommons: http://repository.upenn.edu/cis_reports/237 Building A Large Annotated Corpus of English: The Penn Treebank MS-CIS-93-87 LINC LAB 260 Mitchell P. Marcus Beatrice Santorini Mary Ann Marcinkiewicz University of Pennsylvania School of Engineering and Applied Science Computer and Information Science Department Philadelphia, PA 19104-6389", "title": "Building a Large Annotated Corpus of English: The Penn Treebank"}, "44c977c18752d8913746efc7ea8635b0e4be4e47": {"paper_id": "44c977c18752d8913746efc7ea8635b0e4be4e47", "abstract": null, "title": "A Practical Guide to Training Restricted Boltzmann Machines"}, "c08d0525bd42fa1c24f9f5df72f4c8fcf7063b22": {"paper_id": "c08d0525bd42fa1c24f9f5df72f4c8fcf7063b22", "abstract": "Offline handwriting recognition\u2014the automatic transcription of images of handwritten text\u2014is a challenging task that combines computer vision with sequence learning. In most systems the two elements are handled separately, with sophisticated preprocessing techniques used to extract the image features and sequential models such as HMMs used to provide the transcriptions. By combining two recent innovations in neural networks\u2014multidimensional recurrent neural networks and connectionist temporal classification\u2014this paper introduces a globally trained offline handwriting recogniser that takes raw pixel data as input. Unlike competing systems, it does not require any alphabet specific preprocessing, and can therefore be used unchanged for any language. Evidence of its generality and power is provided by data from a recent international Arabic recognition competition, where it outperformed all entries (91.4% accuracy compared to 87.2% for the competition winner) despite the fact that neither author understands a word of Arabic.", "title": "Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks"}, "10dae7fca6b65b61d155a622f0c6ca2bc3922251": {"paper_id": "10dae7fca6b65b61d155a622f0c6ca2bc3922251", "abstract": null, "title": "Gradient-based learning algorithms for recurrent networks and their computational complexity"}, "c31019e3fb8a3ffacb22da5744f560294486c257": {"paper_id": "c31019e3fb8a3ffacb22da5744f560294486c257", "abstract": "In this paper, we improve the attention or alignment accuracy of neural machine translation by utilizing the alignments of training sentence pairs. We simply compute the distance between the machine attentions and the \u201ctrue\u201d alignments, and minimize this cost in the training procedure. Our experiments on large-scale Chinese-to-English task show that our model improves both translation and alignment qualities significantly over the large-vocabulary neural machine translation system, and even beats a state-of-the-art traditional syntax-based system.", "title": "Supervised Attentions for Neural Machine Translation"}, "0d42801cda0e6c80a6bcaf31efe6cf853fa052d0": {"paper_id": "0d42801cda0e6c80a6bcaf31efe6cf853fa052d0", "abstract": "Neural Machine translation has shown promising results in recent years. In order to control the computational complexity, NMT has to employ a small vocabulary, and massive rare words outside the vocabulary are all replaced with a single unk symbol. Besides the inability to translate rare words, this kind of simple approach leads to much increased ambiguity of the sentences since meaningless unks break the structure of sentences, and thus hurts the translation and reordering of the in-vocabulary words. To tackle this problem, we propose a novel substitution-translation-restoration method. In substitution step, the rare words in a testing sentence are replaced with similar in-vocabulary words based on a similarity model learnt from monolingual data. In translation and restoration steps, the sentence will be translated with a model trained on new bilingual data with rare words replaced, and finally the translations of the replaced words will be substituted by that of original ones. Experiments on Chinese-to-English translation demonstrate that our proposed method can achieve more than 4 BLEU points over the attention-based NMT. When compared to the recently proposed method handling rare words in NMT, our method can also obtain an improvement by nearly 3 BLEU points.", "title": "Towards Zero Unknown Word in Neural Machine Translation"}, "592a6d781309423ceb95502e92e577ef5656de0d": {"paper_id": "592a6d781309423ceb95502e92e577ef5656de0d", "abstract": "Neural encoder-decoder models of machine translation have achieved impressive results, rivalling traditional translation models. However their modelling formulation is overly simplistic, and omits several key inductive biases built into traditional models. In this paper we extend the attentional neural translation model to include structural biases from word based alignment models, including positional bias, Markov conditioning, fertility and agreement over translation directions. We show improvements over a baseline attentional model and standard phrase-based model over several language pairs, evaluating on difficult languages in a low resource setting.", "title": "Incorporating Structural Alignment Biases into an Attentional Neural Translation Model"}, "18422ae57ec0318fb4c200fdcf7c6e145208d792": {"paper_id": "18422ae57ec0318fb4c200fdcf7c6e145208d792", "abstract": "Modeling the target appearance is critical in many modern visual tracking algorithms. Many tracking-by-detection algorithms formulate the probability of target appearance as exponentially related to the confidence of a classifier output. By contrast, in this paper we directly analyze this probability using Gaussian Processes Regression (GPR), and introduce a latent variable to assist the tracking decision. Our observation model for regression is learnt in a semi-supervised fashion by using both labeled samples from previous frames and the unlabeled samples that are tracking candidates extracted from the current frame. We further divide the labeled samples into two categories: auxiliary samples collected from the very early frames and target samples from most recent frames. The auxiliary samples are dynamically re-weighted by the regression, and the final tracking result is determined by fusing decisions from two individual trackers, one derived from the auxiliary samples and the other from the target samples. All these ingredients together enable our tracker, denoted as TGPR, to alleviate the drifting issue from various aspects. The effectiveness of TGPR is clearly demonstrated by its excellent performances on three recently proposed public benchmarks, involving 161 sequences in total, in comparison with state-of-the-arts.", "title": "Transfer Learning Based Visual Tracking with Gaussian Processes Regression"}, "12a376e621d690f3e94bce14cd03c2798a626a38": {"paper_id": "12a376e621d690f3e94bce14cd03c2798a626a38", "abstract": "This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The first is the introduction of a new image representation called the \u201cIntegral Image\u201d which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features from a larger set and yields extremely efficient classifiers[6]. The third contribution is a method for combining increasingly more complex classifiers in a \u201ccascade\u201d which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection.", "title": "Rapid Object Detection using a Boosted Cascade of Simple Features"}, "51fea461cf3724123c888cb9184474e176c12e61": {"paper_id": "51fea461cf3724123c888cb9184474e176c12e61", "abstract": "Image registration finds a variety of applications in computer vision. Unfortunately, traditional image registration techniques tend to be costly. We present a new image registration technique that makes use of the spatial intensity gradient of the images to find a good match using a type of Newton-Raphson iteration. Our technique is faster because it examines far fewer potential matches between the images than existing techniques. Furthermore, this registration technique can be generalized to handle rotation, scaling and shearing. We show show our technique can be adapted for use in a stereo vision system.", "title": "An Iterative Image Registration Technique with an Application to Stereo Vision"}, "30950db8a2cae3630057efe731b85f7b567848b8": {"paper_id": "30950db8a2cae3630057efe731b85f7b567848b8", "abstract": "The problem of tracking curves in dense visual clutter is challenging. Kalman filtering is inadequate because it is based on Gaussian densities which, being unimo dal, cannot represent simultaneous alternative hypotheses. The Condensation algorithm uses \u201cfactored sampling\u201d, previously applied to the interpretation of static images, in which the probability distribution of possible interpretations is represented by a randomly generated set. Condensation uses learned dynamical models, together with visual observations, to propagate the random set over time. The result is highly robust tracking of agile motion. Notwithstanding the use of stochastic methods, the algorithm runs in near real-time.", "title": "CONDENSATION\u2014Conditional Density Propagation for Visual Tracking"}, "6204776d31359d129a582057c2d788a14f8aadeb": {"paper_id": "6204776d31359d129a582057c2d788a14f8aadeb", "abstract": "We address the problem of tracking and recognizing faces in real-world, noisy videos. We track faces using a tracker that adaptively builds a target model reflecting changes in appearance, typical of a video setting. However, adaptive appearance trackers often suffer from drift, a gradual adaptation of the tracker to non-targets. To alleviate this problem, our tracker introduces visual constraints using a combination of generative and discriminative models in a particle filtering framework. The generative term conforms the particles to the space of generic face poses while the discriminative one ensures rejection of poorly aligned targets. This leads to a tracker that significantly improves robustness against abrupt appearance changes and occlusions, critical for the subsequent recognition phase. Identity of the tracked subject is established by fusing pose-discriminant and person-discriminant features over the duration of a video sequence. This leads to a robust video-based face recognizer with state-of-the-art recognition performance. We test the quality of tracking and face recognition on real-world noisy videos from YouTube as well as the standard Honda/UCSD database. Our approach produces successful face tracking results on over 80% of all videos without video or person-specific parameter tuning. The good tracking performance induces similarly high recognition rates: 100% on Honda/UCSD and over 70% on the YouTube set containing 35 celebrities in 1500 sequences.", "title": "Face tracking and recognition with visual constraints in real-world videos"}, "204de256e7ae41162d1e118adf89864cc9b76b28": {"paper_id": "204de256e7ae41162d1e118adf89864cc9b76b28", "abstract": "This paper addresses the problem of learning from very large databases where batch learning is impractical or even infeasible. Bootstrap is a popular technique applicable in such situations. We show that sampling strategy used for bootstrapping has a significant impact on the resulting classifier performance. We design a new general sampling strategy \u201dquasi-random weighted sampling + trimming\u201d (QWS+) that includes well established strategies as special cases. The QWS+ approach minimizes the variance of hypothesis error estimate and leads to significant improvement in performance compared to standard sampling techniques. The superior performance is demonstrated on several problems including profile and frontal face detection.", "title": "Weighted Sampling for Large-Scale Boosting"}, "0b055f916437e3249a78df78a88dbfe4538e6866": {"paper_id": "0b055f916437e3249a78df78a88dbfe4538e6866", "abstract": "This paper shows that the performance of a binary classifier can be significantly improved by the processing of structured unlabeled data, i.e. data are structured if knowing the label of one example restricts the labeling of the others. We propose a novel paradigm for training a binary classifier from labeled and unlabeled examples that we call P-N learning. The learning process is guided by positive (P) and negative (N) constraints which restrict the labeling of the unlabeled set. P-N learning evaluates the classifier on the unlabeled data, identifies examples that have been classified in contradiction with structural constraints and augments the training set with the corrected samples in an iterative process. We propose a theory that formulates the conditions under which P-N learning guarantees improvement of the initial classifier and validate it on synthetic and real data. P-N learning is applied to the problem of on-line learning of object detector during tracking. We show that an accurate object detector can be learned from a single example and an unlabeled video sequence where the object may occur. The algorithm is compared with related approaches and state-of-the-art is achieved on a variety of objects (faces, pedestrians, cars, motorbikes and animals).", "title": "P-N learning: Bootstrapping binary classifiers by structural constraints"}, "28312c3a47c1be3a67365700744d3d6665b86f22": {"paper_id": "28312c3a47c1be3a67365700744d3d6665b86f22", "abstract": "As one of the most successful applications of image analysis and understanding, face recognition has recently received significant attention, especially during the past several years. At least two reasons account for this trend: the first is the wide range of commercial and law enforcement applications, and the second is the availability of feasible technologies after 30 years of research. Even though current machine recognition systems have reached a certain level of maturity, their success is limited by the conditions imposed by many real applications. For example, recognition of face images acquired in an outdoor environment with changes in illumination and/or pose remains a largely unsolved problem. In other words, current systems are still far away from the capability of the human perception system.This paper provides an up-to-date critical survey of still- and video-based face recognition research. There are two underlying motivations for us to write this survey paper: the first is to provide an up-to-date review of the existing literature, and the second is to offer some insights into the studies of machine recognition of faces. To provide a comprehensive survey, we not only categorize existing recognition techniques but also present detailed descriptions of representative methods within each category. In addition, relevant topics such as psychophysical studies, system evaluation, and issues of illumination and pose variation are covered.", "title": "Face recognition: A literature survey"}, "075bfb99ce2dbaa2005500dff90f893b7caa68c2": {"paper_id": "075bfb99ce2dbaa2005500dff90f893b7caa68c2", "abstract": "Boosting has become very popular in computer vision, showing impressive performance in detection and recognition tasks. Mainly off-line training methods have been used, which implies that all training data has to be a priori given; training and usage of the classifier are separate steps. Training the classifier on-line and incrementally as new data becomes available has several advantages and opens new areas of application for boosting in computer vision. In this paper we propose a novel on-line AdaBoost feature selection method. In conjunction with efficient feature extraction methods the method is real time capable. We demonstrate the multifariousness of the method on such diverse tasks as learning complex background models, visual tracking and object detection. All approaches benefit significantly by the on-line training.", "title": "On-line Boosting and Vision"}, "03ae968c3ecb5ba30a6df7b00518db6ec785d8c3": {"paper_id": "03ae968c3ecb5ba30a6df7b00518db6ec785d8c3", "abstract": "Very recently tracking was approached using classification techniques such as support vector machines. The object to be tracked is discriminated by a classifier from the background. In a similar spirit we propose a novel on-line AdaBoost feature selection algorithm for tracking. The distinct advantage of our method is its capability of on-line training. This allows to adapt the classifier while tracking the object. Therefore appearance changes of the object (e.g. out of plane rotations, illumination changes) are handled quite naturally. Moreover, depending on the background the algorithm selects the most discriminating features for tracking resulting in stable tracking results. By using fast computable features (e.g. Haar-like wavelets, orientation histograms, local binary patterns) the algorithm runs in real-time. We demonstrate the performance of the algorithm on several (publically available) video sequences.", "title": "Real-Time Tracking via On-line Boosting"}, "9ecd3d4c75f5e927fc3167dd185d7825e14a814b": {"paper_id": "9ecd3d4c75f5e927fc3167dd185d7825e14a814b", "abstract": "Your use of the JSTOR archive indicates your acceptance of JSTOR's Terms and Conditions of Use, available at http://www.jstor.org/page/info/about/policies/terms.jsp. JSTOR's Terms and Conditions of Use provides, in part, that unless you have obtained prior permission, you may not download an entire issue of a journal or multiple copies of articles, and you may use content in the JSTOR archive only for your personal, non-commercial use.", "title": "Probabilistic Principal Component Analysis"}, "2cfa006b33084abe8160b001f9a24944cda25d05": {"paper_id": "2cfa006b33084abe8160b001f9a24944cda25d05", "abstract": "A new method for r eal-time tracking of non-rigid objects seen from a moving camera is proposed. The central computational module is based on the mean shift iterations and nds the most probable tar get p osition in the current frame. The dissimilarity between the target model (its c olor distribution) and the target candidates is expr essed by a metric derive d from the Bhattacharyya coe cient. The theoretical analysis of the approach shows that it relates to the Bayesian framework while providing a practical, fast and e cient solution. The capability of the tracker to handle in real-time partial occlusions, signi cant clutter, and target scale variations, is demonstrated for several image sequences.", "title": "Real-Time Tracking of Non-Rigid Objects Using Mean Shift"}, "3d8359c257c6c2dd5f170d4ff22d213af011940e": {"paper_id": "3d8359c257c6c2dd5f170d4ff22d213af011940e", "abstract": "Suppose x is an unknown vector in Ropfm (a digital image or signal); we plan to measure n general linear functionals of x and then reconstruct. If x is known to be compressible by transform coding with a known transform, and we reconstruct via the nonlinear procedure defined here, the number of measurements n can be dramatically smaller than the size m. Thus, certain natural classes of images with m pixels need only n=O(m1/4log5/2(m)) nonadaptive nonpixel samples for faithful recovery, as opposed to the usual m pixel samples. More specifically, suppose x has a sparse representation in some orthonormal basis (e.g., wavelet, Fourier) or tight frame (e.g., curvelet, Gabor)-so the coefficients belong to an lscrp ball for 0<ples1. The N most important coefficients in that expansion allow reconstruction with lscr2 error O(N1/2-1p/). It is possible to design n=O(Nlog(m)) nonadaptive measurements allowing reconstruction with accuracy comparable to that attainable with direct knowledge of the N most important coefficients. Moreover, a good approximation to those N important coefficients is extracted from the n measurements by solving a linear program-Basis Pursuit in signal processing. The nonadaptive measurements have the character of \"random\" linear combinations of basis/frame elements. Our results use the notions of optimal recovery, of n-widths, and information-based complexity. We estimate the Gel'fand n-widths of lscrp balls in high-dimensional Euclidean space in the case 0<ples1, and give a criterion identifying near- optimal subspaces for Gel'fand n-widths. We show that \"most\" subspaces are near-optimal, and show that convex optimization (Basis Pursuit) is a near-optimal way to extract information derived from these near-optimal subspaces", "title": "Compressed sensing"}, "2d31a41e7fbce6c3f0b3340356764f6d52cec40c": {"paper_id": "2d31a41e7fbce6c3f0b3340356764f6d52cec40c", "abstract": "We propose a novel tracking algorithm that can work robustly in a challenging scenario such that several kinds of appearance and motion changes of an object occur at the same time. Our algorithm is based on a visual tracking decomposition scheme for the efficient design of observation and motion models as well as trackers. In our scheme, the observation model is decomposed into multiple basic observation models that are constructed by sparse principal component analysis (SPCA) of a set of feature templates. Each basic observation model covers a specific appearance of the object. The motion model is also represented by the combination of multiple basic motion models, each of which covers a different type of motion. Then the multiple basic trackers are designed by associating the basic observation models and the basic motion models, so that each specific tracker takes charge of a certain change in the object. All basic trackers are then integrated into one compound tracker through an interactive Markov Chain Monte Carlo (IMCMC) framework in which the basic trackers communicate with one another interactively while run in parallel. By exchanging information with others, each tracker further improves its performance, which results in increasing the whole performance of tracking. Experimental results show that our method tracks the object accurately and reliably in realistic videos where the appearance and motion are drastically changing over time.", "title": "Visual tracking decomposition"}, "049504df22c77010e5bb62a2088f70fabc5ecb6d": {"paper_id": "049504df22c77010e5bb62a2088f70fabc5ecb6d", "abstract": "Suppose we are given a vector f in a class FsubeRopf<sup>N </sup>, e.g., a class of digital signals or digital images. How many linear measurements do we need to make about f to be able to recover f to within precision epsi in the Euclidean (lscr<sub>2</sub>) metric? This paper shows that if the objects of interest are sparse in a fixed basis or compressible, then it is possible to reconstruct f to within very high accuracy from a small number of random measurements by solving a simple linear program. More precisely, suppose that the nth largest entry of the vector |f| (or of its coefficients in a fixed basis) obeys |f|<sub>(n)</sub>lesRmiddotn<sup>-1</sup>p/, where R>0 and p>0. Suppose that we take measurements y<sub>k</sub>=langf<sup># </sup>,X<sub>k</sub>rang,k=1,...,K, where the X<sub>k</sub> are N-dimensional Gaussian vectors with independent standard normal entries. Then for each f obeying the decay estimate above for some 0<p<1 and with overwhelming probability, our reconstruction f<sup>t</sup>, defined as the solution to the constraints y<sub>k</sub>=langf<sup># </sup>,X<sub>k</sub>rang with minimal lscr<sub>1</sub> norm, obeys parf-f<sup>#</sup>par<sub>lscr2</sub>lesC<sub>p </sub>middotRmiddot(K/logN)<sup>-r</sup>, r=1/p-1/2. There is a sense in which this result is optimal; it is generally impossible to obtain a higher accuracy from any set of K measurements whatsoever. The methodology extends to various other random measurement ensembles; for example, we show that similar results hold if one observes a few randomly sampled Fourier coefficients of f. In fact, the results are quite general and require only two hypotheses on the measurement ensemble which are detailed", "title": "Near-Optimal Signal Recovery From Random Projections: Universal Encoding Strategies?"}, "1ba7a9c0e658a0d98253f999bf43c2e98c07f4e0": {"paper_id": "1ba7a9c0e658a0d98253f999bf43c2e98c07f4e0", "abstract": "The \u21131 tracker obtains robustness by seeking a sparse representation of the tracking object via \u21131 norm minimization. However, the high computational complexity involved in the \u21131 tracker may hamper its applications in real-time processing scenarios. Here we propose Real-time Com-pressive Sensing Tracking (RTCST) by exploiting the signal recovery power of Compressive Sensing (CS). Dimensionality reduction and a customized Orthogonal Matching Pursuit (OMP) algorithm are adopted to accelerate the CS tracking. As a result, our algorithm achieves a realtime speed that is up to 5,000 times faster than that of the \u21131 tracker. Meanwhile, RTCST still produces competitive (sometimes even superior) tracking accuracy compared to the \u21131 tracker. Furthermore, for a stationary camera, a refined tracker is designed by integrating a CS-based background model (CSBM) into tracking. This CSBM-equipped tracker, termed RTCST-B, outperforms most state-of-the-art trackers in terms of both accuracy and robustness. Finally, our experimental results on various video sequences, which are verified by a new metric \u2014 Tracking Success Probability (TSP), demonstrate the excellence of the proposed algorithms.", "title": "Real-time visual tracking using compressive sensing"}, "3316521a5527c7700af8ae6aef32a79a8b83672c": {"paper_id": "3316521a5527c7700af8ae6aef32a79a8b83672c", "abstract": "Both detection and tracking people are challenging problems, especially in complex real world scenes that commonly involve multiple people, complicated occlusions, and cluttered or even moving backgrounds. People detectors have been shown to be able to locate pedestrians even in complex street scenes, but false positives have remained frequent. The identification of particular individuals has remained challenging as well. Tracking methods are able to find a particular individual in image sequences, but are severely challenged by real-world scenarios such as crowded street scenes. In this paper, we combine the advantages of both detection and tracking in a single framework. The approximate articulation of each person is detected in every frame based on local features that model the appearance of individual body parts. Prior knowledge on possible articulations and temporal coherency within a walking cycle are modeled using a hierarchical Gaussian process latent variable model (hGPLVM). We show how the combination of these results improves hypotheses for position and articulation of each person in several subsequent frames. We present experimental results that demonstrate how this allows to detect and track multiple people in cluttered scenes with reoccurring occlusions.", "title": "People-tracking-by-detection and people-detection-by-tracking"}, "075bc988728788aa033b04dee1753ded711180ee": {"paper_id": "075bc988728788aa033b04dee1753ded711180ee", "abstract": "We consider the problem of automatically recognizing human faces from frontal views with varying expression and illumination, as well as occlusion and disguise. We cast the recognition problem as one of classifying among multiple linear regression models and argue that new theory from sparse signal representation offers the key to addressing this problem. Based on a sparse representation computed by C1-minimization, we propose a general classification algorithm for (image-based) object recognition. This new framework provides new insights into two crucial issues in face recognition: feature extraction and robustness to occlusion. For feature extraction, we show that if sparsity in the recognition problem is properly harnessed, the choice of features is no longer critical. What is critical, however, is whether the number of features is sufficiently large and whether the sparse representation is correctly computed. Unconventional features such as downsampled images and random projections perform just as well as conventional features such as eigenfaces and Laplacianfaces, as long as the dimension of the feature space surpasses certain threshold, predicted by the theory of sparse representation. This framework can handle errors due to occlusion and corruption uniformly by exploiting the fact that these errors are often sparse with respect to the standard (pixel) basis. The theory of sparse representation helps predict how much occlusion the recognition algorithm can handle and how to choose the training images to maximize robustness to occlusion. We conduct extensive experiments on publicly available databases to verify the efficacy of the proposed algorithm and corroborate the above claims.", "title": "Robust Face Recognition via Sparse Representation"}, "62a134740314b4469c83c8921ae2e1beea22b8f5": {"paper_id": "62a134740314b4469c83c8921ae2e1beea22b8f5", "abstract": "We propose in this correspondence a new method to perform two-class clustering of 2-D data in a quick and automatic way by preserving certain features of the input data. The method is analytical, deterministic, unsupervised, automatic, and noniterative. The computation time is of order n if the data size is n, and hence much faster than any other method which requires the computation of an n-by-n dissimilarity matrix. Furthermore, the proposed method does not have the trouble of guessing initial values. This new approach is thus more suitable for fast automatic hierarchical clustering or any other fields requiring fast automatic two-class clustering of 2-D data. The method can be extended to cluster data in higher dimensional space. A 3-D example is included.", "title": "A Database for Handwritten Text Recognition Research"}, "2531b0c79e50b67b9e4baae4f93e8f0f5030eff5": {"paper_id": "2531b0c79e50b67b9e4baae4f93e8f0f5030eff5", "abstract": "Your use of the JSTOR archive indicates your acceptance of JSTOR's Terms and Conditions of Use, available at http://www.jstor.org/page/info/about/policies/terms.jsp. JSTOR's Terms and Conditions of Use provides, in part, that unless you have obtained prior permission, you may not download an entire issue of a journal or multiple copies of articles, and you may use content in the JSTOR archive only for your personal, non-commercial use.", "title": "Bayesian Experimental Design : A Review"}, "0acf1a74e6ed8c323192d2b0424849820fe88715": {"paper_id": "0acf1a74e6ed8c323192d2b0424849820fe88715", "abstract": "Support vector machines have met with significant success in numerous real-world learning tasks. However, like most machine learning algorithms, they are generally applied using a randomly selected training set classified in advance. In many settings, we also have the option of using pool-based active learning. Instead of using a randomly selected training set, the learner has access to a pool of unlabeled instances and can request the labels for some number of them. We introduce a new algorithm for performing active learning with support vector machines, i.e., an algorithm for choosing which instances to request next. We provide a theoretical motivation for the algorithm using the notion of a version space. We present experimental results showing that employing our active learning method can significantly reduce the need for labeled training instances in both the standard inductive and transductive settings.", "title": "Support Vector Machine Active Learning with Applications to Text Classification"}, "86ab4cae682fbd49c5a5bedb630e5a40fa7529f6": {"paper_id": "86ab4cae682fbd49c5a5bedb630e5a40fa7529f6", "abstract": "We present an application of back-propagation networks to handwritten digit recognition. Minimal preprocessing of the data was required, but architecture of the network was highly constrained and specifically designed for the task. The input of the network consists of normalized images of isolated digits. The method has 1 % error rate and about a 9% reject rate on zipcode digits provided by the U.S. Postal Service.", "title": "Handwritten Digit Recognition with a Back-Propagation Network"}, "3b3b54848c1bc6ffea2625ce79302abed8e8deb9": {"paper_id": "3b3b54848c1bc6ffea2625ce79302abed8e8deb9", "abstract": "This paper shows how a text classifier\u2019s need for labeled training documents can be reduced by taking advantage of a large pool of unlabeled documents. We modify the Query-by-Committee (QBC) method of active learning to use the unlabeled pool for explicitly estimating document density when selecting examples for labeling. Then active learning is combined with ExpectationMaximization in order to \u201cfill in\u201d the class labels of those documents that remain unlabeled. Experimental results show that the improvements to active learning require less than two-thirds as many labeled training examples as previous QBC approaches, and that the combination of EM and active learning requires only slightly more than half as many labeled training examples to achieve the same accuracy as either the improved active learning or EM alone.", "title": "Employing EM and Pool-Based Active Learning for Text Classification"}, "29620eaa36ad1caefb871039a7ed4b4891d96666": {"paper_id": "29620eaa36ad1caefb871039a7ed4b4891d96666", "abstract": "The use of computer mediated communication such as emailing, microblogs, Short Messaging System (SMS), and chat rooms has created corpora which contain incredibly noisy text. Tweets, messages sent by users on Twitter.com, are an especially noisy form of communication. Twitter.com contains billions of these tweets, but in their current state they contain so much noise that it is difficult to extract useful information. Tweets often contain highly irregular syntax and nonstandard use of English. This paper describes a novel system which normalizes these Twitter posts, converting them into a more standard form of English, so that standard machine translation (MT) and natural language processing (NLP) techniques can be more easily applied to them. In order to normalize Twitter tweets, we take a two step approach. We first preprocess tweets to remove as much noise as possible and then feed them into a machine translation model to convert them into standard English. Together, these two steps allow us to achieve improvement in BLEU scores comporable to the improvements achieved by SMS normalization", "title": "Syntactic Normalization of Twitter Messages"}, "0e2795b1329b25ba3709584b96fd5cb4c96f6f22": {"paper_id": "0e2795b1329b25ba3709584b96fd5cb4c96f6f22", "abstract": "We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models. In the Appendix, we present an efficient training algorithm for the alignment models presented.", "title": "A Systematic Comparison of Various Statistical Alignment Models"}, "75f764383eccbe5abacb666840f488dde8c5585f": {"paper_id": "75f764383eccbe5abacb666840f488dde8c5585f", "abstract": "This paper presents a method to compute similarity of folktales based on conceptual overlap at various levels of abstraction as defined in Dutch WordNet. The method is applied on a corpus of Dutch folktales and evaluated using a comparison to traditional folktale similarity analysis based on the Aarne\u2013Thompson\u2013Uther (ATU) classification system. Document similarity computed by the presented method is in agreement with traditional analysis for a certain amount of folktale pairs, but differ for other pairs. However, it can be argued that the current approach computes an alternative, data-driven type of similarity. Using WordNet instead of a domainspecific ontology or classification system ensures applicability of the method outside of the folktale domain.", "title": "Folktale similarity based on ontological abstraction Marijn Schraagen Digital"}, "0a32b3d027064798fb31ce42894fec31e834f7db": {"paper_id": "0a32b3d027064798fb31ce42894fec31e834f7db", "abstract": "The quantification of lexical semantic relatedness has many applications in NLP, and many different measures have been proposed. We evaluate five of these measures, all of which use WordNet as their central resource, by comparing their performance in detecting and correcting real-word spelling errors. An information-content-based measure proposed by Jiang and Conrath is found superior to those proposed by Hirst and St-Onge, Leacock and Chodorow, Lin, and Resnik. In addition, we explain why distributional similarity is not an adequate proxy for lexical semantic relatedness.", "title": "Evaluating WordNet-based Measures of Lexical Semantic Relatedness"}, "3baecc04e1341cbae7999e8f61a3946c76504828": {"paper_id": "3baecc04e1341cbae7999e8f61a3946c76504828", "abstract": "In the original PageRank algorithm for improving the ranking of search-query results, a single PageRank vector is computed, using the link structure of the Web, to capture the relative \"importance\" of Web pages, independent of any particular search query. To yield more accurate search results, we propose computing a set of PageRank vectors, biased using a set of representative topics, to capture more accurately the notion of importance with respect to a particular topic. By using these (precomputed) biased PageRank vectors to generate query-specific importance scores for pages at query time, we show that we can generate more accurate rankings than with a single, generic PageRank vector. For ordinary keyword search queries, we compute the topic-sensitive PageRank scores for pages satisfying the query using the topic of the query keywords. For searches done in context (e.g., when the search query is performed by highlighting words in a Web page), we compute the topic-sensitive PageRank scores using the topic of the context in which the query appeared.", "title": "Topic-sensitive PageRank"}, "0e3e3c3d8ae5cb7c4636870d69967c197484d3bb": {"paper_id": "0e3e3c3d8ae5cb7c4636870d69967c197484d3bb", "abstract": "This paper will focus on the semantic representation of verbs in computer systems and its impact on lexical selection problems in machine translation (MT). Two groups of English and Chinese verbs are examined to show that lexical selection must be based on interpretation of the sentence as well as selection restrictions placed on the verb arguments. A novel representation scheme is suggested, and is compared to representations with selection restrictions used in transfer-based MT. We see our approach as closely aligned with knowledge-based MT approaches (KBMT), and as a separate component that could be incorporated into existing systems. Examples and experimental results will show that, using this scheme, inexact matches can achieve correct lexical selection.", "title": "Verb Semantics and Lexical Selection"}, "7441116c5b5a745708a9d7c5aa0ecf04e0c76c93": {"paper_id": "7441116c5b5a745708a9d7c5aa0ecf04e0c76c93", "abstract": "Traditionally, vector-based semantic space models use word co-occurrence counts from large corpora to represent lexical meaning. In this article we present a novel framework for constructing semantic spaces that takes syntactic relations into account. We introduce a formalization for this class of models, which allows linguistic knowledge to guide the construction process. We evaluate our framework on a range of tasks relevant for cognitive science and natural language processing: semantic priming, synonymy detection, and word sense disambiguation. In all cases, our framework obtains results that are comparable or superior to the state of the art.", "title": "Dependency-Based Construction of Semantic Space Models"}, "11157109b8f3a098c5c3f801ba9acbffd2aa49b1": {"paper_id": "11157109b8f3a098c5c3f801ba9acbffd2aa49b1", "abstract": "Bootstrapping semantics from text is one of the greatest challenges in natural language learning. We first define a word similarity measure based on the distributional pattern of words. The similarity measure allows us to construct a thesaurus using a parsed corpus. We then present a new evaluation methodology for the automatically constructed thesaurus. The evaluation results show that the thesaurus is significantly closer to WordNet than Roget Thesaurus is.", "title": "Automatic Retrieval and Clustering of Similar Words"}, "3f2f6772d96d972e3b2da5aaa8a0f2feefdf827f": {"paper_id": "3f2f6772d96d972e3b2da5aaa8a0f2feefdf827f", "abstract": null, "title": "Automatic Text Processing: The Transformation, Analysis, and Retrieval of Information by Computer"}, "9bacdc10f1d2d395af8c3a97dbdf510709ddc35f": {"paper_id": "9bacdc10f1d2d395af8c3a97dbdf510709ddc35f", "abstract": "Query logs are of great interest for scientists and companies for research, statistical and commercial purposes. However, the availability of query logs for secondary uses raises privacy issues since they allow the identification and/or revelation of sensitive information about individual users. Hence, query anonymization is crucial to avoid identity disclosure. To enable the publication of privacy-preserved -but still usefulquery logs, in this paper, we present an anonymization method based on semantic microaggregation. Our proposal aims at minimizing the disclosure risk of anonymized query logs while retaining their semantics as much as possible. First, a method to map queries to their formal semantics extracted from the structured categories of the Open Directory Project is presented. Then, a microaggregation method is adapted to perform a semantically-grounded anonymization of query logs. To do so, appropriate semantic similarity and semantic aggregation functions are proposed. Experiments performed using real AOL query logs show that our proposal better retains the utility of anonymized query logs than other related works, while also minimizing the disclosure risk.", "title": "Utility preserving query log anonymization via semantic microaggregation"}, "011ffa226a58b6711ce509609b8336911325b0e0": {"paper_id": "011ffa226a58b6711ce509609b8336911325b0e0", "abstract": "This paper presents a new approach for measuring semantic similarity/distance between words and concepts. It combines a lexical taxonomy structure with corpus statistical information so that the semantic distance between nodes in the semantic space constructed by the taxonomy can be better quantified with the computational evidence derived from a distributional analysis of corpus data. Specifically, the proposed measure is a combined approach that inherits the edge-based approach of the edge counting scheme, which is then enhanced by the node-based approach of the information content calculation. When tested on a common data set of word pair similarity ratings, the proposed approach outperforms other computational models. It gives the highest correlation value (r = 0.828) with a benchmark based on human similarity judgements, whereas an upper bound (r = 0.885) is observed when human subjects replicate the same task.", "title": "Semantic Similarity Based on Corpus Statistics and Lexical Taxonomy"}, "0f10c6f57e7b640a2e89e94b5ca7d2b0d46d3925": {"paper_id": "0f10c6f57e7b640a2e89e94b5ca7d2b0d46d3925", "abstract": "In this paper, we introduce a WordNetbased measure of semantic relatedness by combining the structure and content of WordNet with co\u2013occurrence information derived from raw text. We use the co\u2013occurrence information along with the WordNet definitions to build gloss vectors corresponding to each concept in WordNet. Numeric scores of relatedness are assigned to a pair of concepts by measuring the cosine of the angle between their respective gloss vectors. We show that this measure compares favorably to other measures with respect to human judgments of semantic relatedness, and that it performs well when used in a word sense disambiguation algorithm that relies on semantic relatedness. This measure is flexible in that it can make comparisons between any two concepts without regard to their part of speech. In addition, it can be adapted to different domains, since any plain text corpus can be used to derive the co\u2013occurrence information.", "title": "Using WordNet-Based Context Vectors To Estimate The Semantic Relatedness Of Concepts"}, "45c775fd1ab58da151d1f81a27f12d5f0ae5e00a": {"paper_id": "45c775fd1ab58da151d1f81a27f12d5f0ae5e00a", "abstract": "In this paper we present a new semantic smoothing vector space kernel (S-VSM) for text documents clustering. In the suggested approach semantic relatedness between words is used to smooth the similarity and the representation of text documents. The basic hypothesis examined is that considering semantic relatedness between two text documents may improve the performance of the text document clustering task. For our experimental evaluation we analyze the performance of several semantic relatedness measures when embedded in the proposed (S-VSM) and present results with respect to different experimental conditions, such as: (i) the datasets used, (ii) the underlying knowledge sources of the utilized measures, and (iii) the clustering algorithms employed. To the best of our knowledge, the current study is the first to systematically compare, analyze and evaluate the impact of semantic smoothing in text clustering based on \u2018wisdom of linguists\u2019, e.g., WordNets, \u2018wisdom of crowds\u2019, e.g., Wikipedia, and \u2018wisdom of corpora\u2019, e.g., large text corpora represented with the traditional Bag of Words (BoW) model. Three semantic relatedness measures for text are considered; two knowledge-based (Omiotis [1] that uses WordNet, and WLM [2] that uses Wikipedia), and one corpus-based (PMI [3] trained on a semantically tagged SemCor version). For the comparison of different experimental conditions we use the BCubed F-Measure evaluation metric which satisfies all formal constraints of good quality cluster. The experimental results show that the clustering performance based on the S-VSM is better compared to the traditional VSM model and compares favorably against the standard GVSM kernel which uses word co-occurrences to compute the latent similarities between document terms. 2013 Elsevier B.V. All rights reserved.", "title": "Semantic smoothing for text clustering"}, "0bf128305b68cdc4a216f165b77b442c986e368d": {"paper_id": "0bf128305b68cdc4a216f165b77b442c986e368d", "abstract": "In many research fields such as Psychology, Linguistics, Cognitive Science and Artificial Intelligence, computing semantic similarity between words is an important issue. In this paper a new semantic similarity metric, that exploits some notions of the feature based theory of similarity and translates it into the information theoretic domain, which leverages the notion of Information Content (IC), is presented. In particular, the proposed metric exploits the notion of intrinsic IC which quantifies IC values by scrutinizing how concepts are arranged in an ontological structure. In order to evaluate this metric, an on line experiment asking the community of researchers to rank a list of 65 word pairs has been conducted. The experiment\u2019s web setup allowed to collect 101 similarity ratings and to differentiate native and non-native English speakers. Such a large and diverse dataset enables to confidently evaluate similarity metrics by correlating them with human assessments. Experimental evaluations using WordNet indicate that the proposed metric, coupled with the notion of intrinsic IC, yields results above the state of the art. Moreover, the intrinsic IC formulation also improves the accuracy of other IC-based metrics. In order to investigate the generality of both the intrinsic IC formulation and proposed similarity metric a further evaluation using the MeSH biomedical ontology has been performed. Even in this case significant results were obtained. The proposed metric and several others have been implemented in the Java WordNet Similarity Library.", "title": "A semantic similarity metric combining features and intrinsic information content"}, "259d0304adcb49e40436137684b78a80c9ef097b": {"paper_id": "259d0304adcb49e40436137684b78a80c9ef097b", "abstract": "In this paper, I will discuss four major topics relating to current research in lexical semantics: methodology, descriptive coverage, adequacy of the representation, and the computational usefulness of representations. In addressing these issues, I will discuss what I think are some of the central problems facing the lexical semantics community, and suggest ways of best approaching these issues. Then, I will provide a method for the decomposition of lexical categories and outline a theory of lexical semantics embodying a notion of cocompositionality and type coercion, as well as several levels of semantic description, where the semantic load is spread more evenly throughout the lexicon. I argue that lexical decomposition is possible if it is performed generatively. Rather than assuming a fixed set of primitives, I will assume a fixed number of generative devices that can be seen as constructing semantic expressions. I develop a theory of Qualia Structure, a representation language for lexical items, which renders much lexical ambiguity in the lexicon unnecessary, while still explaining the systematic polysemy that words carry. Finally, I discuss how individual lexical structures can be integrated into the larger lexical knowledge base through a theory of lexical inheritance. This provides us with the necessary principles of global organization for the lexicon, enabling us to fully integrate our natural language lexicon into a conceptual whole.", "title": "The Generative Lexicon"}, "e17e38efc2f21b6cf1ce3b424a3a672fe33b5d37": {"paper_id": "e17e38efc2f21b6cf1ce3b424a3a672fe33b5d37", "abstract": "Deep Neural Networks (DNNs) are universal function approximators providing state-ofthe-art solutions on wide range of applications. Common perceptual tasks such as speech recognition, image classification, and object tracking are now commonly tackled via DNNs. Some fundamental problems remain: (1) the lack of a mathematical framework providing an explicit and interpretable input-output formula for any topology, (2) quantification of DNNs stability regarding adversarial examples (i.e. modified inputs fooling DNN predictions whilst undetectable to humans), (3) absence of generalization guarantees and controllable behaviors for ambiguous patterns, (4) leverage unlabeled data to apply DNNs to domains where expert labeling is scarce as in the medical field. Answering those points would provide theoretical perspectives for further developments based on a common ground. Furthermore, DNNs are now deployed in tremendous societal applications, pushing the need to fill this theoretical gap to ensure control, reliability, and interpretability. 1 ar X iv :1 71 0. 09 30 2v 3 [ st at .M L ] 6 N ov 2 01 7", "title": "Adaptive Partitioning Spline Neural Networks: Template Matching, Memorization, Inhibitor Connections, Inversion, Semi-Sup, Topology Search"}, "178631e0f0e624b1607c7a7a2507ed30d4e83a42": {"paper_id": "178631e0f0e624b1607c7a7a2507ed30d4e83a42", "abstract": "Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates deep recurrent neural networks, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.", "title": "Speech recognition with deep recurrent neural networks"}, "1b4e04381ddd2afab1660437931cd62468370a98": {"paper_id": "1b4e04381ddd2afab1660437931cd62468370a98", "abstract": "Text corpora which are tagged with part-of-speech information are useful in many areas of linguistic research. In this paper, a new part-of-speech tagging method hased on neural networks (Net-Tagger) is presented and its performance is compared to that of a llMM-tagger (Cutting et al., 1992) and a trigrambased tagger (Kempe, 1993). It is shown that the Net-Tagger performs as well as the trigram-based tagger and better than the iIMM-tagger.", "title": "Part-of-Speech Tagging with Neural Networks"}, "22da4cbfa4e29da9a43996324c318dd33b947b48": {"paper_id": "22da4cbfa4e29da9a43996324c318dd33b947b48", "abstract": "We address the problem of learning topic hierarchies from data. The model selection problem in this domain is daunting\u2014which of the large collection of possible trees to use? We take a Bayesian approach, generating an appropriate prior via a distribution on partitions that we refer to as the nested Chinese restaurant process. This nonparametric prior allows arbitrarily large branching factors and readily accommodates growing data collections. We build a hierarchical topic model by combining this prior with a likelihood that is based on a hierarchical variant of latent Dirichlet allocation. We illustrate our approach on simulated data and with an application to the modeling of NIPS abstracts.", "title": "Hierarchical Topic Models and the Nested Chinese Restaurant Process"}, "2329a46590b2036d508097143e65c1b77e571e8c": {"paper_id": "2329a46590b2036d508097143e65c1b77e571e8c", "abstract": "We present a state-of-the-art speech recognition system developed using end-toend deep learning. Our architecture is significantly simpler than traditional speech systems, which rely on laboriously engineered processing pipelines; these traditional systems also tend to perform poorly when used in noisy environments. In contrast, our system does not need hand-designed components to model background noise, reverberation, or speaker variation, but instead directly learns a function that is robust to such effects. We do not need a phoneme dictionary, nor even the concept of a \u201cphoneme.\u201d Key to our approach is a well-optimized RNN training system that uses multiple GPUs, as well as a set of novel data synthesis techniques that allow us to efficiently obtain a large amount of varied data for training. Our system, called Deep Speech, outperforms previously published results on the widely studied Switchboard Hub5\u201900, achieving 16.0% error on the full test set. Deep Speech also handles challenging noisy environments better than widely used, state-of-the-art commercial speech systems.", "title": "Deep Speech: Scaling up end-to-end speech recognition"}, "0c4867f11c9758014d591381d8b397a1d38b04a7": {"paper_id": "0c4867f11c9758014d591381d8b397a1d38b04a7", "abstract": "The first \u20ac price and the \u00a3 and $ price are net prices, subject to local VAT. Prices indicated with * include VAT for books; the \u20ac(D) includes 7% for Germany, the \u20ac(A) includes 10% for Austria. Prices indicated with ** include VAT for electronic products; 19% for Germany, 20% for Austria. All prices exclusive of carriage charges. Prices and other details are subject to change without notice. All errors and omissions excepted. C. Bishop Pattern Recognition and Machine Learning", "title": "Pattern Recognition and Machine Learning"}, "656a33c1db546da8490d6eba259e2a849d73a001": {"paper_id": "656a33c1db546da8490d6eba259e2a849d73a001", "abstract": "The premise of this article is that learning procedures used to train artificial neural networks are inherently statistical techniques. It follows that statistical theory can provide considerable insight into the properties, advantages, and disadvantages of different network learning methods. We review concepts and analytical results from the literatures of mathematical statistics, econometrics, systems identification, and optimization theory relevant to the analysis of learning in artificial neural networks. Because of the considerable variety of available learning procedures and necessary limitations of space, we cannot provide a comprehensive treatment. Our focus is primarily on learning procedures for feedforward networks. However, many of the concepts and issues arising in this framework are also quite broadly relevant to other network learning paradigms. In addition to providing useful insights, the material reviewed here suggests some potentially useful new training methods for artificial neural networks.", "title": "Learning in Artificial Neural Networks: A Statistical Perspective"}, "7e0dab4fe4299bc2f8b4b18f82702af717cf3924": {"paper_id": "7e0dab4fe4299bc2f8b4b18f82702af717cf3924", "abstract": "We present an analysis of how the generalization performance (expected test set error) relates to the expected training set error for nonlinear learning systems, such as multilayer perceptrons and radial basis functions. The principal result is the following relationship (computed to second order) between the expected test set and tlaining set errors: (1) Here, n is the size of the training sample e, u;f f is the effective noise variance in the response variable( s), ,x is a regularization or weight decay parameter, and Peff(,x) is the effective number of parameters in the nonlinear model. The expectations ( ) of training set and test set errors are taken over possible training sets e and training and test sets e' respectively. The effective number of parameters Peff(,x) usually differs from the true number of model parameters P for nonlinear or regularized models; this theoretical conclusion is supported by Monte Carlo experiments. In addition to the surprising result that Peff(,x) ;/; p, we propose an estimate of (1) called the generalized prediction error (GPE) which generalizes well established estimates of prediction risk such as Akaike's F P E and AI C, Mallows Cp, and Barron's PSE to the nonlinear setting.! lCPE and Peff(>\") were previously introduced in Moody (1991). 847", "title": "The Effective Number of Parameters: An Analysis of Generalization and Regularization in Nonlinear Learning Systems"}, "58ceeb151558c1f322b9f6273b47e90e9c04e6b1": {"paper_id": "58ceeb151558c1f322b9f6273b47e90e9c04e6b1", "abstract": "Feedforward neural networks trained by error backpropagation are examples of nonparametric regression estimators. We present a tutorial on nonparametric inference and its relation to neural networks, and we use the statistical viewpoint to highlight strengths and weaknesses of neural models. We illustrate the main points with some recognition experiments involving artificial data as well as handwritten numerals. In way of conclusion, we suggest that current-generation feedforward neural networks are largely inadequate for difficult problems in machine perception and machine learning, regardless of parallel-versus-serial hardware or other implementation issues. Furthermore, we suggest that the fundamental challenges in neural modeling are about representation rather than learning per se. This last point is supported by additional experiments with handwritten numerals.", "title": "Neural Networks and the Bias/Variance Dilemma"}, "2904a9932f4cd0f0886121dc1f2d4aaac0455176": {"paper_id": "2904a9932f4cd0f0886121dc1f2d4aaac0455176", "abstract": "We consider the problem of learning deep generative models from data. We formulate a method that generates an independent sample via a single feedforward pass through a multilayer preceptron, as in the recently proposed generative adversarial networks (Goodfellow et al., 2014). Training a generative adversarial network, however, requires careful optimization of a difficult minimax program. Instead, we utilize a technique from statistical hypothesis testing known as maximum mean discrepancy (MMD), which leads to a simple objective that can be interpreted as matching all orders of statistics between a dataset and samples from the model, and can be trained by backpropagation. We further boost the performance of this approach by combining our generative network with an auto-encoder network, using MMD to learn to generate codes that can then be decoded to produce samples. We show that the combination of these techniques yields excellent generative models compared to baseline approaches as measured on MNIST and the Toronto Face Database.", "title": "Generative Moment Matching Networks"}, "0626908dd710b91aece1a81f4ca0635f23fc47f3": {"paper_id": "0626908dd710b91aece1a81f4ca0635f23fc47f3", "abstract": "Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2% top-1 and 5:6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5% top-5 error and 17:3% top-1 error on the validation set and 3:6% top-5 error on the official test set.", "title": "Rethinking the Inception Architecture for Computer Vision"}, "245b46b10b46e765bcc36ee10df402d1d541a9ec": {"paper_id": "245b46b10b46e765bcc36ee10df402d1d541a9ec", "abstract": "Deep generative models parameterized by neural networks have recently achieved state-ofthe-art performance in unsupervised and semisupervised learning. We extend deep generative models with auxiliary variables which improves the variational approximation. The auxiliary variables leave the generative model unchanged but make the variational distribution more expressive. Inspired by the structure of the auxiliary variable we also propose a model with two stochastic layers and skip connections. Our findings suggest that more expressive and properly specified deep generative models converge faster with better results. We show state-of-theart performance within semi-supervised learning on MNIST (0.96%), SVHN (16.61%) and NORB (9.40%) datasets.", "title": "Auxiliary Deep Generative Models"}, "08969cbeb4ac3fe0b20754cbcf221f8031fc682f": {"paper_id": "08969cbeb4ac3fe0b20754cbcf221f8031fc682f", "abstract": "General unsupervised learning is a long-standing conceptual problem in machine learning. Supervised learning is successful because it can be solved by the minimization of the training error cost function. Unsupervised learning is not as successful, because the unsupervised objective may be unrelated to the supervised task of interest. For an example, density modelling and reconstruction have often been used for unsupervised learning, but they did not produced the sought-after performance gains, because they have no knowledge of the sought-after supervised tasks. In this paper, we present an unsupervised cost function which we name the Output Distribution Matching (ODM) cost, which measures a divergence between the distribution of predictions and distributions of labels. The ODM cost is appealing because it is consistent with the supervised cost in the following sense: a perfect supervised classifier is also perfect according to the ODM cost. Therefore, by aggressively optimizing the ODM cost, we are almost guaranteed to improve our supervised performance whenever the space of possible predictions is exponentially large. We demonstrate that the ODM cost works well on number of small and semiartificial datasets using no (or almost no) labelled training cases. Finally, we show that the ODM cost can be used for one-shot domain adaptation, which allows the model to classify inputs that differ from the input distribution in significant ways without the need for prior exposure to the new domain.", "title": "Towards Principled Unsupervised Learning"}, "83bfdd6a2b28106b9fb66e52832c45f08b828541": {"paper_id": "83bfdd6a2b28106b9fb66e52832c45f08b828541", "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extent. We can cause the network to misclassify an image by applying a certain hardly perceptible perturbation, which is found by maximizing the network\u2019s prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.", "title": "Intriguing properties of neural networks"}, "1827de6fa9c9c1b3d647a9d707042e89cf94abf0": {"paper_id": "1827de6fa9c9c1b3d647a9d707042e89cf94abf0", "abstract": "Training Deep Neural Networks is complicated by the fact that the distribution of each layer\u2019s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a stateof-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.", "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"}, "4017f984d1b4b8748a06da2739183782bbe9b46d": {"paper_id": "4017f984d1b4b8748a06da2739183782bbe9b46d", "abstract": null, "title": "Optimization by simulated annealing"}, "2e3e09e48a7a62dc30efd8ef7fc4665a53e84d7a": {"paper_id": "2e3e09e48a7a62dc30efd8ef7fc4665a53e84d7a", "abstract": "The computotionol power of massively parallel networks of simple processing elements resides in the communication bandwidth provided by the hardware connections between elements. These connections con allow a significant fraction of the knowledge of the system to be applied to an instance of a problem in o very short time. One kind of computation for which massively porollel networks appear to be well suited is large constraint satisfaction searches, but to use the connections efficiently two conditions must be met: First, a search technique that is suitable for parallel networks must be found. Second, there must be some way of choosing internal representations which allow the preexisting hardware connections to be used efficiently for encoding the constraints in the domain being searched. We describe a generol parallel search method, based on statistical mechanics, and we show how it leads to a general learning rule for modifying the connection strengths so as to incorporate knowledge obout o task domain in on efficient way. We describe some simple examples in which the learning algorithm creates internal representations thot ore demonstrobly the most efficient way of using the preexisting connectivity structure.", "title": "A Learning Algorithm for Boltzmann Machines"}, "c7788fe99735eceff2bcc37401fc02d2825f739a": {"paper_id": "c7788fe99735eceff2bcc37401fc02d2825f739a", "abstract": "This paper shows that the accuracy of learned text classifiers can be improved by augmenting a small number of labeled training documents with a large pool of unlabeled documents. This is important because in many text classification problems obtaining training labels is expensive, while large quantities of unlabeled documents are readily available. We introduce an algorithm for learning from labeled and unlabeled documents based on the combination of Expectation-Maximization (EM) and a naive Bayes classifier. The algorithm first trains a classifier using the available labeled documents, and probabilistically labels the unlabeled documents. It then trains a new classifier using the labels for all the documents, and iterates to convergence. This basic EM procedure works well when the data conform to the generative assumptions of the model. However these assumptions are often violated in practice, and poor performance can result. We present two extensions to the algorithm that improve classification accuracy under these conditions: (1) a weighting factor to modulate the contribution of the unlabeled data, and (2) the use of multiple mixture components per class. Experimental results, obtained using text from three different real-world tasks, show that the use of unlabeled data reduces classification error by up to 30%.", "title": "Text Classification from Labeled and Unlabeled Documents using EM"}, "4faec77e6051063ba44ed9d7d390e202bc10b07c": {"paper_id": "4faec77e6051063ba44ed9d7d390e202bc10b07c", "abstract": "It is well known that the addition of noise to the input data of a neural network during training can, in some circumstances, lead to significant improvements in generalization performance. Previous work has shown that such training with noise is equivalent to a form of regularization in which an extra term is added to the error function. However, the regularization term, which involves second derivatives of the error function, is not bounded below, and so can lead to difficulties if used directly in a learning algorithm based on error minimization. In this paper we show that for the purposes of network training, the regularization term can be reduced to a positive semi-definite form that involves only first derivatives of the network mapping. For a sum-of-squares error function, the regularization term belongs to the class of generalized Tikhonov regularizers. Direct minimization of the regularized error function provides a practical alternative to training with noise.", "title": "Training with Noise is Equivalent to Tikhonov Regularization"}, "17accbdd4aa3f9fad6af322bc3d7f4d5b648d9cd": {"paper_id": "17accbdd4aa3f9fad6af322bc3d7f4d5b648d9cd", "abstract": "This paper introduces Transductive Support Vector Machines (TSVMs) for text classi cation. While regular Support Vector Machines (SVMs) try to induce a general decision function for a learning task, Transductive Support Vector Machines take into account a particular test set and try to minimize misclassi cations of just those particular examples. The paper presents an analysis of why TSVMs are well suited for text classi cation. These theoretical ndings are supported by experiments on three test collections. The experiments show substantial improvements over inductive methods, especially for small training sets, cutting the number of labeled training examples down to a twentieth on some tasks. This work also proposes an algorithm for training TSVMs e ciently, handling 10,000 examples and more.", "title": "Transductive Inference for Text Classification using Support Vector Machines"}, "1a736409c7711f8673f31d366f583ddc8759547f": {"paper_id": "1a736409c7711f8673f31d366f583ddc8759547f", "abstract": "The long short-term memory (LSTM) network trained by gradient descent solves difficult problems which traditional recurrent neural networks in general cannot. We have recently observed that the decoupled extended Kalman filter training algorithm allows for even better performance, reducing significantly the number of training steps when compared to the original gradient descent training algorithm. In this paper we present a set of experiments which are unsolvable by classical recurrent networks but which are solved elegantly and robustly and quickly by LSTM combined with Kalman filters.", "title": "Kalman filters improve LSTM network performance in problems unsolvable by traditional recurrent nets"}, "cccd3fd7a45e7643f26391bd539ffbede0690f36": {"paper_id": "cccd3fd7a45e7643f26391bd539ffbede0690f36", "abstract": "Recurrent connectionist networks are important because they can perform temporally extended tasks, giving them considerable power beyond the static mappings performed by the now-familiar multilayer feedforward networks. This ability to perform highly nonlinear dynamic mappings makes these networks particularly interesting to study and potentially quite useful in tasks which have an important temporal component not easily handled through the use of simple tapped delay lines. Some examples are tasks involving recognition or generation of sequential patterns and sensorimotor control. This report examines a number of learning procedures for adjusting the weights in recurrent networks in order to train such networks to produce desired temporal behaviors from input-output stream examples. The procedures are all based on the computation of the gradient of performance error with respect to network weights, and a number of strategies for computing the necessary gradient information are described. Included here are approaches which are familiar and have been rst described elsewhere, along with several novel approaches. One particular purpose of this report is to provide uniform and detailed descriptions and derivations of the various techniques in order to emphasize how they relate to one another. Another important contribution of this report is a detailed analysis of the computational requirements of the various approaches discussed.", "title": "Gradient-based learning algorithms for recurrent connectionist networks"}, "3e38c093d1e5b142a8754d834d152a0921f4f440": {"paper_id": "3e38c093d1e5b142a8754d834d152a0921f4f440", "abstract": "In this paper we present a novel genetic algorithm (GA) solution to a simple yet challenging commercial puzzle game known as Zen Puzzle Garden (ZPG). We describe the game in detail, before presenting a suitable encoding scheme and fitness function for candidate solutions. We then compare the performance of the genetic algorithm with that of the A* algorithm. Our results show that the GA is competitive with informed search in terms of solution quality, and significantly out-performs it in terms of computational resource requirements. We conclude with a brief discussion of the implications of our findings for game solving and other \u201creal world\u201d problems.", "title": "Genetic algorithms and the art of Zen"}, "6f3b7da2ac69f033a0fc9f492a146d6d9e90a788": {"paper_id": "6f3b7da2ac69f033a0fc9f492a146d6d9e90a788", "abstract": "Distant supervision, a widely applied approach in the field of relation extraction can automatically generate large amounts of labeled training corpus with minimal manual effort. However, the labeled training corpus may have many false-positive data, which would hurt the performance of relation extraction. Moreover, in traditional feature-based distant supervised approaches, extraction models adopt human design features with natural language processing. It may also cause poor performance. To address these two shortcomings, we propose a customized attention-based long short-term memory network. Our approach adopts word-level attention to achieve better data representation for relation extraction without manually designed features to perform distant supervision instead of fully supervised relation extraction, and it utilizes instance-level attention to tackle the problem of false-positive data. Experimental results demonstrate that our proposed approach is effective and achieves better performance than traditional methods.", "title": "A Customized Attention-Based Long Short-Term Memory Network for Distant Supervised Relation Extraction"}, "39f63dbdce9207b87878290c0e3983e84cfcecd9": {"paper_id": "39f63dbdce9207b87878290c0e3983e84cfcecd9", "abstract": "We propose a novel context-dependent (CD) model for large-vocabulary speech recognition (LVSR) that leverages recent advances in using deep belief networks for phone recognition. We describe a pre-trained deep neural network hidden Markov model (DNN-HMM) hybrid architecture that trains the DNN to produce a distribution over senones (tied triphone states) as its output. The deep belief network pre-training algorithm is a robust and often helpful way to initialize deep neural networks generatively that can aid in optimization and reduce generalization error. We illustrate the key components of our model, describe the procedure for applying CD-DNN-HMMs to LVSR, and analyze the effects of various modeling choices on performance. Experiments on a challenging business search dataset demonstrate that CD-DNN-HMMs can significantly outperform the conventional context-dependent Gaussian mixture model (GMM)-HMMs, with an absolute sentence accuracy improvement of 5.8% and 9.2% (or relative error reduction of 16.0% and 23.2%) over the CD-GMM-HMMs trained using the minimum phone error rate (MPE) and maximum-likelihood (ML) criteria, respectively.", "title": "Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition"}, "7d9089cbe958da21cbd943bdbcb996f4499e701b": {"paper_id": "7d9089cbe958da21cbd943bdbcb996f4499e701b", "abstract": "Document level sentiment classification remains a challenge: encoding the intrinsic relations between sentences in the semantic meaning of a document. To address this, we introduce a neural network model to learn vector-based document representation in a unified, bottom-up fashion. The model first learns sentence representation with convolutional neural network or long short-term memory. Afterwards, semantics of sentences and their relations are adaptively encoded in document representation with gated recurrent neural network. We conduct document level sentiment classification on four large-scale review datasets from IMDB and Yelp Dataset Challenge. Experimental results show that: (1) our neural model shows superior performances over several state-of-the-art algorithms; (2) gated recurrent neural network dramatically outperforms standard recurrent neural network in document modeling for sentiment classification.1", "title": "Document Modeling with Gated Recurrent Neural Network for Sentiment Classification"}, "1967ad3ac8a598adc6929e9e6b9682734f789427": {"paper_id": "1967ad3ac8a598adc6929e9e6b9682734f789427", "abstract": "We propose a hierarchical attention network for document classification. Our model has two distinctive characteristics: (i) it has a hierarchical structure that mirrors the hierarchical structure of documents; (ii) it has two levels of attention mechanisms applied at the wordand sentence-level, enabling it to attend differentially to more and less important content when constructing the document representation. Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin. Visualization of the attention layers illustrates that the model selects qualitatively informative words and sentences.", "title": "Hierarchical Attention Networks for Document Classification"}, "364da079f91a6cb385997be990af06e9ddf6e888": {"paper_id": "364da079f91a6cb385997be990af06e9ddf6e888", "abstract": "Convolutional neural network (CNN) is a neural network that can make use of the internal structure of data such as the 2D structure of image data. This paper studies CNN on text categorization to exploit the 1D structure (namely, word order) of text data for accurate prediction. Instead of using low-dimensional word vectors as input as is often done, we directly apply CNN to high-dimensional text data, which leads to directly learning embedding of small text regions for use in classification. In addition to a straightforward adaptation of CNN from image to text, a simple but new variation which employs bag-ofword conversion in the convolution layer is proposed. An extension to combine multiple convolution layers is also explored for higher accuracy. The experiments demonstrate the effectiveness of our approach in comparison with state-of-the-art methods.", "title": "Effective Use of Word Order for Text Categorization with Convolutional Neural Networks"}, "4e88de2930a4435f737c3996287a90ff87b95c59": {"paper_id": "4e88de2930a4435f737c3996287a90ff87b95c59", "abstract": "Because of their superior ability to preserve sequence information over time, Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. TreeLSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank).", "title": "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks"}, "27e38351e48fe4b7da2775bf94341738bc4da07e": {"paper_id": "27e38351e48fe4b7da2775bf94341738bc4da07e", "abstract": "Single-word vector space models have been very successful at learning lexical information. However, they cannot capture the compositional meaning of longer phrases, preventing them from a deeper understanding of language. We introduce a recursive neural network (RNN) model that learns compositional vector representations for phrases and sentences of arbitrary syntactic type and length. Our model assigns a vector and a matrix to every node in a parse tree: the vector captures the inherent meaning of the constituent, while the matrix captures how it changes the meaning of neighboring words or phrases. This matrix-vector RNN can learn the meaning of operators in propositional logic and natural language. The model obtains state of the art performance on three different experiments: predicting fine-grained sentiment distributions of adverb-adjective pairs; classifying sentiment labels of movie reviews and classifying semantic relationships such as cause-effect or topic-message between nouns using the syntactic path between them.", "title": "Semantic Compositionality through Recursive Matrix-Vector Spaces"}, "4f1fe957a29a2e422d4034f4510644714d33fb20": {"paper_id": "4f1fe957a29a2e422d4034f4510644714d33fb20", "abstract": "We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative. Using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines. However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization. We conclude by examining factors that make the sentiment classification problem more challenging. Publication info: Proceedings of EMNLP 2002, pp. 79\u201386.", "title": "Thumbs up? Sentiment Classification using Machine Learning Techniques"}, "1368d828a439ba160130e684a3280beb400880f6": {"paper_id": "1368d828a439ba160130e684a3280beb400880f6", "abstract": "Discourse structure is the hidden link between surface features and document-level properties, such as sentiment polarity. We show that the discourse analyses produced by Rhetorical Structure Theory (RST) parsers can improve document-level sentiment analysis, via composition of local information up the discourse tree. First, we show that reweighting discourse units according to their position in a dependency representation of the rhetorical structure can yield substantial improvements on lexicon-based sentiment analysis. Next, we present a recursive neural network over the RST structure, which offers significant improvements over classificationbased methods.", "title": "Better Document-level Sentiment Analysis from RST Discourse Parsing"}, "4d06b6d3bf332316446d35817cb4b481a716bf90": {"paper_id": "4d06b6d3bf332316446d35817cb4b481a716bf90", "abstract": "Recursive neural models have achieved promising results in many natural language processing tasks. The main difference among these models lies in the composition function, i.e., how to obtain the vector representation for a phrase or sentence using the representations of words it contains. This paper introduces a novel Adaptive Multi-Compositionality (AdaMC) layer to recursive neural models. The basic idea is to use more than one composition functions and adaptively select them depending on the input vectors. We present a general framework to model each semantic composition as a distribution over these composition functions. The composition functions and parameters used for adaptive selection are learned jointly from data. We integrate AdaMC into existing recursive neural models and conduct extensive experiments on the Stanford Sentiment Treebank. The results illustrate that AdaMC significantly outperforms state-of-the-art sentiment classification methods. It helps push the best accuracy of sentence-level negative/positive classification from 85.4% up to 88.5%. Introduction Recursive Neural Models (RNMs), which utilize the recursive structure of the input (e.g., a sentence), are one family of popular deep learning models. They are particularly effective for many Natural Language Processing (NLP) tasks due to the compositional nature of natural language. Recently, many promising results have been reported on semantic relationship classification (Socher et al. 2012), syntactic parsing (Socher et al. 2013a), sentiment analysis (Socher et al. 2013b), and so on. The main difference among RNMs lies in the semantic composition method, i.e., how to obtain the vector representation for a phrase or sentence using the representations of words and phrases it contains. For instance, we can compute the word vector for the phrase \u201cnot good\u201d with the vectors of the words \u201cnot\u201d and \u201cgood\u201d. For many tasks, we even need to obtain the vector representations for sentences. The composition algorithm becomes the key to make the vector representations go beyond words to phrases and sentences. \u21e4Contribution during internship at Microsoft Research. Copyright c 2014, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. There have been several attempts in literature to address the semantic composition for RNMs. Specifically, RNN (Socher et al. 2011) uses a global matrix to linearly combine the elements of vectors, while RNTN (Socher et al. 2013b) employs a global tensor to model the products of dimensions. Sometimes it is challenging to find a single powerful function to model the semantic composition. Intuitively, we can employ multiple composition functions, instead of only using a single global one. Instead of finding more complex composition functions, MV-RNN (Socher et al. 2012) assigns matrices for every words to make the compositions specific. However, the number of composition matrices is the same as vocabulary size, which makes the number of parameters quite large. It is easy to overfit the training data and difficult to be optimized. Moreover, MVRNN needs another global matrix to linearly combine the composition matrices for phrases, which still makes these compositions not specific. In order to overcome these shortcomings and make the compositions specific, it is better to use a certain number of composition functions, and embed the role-sensitive (linguistic and semantic) information into word vectors to adaptively select these compositions rather than concrete words. The example \u201cnot (so good)\u201d in sentiment analysis illustrates this point. To obtain the polarity of this phrase, we firstly combine the words \u201cso\u201d and \u201cgood\u201d, then combine the \u201cnot\u201d and \u201cso good\u201d. Specifically, the first combination is a strengthen composition which makes the sentiment polarity stronger, and the second step is a negation composition which negates the positive polarity to negative. In this paper, we introduce a novel Adaptive MultiCompositionality (AdaMC) method for RNMs. AdaMC consists of more than one composition functions, and adaptively selects them depending on the input vectors. The model learns to embed the semantic categories of words into their corresponding word vectors, and uses them to choose these composition functions adaptively. Specifically, we propose a parametrization method to compute the probability distribution for every function given the child vectors. We also introduce a hyper-parameter to model the adaptive preferences over the different composition functions and show three special cases of AdaMC. By adjusting this hyperparameter, there is a continuous transition between these three special cases. Moreover, all these composition functions and how to select them are automatically learned from Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence", "title": "Adaptive Multi-Compositionality for Recursive Neural Models with Applications to Sentiment Analysis"}, "466cbea4e1e7b1f6ef4d2395936ff771e85fd8a7": {"paper_id": "466cbea4e1e7b1f6ef4d2395936ff771e85fd8a7", "abstract": "Traditional information extraction systems have focused on satisfying precise, narrow, pre-specified requests from small, homogeneous corpora. In contrast, the TextRunner system demonstrates a new kind of information extraction, called Open Information Extraction (OIE), in which the system makes a single, data-driven pass over the entire corpus and extracts a large set of relational tuples, without requiring any human input. (Banko et al., 2007) TextRunner is a fullyimplemented, highly scalable example of OIE. TextRunner\u2019s extractions are indexed, allowing a fast query mechanism. Our first public demonstration of the TextRunner system shows the results of performing OIE on a set of 117 million web pages. It demonstrates the power of TextRunner in terms of the raw number of facts it has extracted, as well as its precision using our novel assessment mechanism. And it shows the ability to automatically determine synonymous relations and objects using large sets of extractions. We have built a fast user interface for querying the results.", "title": "TextRunner: Open Information Extraction on the Web"}, "233d861338cfcd479b1d21897453fcc66418d5e1": {"paper_id": "233d861338cfcd479b1d21897453fcc66418d5e1", "abstract": "Information-extraction (IE) systems seek to distill semantic relations from naturallanguage text, but most systems use supervised learning of relation-specific examples and are thus limited by the availability of training data. Open IE systems such as TextRunner, on the other hand, aim to handle the unbounded number of relations found on the Web. But how well can these open systems perform? This paper presents WOE, an open IE system which improves dramatically on TextRunner\u2019s precision and recall. The key to WOE\u2019s performance is a novel form of self-supervised learning for open extractors \u2014 using heuristic matches between Wikipedia infobox attribute values and corresponding sentences to construct training data. Like TextRunner, WOE\u2019s extractor eschews lexicalized features and handles an unbounded set of semantic relations. WOE can operate in two modes: when restricted to POS tag features, it runs as quickly as TextRunner, but when set to use dependency-parse features its precision and recall rise even higher.", "title": "Open Information Extraction Using Wikipedia"}, "69ad45555c2fda94f5fc8e6064255e90a6bf80f6": {"paper_id": "69ad45555c2fda94f5fc8e6064255e90a6bf80f6", "abstract": null, "title": "Open information extraction from the web"}, "0796f6cd7f0403a854d67d525e9b32af3b277331": {"paper_id": "0796f6cd7f0403a854d67d525e9b32af3b277331", "abstract": "Open Information Extraction (IE) is the task of extracting assertions from massive corpora without requiring a pre-specified vocabulary. This paper shows that the output of state-ofthe-art Open IE systems is rife with uninformative and incoherent extractions. To overcome these problems, we introduce two simple syntactic and lexical constraints on binary relations expressed by verbs. We implemented the constraints in the REVERB Open IE system, which more than doubles the area under the precision-recall curve relative to previous extractors such as TEXTRUNNER and WOE. More than 30% of REVERB\u2019s extractions are at precision 0.8 or higher\u2014 compared to virtually none for earlier systems. The paper concludes with a detailed analysis of REVERB\u2019s errors, suggesting directions for future work.1", "title": "Identifying Relations for Open Information Extraction"}, "48e222ca4046007c58f425cc05aa31dbbe8f540a": {"paper_id": "48e222ca4046007c58f425cc05aa31dbbe8f540a", "abstract": "Traditional Information Extraction (IE) takes a relation name and hand-tagged examples of that relation as input. Open IE is a relationindependent extraction paradigm that is tailored to massive and heterogeneous corpora such as the Web. An Open IE system extracts a diverse set of relational tuples from text without any relation-specific input. How is Open IE possible? We analyze a sample of English sentences to demonstrate that numerous relationships are expressed using a compact set of relation-independent lexico-syntactic patterns, which can be learned by an Open IE system. What are the tradeoffs between Open IE and traditional IE? We consider this question in the context of two tasks. First, when the number of relations is massive, and the relations themselves are not pre-specified, we argue that Open IE is necessary. We then present a new model for Open IE called O-CRF and show that it achieves increased precision and nearly double the recall than the model employed by TEXTRUNNER, the previous stateof-the-art Open IE system. Second, when the number of target relations is small, and their names are known in advance, we show that O-CRF is able to match the precision of a traditional extraction system, though at substantially lower recall. Finally, we show how to combine the two types of systems into a hybrid that achieves higher precision than a traditional extractor, with comparable recall.", "title": "The Tradeoffs Between Open and Traditional Relation Extraction"}, "8f8139b63a2fc0b3ae8413acaef47acd35a356e0": {"paper_id": "8f8139b63a2fc0b3ae8413acaef47acd35a356e0", "abstract": "Modern models of relation extraction for tasks like ACE are based on supervised learning of relations from small hand-labeled corpora. We investigate an alternative paradigm that does not require labeled corpora, avoiding the domain dependence of ACEstyle algorithms, and allowing the use of corpora of any size. Our experiments use Freebase, a large semantic database of several thousand relations, to provide distant supervision. For each pair of entities that appears in some Freebase relation, we find all sentences containing those entities in a large unlabeled corpus and extract textual features to train a relation classifier. Our algorithm combines the advantages of supervised IE (combining 400,000 noisy pattern features in a probabilistic classifier) and unsupervised IE (extracting large numbers of relations from large corpora of any domain). Our model is able to extract 10,000 instances of 102 relations at a precision of 67.6%. We also analyze feature performance, showing that syntactic parse features are particularly helpful for relations that are ambiguous or lexically distant in their expression.", "title": "Distant supervision for relation extraction without labeled data"}, "2d208d551ff9000ca189034fa683edb826f4c941": {"paper_id": "2d208d551ff9000ca189034fa683edb826f4c941", "abstract": "We consider the problem of semi-supervised learning to extract categories (e.g., academic fields, athletes) and relations (e.g., PlaysSport(athlete, sport)) from web pages, starting with a handful of labeled training examples of each category or relation, plus hundreds of millions of unlabeled web documents. Semi-supervised training using only a few labeled examples is typically unreliable because the learning task is underconstrained. This paper pursues the thesis that much greater accuracy can be achieved by further constraining the learning task, by coupling the semi-supervised training of many extractors for different categories and relations. We characterize several ways in which the training of category and relation extractors can be coupled, and present experimental results demonstrating significantly improved accuracy as a result.", "title": "Coupled semi-supervised learning for information extraction"}, "049f4f88fdb42c91796ab93598fc4e23f3cbdebe": {"paper_id": "049f4f88fdb42c91796ab93598fc4e23f3cbdebe", "abstract": "Supervised machine learning algorithms for information extraction generally require large amounts of training data. In many cases where labeling training data is burdensome, there may, however, already exist an incomplete database relevant to the task at hand. Records from this database can be used to label text strings that express the same information. For tasks where text strings do not follow the same format or layout, and additionally may contain extra information, labeling the strings completely may be problematic. This paper presents a method for training extractors which fill in missing labels of a text sequence that is partially labeled using simple high-precision heuristics. Furthermore, we improve the algorithm by utilizing labeled fields from the database. In experiments with BibTeX records and research paper citation strings, we show a significant improvement in extraction accuracy over a baseline that only relies on the database for training data.", "title": "Learning Extractors from Unlabeled Text using Relevant Databases"}, "2c5135a0531bc5ad7dd890f018e67a40529f5bcb": {"paper_id": "2c5135a0531bc5ad7dd890f018e67a40529f5bcb", "abstract": "One of the most important issues in machine learning is whether one can improve the performance of a supervised learning algorithm by including unlabeled data. Methods that use both labeled and unlabeled data are generally referred to as semi-supervised learning. Although a number of such methods are proposed, at the current stage, we still don\u2019t have a complete understanding of their effectiveness. This paper investigates a closely related problem, which leads to a novel approach to semi-supervised learning. Specifically we consider learning predictive structures on hypothesis spaces (that is, what kind of classifiers have good predictive power) from multiple learning tasks. We present a general framework in which the structural learning problem can be formulated and analyzed theoretically, and relate it to learning with unlabeled data. Under this framework, algorithms for structural learning will be proposed, and computational issues will be investigated. Experiments will be given to demonstrate the effectiveness of the proposed algorithms in the semi-supervised learning setting.", "title": "A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data"}, "31274eabb84407e3bc2c1d14b804cb7bcc068111": {"paper_id": "31274eabb84407e3bc2c1d14b804cb7bcc068111", "abstract": "We provide an experimental study of the role of syntactic parsing in semantic role labeling. Our conclusions demonstrate that syntactic parse information is clearly most relevant in the very first stage \u2013 the pruning stage. In addition, the quality of the pruning stage cannot be determined solely based on its recall and precision. Instead it depends on the characteristics of the output candidates that make downstream problems easier or harder. Motivated by this observation, we suggest an effective and simple approach of combining different semantic role labeling systems through joint inference, which significantly improves the performance.", "title": "The Necessity of Syntactic Parsing for Semantic Role Labeling"}, "32a175b36ec7f2f08cb3dfac30ce141e144ec9e9": {"paper_id": "32a175b36ec7f2f08cb3dfac30ce141e144ec9e9", "abstract": "Statistical methods useful in automatic recognition of continuous speech are described. They concern modeling of a speaker and of an acoustic processor, extraction of the models' statistical parameters and hypothesis search procedures and likelihood computations of linguistic decoding. Experimental results are presented that indicate the power of the methods.", "title": "Continuous speech recognition by statistical methods"}, "2abe6b9ea1b13653b7384e9c8ef14b0d87e20cfc": {"paper_id": "2abe6b9ea1b13653b7384e9c8ef14b0d87e20cfc", "abstract": "Reuters Corpus Volume I (RCV1) is an archive of over 800,000 manually categorized newswire stories recently made available by Reuters, Ltd. for research purposes. Use of this data for research on text categorization requires a detailed understanding of the real world constraints under which the data was produced. Drawing on interviews with Reuters personnel and access to Reuters documentation, we describe the coding policy and quality control procedures used in producing the RCV1 data, the intended semantics of the hierarchical category taxonomies, and the corrections necessary to remove errorful data. We refer to the original data as RCV1-v1, and the corrected data as RCV1-v2. We benchmark several widely used supervised learning methods on RCV1-v2, illustrating the collection\u2019s properties, suggesting new directions for research, and providing baseline results for future studies. We make available detailed, per-category experimental results, as well as corrected versions of the category assignments and taxonomy structures, via online appendices.", "title": "RCV1: A New Benchmark Collection for Text Categorization Research"}, "ac7332fcd8895d50a38c49f4ccb3c9f6be0505f3": {"paper_id": "ac7332fcd8895d50a38c49f4ccb3c9f6be0505f3", "abstract": "This paper describes our system for SemEval-2010 Task 8 on multi-way classification of semantic relations between nominals. First, the type of semantic relation is classified. Then a relation typespecific classifier determines the relation direction. Classification is performed using SVM classifiers and a number of features that capture the context, semantic role affiliation, and possible pre-existing relations of the nominals. This approach achieved an F1 score of 82.19% and an accuracy of 77.92%.", "title": "UTD: Classifying Semantic Relations by Combining Lexical and Semantic Resources"}, "d682218db1296b997e977d9140787d0ef932b145": {"paper_id": "d682218db1296b997e977d9140787d0ef932b145", "abstract": "In this study, we provide a direct comparison of the Stochastic Maximum Likelihood algorithm and Contrastive Divergence for training Restricted Boltzmann Machines using the MNIST data set. We demonstrate that Stochastic Maximum Likelihood is superior when using the Restricted Boltzmann Machine as a classifier, and that the algorithm can be greatly improved using the technique of iterate averaging from the field of stochastic approximation. We further show that training with optimal parameters for classification does not necessarily lead to optimal results when Restricted Boltzmann Machines are stacked to form a Deep Belief Network. In our experiments we observe that fine tuning a Deep Belief Network significantly changes the distribution of the latent data, even though the parameter changes are negligible.", "title": "A tutorial on stochastic approximation algorithms for training Restricted Boltzmann Machines and Deep Belief Nets"}, "db073b550a4d3354a72468903eba2d3c88620b8f": {"paper_id": "db073b550a4d3354a72468903eba2d3c88620b8f", "abstract": "This paper proposes a novel composite kernel for relation extraction. The composite kernel consists of two individual kernels: an entity kernel that allows for entity-related features and a convolution parse tree kernel that models syntactic information of relation examples. The motivation of our method is to fully utilize the nice properties of kernel methods to explore diverse knowledge for relation extraction. Our study illustrates that the composite kernel can effectively capture both flat and structured features without the need for extensive feature engineering, and can also easily scale to include more features. Evaluation on the ACE corpus shows that our method outperforms the previous best-reported methods and significantly outperforms previous two dependency tree kernels for relation extraction.", "title": "A Composite Kernel to Extract Relations between Entities with Both Flat and Structured Features"}, "68981d738295a4abf860df5bcab080a96f2d93ee": {"paper_id": "68981d738295a4abf860df5bcab080a96f2d93ee", "abstract": "CONTEXT\nLittle is known about the extent or severity of untreated mental disorders, especially in less-developed countries.\n\n\nOBJECTIVE\nTo estimate prevalence, severity, and treatment of Diagnostic and Statistical Manual of Mental Disorders, Fourth Edition (DSM-IV) mental disorders in 14 countries (6 less developed, 8 developed) in the World Health Organization (WHO) World Mental Health (WMH) Survey Initiative.\n\n\nDESIGN, SETTING, AND PARTICIPANTS\nFace-to-face household surveys of 60 463 community adults conducted from 2001-2003 in 14 countries in the Americas, Europe, the Middle East, Africa, and Asia.\n\n\nMAIN OUTCOME MEASURES\nThe DSM-IV disorders, severity, and treatment were assessed with the WMH version of the WHO Composite International Diagnostic Interview (WMH-CIDI), a fully structured, lay-administered psychiatric diagnostic interview.\n\n\nRESULTS\nThe prevalence of having any WMH-CIDI/DSM-IV disorder in the prior year varied widely, from 4.3% in Shanghai to 26.4% in the United States, with an interquartile range (IQR) of 9.1%-16.9%. Between 33.1% (Colombia) and 80.9% (Nigeria) of 12-month cases were mild (IQR, 40.2%-53.3%). Serious disorders were associated with substantial role disability. Although disorder severity was correlated with probability of treatment in almost all countries, 35.5% to 50.3% of serious cases in developed countries and 76.3% to 85.4% in less-developed countries received no treatment in the 12 months before the interview. Due to the high prevalence of mild and subthreshold cases, the number of those who received treatment far exceeds the number of untreated serious cases in every country.\n\n\nCONCLUSIONS\nReallocation of treatment resources could substantially decrease the problem of unmet need for treatment of mental disorders among serious cases. Structural barriers exist to this reallocation. Careful consideration needs to be given to the value of treating some mild cases, especially those at risk for progressing to more serious disorders.", "title": "Prevalence, severity, and unmet need for treatment of mental disorders in the World Health Organization World Mental Health Surveys."}, "41c8b6c59ec604c5bb6dc90f08b50022ead8fcc7": {"paper_id": "41c8b6c59ec604c5bb6dc90f08b50022ead8fcc7", "abstract": "OBJECTIVES\nGlycyrrhizin is the main water-soluble constituent of the root of liquorice (Glycyrrhiza glabra). The study investigates the effect of glycyrrhizin on streptozotocin (STZ)-induced diabetic changes and associated oxidative stress, including haemoglobin-induced free iron-mediated oxidative reactions.\n\n\nMETHODS\nMale Wistar rats were grouped as normal control, STZ-induced diabetic control, normal treated with glycyrrhizin, diabetic treated with glycyrrhizin and diabetic treated with a standard anti-hyperglycaemic drug, glibenclamide. Different parameters were studied in blood and tissue samples of the rats.\n\n\nKEY FINDINGS\nGlycyrrhizin treatment improved significantly the diabetogenic effects of STZ, namely enhanced blood glucose level, glucose intolerant behaviour, decreased serum insulin level including pancreatic islet cell numbers, increased glycohaemoglobin level and enhanced levels of cholesterol and triglyceride. The treatment significantly reduced diabetes-induced abnormalities of pancreas and kidney tissues. Oxidative stress parameters, namely, serum superoxide dismutase, catalase, malondialdehyde and fructosamine in diabetic rats were reverted to respective normal values after glycyrrhizin administration. Free iron in haemoglobin, iron-mediated free radical reactions and carbonyl formation in haemoglobin were pronounced in diabetes, and were counteracted by glycyrrhizin. Effects of glycyrrhizin and glibenclamide treatments appeared comparable.\n\n\nCONCLUSION\nGlycyrrhizin is quite effective against hyperglycaemia, hyperlipidaemia and associated oxidative stress, and may be a potential therapeutic agent for diabetes treatment.", "title": "Ameliorative effects of glycyrrhizin on streptozotocin-induced diabetes in rats."}, "734a073f60b64e4034c134683ef86904e997507b": {"paper_id": "734a073f60b64e4034c134683ef86904e997507b", "abstract": "The contour-guided color palette (CCP) is proposed for robust image segmentation. It efficiently integrates contour and color cues of an image. To find representative colors of an image, color samples along long contours between regions, similar in spirit to machine learning methodology that focus on samples near decision boundaries, are collected followed by the mean-shift (MS) algorithm in the sampled color space to achieve an image-dependent color palette. This color palette provides a preliminary segmentation in the spatial domain, which is further fine-tuned by post-processing techniques such as leakage avoidance, fake boundary removal, and small region mergence. Segmentation performances of CCP and MS are compared and analyzed. While CCP offers an acceptable standalone segmentation result, it can be further integrated into the framework of layered spectral segmentation to produce a more robust segmentation. The superior performance of CCP-based segmentation algorithm is demonstrated by experiments on the Berkeley Segmentation Dataset.", "title": "Robust Image Segmentation Using Contour-Guided Color Palettes"}, "64b3435826a94ddd269b330e6254579f3244f214": {"paper_id": "64b3435826a94ddd269b330e6254579f3244f214", "abstract": null, "title": "Matrix computations (3. ed.)"}, "0161e4348a7079e9c37434c5af47f6372d4b412d": {"paper_id": "0161e4348a7079e9c37434c5af47f6372d4b412d", "abstract": "We propose a method to identify and localize object classes in images. Instead of operating at the pixel level, we advocate the use of superpixels as the basic unit of a class segmentation or pixel localization scheme. To this end, we construct a classifier on the histogram of local features found in each superpixel. We regularize this classifier by aggregating histograms in the neighborhood of each superpixel and then refine our results further by using the classifier in a conditional random field operating on the superpixel graph. Our proposed method exceeds the previously published state-of-the-art on two challenging datasets: Graz-02 and the PASCAL VOC 2007 Segmentation Challenge.", "title": "Class segmentation and object localization with superpixel neighborhoods"}, "13dd25c5e7df2b23ec9a168a233598702c2afc97": {"paper_id": "13dd25c5e7df2b23ec9a168a233598702c2afc97", "abstract": "This paper addresses the problem of segmenting an image into regions. We define a predicate for measuring the evidence for a boundary between two regions using a graph-based representation of the image. We then develop an efficient segmentation algorithm based on this predicate, and show that although this algorithm makes greedy decisions it produces segmentations that satisfy global properties. We apply the algorithm to image segmentation using two different kinds of local neighborhoods in constructing the graph, and illustrate the results with both real and synthetic images. The algorithm runs in time nearly linear in the number of graph edges and is also fast in practice. An important characteristic of the method is its ability to preserve detail in low-variability image regions while ignoring detail in high-variability regions.", "title": "Efficient Graph-Based Image Segmentation"}, "2dec8923ce85884a72706e9fbd44f36938d492be": {"paper_id": "2dec8923ce85884a72706e9fbd44f36938d492be", "abstract": "In this paper, we present Lazy Snapping, an interactive image cutout tool. Lazy Snapping separates coarse and fine scale processing, making object specification and detailed adjustment easy. Moreover, Lazy Snapping provides instant visual feedback, snapping the cutout contour to the true object boundary efficiently despite the presence of ambiguous or low contrast edges. Instant feedback is made possible by a novel image segmentation algorithm which combines graph cut with pre-computed over-segmentation. A set of intuitive user interface (UI) tools is designed and implemented to provide flexible control and editing for the users. Usability studies indicate that Lazy Snapping provides a better user experience and produces better segmentation results than the state-of-the-art interactive image cutout tool, Magnetic Lasso in Adobe Photoshop.", "title": "Lazy snapping"}, "6f9c2c712131b80c2b26a6f6615131fbeee65c29": {"paper_id": "6f9c2c712131b80c2b26a6f6615131fbeee65c29", "abstract": "A linear programming (LP) method for security dispatch and emergency control calculations on large power systems is presented. The method is reliable, fast, flexible, easy to program, and requires little computer storage. It works directly with the normal power-system variables and limits, and incorporates the usual sparse matrix techniques. An important feature of the method is that it handles multi-segment generator cost curves neatly and efficiently.", "title": "Linear Programming for Power-System Network Security Applications"}, "a155264f143aafd380f40fd0167c9b7960f64ea2": {"paper_id": "a155264f143aafd380f40fd0167c9b7960f64ea2", "abstract": "In this paper, we propose a browser fingerprinting technique that can track users not only within a single browser but also across different browsers on the same machine. Specifically, our approach utilizes many novel OS and hardware level features, such as those from graphics cards, CPU, and installed writing scripts. We extract these features by asking browsers to perform tasks that rely on corresponding OS and hardware functionalities. Our evaluation shows that our approach can successfully identify 99.24% of users as opposed to 90.84% for state of the art on single-browser fingerprinting against the same dataset. Further, our approach can achieve higher uniqueness rate than the only cross-browser approach in the literature with similar stability.", "title": "(Cross-)Browser Fingerprinting via OS and Hardware Level Features"}, "51b0ce84988e083d6253af098542f905e1fea0a8": {"paper_id": "51b0ce84988e083d6253af098542f905e1fea0a8", "abstract": "We describe a web browser fingerprinting technique based on measuring the onscreen dimensions of font glyphs. Font rendering in web browsers is affected by many factors\u2014browser version, what fonts are installed, and hinting and antialiasing settings, to name a few\u2014 that are sources of fingerprintable variation in end-user systems. We show that even the relatively crude tool of measuring glyph bounding boxes can yield a strong fingerprint, and is a threat to users\u2019 privacy. Through a user experiment involving over 1,000 web browsers and an exhaustive survey of the allocated space of Unicode, we find that font metrics are more diverse than User-Agent strings, uniquely identifying 34% of participants, and putting others into smaller anonymity sets. Fingerprinting is easy and takes only milliseconds. We show that of the over 125,000 code points examined, it suffices to test only 43 in order to account for all the variation seen in our experiment. Font metrics, being orthogonal to many other fingerprinting techniques, can augment and sharpen those other techniques. We seek ways for privacy-oriented web browsers to reduce the effectiveness of font metric\u2013based fingerprinting, without unduly harming usability. As part of the same user experiment of 1,000 web browsers, we find that whitelisting a set of standard font files has the potential to more than quadruple the size of anonymity sets on average, and reduce the fraction of users with a unique font fingerprint below 10%. We discuss other potential countermeasures.", "title": "Fingerprinting Web Users Through Font Metrics"}, "fe2f4faec5cf209ae7d8a73100db9cce46ce53d4": {"paper_id": "fe2f4faec5cf209ae7d8a73100db9cce46ce53d4", "abstract": "Worldwide, the number of people and the time spent browsing the web keeps increasing. Accordingly, the technologies to enrich the user experience are evolving at an amazing pace. Many of these evolutions provide for a more interactive web (e.g., boom of JavaScript libraries, weekly innovations in HTML5), a more available web (e.g., explosion of mobile devices), a more secure web (e.g., Flash is disappearing, NPAPI plugins are being deprecated), and a more private web (e.g., increased legislation against cookies, huge success of extensions such as Ghostery and AdBlock). Nevertheless, modern browser technologies, which provide the beauty and power of the web, also provide a darker side, a rich ecosystem of exploitable data that can be used to build unique browser fingerprints. Our work explores the validity of browser fingerprinting in today's environment. Over the past year, we have collected 118,934 fingerprints composed of 17 attributes gathered thanks to the most recent web technologies. We show that innovations in HTML5 provide access to highly discriminating attributes, notably with the use of the Canvas API which relies on multiple layers of the user's system. In addition, we show that browser fingerprinting is as effective on mobile devices as it is on desktops and laptops, albeit for radically different reasons due to their more constrained hardware and software environments. We also evaluate how browser fingerprinting could stop being a threat to user privacy if some technological evolutions continue (e.g., disappearance of plugins) or are embraced by browser vendors (e.g., standard HTTP headers).", "title": "Beauty and the Beast: Diverting Modern Web Browsers to Build Unique Browser Fingerprints"}, "5a032460c589a67e7c73b19c93aa591331758139": {"paper_id": "5a032460c589a67e7c73b19c93aa591331758139", "abstract": "We investigate the degree to which modern web browsers are subject to \u201cdevice fingerprinting\u201d via the version and configuration information that they will transmit to websites upon request. We implemented one possible fingerprinting algorithm, and collected these fingerprints from a large sample of browsers that visited our test side, panopticlick.eff.org. We observe that the distribution of our fingerprint contains at least 18.1 bits of entropy, meaning that if we pick a browser at random, at best we expect that only one in 286,777 other browsers will share its fingerprint. Among browsers that support Flash or Java, the situation is worse, with the average browser carrying at least 18.8 bits of identifying information. 94.2% of browsers with Flash or Java were unique in our sample. By observing returning visitors, we estimate how rapidly browser fingerprints might change over time. In our sample, fingerprints changed quite rapidly, but even a simple heuristic was usually able to guess when a fingerprint was an \u201cupgraded\u201d version of a previously observed browser\u2019s fingerprint, with 99.1% of guesses correct and a false positive rate of only 0.86%. We discuss what privacy threat browser fingerprinting poses in practice, and what countermeasures may be appropriate to prevent it. There is a tradeoff between protection against fingerprintability and certain kinds of debuggability, which in current browsers is weighted heavily against privacy. Paradoxically, anti-fingerprinting privacy technologies can be selfdefeating if they are not used by a sufficient number of people; we show that some privacy measures currently fall victim to this paradox, but others do not.", "title": "How Unique Is Your Web Browser?"}, "1f38c11fe8511c77fb7d383126214c9e7dc28e4a": {"paper_id": "1f38c11fe8511c77fb7d383126214c9e7dc28e4a", "abstract": "To date, many attempts have been made to fingerprint users on the web. These fingerprints allow browsing sessions to be linked together and possibly even tied to a user\u2019s identity. They can be used constructively by sites to supplement traditional means of user authentication such as passwords; and they can be used destructively to counter attempts to stay anonymous online. In this paper, we identify two new avenues for browser fingerprinting. The new fingerprints arise from the browser\u2019s JavaScript execution characteristics, making them difficult to simulate or mitigate in practice. The first uses the innate performance signature of each browser\u2019s JavaScript engine, allowing the detection of browser version, operating system and microarchitecture, even when traditional forms of system identification (such as the user-agent header) are modified or hidden. The second subverts the whitelist mechanism of the popular NoScript Firefox extension, which selectively enables web pages\u2019 scripting privileges to increase privacy by allowing a site to determine if particular domains exist in a user\u2019s NoScript whitelist. We have experimentally verified the effectiveness of our system fingerprinting technique using a 1,015-person study on Amazon\u2019s Mechanical Turk platform.", "title": "Fingerprinting Information in JavaScript Implementations"}, "03667897fd22a31eb96232e14b7e08e26a5b9ad7": {"paper_id": "03667897fd22a31eb96232e14b7e08e26a5b9ad7", "abstract": "We study the security and privacy of private browsing modes recently added to all major browsers. We first propose a clean definition of the goals of private browsing and survey its implementation in different browsers. We conduct a measurement study to determine how often it is used and on what categories of sites. Our results suggest that private browsing is used differently from how it is marketed. We then describe an automated technique for testing the security of private browsing modes and report on a few weaknesses found in the Firefox browser. Finally, we show that many popular browser extensions and plugins undermine the security of private browsing. We propose and experiment with a workable policy that lets users safely run extensions in private browsing mode.", "title": "An Analysis of Private Browsing Modes in Modern Browsers"}, "568c44678d2bba4ae9d735b555e847437a7e6f15": {"paper_id": "568c44678d2bba4ae9d735b555e847437a7e6f15", "abstract": "We present Tor, a circuit-based low-latency anonymous communication service. This second-generation Onion Routing system addresses limitations in the original design. Tor adds perfect forward secrecy, congestion control, directory servers, integrity checking, configurable exit policies, and a practical design for rendezvous points. Tor works on the real-world Internet, requires no special privileges or kernel modifications, requires little synchronization or coordination between nodes, and provides a reasonable tradeoff between anonymity, usability, and efficiency. We briefly describe our experiences with an international network of more than a dozen hosts. We close with a list of open problems in anonymous communication.", "title": "Tor: The Second-Generation Onion Router"}, "15531225a3c801f17ae62631d473691104f13161": {"paper_id": "15531225a3c801f17ae62631d473691104f13161", "abstract": "Like conventional cookies, cache cookies are data objects that servers store in Web browsers. Cache cookies, however, are unintentional byproducts of protocol design for browser caches. They do not enjoy any explicit interface support or security policies. In this paper, we show that despite limitations, cache cookies can play a useful role in the identification and authentication of users. Many users today block conventional cookies in their browsers as a privacy measure. The cache-cookie tools we propose can help restore lost usability and convenience to such users while maintaining good privacy. As we show, our techniques can also help combat online security threats such as phishing and pharming that ordinary cookies cannot. The ideas we introduce for cache-cookie management can strengthen ordinary cookies as well. The full version of this paper may be referenced at www.ravenwhite.com", "title": "Cache cookies for browser authentication"}, "226242629f3d21b9e86afe76b1849048148351de": {"paper_id": "226242629f3d21b9e86afe76b1849048148351de", "abstract": "Timing attacks are usually used to attack weak computing devices such as smartcards. We show that timing attacks apply to general software systems. Specifically, we devise a timing attack against OpenSSL. Our experiments show that we can extract private keys from an OpenSSL-based web server running on a machine in the local network. Our results demonstrate that timing attacks against network servers are practical and therefore security systems should defend against them.", "title": "Remote Timing Attacks Are Practical"}, "05575e85d4b0c09c09552921b2ee0db79e5e9cf9": {"paper_id": "05575e85d4b0c09c09552921b2ee0db79e5e9cf9", "abstract": "Many web services aim to track clients as a basis for analyzing their behavior and providing personalized services. Despite much debate regarding the collection of client information, there have been few quantitative studies that analyze the effectiveness of host-tracking and the associated privacy risks. In this paper, we perform a large-scale study to quantify the amount of information revealed by common host identifiers. We analyze month-long anonymized datasets collected by the Hotmail web-mail service and the Bing search engine, which include millions of hosts across the global IP address space. In this setting, we compare the use of multiple identifiers, including browser information, IP addresses, cookies, and user login IDs. We further demonstrate the privacy and security implications of host-tracking in two contexts. In the first, we study the causes of cookie churn in web services, and show that many returning users can still be tracked even if they clear cookies or utilize private browsing. In the second, we show that host-tracking can be leveraged to improve security. Specifically, by aggregating information across hosts, we uncover a stealthy malicious attack associated with over 75,000 bot accounts that forward cookies to distributed locations. This work was done while Ting-Fang was an intern at Microsoft Research. Mart\u0131\u0301n Abadi is also affiliated with the University of California, Santa Cruz.", "title": "Host Fingerprinting and Tracking on the Web: Privacy and Security Implications"}, "1eeac96f45de0be69c0aa6ed21e48a6345002eef": {"paper_id": "1eeac96f45de0be69c0aa6ed21e48a6345002eef", "abstract": "Advertisers use online customer data to target their marketing appeals. This has heightened consumers\u2019 privacy concerns, leading governments to pass laws designed to protect consumer privacy by restricting the use of data and by restricting online tracking techniques used by websites. We use the responses of 3.3 million survey-takers who had been randomly exposed to 9,596 online display (banner) advertising campaigns to explore how strong privacy regulation in the European Union has influenced advertising effectiveness. We find that display advertising became far less effective at changing stated purchase intent after the laws were enacted relative to other countries. The loss in effectiveness was more pronounced for websites that had general content (such as news sites), where non-data-driven targeting is particularly hard to do. The loss of effectiveness was also more pronounced for ads with a smaller page presence and for ads that did not have additional interactive, video, or audio features. \u2217Avi Goldfarb is Associate Professor of Marketing, Rotman School of Management, University of Toronto, 105 St George St., Toronto, ON. Tel. 416-946-8604. Email: agoldfarb@rotman.utoronto.ca. Catherine Tucker is Assistant Professor of Marketing, MIT Sloan School of Business, 1 Amherst St., E40-167, Cambridge, MA. Tel. 617-252-1499. Email: cetucker@mit.edu. We thank Glen Urban and participants at workshops at IDC, MIT, Michigan, Northwestern, UC Berkeley and Wharton for helpful comments.", "title": "Privacy Regulation and Online Advertising"}, "0d2f693901fba451ede4d388724b0e3f57029cd3": {"paper_id": "0d2f693901fba451ede4d388724b0e3f57029cd3", "abstract": "The web has become an essential part of our society and is currently the main medium of information delivery. Billions of users browse the web on a daily basis, and there are single websites that have reached over one billion user accounts. In this environment, the ability to track users and their online habits can be very lucrative for advertising companies, yet very intrusive for the privacy of users. In this paper, we examine how web-based device fingerprinting currently works on the Internet. By analyzing the code of three popular browser-fingerprinting code providers, we reveal the techniques that allow websites to track users without the need of client-side identifiers. Among these techniques, we show how current commercial fingerprinting approaches use questionable practices, such as the circumvention of HTTP proxies to discover a user's real IP address and the installation of intrusive browser plugins. At the same time, we show how fragile the browser ecosystem is against fingerprinting through the use of novel browser-identifying techniques. With so many different vendors involved in browser development, we demonstrate how one can use diversions in the browsers' implementation to distinguish successfully not only the browser-family, but also specific major and minor versions. Browser extensions that help users spoof the user-agent of their browsers are also evaluated. We show that current commercial approaches can bypass the extensions, and, in addition, take advantage of their shortcomings by using them as additional fingerprinting features.", "title": "Cookieless Monster: Exploring the Ecosystem of Web-Based Device Fingerprinting"}, "0dfc79f36528411a9f1350545ca6f1754b18dfc7": {"paper_id": "0dfc79f36528411a9f1350545ca6f1754b18dfc7", "abstract": "As mobile begins to overtake the fixed Internet access, ad networks have aggressively sought methods to track users on their mobile devices. While existing countermeasures and regulation focus on thwarting cookies and various device IDs, this paper submits a hypothesis that smartphone/tablet accelerometers possess unique fingerprints, which can be exploited for tracking users. We believe that the fingerprints arise from hardware imperfections during the sensor manufacturing process, causing every sensor chip to respond differently to the same motion stimulus. The differences in responses are subtle enough that they do not affect most of the higher level functions computed on them. Nonetheless, upon close inspection, these fingerprints emerge with consistency, and can even be somewhat independent of the stimulus that generates them. Measurements and classification on 80 standalone accelerometer chips, 25 Android phones, and 2 tablets, show precision and recall upward of 96%, along with good robustness to realworld conditions. Utilizing accelerometer fingerprints, a crowdsourcing application running in the cloud could segregate sensor data for each device, making it easy to track a user over space and time. Such attacks are almost trivial to launch, while simple solutions may not be adequate to counteract them.", "title": "AccelPrint: Imperfections of Accelerometers Make Smartphones Trackable"}, "01dbc5466cce6abd567cc5b34a481f5c438fb15a": {"paper_id": "01dbc5466cce6abd567cc5b34a481f5c438fb15a", "abstract": "We present the first large-scale studies of three advanced web tracking mechanisms - canvas fingerprinting, evercookies and use of \"cookie syncing\" in conjunction with evercookies. Canvas fingerprinting, a recently developed form of browser fingerprinting, has not previously been reported in the wild; our results show that over 5% of the top 100,000 websites employ it. We then present the first automated study of evercookies and respawning and the discovery of a new evercookie vector, IndexedDB. Turning to cookie syncing, we present novel techniques for detection and analysing ID flows and we quantify the amplification of privacy-intrusive tracking practices due to cookie syncing.\n Our evaluation of the defensive techniques used by privacy-aware users finds that there exist subtle pitfalls --- such as failing to clear state on multiple browsers at once - in which a single lapse in judgement can shatter privacy defenses. This suggests that even sophisticated users face great difficulties in evading tracking techniques.", "title": "The Web Never Forgets: Persistent Tracking Mechanisms in the Wild"}, "00a8accbc9f8ac29e226a5eec5d11e1c278f7043": {"paper_id": "00a8accbc9f8ac29e226a5eec5d11e1c278f7043", "abstract": "In this paper, we present a critical assessment of the use of device fingerprinting for risk-based authentication in a state-of-practice identity and access management system. Risk-based authentication automatically elevates the level of authentication whenever a particular risk threshold is exceeded. Contemporary identity and access management systems frequently leverage browser-based device fingerprints to recognize trusted devices of a certain individual. We analyzed the variability and the predictability of mobile device fingerprints. Our research shows that particularly for mobile devices the fingerprints carry a lot of similarity, even across models and brands, making them less reliable for risk assessment and step-up authentication.", "title": "Mobile device fingerprinting considered harmful for risk-based authentication"}, "c0d57c8bfe5111bd01ac0b09fec322fc1add8311": {"paper_id": "c0d57c8bfe5111bd01ac0b09fec322fc1add8311", "abstract": "Nanosecond-level clock synchronization can be an enabler of a new spectrum of timingand delay-critical applications in data centers. However, the popular clock synchronization algorithm, NTP, can only achieve millisecond-level accuracy. Current solutions for achieving a synchronization accuracy of 10s-100s of nanoseconds require specially designed hardware throughout the network for combatting random network delays and component noise or to exploit clock synchronization inherent in Ethernet standards for the PHY. In this paper, we present HUYGENS, a software clock synchronization system that uses a synchronization network and leverages three key ideas. First, coded probes identify and reject impure probe data\u2014data captured by probes which suffer queuing delays, random jitter, and NIC timestamp noise. Next, HUYGENS processes the purified data with Support Vector Machines, a widely-used and powerful classifier, to accurately estimate one-way propagation times and achieve clock synchronization to within 100 nanoseconds. Finally, HUYGENS exploits a natural network effect\u2014the idea that a group of pair-wise synchronized clocks must be transitively synchronized\u2014 to detect and correct synchronization errors even further. Through evaluation of two hardware testbeds, we quantify the imprecision of existing clock synchronization across server-pairs, and the effect of temperature on clock speeds. We find the discrepancy between clock frequencies is typically 5-10\u03bcs/sec, but it can be as much as 30\u03bcs/sec. We show that HUYGENS achieves synchronization to within a few 10s of nanoseconds under varying loads, with a negligible overhead upon link bandwidth due to probes. Because HUYGENS is implemented in software running on standard hardware, it can be readily deployed in current data centers.", "title": "Exploiting a Natural Network Effect for Scalable, Fine-grained Clock Synchronization"}, "c20baa16cb57ff4979569871d15294fa720bbc23": {"paper_id": "c20baa16cb57ff4979569871d15294fa720bbc23", "abstract": "Many decades of work have been invested in the area of distributed transactions including protocols such as 2PC, Paxos, and various approaches to quorum. These protocols provide the application programmer a fa\u00e7ade of global serializability. Personally, I have invested a nontrivial portion of my career as a strong advocate for the implementation and use of platforms providing guarantees of global serializability. My experience over the last decade has led me to liken these platforms to the Maginot Line. In general, application developers simply do not implement large scalable applications assuming distributed transactions. When they attempt to use distributed transactions, the projects founder because the performance costs and fragility make them impractical. Natural selection kicks in... 1 The Maginot Line was a huge fortress that ran the length of the Franco-German border and was constructed at great expense between World War I and World War II. It successfully kept the German army from directly crossing the border between France and Germany. It was quickly bypassed by the Germans in 1940 who invaded through Belgium. This article is published under a Creative Commons License Agreement (http://creativecommons.org/licenses/by/2.5/). You may copy, distribute, display, and perform the work, make derivative works and make commercial use of the work, but you must attribute the work to the author and CIDR 2007. 3 Biennial Conference on Innovative DataSystems Research (CIDR) January 7-10, Asilomar, California USA. Instead, applications are built using different techniques which do not provide the same transactional guarantees but still meet the needs of their businesses. This paper explores and names some of the practical approaches used in the implementations of large-scale mission-critical applications in a world which rejects distributed transactions. We discuss the management of fine-grained pieces of application data which may be repartitioned over time as the application grows. We also discuss the design patterns used in sending messages between these repartitionable pieces of data. The reason for starting this discussion is to raise awareness of new patterns for two reasons. First, it is my belief that this awareness can ease the challenges of people hand-crafting very large scalable applications. Second, by observing the patterns, hopefully the industry can work towards the creation of platforms that make it easier to build these very large applications.", "title": "Life beyond Distributed Transactions: an Apostate's Opinion"}, "9748241beb02ef1e2d0e6dc877c04b354033a838": {"paper_id": "9748241beb02ef1e2d0e6dc877c04b354033a838", "abstract": "Many distributed storage systems achieve high data access throughput via partitioning and replication, each system with its own advantages and tradeoffs. In order to achieve high scalability, however, today's systems generally reduce transactional support, disallowing single transactions from spanning multiple partitions. Calvin is a practical transaction scheduling and data replication layer that uses a deterministic ordering guarantee to significantly reduce the normally prohibitive contention costs associated with distributed transactions. Unlike previous deterministic database system prototypes, Calvin supports disk-based storage, scales near-linearly on a cluster of commodity machines, and has no single point of failure. By replicating transaction inputs rather than effects, Calvin is also able to support multiple consistency levels---including Paxos-based strong consistency across geographically distant replicas---at no cost to transactional throughput.", "title": "Calvin: fast distributed transactions for partitioned database systems"}, "327014c96c13c183f1e62c45e25ba4aa30c80dbe": {"paper_id": "327014c96c13c183f1e62c45e25ba4aa30c80dbe", "abstract": "MapReduce advantages over parallel databases include storage-system independence and fine-grain fault tolerance for large jobs.", "title": "MapReduce: a flexible data processing tool"}, "517c5cd1dbafb3cfa0eea4fc78d0b5cd085209b2": {"paper_id": "517c5cd1dbafb3cfa0eea4fc78d0b5cd085209b2", "abstract": "Errors in dynamic random access memory (DRAM) are a common form of hardware failure in modern compute clusters. Failures are costly both in terms of hardware replacement costs and service disruption. While a large body of work exists on DRAM in laboratory conditions, little has been reported on real DRAM failures in large production clusters. In this paper, we analyze measurements of memory errors in a large fleet of commodity servers over a period of 2.5 years. The collected data covers multiple vendors, DRAM capacities and technologies, and comprises many millions of DIMM days.\n The goal of this paper is to answer questions such as the following: How common are memory errors in practice? What are their statistical properties? How are they affected by external factors, such as temperature and utilization, and by chip-specific factors, such as chip density, memory technology and DIMM age?\n We find that DRAM error behavior in the field differs in many key aspects from commonly held assumptions. For example, we observe DRAM error rates that are orders of magnitude higher than previously reported, with 25,000 to 70,000 errors per billion device hours per Mbit and more than 8% of DIMMs affected by errors per year. We provide strong evidence that memory errors are dominated by hard errors, rather than soft errors, which previous work suspects to be the dominant error mode. We find that temperature, known to strongly impact DIMM error rates in lab conditions, has a surprisingly small effect on error behavior in the field, when taking all other factors into account. Finally, unlike commonly feared, we don't observe any indication that newer generations of DIMMs have worse error behavior.", "title": "DRAM errors in the wild: a large-scale field study"}, "7b9f0330d698580902d4ecd65b0dc1d86d78d345": {"paper_id": "7b9f0330d698580902d4ecd65b0dc1d86d78d345", "abstract": "We describe PNUTS, a massively parallel and geographically distributed database system for Yahoo!\u2019s web applications. PNUTS provides data storage organized as hashed or ordered tables, low latency for large numbers of concurrent requests including updates and queries, and novel per-record consistency guarantees. It is a hosted, centrally managed, and geographically distributed service, and utilizes automated load-balancing and failover to reduce operational complexity. The first version of the system is currently serving in production. We describe the motivation for PNUTS and the design and implementation of its table storage and replication layers, and then present experimental results.", "title": "PNUTS: Yahoo!'s hosted data serving platform"}, "09d1a6f5a50a8c3e066fb05a8833bc00663ada0e": {"paper_id": "09d1a6f5a50a8c3e066fb05a8833bc00663ada0e", "abstract": "Reliability at massive scale is one of the biggest challenges we face at Amazon.com, one of the largest e-commerce operations in the world; even the slightest outage has significant financial consequences and impacts customer trust. The Amazon.com platform, which provides services for many web sites worldwide, is implemented on top of an infrastructure of tens of thousands of servers and network components located in many datacenters around the world. At this scale, small and large components fail continuously and the way persistent state is managed in the face of these failures drives the reliability and scalability of the software systems.\n This paper presents the design and implementation of Dynamo, a highly available key-value storage system that some of Amazon's core services use to provide an \"always-on\" experience. To achieve this level of availability, Dynamo sacrifices consistency under certain failure scenarios. It makes extensive use of object versioning and application-assisted conflict resolution in a manner that provides a novel interface for developers to use.", "title": "Dynamo: amazon's highly available key-value store"}, "136eefe33796c388a15d25ca03cb8d5077d14f37": {"paper_id": "136eefe33796c388a15d25ca03cb8d5077d14f37", "abstract": "In previous papers [SC05, SBC+07], some of us predicted the end of \u201cone size fits all\u201d as a commercial relational DBMS paradigm. These papers presented reasons and experimental evidence that showed that the major RDBMS vendors can be outperformed by 1-2 orders of magnitude by specialized engines in the data warehouse, stream processing, text, and scientific database markets. Assuming that specialized engines dominate these markets over time, the current relational DBMS code lines will be left with the business data processing (OLTP) market and hybrid markets where more than one kind of capability is required. In this paper we show that current RDBMSs can be beaten by nearly two orders of magnitude in the OLTP market as well. The experimental evidence comes from comparing a new OLTP prototype, H-Store, which we have built at M.I.T., to a popular RDBMS on the standard transactional benchmark, TPC-C. We conclude that the current RDBMS code lines, while attempting to be a \u201cone size fits all\u201d solution, in fact, excel at nothing. Hence, they are 25 year old legacy code lines that should be retired in favor of a collection of \u201cfrom scratch\u201d specialized engines. The DBMS vendors (and the research community) should start with a clean sheet of paper and design systems for tomorrow\u2019s requirements, not continue to push code lines and architectures designed for yesterday\u2019s needs.", "title": "The End of an Architectural Era (It's Time for a Complete Rewrite)"}, "02cbb22e2011938d8d2c0a42b175e96d59bb377f": {"paper_id": "02cbb22e2011938d8d2c0a42b175e96d59bb377f", "abstract": "Cloud Computing, the long-held dream of computing as a utility, has the potential to transform a large part of the IT industry, making software even more attractive as a service and shaping the way IT hardware is designed and purchased. Developers with innovative ideas for new Internet services no longer require the large capital outlays in hardware to deploy their service or the human expense to operate it. They need not be concerned about overprovisioning for a service whose popularity does not meet their predictions, thus wasting costly resources, or underprovisioning for one that becomes wildly popular, thus missing potential customers and revenue. Moreover, companies with large batch-oriented tasks can get results as quickly as their programs can scale, since using 1000 servers for one hour costs no more than using one server for 1000 hours. This elasticity of resources, without paying a premium for large scale, is unprecedented in the history of IT. Cloud Computing refers to both the applications delivered as services over the Internet and the hardware and systems software in the datacenters that provide those services. The services themselves have long been referred to as Software as a Service (SaaS). The datacenter hardware and software is what we will call a Cloud. When a Cloud is made available in a pay-as-you-go manner to the general public, we call it a Public Cloud; the service being sold is Utility Computing. We use the term Private Cloud to refer to internal datacenters of a business or other organization, not made available to the general public. Thus, Cloud Computing is the sum of SaaS and Utility Computing, but does not include Private Clouds. People can be users or providers of SaaS, or users or providers of Utility Computing. We focus on SaaS Providers (Cloud Users) and Cloud Providers, which have received less attention than SaaS Users. From a hardware point of view, three aspects are new in Cloud Computing.", "title": "Above the Clouds: A Berkeley View of Cloud Computing"}, "7b8dd47c5044218b897d0d09eeb1f43b83bb245a": {"paper_id": "7b8dd47c5044218b897d0d09eeb1f43b83bb245a", "abstract": "In fact, nowadays there are sophisticated computer sciences capable of identifying and prevent the academic desertion by means of using predictive algorithms which allow being aware about the requirements and needs of the students. In this sense, the present paper proposes a new methodology to reduce the students desertion using the advantages of the data mining theory jointly with advanced artificial intelligence techniques such that, fuzzy logic. Such methodology aims to detect in a suitable moment, when a student tends to leave their studies. The design and some relative works are detailed in order to emphasize the relevance of this promising research area. To the end, some conclusions and a discussion is presented to propose news paths for the implementation of the proposed methodology and future work. .", "title": "A Framework to avoid Scholar Desertion using Artificial Intelligence"}, "c9c2de3628be7e249722b12911bebad84b567ce6": {"paper_id": "c9c2de3628be7e249722b12911bebad84b567ce6", "abstract": "Face analysis in images in the wild still pose a challenge for automatic age and gender recognition tasks, mainly due to their high variability in resolution, deformation, and occlusion. Although the performance has highly increased thanks to Convolutional Neural Networks (CNNs), it is still far from optimal when compared to other image recognition tasks, mainly because of the high sensitiveness of CNNs to facial variations. In this paper, inspired by biology and the recent success of attention mechanisms on visual question answering and fine-grained recognition, we propose a novel feedforward attention mechanism that is able to discover the most informative and reliable parts of a given face for improving age and gender classification. In particular, given a downsampled facial image, the proposed model is trained based on a novel end-to-end learning framework to extract the most discriminative patches from the original high-resolution image. Experimental validation on the standard Adience, Images of Groups, and MORPH II benchmarks show Preprint submitted to Pattern Recognition June 30, 2017", "title": "Age and gender recognition in the wild with deep attention"}, "36119c10f75094e0568cae8256400c94546d973b": {"paper_id": "36119c10f75094e0568cae8256400c94546d973b", "abstract": "In recent years, heterogeneous face biometrics has attracted more attentions in the face recognition community. After published in 2009, the HFB database has been applied by tens of research groups and widely used for Near infrared vs. Visible light (NIR-VIS) face recognition. Despite its success the HFB database has two disadvantages: a limited number of subjects, lacking specific evaluation protocols. To address these issues we collected the NIR-VIS 2.0 database. It contains 725 subjects, imaged by VIS and NIR cameras in four recording sessions. Because the 3D modality in the HFB database was less used in the literature, we don't consider it in the current version. In this paper, we describe the composition of the database, evaluation protocols and present the baseline performance of PCA on the database. Moreover, two interesting tricks, the facial symmetry and heterogeneous component analysis (HCA) are also introduced to improve the performance.", "title": "The CASIA NIR-VIS 2.0 Face Database"}, "5a5f0287484f0d480fed1ce585dbf729586f0edc": {"paper_id": "5a5f0287484f0d480fed1ce585dbf729586f0edc", "abstract": "Access to well-labeled recordings of facial expression is critical to progress in automated facial expression recognition. With few exceptions, publicly available databases are limited to posed facial behavior that can differ markedly in conformation, intensity, and timing from what occurs spontaneously. To meet the need for publicly available corpora of well-labeled video, we collected, ground-truthed, and prepared for distribution the Denver intensity of spontaneous facial action database. Twenty-seven young adults were video recorded by a stereo camera while they viewed video clips intended to elicit spontaneous emotion expression. Each video frame was manually coded for presence, absence, and intensity of facial action units according to the facial action unit coding system. Action units are the smallest visibly discriminable changes in facial action; they may occur individually and in combinations to comprise more molar facial expressions. To provide a baseline for use in future research, protocols and benchmarks for automated action unit intensity measurement are reported. Details are given for accessing the database for research in computer vision, machine learning, and affective and behavioral science.", "title": "DISFA: A Spontaneous Facial Action Intensity Database"}, "614a7c42aae8946c7ad4c36b53290860f6256441": {"paper_id": "614a7c42aae8946c7ad4c36b53290860f6256441", "abstract": "Face detection and alignment in unconstrained environment are challenging due to various poses, illuminations, and occlusions. Recent studies show that deep learning approaches can achieve impressive performance on these two tasks. In this letter, we propose a deep cascaded multitask framework that exploits the inherent correlation between detection and alignment to boost up their performance. In particular, our framework leverages a cascaded architecture with three stages of carefully designed deep convolutional networks to predict face and landmark location in a coarse-to-fine manner. In addition, we propose a new online hard sample mining strategy that further improves the performance in practice. Our method achieves superior accuracy over the state-of-the-art techniques on the challenging face detection dataset and benchmark and WIDER FACE benchmarks for face detection, and annotated facial landmarks in the wild benchmark for face alignment, while keeps real-time performance.", "title": "Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks"}, "853bd61bc48a431b9b1c7cab10c603830c488e39": {"paper_id": "853bd61bc48a431b9b1c7cab10c603830c488e39", "abstract": "Pushing by big data and deep convolutional neural network (CNN), the performance of face recognition is becoming comparable to human. Using private large scale training datasets, several groups achieve very high performance on LFW, i.e., 97% to 99%. While there are many open source implementations of CNN, none of large scale face dataset is publicly available. The current situation in the field of face recognition is that data is more important than algorithm. To solve this problem, this paper proposes a semi-automatical way to collect face images from Internet and builds a large scale dataset containing about 10,000 subjects and 500,000 images, called CASIAWebFace. Based on the database, we use a 11-layer CNN to learn discriminative representation and obtain state-of-theart accuracy on LFW and YTF. The publication of CASIAWebFace will attract more research groups entering this field and accelerate the development of face recognition in the wild.", "title": "Learning Face Representation from Scratch"}, "13e415ed39f406f1a7c687cb55b6129b1c40ddd5": {"paper_id": "13e415ed39f406f1a7c687cb55b6129b1c40ddd5", "abstract": "Visual processing in cortex is classically modeled as a hierarchy of increasingly sophisticated representations, naturally extending the model of simple to complex cells of Hubel and Wiesel. Surprisingly, little quantitative modeling has been done to explore the biological feasibility of this class of models to explain aspects of higher-level visual processing such as object recognition. We describe a new hierarchical model consistent with physiological data from inferotemporal cortex that accounts for this complex visual task and makes testable predictions. The model is based on a MAX-like operation applied to inputs to certain cortical neurons that may have a general role in cortical function.", "title": "Hierarchical models of object recognition in cortex"}, "213d7af7107fa4921eb0adea82c9f711fd105232": {"paper_id": "213d7af7107fa4921eb0adea82c9f711fd105232", "abstract": "High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such \"autoencoder\" networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.", "title": "Reducing the dimensionality of data with neural networks."}, "29f92d6d4e7acf83a7ae928f796aec442f9a6857": {"paper_id": "29f92d6d4e7acf83a7ae928f796aec442f9a6857", "abstract": "The merging of audiences in social media and the variety of participation structures they present, including different audience sizes and interaction targets, pose questions about how people respond to these new communication situations. This research examined self-presentational and relational concerns through the analysis of language styles on Facebook. The authors collected a corpus of status updates, wall posts, and private messages from 79 participants. These messages varied in certain characteristics of language style, revealing differences in underlying self-presentational and relational concerns based on the publicness and directedness of the interaction. Positive emotion words correlated with self-reported self-presentational concerns in status updates, suggesting a strategic use of sharing positive emotions in public and nondirected communication via status updates. Verbal immediacy correlated with partner familiarity in wall posts but not in private messages, suggesting that verbal immediacy cues serve as markers to differentiate between more and less familiar partners in public wall posts.", "title": "Analysis of Language Style Self-Presentational and Relational Concerns Revealed Through the Managing Impressions and Relationships on Facebook :"}, "2d2b1f9446e9b4cdb46327cda32a8d9621944e29": {"paper_id": "2d2b1f9446e9b4cdb46327cda32a8d9621944e29", "abstract": "Participation in social networking sites has dramatically increased in recent years. Services such as Friendster, Tribe, or the Facebook allow millions of individuals to create online profiles and share personal information with vast networks of friends - and, often, unknown numbers of strangers. In this paper we study patterns of information revelation in online social networks and their privacy implications. We analyze the online behavior of more than 4,000 Carnegie Mellon University students who have joined a popular social networking site catered to colleges. We evaluate the amount of information they disclose and study their usage of the site's privacy settings. We highlight potential attacks on various aspects of their privacy, and we show that only a minimal percentage of users changes the highly permeable privacy preferences.", "title": "Information revelation and privacy in online social networks"}, "2311a88803728111ba7bdb327b127ec3f54d282a": {"paper_id": "2311a88803728111ba7bdb327b127ec3f54d282a", "abstract": "Social resources like trust and shared identity make it easier for people to work and play together. Such social resources are sometimes referred to as social capital. Thirty years ago, Americans built social capital as a side effect of participation in civic organizations and social activities, including bowling leagues. Today, they do so far less frequently (Putnam 2000). HCI researchers and practitioners need to find new ways for people to interact that will generate even more social capital than bowling together does. A new theoretical construct, SocioTechnical Capital, provides a framework for generating and evaluating technology-mediated social relations.", "title": "Beyond Bowling Together : SocioTechnical Capital"}, "406214cb3da048255c7d4c167d2d64dc2455abc9": {"paper_id": "406214cb3da048255c7d4c167d2d64dc2455abc9", "abstract": "Your use of the JSTOR archive indicates your acceptance of JSTOR's Terms and Conditions of Use, available at http://dv1litvip.jstor.org/page/info/about/policies/terms.jsp. JSTOR's Terms and Conditions of Use provides, in part, that unless you have obtained prior permission, you may not download an entire issue of a journal or multiple copies of articles, and you may use content in the JSTOR archive only for your personal, non-commercial use.", "title": "Social capital : Prospects for a new concept"}, "e6228e0454a00117965c5ed884173531a9246189": {"paper_id": "e6228e0454a00117965c5ed884173531a9246189", "abstract": "Large numbers of college students have become avid Facebook users in a short period of time. In this paper, we explore whether these students are using Facebook to find new people in their offline communities or to learn more about people they initially meet offline. Our data suggest that users are largely employing Facebook to learn more about people they meet offline, and are less likely to use the site to initiate new connections.", "title": "A face(book) in the crowd: social Searching vs. social browsing"}, "4c9b69c992700cf963682c42a207457647f98012": {"paper_id": "4c9b69c992700cf963682c42a207457647f98012", "abstract": "The Internet is the latest in a series of technological breakthroughs in interpersonal communication, following the telegraph, telephone, radio, and television. It combines innovative features of its predecessors, such as bridging great distances and reaching a mass audience. However, the Internet has novel features as well, most critically the relative anonymity afforded to users and the provision of group venues in which to meet others with similar interests and values. We place the Internet in its historical context, and then examine the effects of Internet use on the user's psychological well-being, the formation and maintenance of personal relationships, group memberships and social identity, the workplace, and community involvement. The evidence suggests that while these effects are largely dependent on the particular goals that users bring to the interaction-such as self-expression, affiliation, or competition-they also interact in important ways with the unique qualities of the Internet communication situation.", "title": "The internet and social life."}, "461ac81b6ce10d48a6c342e64c59f86d7566fa68": {"paper_id": "461ac81b6ce10d48a6c342e64c59f86d7566fa68", "abstract": "This publication contains reprint articles for which IEEE does not hold copyright. Full text is not available on IEEE Xplore for these articles.", "title": "Social network sites: definition, history, and scholarship"}, "7c7bd6aac3aea47fb817673a096784cd9a1b3e0c": {"paper_id": "7c7bd6aac3aea47fb817673a096784cd9a1b3e0c", "abstract": "This study examines if Facebook, one of the most popular social network sites among college students in the U.S., is related to attitudes and behaviors that enhance individuals\u2019 social capital. Using data from a random web survey of college students across Texas (n = 2, 603), we find positive relationships between intensity of Facebook use and students\u2019 life satisfaction, social trust, civic engagement, and political participation. While these findings should ease the concerns of those who fear that Facebook has mostly negative effects on young adults, the positive and significant associations between Facebook variables and social capital were small, suggesting that online social networks are not the most effective solution for youth disengagement from civic duty and democracy.", "title": "Is There Social Capital in a Social Network Site?: Facebook Use and College Students' Life Satisfaction, Trust, and Participation"}, "10efde0973c9b8221202bacfcdb79a77e1a47fa0": {"paper_id": "10efde0973c9b8221202bacfcdb79a77e1a47fa0", "abstract": "The Internet could change the lives of average citizens as much as did the telephone in the early part of the 20th century and television in the 1950s and 1960s. Researchers and social critics are debating whether the Internet is improving or harming participation in community life and social relationships. This research examined the social and psychological impact of the Internet on 169 people in 73 households during their first 1 to 2 years on-line. We used longitudinal data to examine the effects of the Internet on social involvement and psychological well-being. In this sample, the Internet was used extensively for communication. Nonetheless, greater use of the Internet was associated with declines in participants' communication with family members in the household, declines in the size of their social circle, and increases in their depression and loneliness. These findings have implications for research, for public policy and for the design of technology.", "title": "Internet paradox. A social technology that reduces social involvement and psychological well-being?"}, "0b33d8210d530fad72ce20bd6565ceaed792cbc0": {"paper_id": "0b33d8210d530fad72ce20bd6565ceaed792cbc0", "abstract": "As more people connect to the Internet, researchers are beginning to examine the effects of Internet use on users' psychological health. Due in part to a study released by Kraut and colleagues in 1998, which concluded that Internet use is positively correlated with depression, loneliness, and stress, public opinion about the Internet has been decidedly negative. In contrast, the present study was designed to test the hypothesis that Internet usage can affect users beneficially. Participants engaged in five chat sessions with an anonymous partner. At three different intervals they were administered scales measuring depression, loneliness, self-esteem, and social support. Changes in their scores were tracked over time. Internet use was found to decrease loneliness and depression significantly, while perceived social support and self-esteem increased significantly.", "title": "In Defense of the Internet: The Relationship between Internet Communication and Depression, Loneliness, Self-Esteem, and Perceived Social Support"}, "682aff80095f4df368e3839ad7895dc77786bdc7": {"paper_id": "682aff80095f4df368e3839ad7895dc77786bdc7", "abstract": "Social media treats all users the same: trusted friend or total stranger, with little or nothing in between. In reality, relationships fall everywhere along this spectrum, a topic social science has investigated for decades under the theme of tie strength. Our work bridges this gap between theory and practice. In this paper, we present a predictive model that maps social media data to tie strength. The model builds on a dataset of over 2,000 social media ties and performs quite well, distinguishing between strong and weak ties with over 85% accuracy. We complement these quantitative findings with interviews that unpack the relationships we could not predict. The paper concludes by illustrating how modeling tie strength can improve social media design elements, including privacy controls, message routing, friend introductions and information prioritization.", "title": "Predicting tie strength with social media"}, "15667225cbad7ddb595a22024b49b9ce63f0e474": {"paper_id": "15667225cbad7ddb595a22024b49b9ce63f0e474", "abstract": "A robotic hand may change its grasp status and relocate some of its fingers in order to perform a large scale manipulation. Such a strategy is called a finger gait. In this paper, a randomized manipulation planning algorithm is proposed to solving the finger gait planning problem. One of the most used finger gaiting primitives, finger substitution, is introduced. Because of its discrete-continuous characteristics, the kinematics model of a finger substitution is formulated into a hybrid automaton. Considering the discrete and continuous topology of the automaton, both the discrete metric and continuous metric are defined on the state space. An improved RRT based planner is proposed to find a feasible finger substitution. Finally, simulation results verify the validity of the proposed finger gait planner.", "title": "Finger gaits planning for multifingered manipulation"}, "f5c4b4281bb81d76658b965ffcdd3458efb917c5": {"paper_id": "f5c4b4281bb81d76658b965ffcdd3458efb917c5", "abstract": "We measure the size of the fiscal multiplier using a model with incomplete markets and rigid prices and wages. Allowing for incomplete markets instead of complete markets\u2014the prevalent assumption in the literature\u2014comes with two advantages. First, the incomplete markets model delivers a realistic distribution of the marginal propensity to consume across the population, whereas all households counterfactually behave according to the permanent income hypothesis if markets are complete. Second, in our model the equilibrium response of prices, output, consumption and employment to fiscal stimulus is uniquely determined for any monetary policy including the zero-lower bound. We find that market incompleteness plays the key role in determining the size of the fiscal multiplier, which is slightly above or below 1 depending on whether spending is tax or deficit financed. The size of fiscal multiplier remains similar in a liquidity trap.", "title": "The Fiscal Multiplier \u2217"}, "8e6681c2307b9b875ea580b89b94b405aa63e78e": {"paper_id": "8e6681c2307b9b875ea580b89b94b405aa63e78e", "abstract": "Parallel machine learning workloads have become prevalent in numerous application domains. Many of these workloads are iterative convergent, allowing different threads to compute in an asynchronous manner, relaxing certain read-after-write data dependencies to use stale values. While considerable effort has been devoted to reducing the communication latency between nodes by utilizing asynchronous parallelism, inefficient utilization of relaxed consistency models within a single node have caused parallel implementations to have low execution efficiency. The long latency and serialization caused by atomic operations have a significant impact on performance. The data communication is not overlapped with the main computation, which reduces execution efficiency. The inefficiency comes from the data movement between where they are stored and where they are processed. In this work, we propose Bounded Staled Sync (BSSync), a hardware support for the bounded staleness consistency model, which accompanies simple logic layers in the memory hierarchy. BSSync overlaps the long latency atomic operation with the main computation, targeting iterative convergent machine learning workloads. Compared to previous work that allows staleness for read operations, BSSync utilizes staleness for write operations, allowing stale-writes. We demonstrate the benefit of the proposed scheme for representative machine learning workloads. On average, our approach outperforms the baseline asynchronous parallel implementation by 1.33x times.", "title": "BSSync: Processing Near Memory for Machine Learning Workloads with Bounded Staleness Consistency Models"}, "49ed15db181c74c7067ec01800fb5392411c868c": {"paper_id": "49ed15db181c74c7067ec01800fb5392411c868c", "abstract": "When a database is replicated at many sites, maintaining mutual consistency among the sites in the face of updates is a significant problem. This paper describes several randomized algorithms for distributing updates and driving the replicas toward consistency. The algorithms are very simple and require few guarantees from the underlying communication system, yet they ensure that the effect of every update is eventually reflected in all replicas. The cost and performance of the algorithms are tuned by choosing appropriate distributions in the randomization step. The algorithms are closely analogous to epidemics, and the epidemiology literature aids in understanding their behavior. One of the algorithms has been implemented in the Clearinghouse servers of the Xerox Corporate Internet, solving long-standing problems of high traffic and database inconsistency. An earlier version of this paper appeared in the Proceedings of the Sixth Annual ACM Symposium on Principles of Distributed Computing, Vancouver, August 1987, pages 1-12. CR", "title": "Epidemic Algorithms for Replicated Database Maintenance"}, "4db38057a8f7d310b08b6c8c666fa5a20be7f6b6": {"paper_id": "4db38057a8f7d310b08b6c8c666fa5a20be7f6b6", "abstract": "The paper investigates parameterized approximate message-passing schemes that are based on bounded inference and are inspired by Pearl's belief propagation algorithm (BP). We start with the bounded inference mini-clustering algorithm and then move to the iterative scheme called Iterative Join-Graph Propagation (IJGP), that combines both iteration and bounded inference. Algorithm IJGP belongs to the class of Generalized Belief Propagation algorithms, a framework that allowed connections with approximate algorithms from statistical physics and is shown empirically to surpass the performance of mini-clustering and belief propagation, as well as a number of other state-of-the-art algorithms on several classes of networks. We also provide insight into the accuracy of iterative BP and IJGP by relating these algorithms to well known classes of constraint propagation schemes.", "title": "Join-Graph Propagation Algorithms"}, "19908640236767427ebf0524dc3a4bb09d65145e": {"paper_id": "19908640236767427ebf0524dc3a4bb09d65145e", "abstract": "Recently, researchers have demonstrated that \"loopy belief propagation\" the use of Pearl's polytree algorithm in a Bayesian network with loops can perform well in the context of error-correcting codes. The most dramatic instance of this is the near Shannon-limit performance of \"Turbo Codes\" codes whose decoding algorithm is equivalent to loopy belief propagation in a chain-structured Bayesian network. In this paper we ask: is there something spe\u00ad cial about the error-correcting code context, or does loopy propagation work as an ap\u00ad proximate inference scheme in a more gen\u00ad eral setting? We compare the marginals com\u00ad puted using loopy propagation to the exact ones in four Bayesian network architectures, including two real-world networks: ALARM and QMR. We find that the loopy beliefs of\u00ad ten converge and when they do, they give a good approximation to the correct marginals. However, on the QMR network, the loopy be\u00ad liefs oscillated and had no obvious relation\u00ad ship to the correct posteriors. We present some initial investigations into the cause of these oscillations, and show that some sim\u00ad ple methods of preventing them lead to the wrong results.", "title": "Loopy Belief Propagation for Approximate Inference: An Empirical Study"}, "d5cec31594f7d5330ce55206d56668864e1cb80a": {"paper_id": "d5cec31594f7d5330ce55206d56668864e1cb80a", "abstract": "This study investigates the teaching of vocabulary in ESP courses within the paradigm of task-based language teaching, concentrating on Persian literature students at Birjand University in Iran. Two homogenous groups of students who were taking their ESP courses participated in the study as a control and an experimental group. A teacher-made test of technical vocabulary knowledge was administered as the pre-test. Vocabularies in the control group were taught using a traditional approach, whereas in the experimental group, technical vocabularies were taught on the basis of task-based approach. At the end of the semester, a post-test was given to the students to determine the influence of the treatment on the experimental group. Data analysis showed that the task-based approach was more effective in teaching technical vocabularies compared to the traditional one. Furthermore, the results showed that in the experimental group the male learners outperformed the female learners.", "title": "The Impact of Task-based Approach on Vocabulary Learning in ESP Courses"}, "bb489e4de6f9b835d70ab46217f11e32887931a2": {"paper_id": "bb489e4de6f9b835d70ab46217f11e32887931a2", "abstract": "Deep Learning methods are currently the state-of-the-art in many Computer Vision and Image Processing problems, in particular image classification. After years of intensive investigation, a few models matured and became important tools, including Convolutional Neural Networks (CNNs), Siamese and Triplet Networks, Auto-Encoders (AEs) and Generative Adversarial Networks (GANs). The field is fast-paced and there is a lot of terminologies to catch up for those who want to adventure in Deep Learning waters. This paper has the objective to introduce the most fundamental concepts of Deep Learning for Computer Vision in particular CNNs, AEs and GANs, including architectures, inner workings and optimization. We offer an updated description of the theoretical and practical knowledge of working with those models. After that, we describe Siamese and Triplet Networks, not often covered in tutorial papers, as well as review the literature on recent and exciting topics such as visual stylization, pixel-wise prediction and video processing. Finally, we discuss the limitations of Deep Learning for Computer Vision.", "title": "Everything You Wanted to Know about Deep Learning for Computer Vision but Were Afraid to Ask"}, "389da5d7b37915cf06dc5821ca02e4e412d18a3b": {"paper_id": "389da5d7b37915cf06dc5821ca02e4e412d18a3b", "abstract": "Accumulating neuropsychological, electrophysiological and behavioural evidence suggests that the neural substrates of visual perception may be quite distinct from those underlying the visual control of actions. In other words, the set of object descriptions that permit identification and recognition may be computed independently of the set of descriptions that allow an observer to shape the hand appropriately to pick up an object. We propose that the ventral stream of projections from the striate cortex to the inferotemporal cortex plays the major role in the perceptual identification of objects, while the dorsal stream projecting from the striate cortex to the posterior parietal region mediates the required sensorimotor transformations for visually guided actions directed at such objects.", "title": "Separate visual pathways for perception and action"}, "28bfb9f0e16cb66ba4422bee5902e79c5e89e765": {"paper_id": "28bfb9f0e16cb66ba4422bee5902e79c5e89e765", "abstract": "Detecting humans in films and videos is a challenging problem owing to the motion of the subjects, the camera and the background and to variations in pose, appearance, clothing, illumination and background clutter. We develop a detector for standing and moving people in videos with possibly moving cameras and backgrounds, testing several different motion coding schemes and showing empirically that orientated histograms of differential optical flow give the best overall performance. These motion-based descriptors are combined with our Histogram of Oriented Gradient appearance descriptors. The resulting detector is tested on several databases including a challenging test set taken from feature films and containing wide ranges of pose, motion and background variations, including moving cameras and backgrounds. We validate our results on two challenging test sets containing more than 4400 human examples. The combined detector reduces the false alarm rate by a factor of 10 relative to the best appearance-based detector, for example giving false alarm rates of 1 per 20,000 windows tested at 8% miss rate on our Test Set 1.", "title": "Human Detection Using Oriented Histograms of Flow and Appearance"}, "970b4d2ed1249af97cdf2fffdc7b4beae458db89": {"paper_id": "970b4d2ed1249af97cdf2fffdc7b4beae458db89", "abstract": "With nearly one billion online videos viewed everyday, an emerging new frontier in computer vision research is recognition and search in video. While much effort has been devoted to the collection and annotation of large scalable static image datasets containing thousands of image categories, human action datasets lag far behind. Current action recognition databases contain on the order of ten different action categories collected under fairly controlled conditions. State-of-the-art performance on these datasets is now near ceiling and thus there is a need for the design and creation of new benchmarks. To address this issue we collected the largest action video database to-date with 51 action categories, which in total contain around 7,000 manually annotated clips extracted from a variety of sources ranging from digitized movies to YouTube. We use this database to evaluate the performance of two representative computer vision systems for action recognition and explore the robustness of these methods under various conditions such as camera motion, viewpoint, video quality and occlusion.", "title": "HMDB: A large video database for human motion recognition"}, "18e2202311580318b91f2b4e6073c3afb3db8d7b": {"paper_id": "18e2202311580318b91f2b4e6073c3afb3db8d7b", "abstract": "Local space-time features have recently become a popular video representation for action recognition. Several methods for feature localization and description have been proposed in the literature, and promising recognition results were demonstrated for different action datasets. The comparison of those methods, however, is limited given the different experimental settings and various recognition methods used. The purpose of this paper is first to define a common evaluation setup to compare local space-time detectors and descriptors. All experiments are reported for the same bag-of-features SVM recognition framework. Second, we provide a systematic evaluation of different spatio-temporal features. We evaluate the performance of several space-time interest point detectors and descriptors along with their combinations on datasets with varying degree of difficulty. We also include a comparison with dense features obtained by regular sampling of local space-time patches. Feature detectors. In our experimental evaluation, we consider the following feature detectors. (1) The Harris3D detector [3] extends the Harris detector for images to image sequences. At each video point, a spatio-temporal second-moment matrix \u03bc is computed using a separable Gaussian smoothing function and space-time gradients. Interest points are located at local maxima of H = det(\u03bc)\u2212 k trace3(\u03bc). (2) The Cuboid detector [1] is based on temporal Gabor filters. The response function has the form: R = (I \u2217 g \u2217 hev) + (I \u2217 g \u2217 hod), where g(x,y;\u03c3) is the 2D Gaussian smoothing kernel, and hev and hod are 1D Gabor filters. Interest points are detected at local maxima of R. (3) The Hessian detector [6] is a spatio-temporal extension of the Hessian saliency measure. The determinant of the 3D Hessian matrix is used to measure saliency. The determinant of the Hessian is computed over several spatial and temporal scales. A non-maximum suppression algorithm selects extrema as interest points. (4) Dense sampling extracts multi-scale video blocks at regular positions in space and time and for varying scales. In our experiments, we sample cuboids with 50% spatial and temporal overlap. Feature descriptors. The following feature descriptors are investigated. (1) For the Cuboid descriptor [1], gradients computed for each pixel in a cuboid region are concatenated into a single vector. PCA projects vectors to a lower dimensional space. (2) The HOG/HOF descriptors [4] divide a cuboid region into a grid of cells. For each cell, 4-bin histograms of gradient orientations (HOG) and 5-bin histograms of optic flow (HOF) are computed. Normalized histograms are concatenated into HOG, HOF as well as HOG/HOF descriptor vectors. (3) The HOG3D descriptor [2] is based on histograms of 3D gradient orientations. Gradients are computed via an integral video representations. Regular polyhedrons are used to uniformly quantize the orientation of spatio-temporal gradients. A given 3D volume is divided into a grid of cells. The corresponding descriptor concatenates gradient histograms of all cells. (4) The extended SURF (ESURF) descriptor [6] extends the image SURF descriptor to videos. Again 3D cuboids are divided into a grid of cells. Each cell is represented by a weighted sum of uniformly sampled responses of Haar-wavelets aligned with the three axes. Experimental Setup. We represent video sequences as a bag of local spatio-temporal features [5]. Spatio-temporal features are first quantized HOG3D HOG/HOF HOG HOF Cuboids ESURF Harris3D 89.0% 91.8% 80.9% 92.1% \u2013 \u2013 Cuboids 90.0% 88.7% 82.3% 88.2% 89.1% \u2013 Hessian 84.6% 88.7% 7767% 88.6% \u2013 81.4% Dense 85.3% 86.1% 79.0% 88.0% \u2013 \u2013 Table 1: Average accuracy on the KTH actions dataset.", "title": "Evaluation of Local Spatio-temporal Features for Action Recognition"}, "06bae254319f8d39e80c7254c841787b45baf820": {"paper_id": "06bae254319f8d39e80c7254c841787b45baf820", "abstract": "Recurrent neural networks are powerful sequence learners. They are able to incorporate context information in a flexible way, and are robust to localised distortions of the input data. These properties make them well suited to sequence labelling, where input sequences are transcribed with streams of labels. Long short-term memory is an especially promising recurrent architecture, able to bridge long time delays between relevant input and output events, and thereby access long range context. The aim of this thesis is to advance the state-of-the-art in supervised sequence labelling with recurrent networks in general, and long short-term memory in particular. Its two main contributions are (1) a new type of output layer that allows recurrent networks to be trained directly for sequence labelling tasks where the alignment between the inputs and the labels is unknown, and (2) an extension of long short-term memory to multidimensional data, such as images and video sequences. Experimental results are presented on speech recognition, online and offline handwriting recognition, keyword spotting, image segmentation and image classification, demonstrating the advantages of advanced recurrent networks over other sequential algorithms, such as hidden Markov Models.", "title": "Supervised sequence labelling with recurrent neural networks"}, "0e78074a081f2d3a35fbb6f74ba9b7e27e64757b": {"paper_id": "0e78074a081f2d3a35fbb6f74ba9b7e27e64757b", "abstract": "Most current speech recognition systems use hidden Markov models (HMMs) to deal with the temporal variability of speech and Gaussian mixture models to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input. An alternative way to evaluate the fit is to use a feedforward neural network that takes several frames of coefficients as input and produces posterior probabilities over HMM states as output. Deep neural networks with many hidden layers, that are trained using new methods have been shown to outperform Gaussian mixture models on a variety of speech recognition benchmarks, sometimes by a large margin. This paper provides an overview of this progress and represents the shared views of four research groups who have had recent successes in using deep neural networks for acoustic modeling in speech recognition.", "title": "Deep Neural Networks for Acoustic Modeling in Speech Recognition"}, "a64595934c0f71afd3d96eb890be2445952c5cfe": {"paper_id": "a64595934c0f71afd3d96eb890be2445952c5cfe", "abstract": "Recent work on deep neural networks as acoustic models for automatic speech recognition (ASR) have demonstrated substantial performance improvements. We introduce a model which uses a deep recurrent auto encoder neural network to denoise input features for robust ASR. The model is trained on stereo (noisy and clean) audio features to predict clean features given noisy input. The model makes no assumptions about how noise affects the signal, nor the existence of distinct noise environments. Instead, the model can learn to model any type of distortion or additive noise given sufficient training data. We demonstrate the model is competitive with existing feature denoising approaches on the Aurora2 task, and outperforms a tandem approach where deep networks are used to predict phoneme posteriors directly.", "title": "Recurrent Neural Networks for Noise Reduction in Robust ASR"}, "14318685b5959b51d0f1e3db34643eb2855dc6d9": {"paper_id": "14318685b5959b51d0f1e3db34643eb2855dc6d9", "abstract": "We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.", "title": "Going deeper with convolutions"}, "14b5e8ba23860f440ea83ed4770e662b2a111119": {"paper_id": "14b5e8ba23860f440ea83ed4770e662b2a111119", "abstract": "Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark (Krizhevsky et al., 2012). However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al. on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.", "title": "Visualizing and Understanding Convolutional Networks"}, "722fcc35def20cfcca3ada76c8dd7a585d6de386": {"paper_id": "722fcc35def20cfcca3ada76c8dd7a585d6de386", "abstract": "Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (approx 2 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments.\n Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.", "title": "Caffe: Convolutional Architecture for Fast Feature Embedding"}, "4622265755b2d4683e57c32d638bab841a4d5b45": {"paper_id": "4622265755b2d4683e57c32d638bab841a4d5b45", "abstract": "We train a generative convolutional neural network which is able to generate images of objects given object type, viewpoint, and color. We train the network in a supervised manner on a dataset of rendered 3D chair models. Our experiments show that the network does not merely learn all images by heart, but rather finds a meaningful representation of a 3D chair model allowing it to assess the similarity of different chairs, interpolate between given viewpoints to generate the missing ones, or invent new chair styles by interpolating between chairs from the training set. We show that the network can be used to find correspondences between different chairs from the dataset, outperforming existing approaches on this task.", "title": "Learning to generate chairs with convolutional neural networks"}, "81570ccbf4a1edb8898e8969f9befa3561e46318": {"paper_id": "81570ccbf4a1edb8898e8969f9befa3561e46318", "abstract": "Convolutional neural networks (CNNs) have emerged as the most powerful technique for a range of different tasks in computer vision. Recent work suggested that CNN features are generic and can be used for classification tasks outside the exact domain for which the networks were trained. In this work we use the features from one such network, OverFeat, trained for object detection in natural images, for nodule detection in computed tomography scans. We use 865 scans from the publicly available LIDC data set, read by four thoracic radiologists. Nodule candidates are generated by a state-of-the-art nodule detection system. We extract 2D sagittal, coronal and axial patches for each nodule candidate and extract 4096 features from the penultimate layer of OverFeat and classify these with linear support vector machines. We show for various configurations that the off-the-shelf CNN features perform surprisingly well, but not as good as the dedicated detection system. When both approaches are combined, significantly better results are obtained than either approach alone. We conclude that CNN features have great potential to be used for detection tasks in volumetric medical data.", "title": "Off-the-shelf convolutional neural network features for pulmonary nodule detection in computed tomography scans"}, "60dc90046fbbfd1cbad7c3b9759843780c8bacea": {"paper_id": "60dc90046fbbfd1cbad7c3b9759843780c8bacea", "abstract": "In the last two years, convolutional neural networks (CNNs) have achieved an impressive suite of results on standard recognition datasets and tasks. CNN-based features seem poised to quickly replace engineered representations, such as SIFT and HOG. However, compared to SIFT and HOG, we understand much less about the nature of the features learned by large CNNs. In this paper, we experimentally probe several aspects of CNN feature learning in an attempt to help practitioners gain useful, evidence-backed intuitions about how to apply CNNs to computer vision problems.", "title": "Analyzing the Performance of Multilayer Neural Networks for Object Recognition"}, "701ec23ea4bde6a1c0a9817b803aacd9c8548b75": {"paper_id": "701ec23ea4bde6a1c0a9817b803aacd9c8548b75", "abstract": "Image patch classification is an important task in many different medical imaging applications. In this work, we have designed a customized Convolutional Neural Networks (CNN) with shallow convolution layer to classify lung image patches with interstitial lung disease (ILD). While many feature descriptors have been proposed over the past years, they can be quite complicated and domain-specific. Our customized CNN framework can, on the other hand, automatically and efficiently learn the intrinsic image features from lung image patches that are most suitable for the classification purpose. The same architecture can be generalized to perform other medical image or texture classification tasks.", "title": "Medical image classification with convolutional neural network"}, "65edab091e437d3b9d093dcb8be7c5dc4ce0fe0f": {"paper_id": "65edab091e437d3b9d093dcb8be7c5dc4ce0fe0f", "abstract": "Automatic organ segmentation is an important yet challenging problem for medical image analysis. The pancreas is an abdominal organ with very high anatomical variability. This inhibits previous segmentation methods from achieving high accuracies, especially compared to other organs such as the liver, heart or kidneys. In this paper, we present a probabilistic bottom-up approach for pancreas segmentation in abdominal computed tomography (CT) scans, using multi-level deep convolutional networks (ConvNets). We propose and evaluate several variations of deep ConvNets in the context of hierarchical, coarse-tofine classification on image patches and regions, i.e. superpixels. We first present a dense labeling of local image patches via P-ConvNet and nearest neighbor fusion. Then we describe a regional ConvNet (R1\u2212ConvNet) that samples a set of bounding boxes around each image superpixel at different scales of contexts in a \u201czoom-out\u201d fashion. Our ConvNets learn to assign class probabilities for each superpixel region of being pancreas. Last, we study a stacked R2\u2212ConvNet leveraging the joint space of CT intensities and the P\u2212ConvNet dense probability maps. Both 3D Gaussian smoothing and 2D conditional random fields are exploited as structured predictions for post-processing. We evaluate on CT images of 82 patients in 4-fold cross-validation. We achieve a Dice Similarity Coefficient of 83.6\u00b16.3% in training and 71.8\u00b110.7% in testing.", "title": "DeepOrgan: Multi-level Deep Convolutional Networks for Automated Pancreas Segmentation"}, "245414e768c3b8c8288ac0651604a36b1a44a446": {"paper_id": "245414e768c3b8c8288ac0651604a36b1a44a446", "abstract": "We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent an approximate posterior distribution and uses this for optimisation of a variational lower bound. We develop stochastic backpropagation \u2013 rules for gradient backpropagation through stochastic variables \u2013 and derive an algorithm that allows for joint optimisation of the parameters of both the generative and recognition models. We demonstrate on several real-world data sets that by using stochastic backpropagation and variational inference, we obtain models that are able to generate realistic samples of data, allow for accurate imputations of missing data, and provide a useful tool for high-dimensional data visualisation.", "title": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models"}, "27f3c2b0bb917091f92e4161863ec3559452280f": {"paper_id": "27f3c2b0bb917091f92e4161863ec3559452280f", "abstract": "We present a new estimation principle for parameterized statistical models. The idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially generated noise, using the model log-density function in the regression nonlinearity. We show that this leads to a consistent (convergent) estimator of the parameters, and analyze the asymptotic variance. In particular, the method is shown to directly work for unnormalized models, i.e. models where the density function does not integrate to one. The normalization constant can be estimated just like any other parameter. For a tractable ICA model, we compare the method with other estimation methods that can be used to learn unnormalized models, including score matching, contrastive divergence, and maximum-likelihood where the normalization constant is estimated with importance sampling. Simulations show that noise-contrastive estimation offers the best trade-off between computational and statistical efficiency. The method is then applied to the modeling of natural images: We show that the method can successfully estimate a large-scale two-layer model and a Markov random field.", "title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models"}, "284b18d7196f608448ca3d9496bf220b1dfffcf5": {"paper_id": "284b18d7196f608448ca3d9496bf220b1dfffcf5", "abstract": "We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode, and dataindependent expectations are approximated using persistent Markov chains. The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer \u201cpre-training\u201d phase that allows variational inference to be initialized with a single bottomup pass. We present results on the MNIST and NORB datasets showing that deep Boltzmann machines learn good generative models and perform well on handwritten digit and visual object recognition tasks.", "title": "Deep Boltzmann Machines"}, "3b2bf65ebee91249d1045709200a51d157b0176e": {"paper_id": "3b2bf65ebee91249d1045709200a51d157b0176e", "abstract": "Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite.", "title": "Extracting and composing robust features with denoising autoencoders"}, "52c879bacf63acce5d93df8fc94f0557aec49619": {"paper_id": "52c879bacf63acce5d93df8fc94f0557aec49619", "abstract": "We introduce a novel training principle for probabilistic models that is an alternative to maximum likelihood. The proposed Generative Stochastic Networks (GSN) framework is based on learning the transition operator of a Markov chain whose stationary distribution estimates the data distribution. The transition distribution of the Markov chain is conditional on the previous state, generally involving a small move, so this conditional distribution has fewer dominant modes, being unimodal in the limit of small moves. Thus, it is easier to learn because it is easier to approximate its partition function, more like learning to perform supervised function approximation, with gradients that can be obtained by backprop. We provide theorems that generalize recent work on the probabilistic interpretation of denoising autoencoders and obtain along the way an interesting justification for dependency networks and generalized pseudolikelihood, along with a definition of an appropriate joint distribution and sampling mechanism even when the conditionals are not consistent. GSNs can be used with missing inputs and can be used to sample subsets of variables given the rest. We validate these theoretical results with experiments on two image datasets using an architecture that mimics the Deep Boltzmann Machine Gibbs sampler but allows training to proceed with simple backprop, without the need for layerwise pretraining.", "title": "Deep Generative Stochastic Networks Trainable by Backprop"}, "244ab650b25eb90be76fbbf0c4f8bd9121abe86f": {"paper_id": "244ab650b25eb90be76fbbf0c4f8bd9121abe86f", "abstract": "An unsupervised learning algorithm for a multilayer network of stochastic neurons is described. Bottom-up \"recognition\" connections convert the input into representations in successive hidden layers, and top-down \"generative\" connections reconstruct the representation in one layer from the representation in the layer above. In the \"wake\" phase, neurons are driven by recognition connections, and generative connections are adapted to increase the probability that they would reconstruct the correct activity vector in the layer below. In the \"sleep\" phase, neurons are driven by generative connections, and recognition connections are adapted to increase the probability that they would produce the correct activity vector in the layer above.", "title": "The \"wake-sleep\" algorithm for unsupervised neural networks."}, "b7f3dd82e64d6f99188a9e2ea844b4fd01bb0eca": {"paper_id": "b7f3dd82e64d6f99188a9e2ea844b4fd01bb0eca", "abstract": "The Virtual Empowerment App for the Blinds is an empowerment mobile application for the blind. It is developed based on the modules of the existing Latihan Maya Kejayaan Orang Kurang Upaya app, which was intended to empower persons with physical disabilities. As the requirements of persons with physical disabilities greatly differ from that of vision disabilities, this study aims to develop an app that would help blind users to navigate the module of Latihan Maya Kejayaan OKU by themselves based on the requirements observed from the blind users. An interface design to develop a functional and practical app for the blinds is proposed. A comparison between different gesture platforms was also compared. Three tests were conducted, namely the preliminary testing, pilot testing and final testing to analyze the interaction of users when using the app. Results suggested that the extended app developed based on the design proposed in this study were easily navigated and adapted by persons with vision disabilities.", "title": "Designing A Virtual Empowerment Mobile App for the Blinds"}, "880f681d1ad0fd8aa2c7a8eb8a16ccade1c25147": {"paper_id": "880f681d1ad0fd8aa2c7a8eb8a16ccade1c25147", "abstract": "We present part of a vision system for blind and visually impaired people. It detects obstacles on sidewalks and provides guidance to avoid them. Obstacles are trees, light poles, trash cans, holes, branches, stones and other objects at a distance of 3 to 5 meters from the camera position. The system first detects the sidewalk borders, using edge information in combination with a tracking mask, to obtain straight lines with their slopes and the vanishing point. Once the borders are found, a rectangular window is defined within which two obstacle detection methods are applied. The first determines the variation of the maxima and minima of the gray levels of the pixels. The second uses the binary edge image and searches in the vertical and horizontal histograms for discrepancies of the number of edge points. Together, these methods allow to detect possible obstacles with their position and size, such that the user can be alerted and informed about the best way to avoid them. The system works in realtime and complements normal navigation with the cane.", "title": "Obstacle Detection and Avoidance on Sidewalks"}, "693a614718a96d61968ec573b2932a3301092c9a": {"paper_id": "693a614718a96d61968ec573b2932a3301092c9a", "abstract": "El profesor Haykin ha escrito un libro muy interesante, que ya lleva su segunda edici\u00f3n, lo cual es extrafio dentro de la gran cantidad de libros que se producen en la actualidad. Es f\u00e1cil encontrar en la literatura contempor\u00e1nea la constante referencia que se hace a este libro; sin embargo, el libro no es lo que el t\u00edtulo indica, ya que no es un libro ortodoxo de las teor\u00edas y t\u00e9cnicas que son conocidas como redes neuronales (tambi\u00e9n conocidas como sistemas conexionistas o sistemas neurocomputacionales), tampoco es un libro que presente los fundamentos o bases te\u00f3ricas de las redes neuronales. neuronales, as\u00ed como los grandes modelos de esta \u00e1rea de investigaci\u00f3n, como son los perceptrones y las redes RBFN (Radial-Basis Function Networks). La presentaci\u00f3n de estos cuatro cap\u00edtulos es excelente.", "title": "Neural Networks: A Comprehensive Foundation"}, "7e895b230760ac34ce3d5472fb1cbf92b08566cc": {"paper_id": "7e895b230760ac34ce3d5472fb1cbf92b08566cc", "abstract": "2 Summary A recent WHO analysis has revealed the need for a new world standard population (see attached table). This has become particularly pertinent given the rapid and continued declines in age-specific mortality rates among the oldest old, and the increasing availability of epidemiological data for higher age groups. There is clearly no conceptual justification for choosing one standard over another, hence the choice is arbitrary. However, choosing a standard population with higher proportions in the younger age groups tends to weight events at these ages disproportionately. Similarly, choosing an older standard does the opposite. Hence, rather than selecting a standard to match the current age-structure of some population(s), the WHO adopted a standard based on the average age-structure of those populations to be compared (the world) over the likely period of time that a new standard will be used (some 25-30 years), using the latest UN assessment for 1998 (UN Population Division, 1998). From these estimates, an average world population age-structure was constructed for the period 2000-2025. The use of an average world population, as well as a time series of observations, removes the effects of historical events such as wars and famine on population age composition. The terminal age group in the new WHO standard population has been extended out to 100 years and over, rather than the 85 and over as is the current practice. The WHO World Standard population has fewer children and notably more adults aged 70 and above than the world standard. It is also notably younger than the European standard. It is important to note, however, that the age standardized death rates based on the new standard are not comparable to previous estimates that are based on some earlier standard(s). However, to facilitate comparative analyses, WHO will disseminate trend analyses of the \" complete \" historical mortality data using on the new WHO World Standard Population in future editions of the World Health Statistics Annual.", "title": "AGE STANDARDIZATION OF RATES : A NEW WHO STANDARD"}, "32846efa6eecbc3f521d8b7e9331a23acac4d16e": {"paper_id": "32846efa6eecbc3f521d8b7e9331a23acac4d16e", "abstract": "This paper presents a short-baseline real-time stereo vision system that is capable of the simultaneous and robust estimation of the ego-motion and of the 3D structure and the independent motion of thousands of points of the environment. Kalman filters estimate the position and velocity of world points in 3D Euclidean space. The six degrees of freedom of the ego-motion are obtained by minimizing the projection error of the current and previous clouds of static points. Experimental results with real data in indoor and outdoor environments demonstrate the robustness, accuracy and efficiency of our approach. Since the baseline is as short as 13cm, the device is head-mountable, and can be used by a visually impaired person. Our proposed system can be used to augment the perception of the user in complex dynamic environments.", "title": "A Head-Wearable Short-Baseline Stereo System for the Simultaneous Estimation of Structure and Motion"}, "379f61c6f01b70848c89e49370ce999631007d61": {"paper_id": "379f61c6f01b70848c89e49370ce999631007d61", "abstract": "In this paper we describe a method that estimates the motion of a calibrated camera (settled on an experimental vehicle) and the tridimensional geometry of the environment. The only data used is a video input. In fact, interest points are tracked and matched between frames at video rate. Robust estimates of the camera motion are computed in real-time, key-frames are selected and permit the features 3D reconstruction. The algorithm is particularly appropriate to the reconstruction of long images sequences thanks to the introduction of a fast and local bundle adjustment method that ensures both good accuracy and consistency of the estimated camera poses along the sequence. It also largely reduces computational complexity compared to a global bundle adjustment. Experiments on real data were carried out to evaluate speed and robustness of the method for a sequence of about one kilometer long. Results are also compared to the ground truth measured with a differential GPS.", "title": "Real Time Localization and 3D Reconstruction"}, "5205c4e95d5acf639ae21dede605abfe51bce853": {"paper_id": "5205c4e95d5acf639ae21dede605abfe51bce853", "abstract": "Vision-based tracking systems for augmented reality often require that artificial fiducials be placed in the scene. In this paper we utilize our approach for robust detection and tracking of natural features such as textures or corners. The tracked natural features are automatically calibrated to the fiducials that are used to initialize and facilitate normal tracking. Once calibrated, the natural features are used to extend the system\u2019s tracking range and to stabilize the tracked pose against occlusions and noise. The emphasis of this paper is the integration of natural feature tracking with fiducial tracking to increase the range and robustness of vision-based augmented reality tracking.", "title": "Natural feature tracking for extendible robust augmented realities"}, "a7a3a16355838b62c0fdc69f56d09f70e4ff9dab": {"paper_id": "a7a3a16355838b62c0fdc69f56d09f70e4ff9dab", "abstract": "Estimating the pose of a camera (virtual or real) in which some augmentation takes place is one of the most important parts of an augmented reality (AR) system. Availability of powerful processors and fast frame grabbers have made vision-based trackers commonly used due to their accuracy as well as flexibility and ease of use. Current vision-based trackers are based on tracking of markers. The use of markers increases robustness and reduces computational requirements. However, their use can be very complicated, as they require certain maintenance. Direct use of scene features for tracking, therefore, is desirable. To this end, we describe a general system that tracks the position and orientation of a camera observing a scene without any visual markers. Our method is based on a two-stage process. In the first stage, a set of features is learned with the help of an external tracking system while in action. The second stage uses these learned features for camera tracking when the system in the first stage decides that it is possible to do so. The system is very general so that it can employ any available feature tracking and pose estimation system for learning and tracking. We experimentally demonstrate the viability of the method in real-life examples.", "title": "Marker-less Tracking for AR: A Learning-Based Approach"}, "514fa8d4981cc2b2aecfc02e0e3a8f4be717bcd7": {"paper_id": "514fa8d4981cc2b2aecfc02e0e3a8f4be717bcd7", "abstract": "This paper presents a novel version of the five-point relative orientation algorithm given in Nist\u00e9r (2004). The name of the algorithm arises from the fact that it can operate even on the minimal five point correspondences required for a finite number of solutions to relative orientation. For the minimal five correspondences the algorithm returns up to ten real solutions. The algorithm can also operate on many points. Like the previous version of the five-point algorithm, our method can operate correctly even in the face of critical surfaces, including planar and ruled quadric scenes. The paper presents comparisons with other direct methods, including the previously developed five-point method, two different six-point methods, the seven-point method, and the eight-point method. It is shown that the five-point method is superior in most cases among the direct methods. The new version of the algorithm was developed from the perspective of algebraic geometry and is presented in the context of computing a Gr\u00f6bner basis. The constraints are formulated in terms of polynomial equations in the entries of the fundamental matrix. The polynomial equations generate an algebraic ideal for which a Gr\u00f6bner basis is computed. The Gr\u00f6bner basis is used to compute the action matrix for multiplication by a single variable monomial. The eigenvectors of the action matrix give the solutions for all the variables and thereby also relative orientation. Using a Gr\u00f6bner basis makes the solution clear and easy to explain.", "title": "Recent Developments on Direct Relative Orientation"}, "469f5b07c8927438b79a081efacea82449b338f8": {"paper_id": "469f5b07c8927438b79a081efacea82449b338f8", "abstract": "This paper presents an e cient shape-based object detection method based on Distance Transforms and describes its use for real-time vision on-board vehicles. The method uses a template hierarchy to capture the variety of object shapes; e cient hierarchies can be generated o ine for given shape distributions using stochastic optimization techniques (i.e. simulated annealing). Online, matching involves a simultaneous coarse-tone approach over the shape hierarchy and over the transformation parameters. Very large speedup factors are typically obtained when comparing this approach with the equivalent brute-force formulation; we have measured gains of several orders of magnitudes. We present experimental results on the real-time detection of tra c signs and pedestrians from a moving vehicle. Because of the highly time sensitive nature of these vision tasks, we also discuss some hardwarespeci c implementations of the proposed method as far as SIMD parallelism is concerned.", "title": "Real-Time Object Detection for \"Smart\" Vehicles"}, "1172a5571a6b3f888b3c41f23db20db216195fcd": {"paper_id": "1172a5571a6b3f888b3c41f23db20db216195fcd", "abstract": "The computation of free space available in an environment is an essential task for many intelligent automotive and robotic applications. This paper proposes a new approach, which builds a stochastic occupancy grid to address the free space problem as a dynamic programming task. Stereo measurements are integrated over time reducing disparity uncertainty. These integrated measurements are entered into an occupancy grid, taking into account the noise properties of the measurements. In order to cope with real-time requirements of the application, three occupancy grid types are proposed. Their applicabilities and implementations are also discussed. Experimental results with real stereo sequences show the robustness and accuracy of the method. The current implementation of the method runs on off-the-shelf hardware at 20 Hz.", "title": "Free Space Computation Using Stochastic Occupancy Grids and Dynamic Programming"}, "49bdd3fb166e0faf7ad1c917aee32c22ebc0f9db": {"paper_id": "49bdd3fb166e0faf7ad1c917aee32c22ebc0f9db", "abstract": "Detection and tracking of humans in video streams is important for many applications. We present an approach to automatically detect and track multiple, possibly partially occluded humans in a walking or standing pose from a single camera, which may be stationary or moving. A human body is represented as an assembly of body parts. Part detectors are learned by boosting a number of weak classifiers which are based on edgelet features. Responses of part detectors are combined to form a joint likelihood model that includes an analysis of possible occlusions. The combined detection responses and the part detection responses provide the observations used for tracking. Trajectory initialization and termination are both automatic and rely on the confidences computed from the detection responses. An object is tracked by data association and meanshift methods. Our system can track humans with both inter-object and scene occlusions with static or non-static backgrounds. Evaluation results on a number of images and videos and comparisons with some previous methods are given.", "title": "Detection and Tracking of Multiple, Partially Occluded Humans by Bayesian Combination of Edgelet based Part Detectors"}, "1099983c747a8773c1572a3226373ce4521107b1": {"paper_id": "1099983c747a8773c1572a3226373ce4521107b1", "abstract": "We present a system that estimates the motion of a stereo head or a single moving camera based on video input. The system operates in real-time with low delay and the motion estimates are used for navigational purposes. The front end of the system is a feature tracker. Point features are matched between pairs of frames and linked into image trajectories at video rate. Robust estimates of the camera motion are then produced from the feature tracks using a geometric hypothesize-and-test architecture. This generates what we call visual odometry, i.e. motion estimates from visual input alone. No prior knowledge of the scene nor the motion is necessary. The visual odometry can also be used in conjunction with information from other sources such as GPS, inertia sensors, wheel encoders, etc. The pose estimation method has been applied successfully to video from aerial, automotive and handheld platforms. We focus on results with an autonomous ground vehicle. We give examples of camera trajectories estimated purely from images over previously unseen distances and periods of time.", "title": "Visual odometry"}, "37a15ce03c26ec83d95bf4aaf756a41370d50353": {"paper_id": "37a15ce03c26ec83d95bf4aaf756a41370d50353", "abstract": "We describe the functional and architectural breakdown of a monocular pedestrian detection system. We describe in detail our approach for single-frame classification based on a novel scheme of breaking down the class variability by repeatedly training a set of relatively simple classifiers on clusters of the training set. Single-frame classification performance results and system level performance figures for daytime conditions are presented with a discussion about the remaining gap to meet a daytime normal weather condition production system.", "title": "Pedestrian detection for driving assistance systems: single-frame classification and system level performance"}, "1cf74b1f4fbd05250701c86a45044534a66c7f5e": {"paper_id": "1cf74b1f4fbd05250701c86a45044534a66c7f5e", "abstract": "This paper presents a novel method for detecting and localizing objects of a visual category in cluttered real-world scenes. Our approach considers object categorization and figure-ground segmentation as two interleaved processes that closely collaborate towards a common goal. As shown in our work, the tight coupling between those two processes allows them to benefit from each other and improve the combined performance. The core part of our approach is a highly flexible learned representation for object shape that can combine the information observed on different training examples in a probabilistic extension of the Generalized Hough Transform. The resulting approach can detect categorical objects in novel images and automatically infer a probabilistic segmentation from the recognition result. This segmentation is then in turn used to again improve recognition by allowing the system to focus its efforts on object pixels and to discard misleading influences from the background. Moreover, the information from where in the image a hypothesis draws its support is employed in an MDL based hypothesis verification stage to resolve ambiguities between overlapping hypotheses and factor out the effects of partial occlusion. An extensive evaluation on several large data sets shows that the proposed system is applicable to a range of different object categories, including both rigid and articulated objects. In addition, its flexible representation allows it to achieve competitive object detection performance already from training sets that are between one and two orders of magnitude smaller than those used in comparable systems.", "title": "Robust Object Detection with Interleaved Categorization and Segmentation"}, "9c88bca7b8609121dc73277cfe3c7b9bbf02c692": {"paper_id": "9c88bca7b8609121dc73277cfe3c7b9bbf02c692", "abstract": "A resurgence in the use of medical herbs in the Western world, and the co-use of modern and traditional therapies is becoming more common. Thus there is the potential for both pharmacokinetic and pharmacodynamic herb-drug interactions. For example, systems such as the cytochrome P450 (CYP) may be particularly vulnerable to modulation by the multiple active constituents of herbs, as it is well known that the CYPs are subject to induction and inhibition by exposure to a wide variety of xenobiotics. Using in vitro, in silico, and in vivo approaches, many herbs and natural compounds isolated from herbs have been identified as substrates, inhibitors, and/or inducers of various CYP enzymes. For example, St. John's wort is a potent inducer of CYP3A4, which is mediated by activating the orphan pregnane X receptor. It also contains ingredients that inhibit CYP1A2, CYP2C9, CYP2C19, CYP2D6, and CYP3A4. Many other common medicinal herbs also exhibited inducing or inhibiting effects on the CYP system, with the latter being competitive, noncompetitive, or mechanism-based. It appears that the regulation of CYPs by herbal products complex, depending on the herb type, their administration dose and route, the target organ and species. Due to the difficulties in identifying the active constituents responsible for the modulation of CYP enzymes, prediction of herb-drug metabolic interactions is difficult. However, herb-CYP interactions may have important clinical and toxicological consequences. For example, induction of CYP3A4 by St. John's wort may partly provide an explanation for the enhanced plasma clearance of a number of drugs, such as cyclosporine and innadivir, which are known substrates of CYP3A4, although other mechanisms including modulation of gastric absorption and drug transporters cannot be ruled out. In contrast, many organosulfur compounds, such as diallyl sulfide from garlic, are potent inhibitors of CYP2E1; this may provide an explanation for garlic's chemoproventive effects, as many mutagens require activation by CYP2E1. Therefore, known or potential herb-CYP interactions exist, and further studies on their clinical and toxicological roles are warranted. Given that increasing numbers of people are exposed to a number of herbal preparations that contain many constituents with potential of CYP modulation, high-throughput screening assays should be developed to explore herb-CYP interactions.", "title": "Interactions of herbs with cytochrome P450."}, "b0dc1188881b05b3e1db1d863d49f7ad03c1082a": {"paper_id": "b0dc1188881b05b3e1db1d863d49f7ad03c1082a", "abstract": "KEY POINTS\nThree weeks of intensified training and mild energy deficit in elite race walkers increases peak aerobic capacity independent of dietary support. Adaptation to a ketogenic low carbohydrate, high fat (LCHF) diet markedly increases rates of whole-body fat oxidation during exercise in race walkers over a range of exercise intensities. The increased rates of fat oxidation result in reduced economy (increased oxygen demand for a given speed) at velocities that translate to real-life race performance in elite race walkers. In contrast to training with diets providing chronic or periodised high carbohydrate availability, adaptation to an LCHF diet impairs performance in elite endurance athletes despite a significant improvement in peak aerobic capacity.\n\n\nABSTRACT\nWe investigated the effects of adaptation to a ketogenic low carbohydrate (CHO), high fat diet (LCHF) during 3 weeks of intensified training on metabolism and performance of world-class endurance athletes. We controlled three isoenergetic diets in elite race walkers: high CHO availability (g\u00a0kg-1 \u00a0day-1 : 8.6 CHO, 2.1 protein, 1.2 fat) consumed before, during and after training (HCHO, n\u00a0=\u00a09); identical macronutrient intake, periodised within or between days to alternate between low and high CHO availability (PCHO, n\u00a0=\u00a010); LCHF (<\u00a050\u00a0g\u00a0day-1 CHO; 78% energy as fat; 2.1\u00a0g\u00a0kg-1 \u00a0day-1 protein; LCHF, n\u00a0=\u00a010). Post-intervention, V\u0307O2 peak during race walking increased in all groups (P\u00a0<\u00a00.001, 90% CI: 2.55, 5.20%). LCHF was associated with markedly increased rates of whole-body fat oxidation, attaining peak rates of 1.57\u00a0\u00b1\u00a00.32\u00a0g\u00a0min-1 during 2\u00a0h of walking at \u223c80% V\u0307O2 peak . However, LCHF also increased the oxygen (O2 ) cost of race walking at velocities relevant to real-life race performance: O2 uptake (expressed as a percentage of new V\u0307O2 peak ) at a speed approximating 20\u00a0km race pace was reduced in HCHO and PCHO (90% CI: -7.047, -2.55 and -5.18, -0.86, respectively), but was maintained at pre-intervention levels in LCHF. HCHO and PCHO groups improved times for 10\u00a0km race walk: 6.6% (90% CI: 4.1, 9.1%) and 5.3% (3.4, 7.2%), with no improvement (-1.6% (-8.5, 5.3%)) for the LCHF group. In contrast to training with diets providing chronic or periodised high-CHO availability, and despite a significant improvement in V\u0307O2 peak , adaptation to the topical LCHF diet negated performance benefits in elite endurance athletes, in part due to reduced exercise economy.", "title": "Low carbohydrate, high fat diet impairs exercise economy and negates the performance benefit from intensified training in elite race walkers"}, "4ee2da88189c326f84a06a2ab7248805b74ddc29": {"paper_id": "4ee2da88189c326f84a06a2ab7248805b74ddc29", "abstract": "Sensitive, specific, rapid, inexpensive and easy-to-use nucleic acid tests for use at the point-of-need are critical for the emerging field of personalised medicine for which companion diagnostics are essential, as well as for application in low resource settings. Here we report on the development of a point-of-care nucleic acid lateral flow test for the direct detection of isothermally amplified DNA. The recombinase polymerase amplification method is modified slightly to use tailed primers, resulting in an amplicon with a duplex flanked by two single stranded DNA tails. This tailed amplicon facilitates detection via hybridisation to a surface immobilised oligonucleotide capture probe and a gold nanoparticle labelled reporter probe. A detection limit of 1\u2009\u00d7\u200910-11\u2009M (190\u2009amol), equivalent to 8.67\u2009\u00d7\u2009105 copies of DNA was achieved, with the entire assay, both amplification and detection, being completed in less than 15\u2009minutes at a constant temperature of 37\u2009\u00b0C. The use of the tailed primers obviates the need for hapten labelling and consequent use of capture and reporter antibodies, whilst also avoiding the need for any post-amplification processing for the generation of single stranded DNA, thus presenting an assay that can facilely find application at the point of need.", "title": "Ultrasensitive, rapid and inexpensive detection of DNA using paper based lateral flow assay"}, "f5e3f3c7261a25c724f3eb592a3403b87d3f710e": {"paper_id": "f5e3f3c7261a25c724f3eb592a3403b87d3f710e", "abstract": "This paper presents a road distress detection system involving the phases needed to properly deal with fully automatic road distress assessment. A vehicle equipped with line scan cameras, laser illumination and acquisition HW-SW is used to storage the digital images that will be further processed to identify road cracks. Pre-processing is firstly carried out to both smooth the texture and enhance the linear features. Non-crack features detection is then applied to mask areas of the images with joints, sealed cracks and white painting, that usually generate false positive cracking. A seed-based approach is proposed to deal with road crack detection, combining Multiple Directional Non-Minimum Suppression (MDNMS) with a symmetry check. Seeds are linked by computing the paths with the lowest cost that meet the symmetry restrictions. The whole detection process involves the use of several parameters. A correct setting becomes essential to get optimal results without manual intervention. A fully automatic approach by means of a linear SVM-based classifier ensemble able to distinguish between up to 10 different types of pavement that appear in the Spanish roads is proposed. The optimal feature vector includes different texture-based features. The parameters are then tuned depending on the output provided by the classifier. Regarding non-crack features detection, results show that the introduction of such module reduces the impact of false positives due to non-crack features up to a factor of 2. In addition, the observed performance of the crack detection system is significantly boosted by adapting the parameters to the type of pavement.", "title": "Adaptive Road Crack Detection System by Pavement Classification"}, "3dbe40033c570f4ea9c88ffbee1ad26a6b7d0df6": {"paper_id": "3dbe40033c570f4ea9c88ffbee1ad26a6b7d0df6", "abstract": "In practice the relevant details of images exist only over a restricted range of scale. Hence it is important to study the dependence of image structure on the level of resolution. It seems clear enough that visual perception treats images on several levels of resolution simultaneously and that this fact must be important for the study of perception. However, no applicable mathematically formulated theory to deal with such problems appers to exist. In this paper it is shown that any image can be embedded in a one-parameter family of derived images (with resolution as the parameter) in essentially only one unique way if the constraint that no spurious detail should be generated when the resolution is diminished, is applied. The structure of this family is governed by the well known diffusion equation (a parabolic, linear, partial differential equation of the second order). As such the structure fits into existing theories that treat the front end of the visual system as a continuous tack of homogeneous layer, characterized by iterated local processing schemes. When resolution is decreased the images becomes less articulated because the extrem (\u201clight and dark blobs\u201d) disappear one after the other. This erosion of structure is a simple process that is similar in every case. As a result any image can be described as a juxtaposed and nested set of light and dark blobs, wherein each blod has a limited range of resolution in which it manifests itself. The structure of the family of derived images permits a derivation of the sampling density required to sample the image at multiple scales of resolution. The natural scale along the resolution axis (leading to an informationally uniform sampling density) is logarithmic, thus the structure is apt for the description of size invariances.", "title": "The structure of images"}, "b541df1f9a6172dde4cb96f643e2c21c58449e02": {"paper_id": "b541df1f9a6172dde4cb96f643e2c21c58449e02", "abstract": "This paper introduces the status of social recommender system research in general and collaborative filtering in particular. For the collaborative filtering, the paper shows the basic principles and formulas of two basic approaches, the user-based collaborative filtering and the item-based collaborative filtering. For the user or item similarity calculation, the paper compares the differences between the cosine-based similarity, the revised cosine-based similarity and the Pearson-based similarity. The paper also analyzes the three main challenges of the collaborative filtering algorithm and shows the related works facing the challenges. To solve the Cold Start problem and reduce the cost of best neighborhood calculation, the paper provides several solutions. At last it discusses the future of the collaborative filtering algorithm in social recommender system.", "title": "A Survey of Collaborative Filtering Algorithms for Social Recommender Systems"}, "6aa1c88b810825ee80b8ed4c27d6577429b5d3b2": {"paper_id": "6aa1c88b810825ee80b8ed4c27d6577429b5d3b2", "abstract": "Recommender systems have been evaluated in many, often incomparable, ways. In this article, we review the key decisions in evaluating collaborative filtering recommender systems: the user tasks being evaluated, the types of analysis and datasets being used, the ways in which prediction quality is measured, the evaluation of prediction attributes other than quality, and the user-based evaluation of the system as a whole. In addition to reviewing the evaluation strategies used by prior researchers, we present empirical results from the analysis of various accuracy metrics on one content domain where all the tested metrics collapsed roughly into three equivalence classes. Metrics within each equivalency class were strongly correlated, while metrics from different equivalency classes were uncorrelated.", "title": "Evaluating collaborative filtering recommender systems"}, "399de9508efad3517b00fec4e800f1cd7f383c23": {"paper_id": "399de9508efad3517b00fec4e800f1cd7f383c23", "abstract": "Recommender systems learn about user preferences over time, automatically finding things of similar interest. This reduces the burden of creating explicit queries. Recommender systems do, however, suffer from cold-start problems where no initial information is available early on upon which to base recommendations. Semantic knowledge structures, such as ontologies, can provide valuable domain knowledge and user information. However, acquiring such knowledge and keeping it up to date is not a trivial task and user interests are particularly difficult to acquire and maintain. This paper investigates the synergy between a web-based research paper recommender system and an ontology containing information automatically extracted from departmental databases available on the web. The ontology is used to address the recommender systems cold-start problem. The recommender system addresses the ontology\u2019s interest-acquisition problem. An empirical evaluation of this approach is conducted and the performance of the integrated systems measured. General Terms Design, Experimentation.", "title": "Exploiting Synergy Between Ontologies and Recommender Systems"}, "124a9d3fa4569b2ebda1ca8726166a635f31a36d": {"paper_id": "124a9d3fa4569b2ebda1ca8726166a635f31a36d", "abstract": "This paper focuses on question selection methods for conversational recommender systems. We consider a scenario, where given an initial user query, the recommender system may ask the user to provide additional features describing the searched products. The objective is to generate questions/features that a user would likely reply, and if replied, would effectively reduce the result size of the initial query. Classical entropy-based feature selection methods are effective in term of result size reduction, but they select questions uncorrelated with user needs and therefore unlikely to be replied. We propose two feature-selection methods that combine feature entropy with an appropriate measure of feature relevance. We evaluated these methods in a set of simulated interactions where a probabilistic model of user behavior is exploited. The results show that these methods outperform entropy-based feature selection.", "title": "Feature selection methods for conversational recommender systems"}, "f9806beb3d11127df84f6b25b65613b7640ce3db": {"paper_id": "f9806beb3d11127df84f6b25b65613b7640ce3db", "abstract": "Artificial Neural Network (ANN) design is a complex task because its performance depends on the architecture, the selected transfer function, and the learning algorithm used to train the set of synaptic weights. In this paper we present a methodology that automatically designs an ANN using particle swarm optimization algorithms such as Basic Particle Swarm Optimization (PSO), Second Generation of Particle Swarm Optimization (SGPSO), and a New Model of PSO called NMPSO. The aim of these algorithms is to evolve, at the same time, the three principal components of an ANN: the set of synaptic weights, the connections or architecture, and the transfer functions for each neuron. Eight different fitness functions were proposed to evaluate the fitness of each solution and find the best design. These functions are based on the mean square error (MSE) and the classification error (CER) and implement a strategy to avoid overtraining and to reduce the number of connections in the ANN. In addition, the ANN designed with the proposed methodology is compared with those designed manually using the well-known Back-Propagation and Levenberg-Marquardt Learning Algorithms. Finally, the accuracy of the method is tested with different nonlinear pattern classification problems.", "title": "Designing Artificial Neural Networks Using Particle Swarm Optimization Algorithms"}, "87ba68678a1ae983cee474e4bfdd27257e45ca3d": {"paper_id": "87ba68678a1ae983cee474e4bfdd27257e45ca3d", "abstract": "A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by \u201clearning without a teacher\u201d, and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname \u201cneocognitron\u201d. After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of \u201cS-cells\u201d, which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of \u201cC-cells\u201d similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any \u201cteacher\u201d during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern.", "title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position"}, "d03ae9148a2573bcf959c7fa343cef0be416cb55": {"paper_id": "d03ae9148a2573bcf959c7fa343cef0be416cb55", "abstract": "We present a small object sensitive method for object detection. Our method is built based on SSD (Single Shot MultiBox Detector (Liu et al. 2016)), a simple but effective deep neural network for image object detection. The discrete nature of anchor mechanism used in SSD, however, may cause misdetection for the small objects located at gaps between the anchor boxes. SSD performs better for small object detection after circular shifts of the input image. Therefore, auxiliary feature maps are generated by conducting circular shifts over lower extra feature maps in SSD for small-object detection, which is equivalent to shifting the objects in order to fit the locations of anchor boxes. We call our proposed system Shifted SSD. Moreover, pinpoint accuracy of localization is of vital importance to small objects detection. Hence, two novel methods called Smooth NMS and IoU-Prediction module are proposed to obtain more precise locations. Then for video sequences, we generate trajectory hypothesis to obtain predicted locations in a new frame for further improved performance. Experiments conducted on PASCAL VOC 2007, along with MS COCO, KITTI and our small object video datasets, validate that both mAP and recall are improved with different degrees and the speed is almost the same as SSD.", "title": "Small-objectness sensitive detection based on shifted single shot detector"}, "0ed925cf92023466bd6a8ad886a805b10466e189": {"paper_id": "0ed925cf92023466bd6a8ad886a805b10466e189", "abstract": "We present a novel approach for multi-object tracking which considers object detection and spacetime trajectory estimation as a coupled optimization problem. It is formulated in a hypothesis selection framework and builds upon a state-of-the-art pedestrian detector. At each time instant, it searches for the globally optimal set of spacetime trajectories which provides the best explanation for the current image and for all evidence collected so far, while satisfying the constraints that no two objects may occupy the same physical space, nor explain the same image pixels at any point in time. Successful trajectory hypotheses are fed back to guide object detection in future frames. The optimization procedure is kept efficient through incremental computation and conservative hypothesis pruning. The resulting approach can initialize automatically and track a large and varying number of persons over long periods and through complex scenes with clutter, occlusions, and large-scale background changes. Also, the global optimization framework allows our system to recover from mismatches and temporarily lost tracks. We demonstrate the feasibility of the proposed approach on several challenging video sequences.", "title": "Coupled Detection and Trajectory Estimation for Multi-Object Tracking"}, "72e08cf12730135c5ccd7234036e04536218b6c1": {"paper_id": "72e08cf12730135c5ccd7234036e04536218b6c1", "abstract": "Recently Viola et al. [5] have introduced a rapid object detection scheme based on a boosted cascade of simple features. In this paper we introduce a novel set of rotated haar-like features, which significantly enrich this basic set of simple haar-like features and which can also be calculated very efficiently. At a given hit rate our sample face detector shows off on average a 10% lower false alarm rate by means of using these additional rotated features. We also present a novel post optimization procedure for a given boosted cascade improving on average the false alarm rate further by 12.5%. Using both enhancements the number of false detections is only 24 at a hit rate of 82.3% on the CMU face set [7].", "title": "An extended set of Haar-like features for rapid object detection"}, "25d7da85858a4d89b7de84fd94f0c0a51a9fc67a": {"paper_id": "25d7da85858a4d89b7de84fd94f0c0a51a9fc67a", "abstract": "This paper addresses the problem of generating possible object locations for use in object recognition. We introduce selective search which combines the strength of both an exhaustive search and segmentation. Like segmentation, we use the image structure to guide our sampling process. Like exhaustive search, we aim to capture all possible object locations. Instead of a single technique to generate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible. Our selective search results in a small set of data-driven, class-independent, high quality locations, yielding 99\u00a0% recall and a Mean Average Best Overlap of 0.879 at 10,097 locations. The reduced number of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition. In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition. The selective search software is made publicly available (Software: http://disi.unitn.it/~uijlings/SelectiveSearch.html ).", "title": "Selective Search for Object Recognition"}, "455da02e5048dffb51fb6ab5eb8aeca5926c9d9a": {"paper_id": "455da02e5048dffb51fb6ab5eb8aeca5926c9d9a", "abstract": "Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g., 224<inline-formula><tex-math>$\\times$ </tex-math><alternatives><inline-graphic xlink:type=\"simple\" xlink:href=\"he-ieq1-2389824.gif\"/></alternatives></inline-formula>224) input image. This requirement is \u201cartificial\u201d and may reduce the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with another pooling strategy, \u201cspatial pyramid pooling\u201d, to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. Pyramid pooling is also robust to object deformations. With these advantages, SPP-net should in general improve all CNN-based image classification methods. On the ImageNet 2012 dataset, we demonstrate that SPP-net boosts the accuracy of a variety of CNN architectures despite their different designs. On the Pascal VOC 2007 and Caltech101 datasets, SPP-net achieves state-of-the-art classification results using a single full-image representation and no fine-tuning. The power of SPP-net is also significant in object detection. Using SPP-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors. This method avoids repeatedly computing the convolutional features. In processing test images, our method is 24-102 <inline-formula><tex-math>$\\times$</tex-math><alternatives><inline-graphic xlink:type=\"simple\" xlink:href=\"he-ieq2-2389824.gif\"/> </alternatives></inline-formula> faster than the R-CNN method, while achieving better or comparable accuracy on Pascal VOC 2007. In ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014, our methods rank #2 in object detection and #3 in image classification among all 38 teams. This manuscript also introduces the improvement made for this competition.", "title": "Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition"}, "16fc1065c296840cb0f8ca62601aa17b7f0a02bf": {"paper_id": "16fc1065c296840cb0f8ca62601aa17b7f0a02bf", "abstract": "\u2022 Lighting. When an object is in bright light, it looks brighter than when it\u2019s in shadow, so a program can\u2019t just look at image intensity values. \u2022 Within-class variation. Different instances of the same kind of object can look quite different to one another. For example, a green station wagon and a red convertible are both cars, so a program can\u2019t simply compare a picture to one example. \u2022 Aspect. The same object can look very different when viewed at from different directions\u2014pick up a book and compare its cover and its spine to see this effect. Again, this means that a program might need to have many examples of each type of object. \u2022 Deformation. Many objects can change their appearance significantly without their identity changing. For example, you can move your limbs around, change clothes, paint your face, or have your hair cut. You will look very different indeed, but you will still be a person.", "title": "Object Detection with Discriminatively Trained Part-Based Models"}, "8ade5d29ae9eac7b0980bc6bc1b873d0dd12a486": {"paper_id": "8ade5d29ae9eac7b0980bc6bc1b873d0dd12a486", "abstract": null, "title": "Robust Real-Time Face Detection"}, "76f560991d56ad689ec32f9e9d13291e0193f4cf": {"paper_id": "76f560991d56ad689ec32f9e9d13291e0193f4cf", "abstract": "This paper presents a general trainable framework for object detection in static images of cluttered scenes. The detection technique we develop is based on a wavelet representation of an object class derived from a statistical analysis of the class instances. By learning an object class in terms of a subset of an overcomplete dictionary of wavelet basis functions, we derive a compact representation of an object class which is used as an input to a suppori vector machine classifier. This representation overcomes both the problem of in-class variability and provides a low false detection rate in unconstrained environments. We demonstrate the capabilities of the technique i n two domains whose inherent information content differs significantly. The first system is face detection and the second is the domain of people which, in contrast to faces, vary greatly in color, texture, and patterns. Unlike previous approaches, this system learns from examples and does not rely on any a priori (handcrafted) models or motion-based segmentation. The paper also presents a motion-based extension to enhance the performance of the detection algorithm over video sequences. The results presented here suggest that this architecture may well be quite general.", "title": "A General Framework for Object Detection"}, "07edaa3b52141dcd376a54490df3a91529f1bde3": {"paper_id": "07edaa3b52141dcd376a54490df3a91529f1bde3", "abstract": "We present an accurate, real-time approach to robotic grasp detection based on convolutional neural networks. Our network performs single-stage regression to graspable bounding boxes without using standard sliding window or region proposal techniques. The model outperforms state-of-the-art approaches by 14 percentage points and runs at 13 frames per second on a GPU. Our network can simultaneously perform classification so that in a single step it recognizes the object and finds a good grasp rectangle. A modification to this model predicts multiple grasps per object by using a locally constrained prediction mechanism. The locally constrained model performs significantly better, especially on objects that can be grasped in a variety of ways.", "title": "Real-time grasp detection using convolutional neural networks"}, "009fba8df6bbca155d9e070a9bd8d0959bc693c2": {"paper_id": "009fba8df6bbca155d9e070a9bd8d0959bc693c2", "abstract": "Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.", "title": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation"}, "8b39054a7199ea76c2380b1af11dd46f94185cde": {"paper_id": "8b39054a7199ea76c2380b1af11dd46f94185cde", "abstract": "We propose a unified approach for bottom-up hierarchical image segmentation and object proposal generation for recognition, called Multiscale Combinatorial Grouping (MCG). For this purpose, we first develop a fast normalized cuts algorithm. We then propose a high-performance hierarchical segmenter that makes effective use of multiscale information. Finally, we propose a grouping strategy that combines our multiscale regions into highly-accurate object proposals by exploring efficiently their combinatorial space. We also present Single-scale Combinatorial Grouping (SCG), a faster version of MCG that produces competitive proposals in under five seconds per image. We conduct an extensive and comprehensive empirical validation on the BSDS500, SegVOC12, SBD, and COCO datasets, showing that MCG produces state-of-the-art contours, hierarchical regions, and object proposals.", "title": "Multiscale Combinatorial Grouping for Image Segmentation and Object Proposal Generation."}, "5087d9bdde0ba5440eb8658be7183bf5074a2a94": {"paper_id": "5087d9bdde0ba5440eb8658be7183bf5074a2a94", "abstract": "We propose an object detection system that relies on a multi-region deep convolutional neural network (CNN) that also encodes semantic segmentation-aware features. The resulting CNN-based representation aims at capturing a diverse set of discriminative appearance factors and exhibits localization sensitivity that is essential for accurate object localization. We exploit the above properties of our recognition module by integrating it on an iterative localization mechanism that alternates between scoring a box proposal and refining its location with a deep CNN regression model. Thanks to the efficient use of our modules, we detect objects with very high localization accuracy. On the detection challenges of PASCAL VOC2007 and PASCAL VOC2012 we achieve mAP of 78.2% and 73.9% correspondingly, surpassing any other published work by a significant margin.", "title": "Object Detection via a Multi-region and Semantic Segmentation-Aware CNN Model"}, "1abf6491d1b0f6e8af137869a01843931996a562": {"paper_id": "1abf6491d1b0f6e8af137869a01843931996a562", "abstract": "We present a technique for adding global context to deep convolutional networks for semantic segmentation. The approach is simple, using the average feature for a layer to augment the features at each location. In addition, we study several idiosyncrasies of training, significantly increasing the performance of baseline networks (e.g. from FCN [19]). When we add our proposed global feature, and a technique for learning normalization parameters, accuracy increases consistently even over our improved versions of the baselines. Our proposed approach, ParseNet, achieves state-of-the-art performance on SiftFlow and PASCAL-Context with small additional computational cost over baselines, and near current state-of-theart performance on PASCAL VOC 2012 semantic segmentation with a simple approach. Code is available at https: //github.com/weiliu89/caffe/tree/fcn.", "title": "ParseNet: Looking Wider to See Better"}, "0ee1916a0cb2dc7d3add086b5f1092c3d4beb38a": {"paper_id": "0ee1916a0cb2dc7d3add086b5f1092c3d4beb38a", "abstract": "The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection. This paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension.", "title": "The Pascal Visual Object Classes (VOC) Challenge"}, "7fc3119ab077617e93a8b5b421857820a1ac745f": {"paper_id": "7fc3119ab077617e93a8b5b421857820a1ac745f", "abstract": "The role of anomaly detection in X-ray security imaging, as a supplement to targeted threat detection, is described, and a taxonomy of anomaly types in this domain is presented. Algorithms are described for detecting appearance anomalies of shape, texture, and density, and semantic anomalies of object category presence. The anomalies are detected on the basis of representations extracted from a convolutional neural network pre-trained to identify object categories in photographs, from the final pooling layer for appearance anomalies, and from the logit layer for semantic anomalies. The distribution of representations in normal data is modeled using high-dimensional, full-covariance, Gaussians, and anomalies are scored according to their likelihood relative to those models. The algorithms are tested on X-ray parcel images using stream-of-commerce data as the normal class, and parcels with firearms present the examples of anomalies to be detected. Despite the representations being learned for photographic images and the varied contents of stream-of-commerce parcels, the system, trained on stream-of-commerce images only, is able to detect 90% of firearms as anomalies, while raising false alarms on 18% of stream-of-commerce.", "title": "\u201cUnexpected Item in the Bagging Area\u201d: Anomaly Detection in X-Ray Security Images"}, "bea5780d621e669e8069f05d0f2fc0db9df4b50f": {"paper_id": "bea5780d621e669e8069f05d0f2fc0db9df4b50f", "abstract": "We describe how to train a two-layer convolutional Deep Belief Network (DBN) on the 1.6 million tiny images dataset. When training a convolutional DBN, one must decide what to do with the edge pixels of teh images. As the pixels near the edge of an image contribute to the fewest convolutional lter outputs, the model may see it t to tailor its few convolutional lters to better model the edge pixels. This is undesirable becaue it usually comes at the expense of a good model for the interior parts of the image. We investigate several ways of dealing with the edge pixels when training a convolutional DBN. Using a combination of locally-connected convolutional units and globally-connected units, as well as a few tricks to reduce the e ects of over tting, we achieve state-of-the-art performance in the classi cation task of the CIFAR-10 subset of the tiny images dataset.", "title": "Convolutional Deep Belief Networks on CIFAR-10"}, "10b83e269c1dcb171ae678e28322997f48e2ee6c": {"paper_id": "10b83e269c1dcb171ae678e28322997f48e2ee6c", "abstract": "Building robust low and mid-level image representations, beyond edge primitives, is a long-standing goal in vision. Many existing feature detectors spatially pool edge information which destroys cues such as edge intersections, parallelism and symmetry. We present a learning framework where features that capture these mid-level cues spontaneously emerge from image data. Our approach is based on the convolutional decomposition of images under a spar-sity constraint and is totally unsupervised. By building a hierarchy of such decompositions we can learn rich feature sets that are a robust image representation for both the analysis and synthesis of images.", "title": "Deconvolutional networks"}, "7fc604e1a3e45cd2d2742f96d62741930a363efa": {"paper_id": "7fc604e1a3e45cd2d2742f96d62741930a363efa", "abstract": "Energy-Based Models (EBMs) capture dependencies between v ariables by associating a scalar energy to each configuration of the variab les. Inference consists in clamping the value of observed variables and finding config urations of the remaining variables that minimize the energy. Learning consi sts in finding an energy function in which observed configurations of the variables a re given lower energies than unobserved ones. The EBM approach provides a common the re ical framework for many learning models, including traditional discr minative and generative approaches, as well as graph-transformer networks, co nditi nal random fields, maximum margin Markov networks, and several manifold learn ing methods. Probabilistic models must be properly normalized, which so metimes requires evaluating intractable integrals over the space of all poss ible variable configurations. Since EBMs have no requirement for proper normalizat ion, his problem is naturally circumvented. EBMs can be viewed as a form of non-p robabilistic factor graphs, and they provide considerably more flexibility in th e design of architectures and training criteria than probabilistic approaches .", "title": "A Tutorial on Energy-Based Learning"}, "5d21006fa32ff69f6b0a646f26ce0db84f2f4d33": {"paper_id": "5d21006fa32ff69f6b0a646f26ce0db84f2f4d33", "abstract": "A common practice to gain invariant features in object recognition models is to aggregate multiple low-level features over a small neighborhood. However, the differences between those models makes a comparison of the properties of different aggregation functions hard. Our aim is to gain insight into different functions by directly comparing them on a fixed architecture for several common object recognition tasks. Empirical results show that a maximum pooling operation significantly outperforms subsampling operations. Despite their shift-invariant properties, overlapping pooling windows are no significant improvement over non-overlapping pooling windows. By applying this knowledge, we achieve state-of-the-art error rates of 4.57% on the NORB normalized-uniform dataset and 5.6% on the NORB jittered-cluttered dataset.", "title": "Evaluation of Pooling Operations in Convolutional Architectures for Object Recognition"}, "8213dbed4db44e113af3ed17d6dad57471a0c048": {"paper_id": "8213dbed4db44e113af3ed17d6dad57471a0c048", "abstract": null, "title": "The Nature of Statistical Learning Theory"}, "7a65b1edb60a2c9790958d3b77cc6339ffe83f8f": {"paper_id": "7a65b1edb60a2c9790958d3b77cc6339ffe83f8f", "abstract": "Integrated Network-Based Ohio University Network Detective Service (INBOUNDS) is a network based intrusion detection system being developed at Ohio University. The Anomalous Network-Traffic Detection with Self Organizing Maps (ANDSOM) module for INBOUNDS detects anomalous network traffic based on the Self-Organizing Map algorithm. Each network connection is characterized by six parameters and specified as a six-dimensional vector. The ANDSOM module creates a Self-Organizing Map (SOM) having a two-dimensional lattice of neurons for each network service. During the training phase, normal network traffic is fed to the ANDSOM module, and the neurons in the SOM are trained to capture its characteristic patterns. During real-time operation, a network connection is fed to its respective SOM, and a \u201cwinner\u201d is selected by finding the neuron that is closest in distance to it. The network connection is then classified as an intrusion if this distance is more than a pre-set threshold.", "title": "Detecting Anomalous Network Traffic with Self-organizing Maps"}, "efc9bf5e32ed297e150bf3a38581ef13d9ebc0ad": {"paper_id": "efc9bf5e32ed297e150bf3a38581ef13d9ebc0ad", "abstract": "Security & Inspection X-Ray Systems is widely used by custom to accomplish some security missions by inspecting import-export cargo. Due to the specificity of cargo X-Ray image, such as overlap, viewpoint dependence, and variants of cargo categories, it couldn't be understood easily like natural ones by human. Even for experienced screeners, it's very difficult to judge cargo category and contraband. In this paper, cargo X-Ray image is described by joint shape and texture feature, which could reflect both cargo stacking mode and interior details. Classification performance is compared with the benchmark method by top hit 1, 3, 5 ratio, and it's demonstrated that good performance is achieved here. In addition, we also discuss X-Ray image property and explore some reasons why cargo classification under X-Ray is very difficult.", "title": "Joint Shape and Texture Based X-Ray Cargo Image Classification"}, "385fd7e0f8a973186f9fbdbc31b68a970911445c": {"paper_id": "385fd7e0f8a973186f9fbdbc31b68a970911445c", "abstract": "We present a novel Bag-of-Words (BoW) representation scheme for image classification tasks, where the separation of features distinctive of different classes is enforced via class-specific feature-clustering. We investigate the implementation of this approach for the detection of firearms in baggage security X-ray imagery. We implement our novel BoW model using the Speeded-Up Robust Features (SURF) detector and descriptor within a Support Vector Machine (SVM) classifier framework. Experimentation on a large, diverse data set yields a significant improvement in classification performance over previous works with an optimal true positive rate of 99.07% at a false positive rate of 4.31%. Our results indicate that class-specific clustering primes the feature space and ultimately simplifies the classification process. We further demonstrate the importance of using diverse, representative data and efficient training and testing procedures. The excellent performance of the classifier is a strong indication of the potential advantages of this technique in threat object detection in security screening settings.", "title": "Improving feature-based object recognition for X-ray baggage security screening using primed visualwords"}, "fae8f88e591815e7fbba7088221b8ade21c937dc": {"paper_id": "fae8f88e591815e7fbba7088221b8ade21c937dc", "abstract": "We consider the use of transfer learning, via the use of deep Convolutional Neural Networks (CNN) for the image classification problem posed within the context of X-ray baggage security screening. The use of a deep multi-layer CNN approach, traditionally requires large amounts of training data, in order to facilitate construction of a complex complete end-to-end feature extraction, representation and classification process. Within the context of X-ray security screening, limited availability of training for particular items of interest can thus pose a problem. To overcome this issue, we employ a transfer learning paradigm such that a pre-trained CNN, primarily trained for generalized image classification tasks where sufficient training data exists, can be specifically optimized as a later secondary process that targets specific this application domain. For the classical handgun detection problem we achieve 98.92% detection accuracy outperforming prior work in the field and furthermore extend our evaluation to a multiple object classification task within this context.", "title": "Transfer learning using convolutional neural networks for object classification within X-ray baggage security imagery"}, "6c8b30f63f265c32e26d999aa1fef5286b8308ad": {"paper_id": "6c8b30f63f265c32e26d999aa1fef5286b8308ad", "abstract": "Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different \u201cthinned\u201d networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.", "title": "Dropout: a simple way to prevent neural networks from overfitting"}, "567e9a58f3f45f19aaec40179658f74892ef2960": {"paper_id": "567e9a58f3f45f19aaec40179658f74892ef2960", "abstract": "Intrusion detection systems have traditionally been based on the characterization of an attack and the tracking of the activity on the system to see if it matches that characterization. Recently, new intrusion detection systems based on data mining are making their appearance in the field. This paper describes the design and experiences with the ADAM (Audit Data Analysis and Mining) system, which we use as a testbed to study how useful data mining techniques can be in intrusion detection.", "title": "ADAM: A Testbed for Exploring the Use of Data Mining in Intrusion Detection"}, "6665e03447f989c9bdb3432d93e89b516b9d18a7": {"paper_id": "6665e03447f989c9bdb3432d93e89b516b9d18a7", "abstract": null, "title": "Fast Effective Rule Induction"}, "771dc593e9760ae8decf2a197ddbc2f4cb6f5cfd": {"paper_id": "771dc593e9760ae8decf2a197ddbc2f4cb6f5cfd", "abstract": "A Bayesian nonparametric model is a Bayesian model on an infinite-dimensional parameter space. The parameter space is typically chosen as the set of all possible solutions for a given learning problem. For example, in a regression problem the parameter space can be the set of continuous functions, and in a density estimation problem the space can consist of all densities. A Bayesian nonparametric model uses only a finite subset of the available parameter dimensions to explain a finite sample of observations, with the set of dimensions chosen depending on the sample, such that the effective complexity of the model (as measured by the number of dimensions used) adapts to the data. Classical adaptive problems, such as nonparametric estimation and model selection, can thus be formulated as Bayesian inference problems. Popular examples of Bayesian nonparametric models include Gaussian process regression, in which the correlation structure is refined with growing sample size, and Dirichlet process mixture models for clustering, which adapt the number of clusters to the complexity of the data. Bayesian nonparametric models have recently been applied to a variety of machine learning problems, including regression, classification, clustering, latent variable modeling, sequential modeling, image segmentation, source separation and grammar induction.", "title": "Bayesian Nonparametric Models"}, "143d2e02ab91ae6259576ac50b664b8647af8988": {"paper_id": "143d2e02ab91ae6259576ac50b664b8647af8988", "abstract": "A generalization of the sampling method introduced by Metropolis et al. (1953) is presented along with an exposition of the relevant theory, techniques of application and methods and difficulties of assessing the error in Monte Carlo estimates. Examples of the methods, including the generation of random orthogonal matrices and potential applications of the methods to numerical problems arising in statistics, are discussed.", "title": "Monte Carlo sampling methods using Markov chains and their applications"}, "ac5a2d9efc4113b280dbf19159fc61deecc52126": {"paper_id": "ac5a2d9efc4113b280dbf19159fc61deecc52126", "abstract": "We address the problem of noise management in clustering algorithms. Namely, issues that arise when on top of some cluster structure the data also contains an unstructured set of points. We consider how clustering algorithms can be \u201crobustified\u201d so that they recover the cluster structure in spite of the unstructured part of the input. We introduce some quantitative measures of such robustness that take into account the strength of the embedded cluster structure as well as the mildness of the noise subset. We propose a simple and efficient method to turn any centroid-based clustering algorithm into a noiserobust one, and prove robustness guarantees for our method with respect to these measures. We also prove that more straightforward ways of \u201crobustifying\u201d clustering algorithms fail to achieve similar guarantees.", "title": "Clustering in the Presence of Background Noise"}, "6caeb02e315d594506e3787d09d0d4e79314790b": {"paper_id": "6caeb02e315d594506e3787d09d0d4e79314790b", "abstract": "A family of graph-theoretical algorithms based on the minimal spanning tree are capable of detecting several kinds of cluster structure in arbitrary point sets; description of the detected clusters is possible in some cases by extensions of the method. Development of these clustering algorithms was based on examples from two-dimensional space because we wanted to copy the human perception of gestalts or point groupings. On the other hand, all the methods considered apply to higher dimensional spaces and even to general metric spaces. Advantages of these methods include determinacy, easy interpretation of the resulting clusters, conformity to gestalt principles of perceptual organization, and invariance of results under monotone transformations of interpoint distance. Brief discussion is made of the application of cluster detection to taxonomy and the selection of good feature spaces for pattern recognition. Detailed analyses of several planar cluster detection problems are illustrated by text and figures. The well-known Fisher iris data, in four-dimensional space, have been analyzed by these methods also. PL/1 programs to implement the minimal spanning tree methods have been fully debugged.", "title": "Graph-Theoretical Methods for Detecting and Describing Gestalt Clusters"}, "1e3c4e29e527891089306a330e826927d36f1e36": {"paper_id": "1e3c4e29e527891089306a330e826927d36f1e36", "abstract": "1 2 3 4 5 6 Can we organize sampling entities into discrete classes, such that within-group similarity is maximized and among-group similarity is minimized according to some objective criterion? 2 Important Characteristics of Cluster Analysis Techniques P Family of techniques with similar goals. P Operate on data sets for which pre-specified, well-defined groups do \"not\" exist; characteristics of the data are used to assign entities into artificial groups. P Summarize data redundancy by reducing the information on the whole set of say N entities to information about say g groups of nearly similar entities (where hopefully g is very much smaller than N).", "title": "Cluster Analysis"}, "2740b1c450c7b6fa0d7cb8b52681c2e5b6f72752": {"paper_id": "2740b1c450c7b6fa0d7cb8b52681c2e5b6f72752", "abstract": "We define notions of stability for learning algorithms and show how to use these notions to derive generalization error bounds based on the empirical error and the leave-one-out error. The methods we use can be applied in the regression framework as well as in the classification one when the classifier is obtained by thresholding a real-valued function. We study the stability properties of large classes of learning algorithms such as regularization based algorithms. In particular we focus on Hilbert space regularization and Kullback-Leibler regularization. We demonstrate how to apply the results to SVM for regression and classification.", "title": "Stability and Generalization"}, "5d310f003e63e63b3ab51ac714fa9f792d1f5c12": {"paper_id": "5d310f003e63e63b3ab51ac714fa9f792d1f5c12", "abstract": "We present a simple and natural greedy algorithm for the metric uncapacitated facility location problem achieving an approximation guarantee of 1.61. We use this algorithm to find better approximation algorithms for the capacitated facility location problem with soft capacities and for a common generalization of the k-median and facility location problems. We also prove a lower bound of 1+2/e on the approximability of the k-median problem. At the end, we present a discussion about the techniques we have used in the analysis of our algorithm, including a computer-aided method for proving bounds on the approximation factor.", "title": "A new greedy approach for facility location problems"}, "3f1fdbd748f757541ed04f607f996a74a8c0711b": {"paper_id": "3f1fdbd748f757541ed04f607f996a74a8c0711b", "abstract": "We present the first linear time (1 + /spl epsiv/)-approximation algorithm for the k-means problem for fixed k and /spl epsiv/. Our algorithm runs in O(nd) time, which is linear in the size of the input. Another feature of our algorithm is its simplicity - the only technique involved is random sampling.", "title": "A simple linear time (1 + /spl epsiv/)-approximation algorithm for k-means clustering in any dimensions"}, "0b33bbda1a4e46e334e49e3b726b02f2bf42553f": {"paper_id": "0b33bbda1a4e46e334e49e3b726b02f2bf42553f", "abstract": "Comic books are a kind of storytelling graphic publications mainly expressed by abstract line drawings. As a clue of story lines, comic characters play an important role in the story, and their detection is an essential part of comic book analysis. For this purpose, the task includes (1) locating characters in comics pages and (2) identifying them, which is called specific character detection. Corresponding to different scenes of comic books, one specific character can be represented by various expressions coupled with rotations, occlusions, and other perspective drawing effects, which challenge the detection. In this paper, we focus on stable features regarding the possible transformations and proposed a framework to detect them. Specifically, some discriminative features are selected as detectors for characterizing characters, on the basis of a training dataset. Based on the detectors, the drawings of the same characters in different scenes can be detected. The methodology has been experimented and validated on 6 titles of comics. Despite the terrific changes for different scenes, the proposed method achieved detection of 70% comic characters.", "title": "Specific Comic Character Detection Using Local Feature Matching"}, "19abef760f870ae57a83c9c89f02e6b2b5e64161": {"paper_id": "19abef760f870ae57a83c9c89f02e6b2b5e64161", "abstract": "Progress in understanding the brain mechanisms underlying vision requires the construction of computational models that not only emulate the brain's anatomy and physiology, but ultimately match its performance on visual tasks. In recent years, \"natural\" images have become popular in the study of vision and have been used to show apparently impressive progress in building such models. Here, we challenge the use of uncontrolled \"natural\" images in guiding that progress. In particular, we show that a simple V1-like model--a neuroscientist's \"null\" model, which should perform poorly at real-world visual object recognition tasks--outperforms state-of-the-art object recognition systems (biologically inspired and otherwise) on a standard, ostensibly natural image recognition test. As a counterpoint, we designed a \"simpler\" recognition test to better span the real-world variation in object pose, position, and scale, and we show that this test correctly exposes the inadequacy of the V1-like model. Taken together, these results demonstrate that tests based on uncontrolled natural images can be seriously misleading, potentially guiding progress in the wrong direction. Instead, we reexamine what it means for images to be natural and argue for a renewed focus on the core problem of object recognition--real-world image variation.", "title": "Why is Real-World Visual Object Recognition Hard?"}, "0e650b5e54a3624792952899a0f79b91a1d68e79": {"paper_id": "0e650b5e54a3624792952899a0f79b91a1d68e79", "abstract": "Learning visual models of object categories notoriously requires hundreds or thousands of training examples. We show that it is possible to learn much information about a category from just one, or a handful, of images. The key insight is that, rather than learning from scratch, one can take advantage of knowledge coming from previously learned categories, no matter how different these categories might be. We explore a Bayesian implementation of this idea. Object categories are represented by probabilistic models. Prior knowledge is represented as a probability density function on the parameters of these models. The posterior model for an object category is obtained by updating the prior in the light of one or more observations. We test a simple implementation of our algorithm on a database of 101 diverse object categories. We compare category models learned by an implementation of our Bayesian approach to models learned from by maximum likelihood (ML) and maximum a posteriori (MAP) methods. We find that on a database of more than 100 categories, the Bayesian approach produces informative models when the number of training examples is too small for other methods to operate successfully.", "title": "One-shot learning of object categories"}, "27db63ab642d9c27601a9311d65b63e2d2d26744": {"paper_id": "27db63ab642d9c27601a9311d65b63e2d2d26744", "abstract": "While methods for comparing two learning algorithms on a single data set have been scrutinized for quite some time already, the issue of statistical tests for comparisons of more algorithms on multiple data sets, which is even more essential to typical machine learning studies, has been all but ignored. This article reviews the current practice and then theoretically and empirically examines several suitable tests. Based on that, we recommend a set of simple, yet safe and robust non-parametric tests for statistical comparisons of classifiers: the Wilcoxon signed ranks test for comparison of two classifiers and the Friedman test with the corresponding post-hoc tests for comparison of more classifiers over multiple data sets. Results of the latter can also be neatly presented with the newly introduced CD (critical difference) diagrams.", "title": "Statistical Comparisons of Classifiers over Multiple Data Sets"}, "62df185ddf7cd74040dd7f8dd72a9a2d85b734ed": {"paper_id": "62df185ddf7cd74040dd7f8dd72a9a2d85b734ed", "abstract": "The long-standing rationalist tradition in moral psychology emphasizes the role of reason in moral judgment. A more recent trend places increased emphasis on emotion. Although both reason and emotion are likely to play important roles in moral judgment, relatively little is known about their neural correlates, the nature of their interaction, and the factors that modulate their respective behavioral influences in the context of moral judgment. In two functional magnetic resonance imaging (fMRI) studies using moral dilemmas as probes, we apply the methods of cognitive neuroscience to the study of moral judgment. We argue that moral dilemmas vary systematically in the extent to which they engage emotional processing and that these variations in emotional engagement influence moral judgment. These results may shed light on some puzzling patterns in moral judgment observed by contemporary philosophers.", "title": "An fMRI investigation of emotional engagement in moral judgment."}, "2e03d6ed05869da01ca8dedd825a0d4ca14eca75": {"paper_id": "2e03d6ed05869da01ca8dedd825a0d4ca14eca75", "abstract": "Privacy is ever-growing concern in our society and is becoming a fundamental aspect to take into account when one wants to use, publish and analyze data involving human personal sensitive information. Unfortunately, it is increasingly hard to transform the data in a way that it protects sensitive information: we live in the era of big data characterized by unprecedented opportunities to sense, store and analyze social data describing human activities in great detail and resolution. As a result, privacy preservation simply cannot be accomplished by de-identification alone. In this paper, we propose the privacy-by-design paradigm to develop technological frameworks for countering the threats of undesirable, unlawful effects of privacy violation, without obstructing the knowledge discovery opportunities of social mining and big data analytical technologies. Our main idea is to inscribe privacy protection into the knowledge discovery technology by design, so that the analysis incorporates the relevant privacy requirements from the start.", "title": "Privacy-by-design in big data analytics and social mining"}, "03b01daccd140e7d65358f31f8c4472d18573a5a": {"paper_id": "03b01daccd140e7d65358f31f8c4472d18573a5a", "abstract": "There has been increasing interest in the problem of building accurate data mining models over aggregate data, while protecting privacy at the level of individual records. One approach for this problem is to randomize the values in individual records, and only disclose the randomized values. The model is then built over the randomized data, after first compensating for the randomization (at the aggregate level). This approach is potentially vulnerable to privacy breaches: based on the distribution of the data, one may be able to learn with high confidence that some of the randomized records satisfy a specified property, even though privacy is preserved on average.In this paper, we present a new formulation of privacy breaches, together with a methodology, \"amplification\", for limiting them. Unlike earlier approaches, amplification makes it is possible to guarantee limits on privacy breaches without any knowledge of the distribution of the original data. We instantiate this methodology for the problem of mining association rules, and modify the algorithm from [9] to limit privacy breaches without knowledge of the data distribution. Next, we address the problem that the amount of randomization required to avoid privacy breaches (when mining association rules) results in very long transactions. By using pseudorandom generators and carefully choosing seeds such that the desired items from the original transaction are present in the randomized transaction, we can send just the seed instead of the transaction, resulting in a dramatic drop in communication and storage cost. Finally, we define new information measures that take privacy breaches into account when quantifying the amount of privacy preserved by randomization.", "title": "Limiting privacy breaches in privacy preserving data mining"}, "70fda5147aedd42c64143a464117b5ffde18a2e4": {"paper_id": "70fda5147aedd42c64143a464117b5ffde18a2e4", "abstract": "Over the past five years a new approach to privacy-preserving data analysis has born fruit [13, 18, 7, 19, 5, 37, 35, 8, 32]. This approach differs from much (but not all!) of the related literature in the statistics, databases, theory, and cryptography communities, in that a formal and ad omnia privacy guarantee is defined, and the data analysis techniques presented are rigorously proved to satisfy the guarantee. The key privacy guarantee that has emerged is differential privacy. Roughly speaking, this ensures that (almost, and quantifiably) no risk is incurred by joining a statistical database. In this survey, we recall the definition of differential privacy and two basic techniques for achieving it. We then show some interesting applications of these techniques, presenting algorithms for three specific tasks and three general results on differentially private learning.", "title": "Differential Privacy: A Survey of Results"}, "32334506f746e83367cecb91a0ab841e287cd958": {"paper_id": "32334506f746e83367cecb91a0ab841e287cd958", "abstract": "We consider a statistical database in which a trusted administrator introduces noise to the query responses with the goal of maintaining privacy of individual database entries. In such a database, a query consists of a pair (S, f) where S is a set of rows in the database and f is a function mapping database rows to {0, 1}. The true answer is \u03a3i\u03b5S f(di), and a noisy version is released as the response to the query. Results of Dinur, Dwork, and Nissim show that a strong form of privacy can be maintained using a surprisingly small amount of noise -- much less than the sampling error -- provided the total number of queries is sublinear in the number of database rows. We call this query and (slightly) noisy reply the SuLQ (Sub-Linear Queries) primitive. The assumption of sublinearity becomes reasonable as databases grow increasingly large.We extend this work in two ways. First, we modify the privacy analysis to real-valued functions f and arbitrary row types, as a consequence greatly improving the bounds on noise required for privacy. Second, we examine the computational power of the SuLQ primitive. We show that it is very powerful indeed, in that slightly noisy versions of the following computations can be carried out with very few invocations of the primitive: principal component analysis, k means clustering, the Perceptron Algorithm, the ID3 algorithm, and (apparently!) all algorithms that operate in the in the statistical query learning model [11].", "title": "Practical privacy: the SuLQ framework"}, "17fac85921a6538161b30665f55991f7c7e0f940": {"paper_id": "17fac85921a6538161b30665f55991f7c7e0f940", "abstract": "We continue a line of research initiated in [10, 11] on privacypreserving statistical databases. Consider a trusted server that holds a database of sensitive information. Given a query function f mapping databases to reals, the so-called true answer is the result of applying f to the database. To protect privacy, the true answer is perturbed by the addition of random noise generated according to a carefully chosen distribution, and this response, the true answer plus noise, is returned to the user. Previous work focused on the case of noisy sums, in which f = P i g(xi), where xi denotes the ith row of the database and g maps database rows to [0, 1]. We extend the study to general functions f , proving that privacy can be preserved by calibrating the standard deviation of the noise according to the sensitivity of the function f . Roughly speaking, this is the amount that any single argument to f can change its output. The new analysis shows that for several particular applications substantially less noise is needed than was previously understood to be the case. The first step is a very clean characterization of privacy in terms of indistinguishability of transcripts. Additionally, we obtain separation results showing the increased value of interactive sanitization mechanisms over non-interactive.", "title": "Calibrating Noise to Sensitivity in Private Data Analysis"}, "08a0a828c4875c847763585d9234e91f254552d1": {"paper_id": "08a0a828c4875c847763585d9234e91f254552d1", "abstract": "Existing trajectory clustering algorithms group similar trajectories as a whole, thus discovering common trajectories. Our key observation is that clustering trajectories as a whole could miss common sub-trajectories. Discovering common sub-trajectories is very useful in many applications, especially if we have regions of special interest for analysis. In this paper, we propose a new partition-and-group framework for clustering trajectories, which partitions a trajectory into a set of line segments, and then, groups similar line segments together into a cluster. The primary advantage of this framework is to discover common sub-trajectories from a trajectory database. Based on this partition-and-group framework, we develop a trajectory clustering algorithm TRACLUS. Our algorithm consists of two phases: partitioning and grouping. For the first phase, we present a formal trajectory partitioning algorithm using the minimum description length(MDL) principle. For the second phase, we present a density-based line-segment clustering algorithm. Experimental results demonstrate that TRACLUS correctly discovers common sub-trajectories from real trajectory data.", "title": "Trajectory clustering: a partition-and-group framework"}, "4ec6d5e2300da16f0e007d5a72d5c9f12aacb989": {"paper_id": "4ec6d5e2300da16f0e007d5a72d5c9f12aacb989", "abstract": "Individual human trajectories are characterized by fat-tailed distributions of jump sizes and waiting times, suggesting the relevance of continuous-time random-walk (CTRW) models for human mobility. However, human traces are barely random. Given the importance of human mobility, from epidemic modelling to traffic prediction and urban planning, we need quantitative models that can account for the statistical characteristics of individual human trajectories. Here we use empirical data on human mobility, captured by mobile-phone traces, to show that the predictions of the CTRW models are in systematic conflict with the empirical results. We introduce two principles that govern human trajectories, allowing us to build a statistically self-consistent microscopic model for individual human mobility. The model accounts for the empirically observed scaling laws, but also allows us to analytically predict most of the pertinent scaling exponents.", "title": "Modelling the scaling properties of human mobility"}, "b37bf823f74990fcd0df57a8ced5944d3a3b2576": {"paper_id": "b37bf823f74990fcd0df57a8ced5944d3a3b2576", "abstract": "Recently, deep reinforcement learning (RL) methods have been applied successfully to multi-agent scenarios. Typically, these methods rely on a concatenation of agent states to represent the information content required for decentralized decision making. However, concatenation scales poorly to swarm systems with a large number of homogeneous agents as it does not exploit the fundamental properties inherent to these systems: (i) the agents in the swarm are interchangeable and (ii) the exact number of agents in the swarm is irrelevant. Therefore, we propose a new state representation for deep multi-agent RL based on mean embeddings of distributions. We treat the agents as samples of a distribution and use the empirical mean embedding as input for a decentralized policy. We define different feature spaces of the mean embedding using histograms, radial basis functions and a neural network learned end\u2013to\u2013end. We evaluate the representation on two well known problems from the swarm literature (rendezvous and pursuit evasion), in a globally and locally observable setup. For the local setup we furthermore introduce simple communication protocols. Of all approaches, the mean embedding representation using neural network features enables the richest information exchange between neighboring agents facilitating the development of more complex collective strategies.", "title": "Deep Reinforcement Learning for Swarm Systems"}, "2fc09d1d7562dfcc313e35f55e197b7f56ddaacd": {"paper_id": "2fc09d1d7562dfcc313e35f55e197b7f56ddaacd", "abstract": "For problems of data compression, gambling, and prediction of individual sequences 1 the following questions arise. Given a target family of probability mass functions ( 1 ), how do we choose a probability mass function ( 1 ) so that it approximately minimizes the maximum regret /belowdisplayskip10ptminus6pt max (log 1 ( 1 ) log 1 ( 1 )\u0302) and so that it achieves the best constant in the asymptotics of the minimax regret, which is of the form ( 2) log( 2 ) + + (1), where is the parameter dimension? Are there easily implementable strategies that achieve those asymptotics? And how does the solution to the worst case sequence problem relate to the solution to the corresponding expectation version min max (log 1 ( 1 ) log 1 ( 1 ))? In the discrete memoryless case, with a given alphabet of size , the Bayes procedure with the Dirichlet(1 2 1 2) prior is asymptotically maximin. Simple modifications of it are shown to be asymptotically minimax. The best constant is = log( (1 2) ( ( 2)) which agrees with the logarithm of the integral of the square root of the determinant of the Fisher information. Moreover, our asymptotically optimal strategies for the worst case problem are also asymptotically optimal for the expectation version. Analogous conclusions are given for the case of prediction, gambling, and compression when, for each observation, one has access to side information from an alphabet of size . In this setting the minimax regret is shown to be ( 1) 2 log 2 + + (1)", "title": "Asymptotic minimax regret for data compression, gambling, and prediction"}, "b6567e89efb00aed127d0a11294f6ac64d726308": {"paper_id": "b6567e89efb00aed127d0a11294f6ac64d726308", "abstract": "With the tremendous amount of textual data available in the Internet, techniques for abstractive text summarization become increasingly appreciated. In this paper, we present work in progress that tackles the problem of multilingual text summarization using semantic representations. Our system is based on abstract linguistic structures obtained from an analysis pipeline of disambiguation, syntactic and semantic parsing tools. The resulting structures are stored in a semantic repository, from which a text planning component produces content plans that go through a multilingual generation pipeline that produces texts in English, Spanish, French, or German. In this paper we focus on the lingusitic components of the summarizer, both analysis and generation.", "title": "Multilingual Natural Language Generation within Abstractive Summarization"}, "346bcbebafd193aba689269da2de51bfaf163df2": {"paper_id": "346bcbebafd193aba689269da2de51bfaf163df2", "abstract": "This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75 F-measure, a 13 relative decrease in F-measure error over the baseline model's score of 88.2. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative-in terms of both simplicity and efficiency-to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation.", "title": "Discriminative Reranking for Natural Language Parsing"}, "24ff26b9cf71d9f9a8e7dedf0cfe56d105363cd3": {"paper_id": "24ff26b9cf71d9f9a8e7dedf0cfe56d105363cd3", "abstract": "FrameNet is a three-year NSF-supported project in corpus-based computational lexicography, now in its second year (NSF IRI-9618838, \"Tools for Lexicon Building\"). The project's key features are (a) a commitment to corpus evidence for semantic and syntactic generalizations, and (b) the representation of the valences of its target words (mostly nouns, adjectives, and verbs) in which the semantic portion makes use of frame semantics. The resulting database will contain (a) descriptions of the semantic frames underlying the meanings of the words described, and (b) the valence representation (semantic and syntactic) of several thousand words and phrases, each accompanied by (c) a representative collection of annotated corpus attestations, which jointly exemplify the observed linkings between \"frame elements\" and their syntactic realizations (e.g. grammatical function, phrase type, and other syntactic traits). This report will present the project's goals and workflow, and information about the computational tools that have been adapted or created in-house for this work. 1 I n t r o d u c t i o n The Berkeley FrameNet project 1 is producing frame-semantic descriptions of several thousand English lexical items and backing up these descriptions with semantically annotated attestations from contemporary English corpora 2. 1The project is based at the International Computer Science Institute (1947 Center Street, Berkeley, CA). A fuller bibliography may be found in (Lowe et ai., 1997) 2Our main corpus is the British National Corpus. We have access to it through the courtesy of Oxford University Press; the POS-tagged and lemmatized version we use was prepared by the Institut flit Maschinelle Sprachverarbeitung of the University of Stuttgart). The These descriptions are based on hand-tagged semantic annotations of example sentences extracted from large text corpora and systematic analysis of the semantic patterns they exemplify by lexicographers and linguists. The primary emphasis of the project therefore is the encoding, by humans, of semantic knowledge in machine-readable form. The intuition of the lexicographers is guided by and constrained by the results of corpus-based research using highperformance software tools. The semantic domains to be covered are\" HEALTH CARE, CHANCE, PERCEPTION, COMMUNICATION, TRANSACTION, TIME, SPACE, BODY (parts and functions of the body), MOTION, LIFE STAGES, SOCIAL CONTEXT, EMOTION and COGNITION. 1.1 Scope of t h e P r o j e c t The results of the project are (a) a lexical resource, called the FrameNet database 3, and (b) associated software tools. The database has three major components (described in more detail below: \u2022 Lexicon containing entries which are composed of: (a) some conventional dictionary-type data, mainly for the sake of human readers; (b) FORMULAS which capture the morphosyntactic ways in which elements of the semantic frame can be realized within the phrases or sentences built up around the word; (c) links to semantically ANNOTATED EXAMEuropean collaborators whose participation has made this possible are Sue Atkins, Oxford University Press, and Ulrich Held, IMS-Stuttgart. SThe database will ultimately contain at least 5,000 lexical entries together with a parallel annotated corpus, these in formats suitable for integration into applications which use other lexical resources such as WordNet and COMLEX. The final design of the database will be selected in consultation with colleagues at Princeton (WordNet), ICSI, and IMS, and with other members of the NLP community.", "title": "The Berkeley FrameNet Project"}, "ee87e33ed914cdeddfe9ae6a20995f246fb24dec": {"paper_id": "ee87e33ed914cdeddfe9ae6a20995f246fb24dec", "abstract": "Although single dialyzer use and reuse by chemical reprocessing are both associated with some complications, there is no definitive advantage to either in this respect. Some complications occur mainly at the first use of a dialyzer: a new cellophane or cuprophane membrane may activate the complement system, or a noxious agent may be introduced to the dialyzer during production or generated during storage. These agents may not be completely removed during the routine rinsing procedure. The reuse of dialyzers is associated with environmental contamination, allergic reactions, residual chemical infusion (rebound release), inadequate concentration of disinfectants, and pyrogen reactions. Bleach used during reprocessing causes a progressive increase in dialyzer permeability to larger molecules, including albumin. Reprocessing methods without the use of bleach are associated with progressive decreases in membrane permeability, particularly to larger molecules. Most comparative studies have not shown differences in mortality between centers reusing and those not reusing dialyzers, however, the largest cluster of dialysis-related deaths occurred with single-use dialyzers due to the presence of perfluorohydrocarbon introduced during the manufacturing process and not completely removed during preparation of the dialyzers before the dialysis procedure. The cost savings associated with reuse is substantial, especially with more expensive, high-flux synthetic membrane dialyzers. With reuse, some dialysis centers can afford to utilize more efficient dialyzers that are more expensive; consequently they provide a higher dose of dialysis and reduce mortality. Some studies have shown minimally higher morbidity with chemical reuse, depending on the method. Waste disposal is definitely decreased with the reuse of dialyzers, thus environmental impacts are lessened, particularly if reprocessing is done by heat disinfection. It is safe to predict that dialyzer reuse in dialysis centers will continue because it also saves money for the providers. Saving both time for the patient and money for the provider were the main motivations to design a new machine for daily home hemodialysis. The machine, developed in the 1990s, cleans and heat disinfects the dialyzer and lines in situ so they do not need to be changed for a month. In contrast, reuse of dialyzers in home hemodialysis patients treated with other hemodialysis machines is becoming less popular and is almost extinct.", "title": "Dialyzer reuse--part II: advantages and disadvantages."}, "addd6acd5fd1b824eede16f97da3f3f2fca677ba": {"paper_id": "addd6acd5fd1b824eede16f97da3f3f2fca677ba", "abstract": "Network security systems inspect packet payloads for signatures of attacks. These systems use regular expression matching at their core. Many techniques for implementing regular expression matching at line rate have been proposed. Solutions differ in the type of automaton used (i.e., deterministic vs. non-deterministic) and in the configuration of implementation-specific parameters. While each solution has been shown to perform well on specific rule sets and traffic patterns, there has been no systematic comparison across a large set of solutions, rule sets and traffic patterns. Thus, it is extremely challenging for a practitioner to make an informed decision within the plethora of existing algorithmic and architectural proposals. To address this problem, we present a comprehensive evaluation of a broad set of regular expression matching techniques. We consider both algorithmic and architectural aspects. Specifically, we explore the performance, area requirements, and power consumption of implementations targeting processors and field programmable gate arrays using rule sets of practical size and complexity. We present detailed performance results and specific guidelines for determining optimal configurations based on a simple evaluation of the rule set. These guidelines can help significantly when implementing regular expression matching systems in practice.", "title": "Picking Pesky Parameters: Optimizing Regular Expression Matching in Practice"}, "b5de05988cf1b8a9949db315ad4d2f36b35ba21f": {"paper_id": "b5de05988cf1b8a9949db315ad4d2f36b35ba21f", "abstract": "Online health communities and support groups are a valuable source of information for users suffering from a physical or mental illness. Users turn to these forums for moral support or advice on specific conditions, symptoms, or side effects of medications. This paper describes and studies the linguistic patterns of a community of support forum users over time focused on the used of anxious related words. We introduce a methodology to identify groups of individuals exhibiting linguistic patterns associated with anxiety and the correlations between this linguistic pattern and other word usage. We find some evidence that participation in these groups does yield positive effects on their users by reducing the frequency of anxious related word used over time.", "title": "Analysis of Anxious Word Usage on Online Health Forums"}, "50e7733dd260760abff88b793c3b77cc8aeb6ccf": {"paper_id": "50e7733dd260760abff88b793c3b77cc8aeb6ccf", "abstract": "We connect measures of public opinion measured from polls with sentiment measured from text. We analyze several surveys on consumer confidence and political opinion over the 2008 to 2009 period, and find they correlate to sentiment word frequencies in contemporaneous Twitter messages. While our results vary across datasets, in several cases the correlations are as high as 80%, and capture important large-scale trends. The results highlight the potential of text streams as a substitute and supplement for traditional polling. Introduction If we want to know, say, the extent to which the U.S. population likes or dislikes Barack Obama, an obvious thing to do is to ask a random sample of people (i.e., poll). Survey and polling methodology, extensively developed through the 20th century (Krosnick, Judd, and Wittenbrink 2005), gives numerous tools and techniques to accomplish representative public opinion measurement. With the dramatic rise of text-based social media, millions of people broadcast their thoughts and opinions on a great variety of topics. Can we analyze publicly available data to infer population attitudes in the same manner that public opinion pollsters query a population? If so, then mining public opinion from freely available text content could be a faster and less expensive alternative to traditional polls. (A standard telephone poll of one thousand respondents easily costs tens of thousands of dollars to run.) Such analysis would also permit us to consider a greater variety of polling questions, limited only by the scope of topics and opinions people broadcast. Extracting the public opinion from social media text provides a challenging and rich context to explore computational models of natural language, motivating new research in computational linguistics. In this paper, we connect measures of public opinion derived from polls with sentiment measured from analysis of text from the popular microblogging site Twitter. We explicitly link measurement of textual sentiment in microblog messages through time, comparing to contemporaneous polling data. In this preliminary work, summary Copyright c \u00a9 2010, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. statistics derived from extremely simple text analysis techniques are demonstrated to correlate with polling data on consumer confidence and political opinion, and can also predict future movements in the polls. We find that temporal smoothing is a critically important issue to support a successful model. Data We begin by discussing the data used in this study: Twitter for the text data, and public opinion surveys from multiple polling organizations. Twitter Corpus Twitter is a popular microblogging service in which users post messages that are very short: less than 140 characters, averaging 11 words per message. It is convenient for research because there are a very large number of messages, many of which are publicly available, and obtaining them is technically simple compared to scraping blogs from the web. We use 1 billion Twitter messages posted over the years 2008 and 2009, collected by querying the Twitter API,1 as well as archiving the \u201cGardenhose\u201d real-time stream. This comprises a roughly uniform sample of public messages, in the range of 100,000 to 7 million messages per day. (The primary source of variation is growth of Twitter itself; its message volume increased by a factor of 50 over this twoyear time period.) Most Twitter users appear to live in the U.S., but we made no systematic attempt to identify user locations or even message language, though our analysis technique should largely ignore non-English messages. There probably exist many further issues with this text sample; for example, the demographics and communication habits of the Twitter user population probably changed over this time period, which should be adjusted for given our desire to measure attitudes in the general population. There are clear opportunities for better preprocessing and stratified sampling to exploit these data. This scraping effort was conducted by Brendan Meeder. Public Opinion Polls We consider several measures of consumer confidence and political opinion, all obtained from telephone surveys to participants selected through random-digit dialing, a standard technique in traditional polling (Chang and Krosnick 2003). Consumer confidence refers to how optimistic the public feels, collectively, about the health of the economy and their personal finances. It is thought that high consumer confidence leads to more consumer spending; this line of argument is often cited in the popular media and by policymakers (Greenspan 2002), and further relationships with economic activity have been studied (Ludvigson 2004; Wilcox 2007). Knowing the public\u2019s consumer confidence is of great utility for economic policy making as well as business planning. Two well-known surveys that measure U.S. consumer confidence are the Consumer Confidence Index from the Consumer Board, and the Index of Consumer Sentiment (ICS) from the Reuters/University of Michigan Surveys of Consumers.2 We use the latter, as it is more extensively studied in economics, having been conducted since the 1950s. The ICS is derived from answers to five questions administered monthly in telephone interviews with a nationally representative sample of several hundred people; responses are combined into the index score. Two of the questions, for example, are: \u201cWe are interested in how people are getting along financially these days. Would you say that you (and your family living there) are better off or worse off financially than you were a year ago?\u201d \u201cNow turning to business conditions in the country as a whole\u2014do you think that during the next twelve months we\u2019ll have good times financially, or bad times, or what?\u201d We also use another poll, the Gallup Organization\u2019s \u201cEconomic Confidence\u201d index,3 which is derived from answers to two questions that ask interviewees to to rate the overall economic health of the country. This only addresses a subset of the issues that are incorporated into the ICS. We are interested in it because, unlike the ICS, it is administered daily (reported as three-day rolling averages). Frequent polling data are more convenient for our comparison purpose, since we have fine-grained, daily Twitter data, but only over a twoyear period. Both datasets are shown in Figure 1. For political opinion, we use two sets of polls. The first is Gallup\u2019s daily tracking poll for the presidential job approval rating for Barack Obama over the course of 2009, which is reported as 3-day rolling averages.4 These data are shown in Figure 2. The second is a set of tracking polls during the 2008 U.S. presidential election cycle, asking potential voters Downloaded from http://www.sca.isr.umich. edu/. Downloaded from http://www.gallup.com/poll/ 122840/gallup-daily-economic-indexes.aspx. Downloaded from http://www.gallup.com/poll/ 113980/Gallup-Daily-Obama-Job-Approval.aspx. Index G al lu p E co n. C on f.", "title": "From Tweets to Polls: Linking Text Sentiment to Public Opinion Time Series"}, "391d9ef4395cf2f69e7a2f0483d40b6addd95888": {"paper_id": "391d9ef4395cf2f69e7a2f0483d40b6addd95888", "abstract": "In this paper, we propose an approach to automatically detect sentiments on Twitter messages (tweets) that explores some characteristics of how tweets are written and meta-information of the words that compose these messages. Moreover, we leverage sources of noisy labels as our training data. These noisy labels were provided by a few sentiment detection websites over twitter data. In our experiments, we show that since our features are able to capture a more abstract representation of tweets, our solution is more effective than previous ones and also more robust regarding biased and noisy data, which is the kind of data provided by these", "title": "Robust Sentiment Detection on Twitter from Biased and Noisy Data"}, "49b2a1b9606c0ccb95a36895760fc91b8b830266": {"paper_id": "49b2a1b9606c0ccb95a36895760fc91b8b830266", "abstract": "Twitter, a popular microblogging service, has received much attention recently. An important characteristic of Twitter is its real-time nature. For example, when an earthquake occurs, people make many Twitter posts (tweets) related to the earthquake, which enables detection of earthquake occurrence promptly, simply by observing the tweets. As described in this paper, we investigate the real-time interaction of events such as earthquakes in Twitter and propose an algorithm to monitor tweets and to detect a target event. To detect a target event, we devise a classifier of tweets based on features such as the keywords in a tweet, the number of words, and their context. Subsequently, we produce a probabilistic spatiotemporal model for the target event that can find the center and the trajectory of the event location. We consider each Twitter user as a sensor and apply Kalman filtering and particle filtering, which are widely used for location estimation in ubiquitous/pervasive computing. The particle filter works better than other comparable methods for estimating the centers of earthquakes and the trajectories of typhoons. As an application, we construct an earthquake reporting system in Japan. Because of the numerous earthquakes and the large number of Twitter users throughout the country, we can detect an earthquake with high probability (96% of earthquakes of Japan Meteorological Agency (JMA) seismic intensity scale 3 or more are detected) merely by monitoring tweets. Our system detects earthquakes promptly and sends e-mails to registered users. Notification is delivered much faster than the announcements that are broadcast by the JMA.", "title": "Earthquake shakes Twitter users: real-time event detection by social sensors"}, "09d7e1da6754d7b894b03319e764f3e8d6fdd915": {"paper_id": "09d7e1da6754d7b894b03319e764f3e8d6fdd915", "abstract": "The rapid growth of geotagged social media raises new computational possibilities for investigating geographic linguistic variation. In this paper, we present a multi-level generative model that reasons jointly about latent topics and geographical regions. High-level topics such as \u201csports\u201d or \u201centertainment\u201d are rendered differently in each geographic region, revealing topic-specific regional distinctions. Applied to a new dataset of geotagged microblogs, our model recovers coherent topics and their regional variants, while identifying geographic areas of linguistic consistency. The model also enables prediction of an author\u2019s geographic location from raw text, outperforming both text regression and supervised topic models.", "title": "A Latent Variable Model for Geographic Lexical Variation"}, "2b5081abd1c0a606f63c58bfcf47bea6d2075ac5": {"paper_id": "2b5081abd1c0a606f63c58bfcf47bea6d2075ac5", "abstract": "Seasonal influenza epidemics are a major public health concern, causing tens of millions of respiratory illnesses and 250,000 to 500,000 deaths worldwide each year. In addition to seasonal influenza, a new strain of influenza virus against which no previous immunity exists and that demonstrates human-to-human transmission could result in a pandemic with millions of fatalities. Early detection of disease activity, when followed by a rapid response, can reduce the impact of both seasonal and pandemic influenza. One way to improve early detection is to monitor health-seeking behaviour in the form of queries to online search engines, which are submitted by millions of users around the world each day. Here we present a method of analysing large numbers of Google search queries to track influenza-like illness in a population. Because the relative frequency of certain queries is highly correlated with the percentage of physician visits in which a patient presents with influenza-like symptoms, we can accurately estimate the current level of weekly influenza activity in each region of the United States, with a reporting lag of about one day. This approach may make it possible to use search queries to detect influenza epidemics in areas with a large population of web search users.", "title": "Detecting influenza epidemics using search engine query data"}, "ad90445eb2b6615174981e3c36c9aa25754276b8": {"paper_id": "ad90445eb2b6615174981e3c36c9aa25754276b8", "abstract": "The Dirichlet distribution and its compound variant, the Dirichlet-multinomial, are two of the most basic models for proportional data, such as the mix of vocabulary words in a text document. Yet the maximum-likelihood estimate of these distributions is not available in closed-form. This paper describes simple and efficient iterative schemes for obtaining parameter estimates in these models. In each case, a fixed-point iteration and a Newton-Raphson (or generalized Newton-Raphson) iteration is provided. 1 The Dirichlet distribution The Dirichlet distribution is a model of how proportions vary. Let p denote a random vector whose elements sum to 1, so that pk represents the proportion of item k. Under the Dirichlet model with parameter vector \u03b1, the probability density at p is p(p) \u223c D(\u03b11, ..., \u03b1K) = \u0393( \u2211 k \u03b1k) \u220f k \u0393(\u03b1k) \u220f k pk k (1) where pk > 0 (2)", "title": "Estimating a Dirichlet distribution"}, "0d0da3e46ad8a8fd79ec4b689e4601e84dad9595": {"paper_id": "0d0da3e46ad8a8fd79ec4b689e4601e84dad9595", "abstract": "We present a content-driven reputation system for Wikipedia authors. In our system, authors gain reputation when the edits they perform to Wikipedia articles are preserved by subsequent authors, and they lose reputation when their edits are rolled back or undone in short order. Thus, author reputation is computed solely on the basis of content evolution; user-to-user comments or ratings are not used. The author reputation we compute could be used to flag new contributions from low-reputation authors, or it could be used to allow only authors with high reputation to contribute to controversialor critical pages. A reputation system for the Wikipedia could also provide an incentive for high-quality contributions. We have implemented the proposed system, and we have used it to analyze the entire Italian and French Wikipedias, consisting of a total of 691, 551 pages and 5, 587, 523 revisions. Our results show that our notion of reputation has good predictive value: changes performed by low-reputation authors have a significantly larger than average probability of having poor quality, as judged by human observers, and of being later undone, as measured by our algorithms.", "title": "A content-driven reputation system for the wikipedia"}, "f1ae781dfe425c8026a20dbb474770b696cfb172": {"paper_id": "f1ae781dfe425c8026a20dbb474770b696cfb172", "abstract": "MOTIVATION\nThere has recently been a notable shift in biomedical information extraction (IE) from relation models toward the more expressive event model, facilitated by the maturation of basic tools for biomedical text analysis and the availability of manually annotated resources. The event model allows detailed representation of complex natural language statements and can support a number of advanced text mining applications ranging from semantic search to pathway extraction. A recent collaborative evaluation demonstrated the potential of event extraction systems, yet there have so far been no studies of the generalization ability of the systems nor the feasibility of large-scale extraction.\n\n\nRESULTS\nThis study considers event-based IE at PubMed scale. We introduce a system combining publicly available, state-of-the-art methods for domain parsing, named entity recognition and event extraction, and test the system on a representative 1% sample of all PubMed citations. We present the first evaluation of the generalization performance of event extraction systems to this scale and show that despite its computational complexity, event extraction from the entire PubMed is feasible. We further illustrate the value of the extraction approach through a number of analyses of the extracted information.\n\n\nAVAILABILITY\nThe event detection system and extracted data are open source licensed and available at http://bionlp.utu.fi/.", "title": "Complex event extraction at PubMed scale"}, "04f39720b9b20f8ab990228ae3fe4f473e750fe3": {"paper_id": "04f39720b9b20f8ab990228ae3fe4f473e750fe3", "abstract": null, "title": "Probabilistic Graphical Models - Principles and Techniques"}, "87fe8163dcaff9b5cb759439c47c2c4661c78829": {"paper_id": "87fe8163dcaff9b5cb759439c47c2c4661c78829", "abstract": "Opinion Mining and Sentiment Analysis is the field of study that analyzes customer opinions, feedback, sentiments, evaluations, attitudes, and emotions from written language. It is one of the most active research areas in Natural Language Processing (NLP) and is also widely studied in data mining, Web mining, and text mining. The growing importance of sentiment analysis coincides with the growth of social media and Web 2.0 technologies such as reviews, discussion forums, blogs, micro-blogs, and social networks. For the first time in human history, we now have a huge volume of opinionated data recorded in digital form for analysis. In this paper we do a survey of papers on Opinion Mining and Sentiment Analysis and detail the techniques used.", "title": "Opinion Mining and Sentiment Analysis : A Survey"}, "f5653389328cdd9a7562cedf4a4f41b209038c6a": {"paper_id": "f5653389328cdd9a7562cedf4a4f41b209038c6a", "abstract": "1.1 Introduction Relational data has two characteristics: first, statistical dependencies exist between the entities we wish to model, and second, each entity often has a rich set of features that can aid classification. For example, when classifying Web documents, the page's text provides much information about the class label, but hyperlinks define a relationship between pages that can improve classification [Taskar et al., 2002]. Graphical models are a natural formalism for exploiting the dependence structure among entities. Traditionally, graphical models have been used to represent the joint probability distribution p(y, x), where the variables y represent the attributes of the entities that we wish to predict, and the input variables x represent our observed knowledge about the entities. But modeling the joint distribution can lead to difficulties when using the rich local features that can occur in relational data, because it requires modeling the distribution p(x), which can include complex dependencies. Modeling these dependencies among inputs can lead to intractable models, but ignoring them can lead to reduced performance. A solution to this problem is to directly model the conditional distribution p(y|x), which is sufficient for classification. This is the approach taken by conditional random fields [Lafferty et al., 2001]. A conditional random field is simply a conditional distribution p(y|x) with an associated graphical structure. Because the model is", "title": "1 An Introduction to Conditional Random Fields for Relational Learning"}, "2a501b074261e81b9126e80a0a308cfa5e76f8c1": {"paper_id": "2a501b074261e81b9126e80a0a308cfa5e76f8c1", "abstract": "Unbiased language is a requirement for reference sources like encyclopedias and scientific texts. Bias is, nonetheless, ubiquitous, making it crucial to understand its nature and linguistic realization and hence detect bias automatically. To this end we analyze real instances of human edits designed to remove bias from Wikipedia articles. The analysis uncovers two classes of bias: framing bias, such as praising or perspective-specific words, which we link to the literature on subjectivity; and epistemological bias, related to whether propositions that are presupposed or entailed in the text are uncontroversially accepted as true. We identify common linguistic cues for these classes, including factive verbs, implicatives, hedges, and subjective intensifiers. These insights help us develop features for a model to solve a new prediction task of practical importance: given a biased sentence, identify the bias-inducing word. Our linguistically-informed model performs almost as well as humans tested on the same task.", "title": "Linguistic Models for Analyzing and Detecting Biased Language"}, "fd26f8069cfa528463fdf8a90864587e997ee86d": {"paper_id": "fd26f8069cfa528463fdf8a90864587e997ee86d", "abstract": "Community question answering aims at choosing the most appropriate answer for a given question, which is important in many NLP applications. Previous neural network-based methods consider several different aspects of information through calculating attentions. These different kinds of attentions are always simply summed up and can be seen as a \u201csingle view\u201d, causing severe information loss. To overcome this problem, we propose a Multi-View Fusion Neural Network, where each attention component generates a \u201cview\u201d of the QA pair and a fusion RNN integrates the generated views to form a more holistic representation. In this fusion RNN method, a filter gate collects important information of input and directly adds it to the output, which borrows the idea of residual networks. Experimental results on the WikiQA and SemEval-2016 CQA datasets demonstrate that our proposed model outperforms the state-of-the-art methods.", "title": "A Multi-View Fusion Neural Network for Answer Selection"}, "03ff3f8f4d5a700fbe8f3a3e63a39523c29bb60f": {"paper_id": "03ff3f8f4d5a700fbe8f3a3e63a39523c29bb60f", "abstract": "The ability to accurately represent sentences is central to language understanding. We describe a convolutional architecture dubbed the Dynamic Convolutional Neural Network (DCNN) that we adopt for the semantic modelling of sentences. The network uses Dynamic k-Max Pooling, a global pooling operation over linear sequences. The network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations. The network does not rely on a parse tree and is easily applicable to any language. We test the DCNN in four experiments: small scale binary and multi-class sentiment prediction, six-way question classification and Twitter sentiment prediction by distant supervision. The network achieves excellent performance in the first three tasks and a greater than 25% error reduction in the last task with respect to the strongest baseline.", "title": "A Convolutional Neural Network for Modelling Sentences"}, "25ac694fa23f733679496a139e9168472e267865": {"paper_id": "25ac694fa23f733679496a139e9168472e267865", "abstract": "In this paper, we study, compare and combine two state-of-the-art approaches to automatic feature engineering: Convolution Tree Kernels (CTKs) and Convolutional Neural Networks (CNNs) for learning to rank answer sentences in a Question Answering (QA) setting. When dealing with QA, the key aspect is to encode relational information between the constituents of question and answer in learning algorithms. For this purpose, we propose novel CNNs using relational information and combined them with relational CTKs. The results show that (i) both approaches achieve the state of the art on a question answering task, where CTKs produce higher accuracy and (ii) combining such methods leads to unprecedented high results.", "title": "Convolutional Neural Networks vs. Convolution Kernels: Feature Engineering for Answer Sentence Reranking"}, "735786a00d50373434d50f8c82889abbcdc8ea12": {"paper_id": "735786a00d50373434d50f8c82889abbcdc8ea12", "abstract": "This paper describes the KeLP system participating in the SemEval-2016 Community Question Answering (cQA) task. The challenge tasks are modeled as binary classification problems: kernel-based classifiers are trained on the SemEval datasets and their scores are used to sort the instances and produce the final ranking. All classifiers and kernels have been implemented within the Kernel-based Learning Platform called KeLP. Our primary submission ranked first in Subtask A, third in Subtask B and second in Subtask C. These ranks are based on MAP, which is the referring challenge system score. Our approach outperforms all the other systems with respect to all the other challenge metrics.", "title": "KeLP at SemEval-2016 Task 3: Learning Semantic Relations between Questions and Answers"}, "07f3f736d90125cb2b04e7408782af411c67dd5a": {"paper_id": "07f3f736d90125cb2b04e7408782af411c67dd5a", "abstract": "Semantic matching is of central importance to many natural language tasks [2, 28]. A successful matching algorithm needs to adequately model the internal structures of language objects and the interaction between them. As a step toward this goal, we propose convolutional neural network models for matching two sentences, by adapting the convolutional strategy in vision and speech. The proposed models not only nicely represent the hierarchical structures of sentences with their layerby-layer composition and pooling, but also capture the rich matching patterns at different levels. Our models are rather generic, requiring no prior knowledge on language, and can hence be applied to matching tasks of different nature and in different languages. The empirical study on a variety of matching tasks demonstrates the efficacy of the proposed model on a variety of matching tasks and its superiority to competitor models.", "title": "Convolutional Neural Network Architectures for Matching Natural Language Sentences"}, "a62b58c267fddfa06545a7fc63a3c62ef7dc9e15": {"paper_id": "a62b58c267fddfa06545a7fc63a3c62ef7dc9e15", "abstract": "Answer sentence selection is the task of identifying sentences that contain the answer to a given question. This is an important problem in its own right as well as in the larger context of open domain question answering. We propose a novel approach to solving this task via means of distributed representations, and learn to match questions with answers by considering their semantic encoding. This contrasts prior work on this task, which typically relies on classifiers with large numbers of hand-crafted syntactic and semantic features and various external resources. Our approach does not require any feature engineering nor does it involve specialist linguistic data, making this model easily applicable to a wide range of domains and languages. Experimental results on a standard benchmark dataset from TREC demonstrate that\u2014despite its simplicity\u2014our model matches state of the art performance on the answer sentence selection task.", "title": "Deep Learning for Answer Sentence Selection"}, "33261d252218007147a71e40f8367ed152fa2fe0": {"paper_id": "33261d252218007147a71e40f8367ed152fa2fe0", "abstract": "This paper presents a system which learns to answer questions on a broad range of topics from a knowledge base using few handcrafted features. Our model learns low-dimensional embeddings of words and knowledge base constituents; these representations are used to score natural language questions against candidate answers. Training our system using pairs of questions and structured representations of their answers, and pairs of question paraphrases, yields competitive results on a recent benchmark of the literature.", "title": "Question Answering with Subgraph Embeddings"}, "3d1427961edccf8940a360d203e44539db58a60f": {"paper_id": "3d1427961edccf8940a360d203e44539db58a60f", "abstract": "The standard unsupervised recurrent neural network language model (RNNLM) generates sentences one word at a time and does not work from an explicit global distributed sentence representation. In this work, we present an RNN-based variational autoencoder language model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the difficult learning problem presented by this model, demonstrate strong performance in the imputation of missing tokens, and explore many interesting properties of the latent sentence space.", "title": "Generating Sentences from a Continuous Space"}, "d1b78d136e9e6be0aeb814027f0f3fd843606155": {"paper_id": "d1b78d136e9e6be0aeb814027f0f3fd843606155", "abstract": "We describe a new model for learning meaningful representations of text documents from an unlabeled collection of documents. This model is inspired by the recently proposed Replicated Softmax, an undirected graphical model of word counts that was shown to learn a better generative model and more meaningful document representations. Specifically, we take inspiration from the conditional mean-field recursive equations of the Replicated Softmax in order to define a neural network architecture that estimates the probability of observing a new word in a given document given the previously observed words. This paradigm also allows us to replace the expensive softmax distribution over words with a hierarchical distribution over paths in a binary tree of words. The end result is a model whose training complexity scales logarithmically with the vocabulary size instead of linearly as in the Replicated Softmax. Our experiments show that our model is competitive both as a generative model of documents and as a document representation learning algorithm.", "title": "A Neural Autoregressive Topic Model"}, "0c14de67cccfa134eaf1b53f50e0257d4b2e764f": {"paper_id": "0c14de67cccfa134eaf1b53f50e0257d4b2e764f", "abstract": "We introduce a deep, generative autoencoder capable of learning hierarchies of distributed representations from data. Successive deep stochastic hidden layers are equipped with autoregressive connections, which enable the model to be sampled from quickly and exactly via ancestral sampling. We derive an efficient approximate parameter estimation method based on the minimum description length (MDL) principle, which can be seen as maximising a variational lower bound on the log-likelihood, with a feedforward neural network implementing approximate inference. We demonstrate state-of-the-art generative performance on a number of classic data sets: several UCI data sets, MNIST and Atari 2600 games.", "title": "Deep AutoRegressive Networks"}, "a129f612a9eff903d9133244a6f0914ef3cbda72": {"paper_id": "a129f612a9eff903d9133244a6f0914ef3cbda72", "abstract": "We develop a semantic parsing framework based on semantic similarity for open domain question answering (QA). We focus on single-relation questions and decompose each question into an entity mention and a relation pattern. Using convolutional neural network models, we measure the similarity of entity mentions with entities in the knowledge base (KB) and the similarity of relation patterns and relations in the KB. We score relational triples in the KB using these measures and select the top scoring relational triple to answer the question. When evaluated on an open-domain QA task, our method achieves higher precision across different recall points compared to the previous approach, and can improve F1 by 7 points.", "title": "Semantic Parsing for Single-Relation Question Answering"}, "0476b7d387d2a6381a784b2b89ccf7baef098f5e": {"paper_id": "0476b7d387d2a6381a784b2b89ccf7baef098f5e", "abstract": "Textual similarity measurement is a challenging problem, as it requires understanding the semantics of input sentences. Most previous neural network models use coarse-grained sentence modeling, which has difficulty capturing fine-grained word-level information for semantic comparisons. As an alternative, we propose to explicitly model pairwise word interactions and present a novel similarity focus mechanism to identify important correspondences for better similarity measurement. Our ideas are implemented in a novel neural network architecture that demonstrates state-ofthe-art accuracy on three SemEval tasks and two answer selection tasks.", "title": "Pairwise Word Interaction Modeling with Deep Neural Networks for Semantic Similarity Measurement"}, "0f6924633c56832b91836b69aedfd024681e427c": {"paper_id": "0f6924633c56832b91836b69aedfd024681e427c", "abstract": "\u2022 J. Wieting, M. Bansal, K. Gimpel, K. Livescu, and D. Roth. 2015. From paraphrase database to compositional paraphrase model and back. TACL. \u2022 K. S. Tai, R. Socher, and C. D. Manning. 2015. Improved semantic representations from treestructured long short-term memory networks. ACL. \u2022 W. Yin and H. Schutze. 2015. Convolutional neural network for paraphrase identification. NAACL. The product also streams internet radio and comes with a 30-day free trial for realnetworks' rhapsody music subscription. The device plays internet radio streams and comes with a 30-day trial of realnetworks rhapsody music service. Given two sentences, measure their similarity:", "title": "Multi-Perspective Sentence Similarity Modeling with Convolutional Neural Networks"}, "6f1b6007638724124e2763f818ee4ebf2da3ae86": {"paper_id": "6f1b6007638724124e2763f818ee4ebf2da3ae86", "abstract": "Most conventional sentence similarity methods only focus on similar parts of two input sentences, and simply ignore the dissimilar parts, which usually give us some clues and semantic meanings about the sentences. In this work, we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences. The model represents each word as a vector, and calculates a semantic matching vector for each word based on all words in the other sentence. Then, each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector. After this, a twochannel CNN model is employed to capture features by composing the similar and dissimilar components. Finally, a similarity score is estimated over the composed feature vectors. Experimental results show that our model gets the state-of-the-art performance on the answer sentence selection task, and achieves a comparable result on the paraphrase identification task.", "title": "Sentence Similarity Learning by Lexical Decomposition and Composition"}, "0825788b9b5a18e3dfea5b0af123b5e939a4f564": {"paper_id": "0825788b9b5a18e3dfea5b0af123b5e939a4f564", "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.", "title": "Glove: Global Vectors for Word Representation"}, "03fe39386ce90e10ec87f10e00532c5cf30b244f": {"paper_id": "03fe39386ce90e10ec87f10e00532c5cf30b244f", "abstract": "We describe the WIKIQA dataset, a new publicly available set of question and sentence pairs, collected and annotated for research on open-domain question answering. Most previous work on answer sentence selection focuses on a dataset created using the TREC-QA data, which includes editor-generated questions and candidate answer sentences selected by matching content words in the question. WIKIQA is constructed using a more natural process and is more than an order of magnitude larger than the previous dataset. In addition, the WIKIQA dataset also includes questions for which there are no correct sentences, enabling researchers to work on answer triggering, a critical component in any QA system. We compare several systems on the task of answer sentence selection on both datasets and also describe the performance of a system on the problem of answer triggering using the WIKIQA dataset.", "title": "WikiQA: A Challenge Dataset for Open-Domain Question Answering"}, "bd433d471af50b571d7284afb5ee435654ace99f": {"paper_id": "bd433d471af50b571d7284afb5ee435654ace99f", "abstract": "Over last several decades, computer vision researchers have been devoted to find good feature to solve different tasks, such as object recognition, object detection, object segmentation, activity recognition and so forth. Ideal features transform raw pixel intensity values to a representation in which these computer vision problems are easier to solve. Recently, deep features from covolutional neural network(CNN) have attracted many researchers in computer vision. In the supervised setting, these hierarchies are trained to solve specific problems by minimizing an objective function. More recently, the feature learned from large scale image dataset have been proved to be very effective and generic for many computer vision task. The feature learned from recognition task can be used in the object detection task. This work uncover the principles that lead to these generic feature representations in the transfer learning, which does not need to train the dataset again but transfer the rich feature from CNN learned from ImageNet dataset. We begin by summarize some related prior works, particularly the paper in object recognition, object detection and segmentation. We introduce the deep feature to computer vision task in intelligent transportation system. We apply deep feature in object detection task, especially in vehicle detection task. To make fully use of objectness proposals, we apply proposal generator on road marking detection and recognition task. Third, to fully understand the transportation situation, we introduce the deep feature into scene understanding. We experiment each task for different public datasets, and prove our framework is robust.", "title": "Going Deeper with Convolutional Neural Network for Intelligent Transportation"}, "e5d9bd94a65f3ed497a4d36e089b1393b0e5520d": {"paper_id": "e5d9bd94a65f3ed497a4d36e089b1393b0e5520d", "abstract": "A range of Natural Language Processing tasks involve making judgments about the semantic relatedness of a pair of sentences, such as Recognizing Textual Entailment (RTE) and answer selection for Question Answering (QA). A key challenge that these tasks face in common is the lack of explicit alignment annotation between a sentence pair. We capture the alignment by using a novel probabilistic model that models tree-edit operations on dependency parse trees. Unlike previous tree-edit models which require a separate alignment-finding phase and resort to ad-hoc distance metrics, our method treats alignments as structured latent variables, and offers a principled framework for incorporating complex linguistic features. We demonstrate the robustness of our model by conducting experiments for RTE and QA, and show that our model performs competitively on both tasks with the same set of general features.", "title": "Probabilistic Tree-Edit Models with Structured Latent Variables for Textual Entailment and Question Answering"}, "f795853dba1b4dc7ead4c4c5d94d4e1666a5df24": {"paper_id": "f795853dba1b4dc7ead4c4c5d94d4e1666a5df24", "abstract": "This paper presents our approach to semantic relatedness and textual entailment subtasks organized as task 1 in SemEval 2014. Specifically, we address two questions: (1) Can we solve these two subtasks together? (2) Are features proposed for textual entailment task still effective for semantic relatedness task? To address them, we extracted seven types of features including text difference measures proposed in entailment judgement subtask, as well as common text similarity measures used in both subtasks. Then we exploited the same feature set to solve the both subtasks by considering them as a regression and a classification task respectively and performed a study of influence of different features. We achieved the first and the second rank for relatedness and entailment task respectively.", "title": "ECNU: One Stone Two Birds: Ensemble of Heterogenous Measures for Semantic Relatedness and Textual Entailment"}, "d0edfd4cafaa66722f2b653d92682a3576f1bb72": {"paper_id": "d0edfd4cafaa66722f2b653d92682a3576f1bb72", "abstract": "This paper describes MITRE\u2019s participation in the Paraphrase and Semantic Similarity in Twitter task (SemEval-2015 Task 1). This effort placed first in Semantic Similarity and second in Paraphrase Identification with scores of Pearson\u2019s r of 61.9%, F1 of 66.7%, and maxF1 of 72.4%. We detail the approaches we explored including mixtures of string matching metrics, alignments using tweet-specific distributed word representations, recurrent neural networks for modeling similarity with those alignments, and distance measurements on pooled latent semantic features. Logistic regression is used to tie the systems together into the ensembles submitted for evaluation.", "title": "MITRE: Seven Systems for Semantic Similarity in Tweets"}, "1682b8b395c7d7fa30b3cec961ac81fdda53e72d": {"paper_id": "1682b8b395c7d7fa30b3cec961ac81fdda53e72d", "abstract": "We present a new deep learning architecture Bi-CNN-MI for paraphrase identification (PI). Based on the insight that PI requires comparing two sentences on multiple levels of granularity, we learn multigranular sentence representations using convolutional neural network (CNN) and model interaction features at each level. These features are then the input to a logistic classifier for PI. All parameters of the model (for embeddings, convolution and classification) are directly optimized for PI. To address the lack of training data, we pretrain the network in a novel way using a language modeling task. Results on the MSRP corpus surpass that of previous NN competitors.", "title": "Convolutional Neural Network for Paraphrase Identification"}, "6a694487451957937adddbd682d3851fabd45626": {"paper_id": "6a694487451957937adddbd682d3851fabd45626", "abstract": "State-of-the-art question answering (QA) systems employ term-density ranking to retrieve answer passages. Such methods often retrieve incorrect passages as relationships among question terms are not considered. Previous studies attempted to address this problem by matching dependency relations between questions and answers. They used strict matching, which fails when semantically equivalent relationships are phrased differently. We propose fuzzy relation matching based on statistical models. We present two methods for learning relation mapping scores from past QA pairs: one based on mutual information and the other on expectation maximization. Experimental results show that our method significantly outperforms state-of-the-art density-based passage retrieval methods by up to 78% in mean reciprocal rank. Relation matching also brings about a 50% improvement in a system enhanced by query expansion.", "title": "Question answering passage retrieval using dependency relations"}, "16c3eaad79a51a9e1a408d768a5096ca7675d4fd": {"paper_id": "16c3eaad79a51a9e1a408d768a5096ca7675d4fd", "abstract": "We describe the application of kernel methods to Natural Language Processing (NLP) problems. In many NLP tasks the objects being modeled are strings, trees, graphs or other discrete structures which require some mechanism to convert them into feature vectors. We describe kernels for various natural language structures, allowing rich, high dimensional representations of these structures. We show how a kernel over trees can be applied to parsing using the voted perceptron algorithm, and we give experimental results on the ATIS corpus of parse trees.", "title": "Convolution Kernels for Natural Language"}, "14c146d457bbd201f3a117ee9c848300d341e5d0": {"paper_id": "14c146d457bbd201f3a117ee9c848300d341e5d0", "abstract": "The Conference on Computational Natural Language Learning is accompanied every year by a shared task whose purpose is to promote natural language processing applications and evaluate them in a standard setting. In 2008 the shared task was dedicated to the joint parsing of syntactic and semantic dependencies. This shared task not only unifies the shared tasks of the previous four years under a unique dependency-based formalism, but also extends them significantly: this year\u2019s syntactic dependencies include more information such as named-entity boundaries; the semantic dependencies model roles of both verbal and nominal predicates. In this paper, we define the shared task and describe how the data sets were created. Furthermore, we report and analyze the results and describe the approaches of the participating systems.", "title": "The CoNLL 2008 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies"}, "0fd0e3854ee696148e978ec33d5c042554cd4d23": {"paper_id": "0fd0e3854ee696148e978ec33d5c042554cd4d23", "abstract": "This document provides additional details about the experiments described in (Heilman and Smith, 2010). Note that while this document provides information about the datasets and experimental methods, it does not provide further results. If you have any further questions, please feel free to contact the first author. The preprocessed datasets (i.e., tagged and parsed) will be made available for research purposes upon request.", "title": "Tree Edit Models for Recognizing Textual Entailments, Paraphrases, and Answers to Questions"}, "7aa63f414a4d7c6e4369a15a04dc5d3eb5da2b0e": {"paper_id": "7aa63f414a4d7c6e4369a15a04dc5d3eb5da2b0e", "abstract": "This paper proposes a framework for automatically engineering features for two important tasks of question answering: answer sentence selection and answer extraction. We represent question and answer sentence pairs with linguistic structures enriched by semantic information, where the latter is produced by automatic classifiers, e.g., question classifier and Named Entity Recognizer. Tree kernels applied to such structures enable a simple way to generate highly discriminative structural features that combine syntactic and semantic information encoded in the input trees. We conduct experiments on a public benchmark from TREC to compare with previous systems for answer sentence selection and answer extraction. The results show that our models greatly improve on the state of the art, e.g., up to 22% on F1 (relative improvement) for answer extraction, while using no additional resources and no manual feature engineering.", "title": "Automatic Feature Engineering for Answer Selection and Extraction"}, "3e393df4a5731fb7b49cf2f527fed1ee4e6e6942": {"paper_id": "3e393df4a5731fb7b49cf2f527fed1ee4e6e6942", "abstract": "In this paper, we study the answer sentence selection problem for question answering. Unlike previous work, which primarily leverages syntactic analysis through dependency tree matching, we focus on improving the performance using models of lexical semantic resources. Experiments show that our systems can be consistently and significantly improved with rich lexical semantic information, regardless of the choice of learning algorithms. When evaluated on a benchmark dataset, the MAP and MRR scores are increased by 8 to 10 points, compared to one of our baseline systems using only surface-form matching. Moreover, our best system also outperforms pervious work that makes use of the dependency tree structure by a wide margin.", "title": "Question Answering Using Enhanced Lexical Semantic Models"}, "118be9cf31ecf5cbdd49a19da8615b593ec61a4c": {"paper_id": "118be9cf31ecf5cbdd49a19da8615b593ec61a4c", "abstract": "This paper presents a syntax-driven approach to question answering, specifically the answer-sentence selection problem for short-answer questions. Rather than using syntactic features to augment existing statistical classifiers (as in previous work), we build on the idea that questions and their (correct) answers relate to each other via loose but predictable syntactic transformations. We propose a probabilistic quasi-synchronous grammar, inspired by one proposed for machine translation (D. Smith and Eisner, 2006), and parameterized by mixtures of a robust nonlexical syntax/alignment model with a(n optional) lexical-semantics-driven log-linear model. Our model learns soft alignments as a hidden variable in discriminative training. Experimental results using the TREC dataset are shown to significantly outperform strong state-of-the-art baselines.", "title": "What is the Jeopardy Model? A Quasi-Synchronous Grammar for QA"}, "fc981c140ae0331b5ea20f48cb92762d8e8316b9": {"paper_id": "fc981c140ae0331b5ea20f48cb92762d8e8316b9", "abstract": "We present MultiGranCNN, a general deep learning architecture for matching text chunks. MultiGranCNN supports multigranular comparability of representations: shorter sequences in one chunk can be directly compared to longer sequences in the other chunk. MultiGranCNN also contains a flexible and modularized match feature component that is easily adaptable to different types of chunk matching. We demonstrate stateof-the-art performance of MultiGranCNN on clause coherence and paraphrase identification tasks.", "title": "MultiGranCNN: An Architecture for General Matching of Text Chunks on Multiple Levels of Granularity"}, "18cd08248be658abcfb6f72a3033b911474541e9": {"paper_id": "18cd08248be658abcfb6f72a3033b911474541e9", "abstract": "We apply a general deep learning framework to address the non-factoid question answering task. Our approach does not rely on any linguistic tools and can be applied to different languages or domains. Various architectures are presented and compared. We create and release a QA corpus and setup a new QA task in the insurance domain. Experimental results demonstrate superior performance compared to the baseline methods and various technologies give further improvements. For this highly challenging task, the top-1 accuracy can reach up to 65.3% on a test set, which indicates a great potential for practical use.", "title": "Applying deep learning to answer selection: A study and an open task"}, "0dab72129b4458d9e3dbf1f109848c2d6d7af8a8": {"paper_id": "0dab72129b4458d9e3dbf1f109848c2d6d7af8a8", "abstract": "Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.", "title": "A large annotated corpus for learning natural language inference"}, "1971425c8e95d75dca4fec126504d7fa6179c926": {"paper_id": "1971425c8e95d75dca4fec126504d7fa6179c926", "abstract": "Knowledge is indispensable to understanding. The ongoing information explosion highlights the need to enable machines to better understand electronic text in human language. Much work has been devoted to creating universal ontologies or taxonomies for this purpose. However, none of the existing ontologies has the needed depth and breadth for universal understanding. In this paper, we present a universal, probabilistic taxonomy that is more comprehensive than any existing ones. It contains 2.7 million concepts harnessed automatically from a corpus of 1.68 billion web pages. Unlike traditional taxonomies that treat knowledge as black and white, it uses probabilities to model inconsistent, ambiguous and uncertain information it contains. We present details of how the taxonomy is constructed, its probabilistic modeling, and its potential applications in text understanding.", "title": "Probase: a probabilistic taxonomy for text understanding"}, "11ec56898a9e7f401a2affe776b5297bd4e25025": {"paper_id": "11ec56898a9e7f401a2affe776b5297bd4e25025", "abstract": "This paper presents the task on the evaluation of Compositional Distributional Semantics Models on full sentences organized for the first time within SemEval2014. Participation was open to systems based on any approach. Systems were presented with pairs of sentences and were evaluated on their ability to predict human judgments on (i) semantic relatedness and (ii) entailment. The task attracted 21 teams, most of which participated in both subtasks. We received 17 submissions in the relatedness subtask (for a total of 66 runs) and 18 in the entailment subtask (65 runs).", "title": "SemEval-2014 Task 1: Evaluation of Compositional Distributional Semantic Models on Full Sentences through Semantic Relatedness and Textual Entailment"}, "12db83e66e50152e170d5009c425c925ad2e2c2a": {"paper_id": "12db83e66e50152e170d5009c425c925ad2e2c2a", "abstract": "Automatically recognizing entailment relations between pairs of natural language sentences has so far been the dominion of classifiers employing hand engineered features derived from natural language processing pipelines. End-to-end differentiable neural architectures have failed to approach state-of-the-art performance until very recently. In this paper, we propose a neural model that reads two sentences to determine entailment using long short-term memory units. We extend this model with a word-by-word neural attention mechanism that encourages reasoning over entailments of pairs of words and phrases. Furthermore, we present a qualitative analysis of attention weights produced by this model, demonstrating such reasoning capabilities. On a large entailment dataset this model outperforms the previous best neural model and a classifier with engineered features by a substantial margin. It is the first generic end-to-end differentiable system that achieves state-of-the-art accuracy on a textual entailment dataset.", "title": "Reasoning about Entailment with Neural Attention"}, "54a0c38cc9e94ed65f6edea08f04ee9839198523": {"paper_id": "54a0c38cc9e94ed65f6edea08f04ee9839198523", "abstract": "Unauthorized or rogue access points (APs) produce security vulnerabilities in enterprise/campus networks by circumventing inherent security mechanisms. We propose to use the round trip time (RTT) of network traffic to distinguish between wired and wireless nodes. This information coupled with a standard wireless AP authorization policy allows the differentiation (at a central location) between wired nodes, authorized APs, and rogue APs. We show that the lower capacity and the higher variability in a wireless network can be used to effectively distinguish between wired and wireless nodes. Further, this detection is not dependant upon the wireless technology (802.11a, 802.11b, or 802.11g), is scalable, does not contain the inefficiencies of current solutions, remains valid as the capacity of wired and wireless links increase, and is independent of the signal range of the rogue APs.", "title": "A Passive Approach to Rogue Access Point Detection"}, "b6a40e6b1775c09348ed822c5300ef1cd58cfe71": {"paper_id": "b6a40e6b1775c09348ed822c5300ef1cd58cfe71", "abstract": "Big Data Analytics and Deep Learning are two high-focus of data science. Big Data has become important as many organizations both public and private have been collecting massive amounts of domain-specific information, which can contain useful information about problems such as national intelligence, cyber security, fraud detection, marketing, and medical informatics. Companies such as Google and Microsoft are analyzing large volumes of data for business analysis and decisions, impacting existing and future technology. Deep Learning algorithms extract high-level, complex abstractions as data representations through a hierarchical learning process. Complex abstractions are learnt at a given level based on relatively simpler abstractions formulated in the preceding level in the hierarchy. A key benefit of Deep Learning is the analysis and learning of massive amounts of unsupervised data, making it a valuable tool for Big Data Analytics where raw data is largely unlabeled and un-categorized. In the present study, we explore how Deep Learning can be utilized for addressing some important problems in Big Data Analytics, including extracting complex patterns from massive volumes of data, semantic indexing, data tagging, fast information retrieval, and simplifying discriminative tasks. We also investigate some aspects of Deep Learning research that need further exploration to incorporate specific challenges introduced by Big Data Analytics, including streaming data, high-dimensional data, scalability of models, and distributed computing. We conclude by presenting insights into relevant future works by posing some questions, including defining data sampling criteria, domain adaptation modeling, defining criteria for obtaining useful data abstractions, improving semantic indexing, semi-supervised learning, and active learning.", "title": "Deep learning applications and challenges in big data analytics"}, "1835227a28b84b8e7c93a7232437b54c31c52a02": {"paper_id": "1835227a28b84b8e7c93a7232437b54c31c52a02", "abstract": "It is possible to combine multiple latent-variable models of the same data by multiplying their probability distributions together and then renormalizing. This way of combining individual expert models makes it hard to generate samples from the combined model but easy to infer the values of the latent variables of each expert, because the combination rule ensures that the latent variables of different experts are conditionally independent when given the data. A product of experts (PoE) is therefore an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary. Training a PoE by maximizing the likelihood of the data is difficult because it is hard even to approximate the derivatives of the renormalization term in the combination rule. Fortunately, a PoE can be trained using a different objective function called contrastive divergence whose derivatives with regard to the parameters can be approximated accurately and efficiently. Examples are presented of contrastive divergence learning using several types of expert on several types of data.", "title": "Training Products of Experts by Minimizing Contrastive Divergence"}, "2077d0f30507d51a0d3bbec4957d55e817d66a59": {"paper_id": "2077d0f30507d51a0d3bbec4957d55e817d66a59", "abstract": "We develop a framework for learning generic, expressive image priors that capture the statistics of natural scenes and can be used for a variety of machine vision tasks. The approach extends traditional Markov random field (MRF) models by learning potential functions over extended pixel neighborhoods. Field potentials are modeled using a Products-of-Experts framework that exploits nonlinear functions of many linear filter responses. In contrast to previous MRF approaches all parameters, including the linear filters themselves, are learned from training data. We demonstrate the capabilities of this Field of Experts model with two example applications, image denoising and image inpainting, which are implemented using a simple, approximate inference scheme. While the model is trained on a generic image database and is not tuned toward a specific application, we obtain results that compete with and even outperform specialized techniques.", "title": "Fields of Experts: a framework for learning image priors"}, "a7a1a298c44b5aff9bdca6ea9908c425796404b7": {"paper_id": "a7a1a298c44b5aff9bdca6ea9908c425796404b7", "abstract": "Practical experience has shown that in order to obtain the best possible performance, prior knowledge about invariances of a classification problem at hand ought to be incorporated into the training procedure. We describe and review all known methods for doing so in support vector machines, provide experimental results, and discuss their respective merits. One of the significant new results reported in this work is our recent achievement of the lowest reported test error on the well-known MNIST digit recognition benchmark task, with SVM training times that are also significantly faster than previous SVM methods.", "title": "Training Invariant Support Vector Machines"}, "7b1cc19dec9289c66e7ab45e80e8c42273509ab6": {"paper_id": "7b1cc19dec9289c66e7ab45e80e8c42273509ab6", "abstract": "Neural networks are a powerful technology for classification of visual inputs arising from documents. However, there is a confusing plethora of different neural network methods that are used in the literature and in industry. This paper describes a set of concrete best practices that document analysis researchers can use to get good results with neural networks. The most important practice is getting a training set as large as possible: we expand the training set by adding a new form of distorted data. The next most important practice is that convolutional neural networks are better suited for visual document tasks than fully connected networks. We propose that a simple \u201cdo-it-yourself\u201d implementation of convolution with a flexible architecture is suitable for many visual document problems. This simple convolutional neural network does not require complex methods, such as momentum, weight decay, structuredependent learning rates, averaging layers, tangent prop, or even finely-tuning the architecture. The end result is a very simple yet general architecture which can yield state-of-the-art performance for document analysis. We illustrate our claims on the MNIST set of English digit", "title": "Best Practices for Convolutional Neural Networks Applied to Visual Document Analysis"}, "589b8659007e1124f765a5d1bd940b2bf4d79054": {"paper_id": "589b8659007e1124f765a5d1bd940b2bf4d79054", "abstract": "Your use of the JSTOR archive indicates your acceptance of JSTOR's Terms and Conditions of Use, available at http://www.jstor.org/page/info/about/policies/terms.jsp. JSTOR's Terms and Conditions of Use provides, in part, that unless you have obtained prior permission, you may not download an entire issue of a journal or multiple copies of articles, and you may use content in the JSTOR archive only for your personal, non-commercial use.", "title": "Projection Pursuit Regression"}, "a120c05ad7cd4ce2eb8fb9697e16c7c4877208a5": {"paper_id": "a120c05ad7cd4ce2eb8fb9697e16c7c4877208a5", "abstract": "Neal, R.M., Connectionist learning of belief networks, Artificial Intelligence 56 (1992) 71-113. Connectionist learning procedures are presented for \"sigmoid\" and \"noisy-OR\" varieties of probabilistic belief networks. These networks have previously been seen primarily as a means of representing knowledge derived from experts. Here it is shown that the \"Gibbs sampling\" simulation procedure for such networks can support maximum-likelihood learning from empirical data through local gradient ascent. This learning procedure resembles that used for \"Boltzmann machines\", and like it, allows the use of \"hidden\" variables to model correlations between visible variables. Due to the directed nature of the connections in a belief network, however, the \"negative phase\" of Boltzmann machine learning is unnecessary. Experimental results show that, as a result, learning in a sigmoid belief network can be faster than in a Boltzmann machine. These networks have other advantages over Boltzmann machines in pattern classification and decision making applications, are naturally applicable to unsupervised learning problems, and provide a link between work on connectionist learning and work on the representation of expert knowledge.", "title": "Connectionist Learning of Belief Networks"}, "6f0144dc7ba19123ddce8cdd4ad0f6dc36dd4ef2": {"paper_id": "6f0144dc7ba19123ddce8cdd4ad0f6dc36dd4ef2", "abstract": "International guidelines recommend the use of Gonadotropin-Releasing Hormone (GnRH) agonists in adolescents with gender dysphoria (GD) to suppress puberty. Little is known about the way gender dysphoric adolescents themselves think about this early medical intervention. The purpose of the present study was (1) to explicate the considerations of gender dysphoric adolescents in the Netherlands concerning the use of puberty suppression; (2) to explore whether the considerations of gender dysphoric adolescents differ from those of professionals working in treatment teams, and if so in what sense. This was a qualitative study designed to identify considerations of gender dysphoric adolescents regarding early treatment. All 13 adolescents, except for one, were treated with puberty suppression; five adolescents were trans girls and eight were trans boys. Their ages ranged between 13 and 18\u00a0years, with an average age of 16\u00a0years and 11\u00a0months, and a median age of 17\u00a0years and 4\u00a0months. Subsequently, the considerations of the adolescents were compared with views of clinicians treating youth with GD. From the interviews with the gender dysphoric adolescents, three themes emerged: (1) the difficulty of determining what is an appropriate lower age limit for starting puberty suppression. Most adolescents found it difficult to define an appropriate age limit and saw it as a dilemma; (2) the lack of data on the long-term effects of puberty suppression. Most adolescents stated that the lack of long-term data did not and would not stop them from wanting puberty suppression; (3) the role of the social context, for which there were two subthemes: (a) increased media-attention, on television, and on the Internet; (b) an imposed stereotype. Some adolescents were positive about the role of the social context, but others raised doubts about it. Compared to clinicians, adolescents were often more cautious in their treatment views. It is important to give voice to gender dysphoric adolescents when discussing the use of puberty suppression in GD. Otherwise, professionals might act based on assumptions about adolescents' opinions instead of their actual considerations. We encourage gathering more qualitative research data from gender dysphoric adolescents in other countries.", "title": "Perceptions of Sex, Gender, and Puberty Suppression: A Qualitative Analysis of Transgender Youth"}, "0222c7dd4562330a2304087d6a669a0b48bc93ae": {"paper_id": "0222c7dd4562330a2304087d6a669a0b48bc93ae", "abstract": "The hypothesis that dopamine is important for reward has been proposed in a number of forms, each of which has been challenged. Normally, rewarding stimuli such as food, water, lateral hypothalamic brain stimulation and several drugs of abuse become ineffective as rewards in animals given performance-sparing doses of dopamine antagonists. Dopamine release in the nucleus accumbens has been linked to the efficacy of these unconditioned rewards, but dopamine release in a broader range of structures is implicated in the 'stamping-in' of memory that attaches motivational importance to otherwise neutral environmental stimuli.", "title": "Dopamine, learning and motivation"}, "84d6aec209bbdf55a2ece7544f6b322817860a01": {"paper_id": "84d6aec209bbdf55a2ece7544f6b322817860a01", "abstract": "The ability to stop motor responses depends critically on the right inferior frontal cortex (IFC) and also engages a midbrain region consistent with the subthalamic nucleus (STN). Here we used diffusion-weighted imaging (DWI) tractography to show that the IFC and the STN region are connected via a white matter tract, which could underlie a \"hyperdirect\" pathway for basal ganglia control. Using a novel method of \"triangulation\" analysis of tractography data, we also found that both the IFC and the STN region are connected with the presupplementary motor area (preSMA). We hypothesized that the preSMA could play a conflict detection/resolution role within a network between the preSMA, the IFC, and the STN region. A second experiment tested this idea with functional magnetic resonance imaging (fMRI) using a conditional stop-signal paradigm, enabling examination of behavioral and neural signatures of conflict-induced slowing. The preSMA, IFC, and STN region were significantly activated the greater the conflict-induced slowing. Activation corresponded strongly with spatial foci predicted by the DWI tract analysis, as well as with foci activated by complete response inhibition. The results illustrate how tractography can reveal connections that are verifiable with fMRI. The results also demonstrate a three-way functional-anatomical network in the right hemisphere that could either brake or completely stop responses.", "title": "Triangulating a cognitive control network using diffusion-weighted magnetic resonance imaging (MRI) and functional MRI."}, "ae518958399c11dd0163f00168e3079228a8c237": {"paper_id": "ae518958399c11dd0163f00168e3079228a8c237", "abstract": "Apart from some general issues related to the Gender Identity Disorder (GID) diagnosis, such as whether it should stay in the DSM-V or not, a number of problems specifically relate to the current criteria of the GID diagnosis for adolescents and adults. These problems concern the confusion caused by similarities and differences of the terms transsexualism and GID, the inability of the current criteria to capture the whole spectrum of gender variance phenomena, the potential risk of unnecessary physically invasive examinations to rule out intersex conditions (disorders of sex development), the necessity of the D criterion (distress and impairment), and the fact that the diagnosis still applies to those who already had hormonal and surgical treatment. If the diagnosis should not be deleted from the DSM, most of the criticism could be addressed in the DSM-V if the diagnosis would be renamed, the criteria would be adjusted in wording, and made more stringent. However, this would imply that the diagnosis would still be dichotomous and similar to earlier DSM versions. Another option is to follow a more dimensional approach, allowing for different degrees of gender dysphoria depending on the number of indicators. Considering the strong resistance against sexuality related specifiers, and the relative difficulty assessing sexual orientation in individuals pursuing hormonal and surgical interventions to change physical sex characteristics, it should be investigated whether other potentially relevant specifiers (e.g., onset age) are more appropriate.", "title": "The DSM diagnostic criteria for gender identity disorder in adolescents and adults."}, "a176dfee19e9120060190fa1707f0a54071d9fd1": {"paper_id": "a176dfee19e9120060190fa1707f0a54071d9fd1", "abstract": "We studied a North American sample of female-to-male (FtM) transsexuals sexually attracted to men, aiming to understand their identity and sexuality in the context of a culture of transgender empowerment. Sex-reassigned FtM transsexuals, 18 years or older and attracted to men, were recruited via an FtM community conference and listserv. Participants (N = 25) responded to open-ended questions about identity development, sexual behavior, and social support. Data were analyzed by content analysis. Scores for sexual identity, self esteem, sexual functioning, and psychological adjustment were compared to those of a comparison group (N = 76 nontransgender gay and bisexual men). Of the 25 FtMs, 15 (60%) identified as gay, 8 (32%) as bisexual, and 2 (8%) as queer. All were comfortable with their gender identity and sexual orientation. The FtM group was more bisexual than the nontransgender gay and bisexual controls. No significant group differences were found in self esteem, sexual satisfaction, or psychological adjustment. For some FtMs, sexual attractions and experiences with men affirmed their gender identity; for others, self-acceptance of a transgender identity facilitated actualization of their attractions toward men. Most were \"out\" as transgender among friends and family, but not on the job or within the gay community. Disclosure and acceptance of their homosexuality was limited. The sexual identity of gay and bisexual FtMs appears to mirror the developmental process for nontransgender homosexual men and women in several ways; however, participants also had experiences unique to being both transgender and gay/bisexual. This signals the emergence of a transgender sexuality.", "title": "Gay and bisexual identity development among female-to-male transsexuals in North America: emergence of a transgender sexuality."}, "f2ffd64a9831805316026f63fa630dd435d2fe56": {"paper_id": "f2ffd64a9831805316026f63fa630dd435d2fe56", "abstract": "This chapter identifies the most robust conclusions and ideas about adolescent development and psychological functioning that have emerged since Petersen's 1988 review. We begin with a discussion of topics that have dominated recent research, including adolescent problem behavior, parent-adolescent relations, puberty, the development of the self, and peer relations. We then identify and examine what seem to us to be the most important new directions that have come to the fore in the last decade, including research on diverse populations, contextual influences on development, behavioral genetics, and siblings. We conclude with a series of recommendations for future research on adolescence.", "title": "Adolescent development."}, "add0be5cff0eafa1059be85ae35869d2ae407177": {"paper_id": "add0be5cff0eafa1059be85ae35869d2ae407177", "abstract": "Oscar is a conversational intelligent tutoring system (CITS) which dynamically predicts and adapts to a student's learning style throughout the tutoring conversation. Oscar aims to mimic a human tutor to improve the effectiveness of the learning experience by leading a natural language tutorial and adapting material to suit an individual's learning style. Prediction of learning style is undertaken through capturing independent variables during the conversation. The variable with the highest value determines the individuals learning style. This paper proposes a new method which uses a fuzzy classification tree to build a fuzzy predictive model using these variables which are captured through natural language dialogue Experiments have been undertaken on two of the learning style dimensions: perception (sensory-intuitive) and understanding (sequential-global). Early results show the model has substantially increased the predictive accuracy of the Oscar CITS and discovered some interesting relationships amongst these variables.", "title": "On predicting learning styles in conversational intelligent tutoring systems using fuzzy classification trees"}, "7b71a46f960f3d6e5e878646a6313e2a6fd927e5": {"paper_id": "7b71a46f960f3d6e5e878646a6313e2a6fd927e5", "abstract": "Students have different levels of motivation, different attitudes about teaching and learning, and different responses to specific classroom environments and instructional practices. The more thoroughly instructors understand the differences, the better chance they have of meeting the diverse learning needs of all of their students. Three categories of diversity that have been shown to have important implications for teaching and learning are differences in students\u2019 learning styles (characteristic ways of taking in and processing information), approaches to learning (surface, deep, and strategic), and intellectual development levels (attitudes about the nature of knowledge and how it should be acquired and evaluated). This article reviews models that have been developed for each of these categories, outlines their pedagogical implications, and suggests areas for further study.", "title": "Understanding Student Differences"}, "33d3896fe9a143dc7eced9ff75321a6f6777aeec": {"paper_id": "33d3896fe9a143dc7eced9ff75321a6f6777aeec", "abstract": "There is an interplay between emotions and learning, but this interaction is far more complex than previous theories have articulated. This article proffers a novel model by which to: 1). regard the interplay of emotions upon learning for, 2). the larger practical aim of crafting computer-based models that will recognize a learner\u2019s affective state and respond appropriately to it so that learning will proceed at an optimal pace. 1. Looking around then moving forward The extent to which emotional upsets can interfere with mental life is no news to teachers. Students who are anxious, angry, or depressed don\u2019t learn; people who are caught in these states do not take in information efficiently or deal with it well. Daniel Goleman, Emotional Intelligence Educators have emphasized conveying information and facts; rarely have they modeled the learning process. When teachers present material to the class, it is usually in a polished form that omits the natural steps of making mistakes (e.g., feeling confused), recovering from them (e.g., overcoming frustration), deconstructing what went wrong (e.g., not becoming dispirited), and starting over again (with hope and enthusiasm). Those who work in science, math, engineering, and technology (SMET) as professions know that learning naturally involves failure and a host of associated affective responses. Yet, educators of SMET learners have rarely illuminated these natural concomitants of the learning experience. The result is that when students see that they are not getting the facts right (on quizzes, exams, etc.), then they tend to believe that they are either \u2018not good at this,\u2019 \u2018can\u2019t do it,\u2019 or that they are simply \u2018stupid\u2019 when it comes to these subjects. What we fail to teach them is that all these feelings associated with various levels of failure are normal parts of learning, and that they can actually be helpful signals for how to learn better. Expert teachers are very adept at recognizing and addressing the emotional state of learners and, based upon their observation they take some action that positively impacts learning. But what do these expert teachers \u2018see\u2019 and how do they decide upon a course of action? How do students who have strayed from learning return to a productive path, such as the one that Csikszentmihalyi [1990] refers to as his \u201czone of flow\u201d? Skilled humans can assess emotional signals with varying degrees of accuracy, and researchers are beginning to make progress giving computers similar abilities at recognizing affective expressions. We believe that accurately identifying a learner\u2019s cognitive-emotional state is a critical mentoring skill. Although computers perform as well as or better than people in selected domains, they do not yet rise to human levels of mentoring. We envision that computers will soon become capable of recognizing human behaviors indicative of the user\u2019s affective state. We have begun research that will lead to our building of a computerized Learning Companion that will track the affective state of a learner through their learning journey. It will recognize cognitive-emotive state (affective state), and respond appropriately. We believe that the first task is to evolve new pedagogical models, which assess whether or not learning is proceeding at a healthy rate and intervene appropriately; then these pedagogical models will be integrated into a computerized environment. Two issues face us, one is to research new educational pedagogy, and the other is a matter of building computerized mechanisms that will accurately and immediately recognize a learner\u2019s state by some ubiquitous method and activate an appropriate response. Axis -1. 0 -0. 5 0 +0. 5 +1. 0 Anxiety-Confidence Anxiety Worry Discomfort Comfort Hopeful Confident Boredom-Fascination Ennui Boredom Indifference Interest Curiosity Intrigue Frustration-Euphoria Frustration Puzzlement Confusion Insight Enlightenment Epiphany Dispirited-Encouraged Dispirited Disappointed Dissatisfied Satisfied Thrilled Enthusiastic Terror-Enchantment Terror Dread Apprehension Calm Anticipatory Excited Figure 1 \u2013 Emotion sets possibly relevant to learning 2. Two sets of research results This research project will have two sets of results. This paper offers the first set of results, which consists of our model and a research method to investigate the issue. A future paper will contain the results of the empirical research\u2014the second set of results. This paper will address two aspects of our current research. Section 3 will outline our theoretical frameworks and define our model (Figures 1 and 2). Section 4 will describe our empirical research methods. 3. Guiding theoretical frameworks: An ideal model of learning process Before describing the model\u2019s dynamics, we should say something about the space of emotions it names. Previous emotion theories have proposed that there are from two to twenty basic or prototype emotions (see for example, Plutchik, 1980; Leidelmeijer, 1991). The four most common emotions appearing on the many theorists\u2019 lists are fear, anger, sadness, and joy. Plutchik [1980] distinguished among eight basic emotions: fear, anger, sorrow, joy, disgust, acceptance, anticipation, and surprise. Ekman [1992] has focused on a set of from six to eight basic emotions that have associated facial expressions. However, none of the existing frameworks address emotions commonly seen in SMET learning experiences, some of which we have noted in Figure 1. Whether all of these are important, and whether the axes shown in Figure 1 are the \u201cright\u201d ones remains to be evaluated, and it will no doubt take many investigations before a \u201cbasic emo tion set for learning\u201d can be established. Such a set may be culturally different and will likely vary with developmental age as well. For example, it has been argued that infants come into this world only expressing interest, distress, and pleasure [Lewis, 1993] and that these three states provide sufficiently rich initial cues to the caregiver that she or he can scaffold the learning experience appropriately in response. We believe that skilled observant human tutors and mentors (teachers) react to assist students based on a few \u2018least common denominators\u2019 of affect as opposed to a large number of complex factors; thus, we expect that the space of emotions presented here might be simplified and refined further as we tease out which states are most important for shaping the companion\u2019s responses. Constructive Learning Disappointment Awe Puzzlement Satisfaction Confusion Curiosity II I Negative Positive Affect Affect III IV Frustration Hopefulness Discard Fresh research Misconceptions", "title": "An Affective Model of Interplay between Emotions and Learning: Reengineering Educational Pedagogy - Building a Learning Companion"}, "9aa7c0f8d963f4c908f5dd21366d9d25872baf16": {"paper_id": "9aa7c0f8d963f4c908f5dd21366d9d25872baf16", "abstract": "Why/AutoTutor is a tutoring system that helps students construct answers to qualitative physics problems by holding a conversation in natural language. Why/AutoTutor provides feedback to the student on what the student types in (positive, neutral, negative feedback), pumps the student for more information, prompts the student to fill in missing words, gives hints, fills in missing information with assertions, identifies and corrects bad answers and misconceptions, answers students\u2019 questions, and summarizes answers. In essence, constructivist learning is implemented in a mixedinitiative dialog. Why/AutoTutor delivers its dialog moves with an animated conversational agent whereas students type in their answers via keyboard. We conducted an experiment that compared Why/AutoTutor with two control conditions (Read textbook, nothing) in assessments of learning gains. The tutoring system performed significantly better than the two control conditions on a test similar to the Force Concept Inventory. AutoTutor and Why/AutoTutor Why/AutoTutor is the most recent tutoring system in the AutoTutor series developed by the Tutoring Research Group at the University of Memphis. Why/AutoTutor was specifically designed to help college students learn Newtonian qualitative physics (Graesser, VanLehn, Rose, Jordan, & Harter, 2001), whereas the previous AutoTutor systems were on topics of introductory computer literacy (Graesser, Person, Harter, & TRG, 2001; Graesser, P. Wiemer-Hastings, K. Wiemer-Hastings, & Kreuz, 1999) and military tactical reasoning (Ryder, Graesser, McNamara, Karnavat, & Pop, 2002). The design of AutoTutor was inspired by three bodies of theoretical, empirical, and applied research. These include explanation-based constructivist theories of learning (Aleven & Koedinger, 2002; Chi, deLeeuw, Chiu, LaVancher, 1994; VanLehn, Jones, & Chi, 1992), intelligent tutoring systems that adaptively respond to student knowledge (Anderson, Corbett, Koedinger, & Pelletier, 1995; VanLehn, Lynch, et al.,2002), and empirical research that has documented the collaborative constructive activities that routinely occur during human tutoring (Chi, Siler, Jeong, Yamauchi, & Hausmann, 2001; Fox, 1993; Graesser, Person, & Magliano, 1995; Moore, 1995; Shah, Evens, Michael, & Rovick, 2002). The process of actively constructing explanations and elaborations of the learning material allegedly produces better learning than merely presenting information to students. This is where human tutors excel in scaffolding learning, because they guide the students in productive constructive processes and simultaneously respond to the student\u2019s information needs. Surprisingly, the dialog moves of most human tutors are not particularly sophisticated from the standpoint of today\u2019s pedagogical theories and those theories implemented in intelligent tutoring systems (Graesser et al., 1995). Human tutors normally coach the student in filling in missing pieces of information in an expected answer and they fix bugs and misconceptions that are manifested by the student during the tutorial dialog. Human tutors rarely implement bona fide Socratic tutoring strategies, modeling-scaffolding-fading, and other intelligent pedagogical techniques (Collins, Brown, & Newman, 1989). The argument has been made that it is the conversational properties of human tutorial dialog, not sophisticated tutoring tactics, that explain why normal human tutors facilitate learning (Graesser et al., 1995). More sophisticated pedagogical techniques will no doubt increase learning even further. Why/AutoTutor was designed to simulate the dialog moves of normal human tutors who coach students in constructing explanations. Why/AutoTutor helps students learn by presenting challenging problems (or questions) from a curriculum script and then engaging in mixed initiative dialog that guides the student in constructing an answer. An example question is \u201cSuppose a boy is in a free-falling elevator and he holds his keys motionless right in front of his face and then lets go. What will happen to the keys? Explain why.\u201d Another example question is \u201cWhen a car without headrests on the seats is struck from behind, the passengers often suffer neck injuries. Why do passengers get neck injuries in this situation?\u201d Such questions are designed to require about a paragraph of information (3-7 sentences) to answer. However, initial answers to these questions are typically only 1 or 2 sentences in length, even though students have more knowledge that is relevant to an answer. This is where tutorial dialog may be particularly helpful. AutoTutor engages the student in a mixed initiative dialog that assists", "title": "Why/AutoTutor: A Test of Learning Gains from a Physics Tutor with Natural Language Dialog"}, "32de3580916a1e5f36bc54779ba5904c19daa403": {"paper_id": "32de3580916a1e5f36bc54779ba5904c19daa403", "abstract": "been developed during the last 20 years have proven to be quite successful, particularly in the domains of mathematics, science, and technology. They produce significant learning gains beyond classroom environments. They are capable of engaging most students\u2019 attention and interest for hours. We have been working on a new generation of intelligent tutoring systems that hold mixedinitiative conversational dialogues with the learner. The tutoring systems present challenging problems and questions to the learner, the learner types in answers in English, and there is a lengthy multiturn dialogue as complete solutions or answers evolve. This article presents the tutoring systems that we have been developing. AUTOTUTOR is a conversational agent, with a talking head, that helps college students learn about computer literacy. ANDES, ATLAS, AND WHY2 help adults learn about physics. Instead of being mere information-delivery systems, our systems help students actively construct knowledge through conversations.", "title": "Intelligent Tutoring Systems with Conversational Dialogue"}, "eb839c06ce9db59ed3279cec7df0f2fb09d9583a": {"paper_id": "eb839c06ce9db59ed3279cec7df0f2fb09d9583a", "abstract": "An onolysis of student learning with the LISP tutor indicates thot while LISP is complex, learning it is simple. The key to factoring out the complexity of LiSP is to monitor the leorning of the 500 productions in the LISP tutor which describe the progromming skill. The learning of these productions follows the power-low learning curve typical of skill acquisition. There is transfer from other progromming experience to the extent that this programming experience involves the some productions. Subjects oppeor to differ only on the general dimensions of how well they acquire the productions ond how well they retain the productions. lnstructionol monipulotions such OS remediotion, content of feedback. ond timing of feedback ore effective to the extent they give students more practice progromming, and explain to students why correct solutions work.", "title": "Skill Acquisition and the LISP Tutor"}, "0b481669a82f9db73033129ef8bf02fe2d728c9f": {"paper_id": "0b481669a82f9db73033129ef8bf02fe2d728c9f", "abstract": "We present AD, a new algorithm for approximate maximum a posteriori (MAP) inference on factor graphs, based on the alternating directions method of multipliers. Like other dual decomposition algorithms, AD has a modular architecture, where local subproblems are solved independently, and their solutions are gathered to compute a global update. The key characteristic of AD is that each local subproblem has a quadratic regularizer, leading to faster convergence, both theoretically and in practice. We provide closed-form solutions for these AD subproblems for binary pairwise factors and factors imposing first-order logic constraints. For arbitrary factors (large or combinatorial), we introduce an active set method which requires only an oracle for computing a local MAP configuration, making AD applicable to a wide range of problems. Experiments on synthetic and real-world problems show that AD compares favorably with the state-of-the-art.", "title": "AD3: alternating directions dual decomposition for MAP inference in graphical models"}, "1fbfa8b590ce4679367d73cb8e4f2d169ae5c624": {"paper_id": "1fbfa8b590ce4679367d73cb8e4f2d169ae5c624", "abstract": "Convex programming involves a convex set F \u2286 R and a convex function c : F \u2192 R. The goal of convex programming is to find a point in F which minimizes c. In this paper, we introduce online convex programming. In online convex programming, the convex set is known in advance, but in each step of some repeated optimization problem, one must select a point in F before seeing the cost function for that step. This can be used to model factory production, farm production, and many other industrial optimization problems where one is unaware of the value of the items produced until they have already been constructed. We introduce an algorithm for this domain, apply it to repeated games, and show that it is really a generalization of infinitesimal gradient ascent, and the results here imply that generalized infinitesimal gradient ascent (GIGA) is universally consistent.", "title": "Online Convex Programming and Generalized Infinitesimal Gradient Ascent"}, "3fa58a0dcbf7392e02f744a0f729ba8010b7334c": {"paper_id": "3fa58a0dcbf7392e02f744a0f729ba8010b7334c", "abstract": "We present a new part-of-speech tagger that demonstrates the following ideas: (i) explicit use of both preceding and following tag contexts via a dependency network representation, (ii) broad use of lexical features, including jointly conditioning on multiple consecutive words, (iii) effective use of priors in conditional loglinear models, and (iv) fine-grained modeling of unknown word features. Using these ideas together, the resulting tagger gives a 97.24% accuracy on the Penn Treebank WSJ, an error reduction of 4.4% on the best previous single automatically learned tagging result.", "title": "Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network"}, "6de9e91967483d78a92270f43ef91b3625ad3810": {"paper_id": "6de9e91967483d78a92270f43ef91b3625ad3810", "abstract": "We study primal solutions obtained as a by-product of subgradient methods when solving the Lagrangian dual of a primal convex constrained optimization problem (possibly nonsmooth). The existing literature on the use of subgradient methods for generating primal optimal solutions is limited to the methods producing such solutions only asymptotically (i.e., in the limit as the number of subgradient iterations increases to infinity). Furthermore, no convergence rate results are known for these algorithms. In this paper, we propose and analyze dual subgradient methods using averaging to generate approximate primal optimal solutions. These algorithms use a constant stepsize as opposed to a diminishing stepsize which is dominantly used in the existing primal recovery schemes. We provide estimates on the convergence rate of the primal sequences. In particular, we provide bounds on the amount of feasibility violation of the generated approximate primal solutions. We also provide upper and lower bounds on the primal function values at the approximate solutions. The feasibility violation and primal value estimates are given per iteration, thus providing practical stopping criteria. Our analysis relies on the Slater condition and the inherited boundedness properties of the dual problem under this condition.", "title": "Approximate Primal Solutions and Rate Analysis for Dual Subgradient Methods"}, "368f3dea4f12c77dfc9b7203f3ab2b9efaecb635": {"paper_id": "368f3dea4f12c77dfc9b7203f3ab2b9efaecb635", "abstract": "We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy wordlevel alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.", "title": "Statistical Phrase-Based Translation"}, "28d9c89f946a39d3c4146bf32ddff546845e1801": {"paper_id": "28d9c89f946a39d3c4146bf32ddff546845e1801", "abstract": "We present a simple and effective semisupervised method for training dependency parsers. We focus on the problem of lexical representation, introducing features that incorporate word clusters derived from a large unannotated corpus. We demonstrate the effectiveness of the approach in a series of dependency parsing experiments on the Penn Treebank and Prague Dependency Treebank, and we show that the cluster-based features yield substantial gains in performance across a wide range of conditions. For example, in the case of English unlabeled second-order parsing, we improve from a baseline accuracy of 92.02% to 93.16%, and in the case of Czech unlabeled second-order parsing, we improve from a baseline accuracy of 86.13% to 87.13%. In addition, we demonstrate that our method also improves performance when small amounts of training data are available, and can roughly halve the amount of supervised data required to reach a desired level of performance.", "title": "Simple Semi-supervised Dependency Parsing"}, "27891ad3fbd8420c1a5a7459bf72b2e299487aab": {"paper_id": "27891ad3fbd8420c1a5a7459bf72b2e299487aab", "abstract": "Integer Linear Programming has recently been used for decoding in a number of probabilistic models in order to enforce global constraints. However, in certain applications, such as non-projective dependency parsing and machine translation, the complete formulation of the decoding problem as an integer linear program renders solving intractable. We present an approach which solves the problem incrementally, thus we avoid creating intractable integer linear programs. This approach is applied to Dutch dependency parsing and we show how the addition of linguistically motivated constraints can yield a significant improvement over stateof-the-art.", "title": "Incremental Integer Linear Programming for Non-projective Dependency Parsing"}, "67cd52465b3cf18137135c94dc0298f8b8255666": {"paper_id": "67cd52465b3cf18137135c94dc0298f8b8255666", "abstract": "Future plastic materials will be very different from those that are used today. The increasing importance of sustainability promotes the development of bio-based and biodegradable polymers, sometimes misleadingly referred to as 'bioplastics'. Because both terms imply \"green\" sources and \"clean\" removal, this paper aims at critically discussing the sometimes-conflicting terminology as well as renewable sources with a special focus on the degradation of these polymers in natural environments. With regard to the former we review innovations in feedstock development (e.g. microalgae and food wastes). In terms of the latter, we highlight the effects that polymer structure, additives, and environmental variables have on plastic biodegradability. We argue that the 'biodegradable' end-product does not necessarily degrade once emitted to the environment because chemical additives used to make them fit for purpose will increase the longevity. In the future, this trend may continue as the plastics industry also is expected to be a major user of nanocomposites. Overall, there is a need to assess the performance of polymer innovations in terms of their biodegradability especially under realistic waste management and environmental conditions, to avoid the unwanted release of plastic degradation products in receiving environments.", "title": "Environmental performance of bio-based and biodegradable plastics: the road ahead."}, "2c39b0daaa22267ce169dd745cbc6dfec78cae3d": {"paper_id": "2c39b0daaa22267ce169dd745cbc6dfec78cae3d", "abstract": "KEMI-TORNIO UNIVERSITY OF APPLIED SCIENCES Degree programme: Business Information Technology Writer: Guo, Shuhang Thesis title: Analysis and evaluation of similarity metrics in collaborative filtering recommender system Pages (of which appendix): 62 (1) Date: May 15, 2014 Thesis instructor: Ryabov, Vladimir This research is focused on the field of recommender systems. The general aims of this thesis are to summary the state-of-the-art in recommendation systems, evaluate the efficiency of the traditional similarity metrics with varies of data sets, and propose an ideology to model new similarity metrics. The literatures on recommender systems were studied for summarizing the current development in this filed. The implementation of the recommendation and evaluation was achieved by Apache Mahout which provides an open source platform of recommender engine. By importing data information into the project, a customized recommender engine was built. Since the recommending results of collaborative filtering recommender significantly rely on the choice of similarity metrics and the types of the data, several traditional similarity metrics provided in Apache Mahout were examined by the evaluator offered in the project with five data sets collected by some academy groups. From the evaluation, I found out that the best performance of each similarity metric was achieved by optimizing the adjustable parameters. The features of each similarity metric were obtained and analyzed with practical data sets. In addition, an ideology by combining two traditional metrics was proposed in the thesis and it was proven applicable and efficient by the metrics combination of Pearson correlation and Euclidean distance. The observation and evaluation of traditional similarity metrics with practical data is helpful to understand their features and suitability, from which new models can be created. Besides, the ideology proposed for modeling new similarity metrics can be found useful both theoretically and practically.", "title": "Analysis and Evaluation of Similarity Metrics in Collaborative Filtering Recommender System"}, "36e59e71a19fba9f61012e8653a9eee884eac93c": {"paper_id": "36e59e71a19fba9f61012e8653a9eee884eac93c", "abstract": "The generation of RDF data has accelerated to the point where many data sets need to be partitioned across multiple machines in order to achieve reasonable performance when querying the data. Although tremendous progress has been made in the Semantic Web community for achieving high performance data management on a single node, current solutions that allow the data to be partitioned across multiple machines are highly inefficient. In this paper, we introduce a scalable RDF data management system that is up to three orders of magnitude more efficient than popular multi-node RDF data management systems. In so doing, we introduce techniques for (1) leveraging state-of-the-art single node RDF-store technology (2) partitioning the data across nodes in a manner that helps accelerate query processing through locality optimizations and (3) decomposing SPARQL queries into high performance fragments that take advantage of how data is partitioned in a cluster.", "title": "Scalable SPARQL Querying of Large RDF Graphs"}, "b78c04c7f29ddaeaeb208d4eae684ffccd71e04f": {"paper_id": "b78c04c7f29ddaeaeb208d4eae684ffccd71e04f", "abstract": "The success of the von Neumann model of sequential computation is attributable to the fact that it is an efficient bridge between software and hardware: high-level languages can be efficiently compiled on to this model; yet it can be effeciently implemented in hardware. The author argues that an analogous bridge between software and hardware in required for parallel computation if that is to become as widely used. This article introduces the bulk-synchronous parallel (BSP) model as a candidate for this role, and gives results quantifying its efficiency both in implementing high-level language features and algorithms, as well as in being implemented in hardware.", "title": "A Bridging Model for Parallel Computation"}, "e7d30fefe1b99c21813873f976e46d03dc82b4fc": {"paper_id": "e7d30fefe1b99c21813873f976e46d03dc82b4fc", "abstract": "Link prediction is an important task for analying social networks which also has applications in other domains like, information retrieval, bioinformatics and e-commerce. There exist a variety of techniques for link prediction, ranging from feature-based classi cation and kernel-based method to matrix factorization and probabilistic graphical models. These methods differ from each other with respect to model complexity, prediction performance, scalability, and generalization ability. In this article, we survey some representative link prediction methods by categorizing them by the type of the models. We largely consider three types of models: rst, the traditional (non-Bayesian) models which extract a set of features to train a binary classi cation model. Second, the probabilistic approaches which model the joint-probability among the entities in a network by Bayesian graphical models. And, nally the linear algebraic approach which computes the similarity between the nodes in a network by rank-reduced similarity matrices. We discuss various existing link prediction models that fall in these broad categories and analyze their strength and weakness. We conclude the survey with a discussion on recent developments and future research direction.", "title": "A Survey of Link Prediction in Social Networks"}, "2b0cc03aa4625a09958c20dc721f4e0a52c13fd0": {"paper_id": "2b0cc03aa4625a09958c20dc721f4e0a52c13fd0", "abstract": "While high-level data parallel frameworks, like MapReduce, simplify the design and implementation of large-scale data processing systems, they do not naturally or efficiently support many important data mining and machine learning algorithms and can lead to inefficient learning systems. To help fill this critical void, we introduced the GraphLab abstraction which naturally expresses asynchronous, dynamic, graph-parallel computation while ensuring data consistency and achieving a high degree of parallel performance in the shared-memory setting. In this paper, we extend the GraphLab framework to the substantially more challenging distributed setting while preserving strong data consistency guarantees. We develop graph based extensions to pipelined locking and data versioning to reduce network congestion and mitigate the effect of network latency. We also introduce fault tolerance to the GraphLab abstraction using the classic Chandy-Lamport snapshot algorithm and demonstrate how it can be easily implemented by exploiting the GraphLab abstraction itself. Finally, we evaluate our distributed implementation of the GraphLab abstraction on a large Amazon EC2 deployment and show 1-2 orders of magnitude performance gains over Hadoop-based implementations.", "title": "Distributed GraphLab : A Framework for Machine Learning and Data Mining in the Cloud"}, "290be78976ac43d8b9a060dad7c3bb1f02a2ebef": {"paper_id": "290be78976ac43d8b9a060dad7c3bb1f02a2ebef", "abstract": "Research papers available on the World Wide Web (WWW or Web) areoften poorly organized, often exist in forms opaque to searchengines (e.g. Postscript), and increase in quantity daily.Significant amounts of time and effort are typically needed inorder to find interesting and relevant publications on the Web. Wehave developed a Web based information agent that assists the userin the process of performing a scientific literature search. Givena set of keywords, the agent uses Web search engines and heuristicsto locate and download papers. The papers are parsed in order toextract information features such as the abstract and individuallyidentified citations. The agents Web interface can be used to findrelevant papers in the database using keyword searches, or bynavigating the links between papers formed by the citations. Linksto both citing and cited publications can be followed. In additionto simple browsing and keyword searches, the agent can find paperswhich are similar to a given paper using word information and byanalyzing common citations made by the papers.", "title": "CiteSeer: An Autonous Web Agent for Automatic Retrieval and Identification of Interesting Publications"}, "09c162757d253a61b4b20f975cea58a1554a8917": {"paper_id": "09c162757d253a61b4b20f975cea58a1554a8917", "abstract": "Recommender systems are now popular both commercially and in the research community, where many approaches have been suggested for providing recommendations. In many cases a system designer that wishes to employ a recommendation system must choose between a set of candidate approaches. A first step towards selecting an appropriate algorithm is to decide which properties of the application to focus upon when making this choice. Indeed, recommendation systems have a variety of properties that may affect user experience, such as accuracy, robustness, scalability, and so forth. In this paper we discuss how to compare recommenders based on a set of properties that are relevant for the application. We focus on comparative studies, where a few algorithms are compared using some evaluation metric, rather than absolute benchmarking of algorithms. We describe experimental settings appropriate for making choices between algorithms. We review three types of experiments, starting with an offline setting, where recommendation approaches are compared without user interaction, then reviewing user studies, where a small group of subjects experiment with the system and report on the experience, and finally describe large scale online experiments, where real user populations interact with the system. In each of these cases we describe types of questions that can be answered, and suggest protocols for experimentation. We also discuss how to draw trustworthy conclusions from the conducted experiments. We then review a large set of properties, and explain how to evaluate systems given relevant properties. We also survey a large set of evaluation metrics in the context of the property that they evaluate. Guy Shani Microsoft Research, One Microsoft Way, Redmond, WA, e-mail: guyshani@microsoft.com Asela Gunawardana Microsoft Research, One Microsoft Way, Redmond, WA, e-mail: aselag@microsoft.com", "title": "Evaluating Recommendation Systems"}, "9740e73b0fe1885aaa3f77e3c1d03f8e86d842bb": {"paper_id": "9740e73b0fe1885aaa3f77e3c1d03f8e86d842bb", "abstract": "The number of research papers available is growing at a staggering rate. Researchers need tools to help them find the papers they should read among all the papers published each year. In this paper, we present and experiment with hybrid recommender algorithms that combine Collaborative Filtering and Content-based. Filtering to recommend research papers to users. Our hybrid algorithms combine the strengths of each filtering approach to address their individual weaknesses. We evaluated our algorithms through offline experiments on a database of 102, 000 research papers, and through an online experiment with 110 users. For both experiments we used a dataset created from the CiteSeer repository of computer science research papers. We developed separate English and Portuguese versions of the interface and specifically recruited American and Brazilian users to test for cross-cultural effects. Our results show that users value paper recommendations, that the hybrid algorithms can be successfully combined, that different algorithms are more suitable for recommending different kinds of papers, and that users with different levels of experience perceive recommendations differently These results can be applied to develop recommender systems for other types of digital libraries.", "title": "Enhancing digital libraries with TechLens"}, "4f6e61d2ab1e2f468cc4bc8fdd8d6f13efaba468": {"paper_id": "4f6e61d2ab1e2f468cc4bc8fdd8d6f13efaba468", "abstract": "Most of the existing approaches to collaborative filtering cannot handle very large data sets. In this paper we show how a class of two-layer undirected graphical models, called Restricted Boltzmann Machines (RBM's), can be used to model tabular data, such as user's ratings of movies. We present efficient learning and inference procedures for this class of models and demonstrate that RBM's can be successfully applied to the Netflix data set, containing over 100 million user/movie ratings. We also show that RBM's slightly outperform carefully-tuned SVD models. When the predictions of multiple RBM models and multiple SVD models are linearly combined, we achieve an error rate that is well over 6% better than the score of Netflix's own system.", "title": "Restricted Boltzmann machines for collaborative filtering"}, "83cdbbfad2d4bb4c348e09cf3db779f020809b05": {"paper_id": "83cdbbfad2d4bb4c348e09cf3db779f020809b05", "abstract": "On-line learning in domains where the target concept depends on some hidden context poses serious problems. A changing context can induce changes in the target concepts, producing what is known as concept drift. We describe a family of learning algorithms that flexibly react to concept drift and can take advantage of situations where contexts reappear. The general approach underlying all these algorithms consists of (1) keeping only a window of currently trusted examples and hypotheses; (2) storing concept descriptions and re-using them when a previous context re-appears; and (3) controlling both of these functions by a heuristic that constantly monitors the system's behavior. The paper reports on experiments that test the systems' performance under various conditions such as different levels of noise and different extent and rate of concept drift.", "title": "Learning in the Presence of Concept Drift and Hidden Contexts"}, "32fb6a88307d80ec5c932de843624d398e04ab28": {"paper_id": "32fb6a88307d80ec5c932de843624d398e04ab28", "abstract": "Collaborative filtering (CF) approaches proved to be effective for recommender systems in predicting user preferences in item selection using known user ratings of items. This subfield of machine learning has gained a lot of popularity with the Netflix Prize competition started in October 2006. Two major approaches for this problem are matrix factorization (MF) and the neighbor based approach (NB). In this work, we propose various variants of MF and NB that can boost the performance of the usual ensemble based scheme. First, we investigate various regularization scenarios for MF. Second, we introduce two NB methods: one is based on correlation coefficients and the other on linear least squares. At the experimentation part, we show that the proposed approaches compare favorably with existing ones in terms of prediction accuracy and/or required training time. We present results of blending the proposed methods.", "title": "Matrix factorization and neighbor based algorithms for the netflix prize problem"}, "d3f811997b68ad3c2fc72f2cabfd420fdefa1549": {"paper_id": "d3f811997b68ad3c2fc72f2cabfd420fdefa1549", "abstract": "The Netflix Prize is a collaborative filtering problem. This subfield of machine learning became popular in the late 1990s with the spread of online services that used recommendation systems (e.g. Amazon, Yahoo! Music, and of course Netflix). The aim of such a system is to predict what items a user might like based on his/her and other users' previous ratings. The Netflix Prize dataset is much larger than former benchmark datasets, therefore the scalability of the algorithms is a must. This paper describes the major components of our blending based solution, called the Gravity Recommendation System (GRS). In the Netflix Prize contest, it attained RMSE 0.8743 as of November 2007. We now compare the effectiveness of some selected individual and combined approaches on a particular subset of the Prize dataset, and discuss their important features and drawbacks.", "title": "Major components of the gravity recommendation system"}, "1f69d28a56246c99e1b7589c55871e1d8ce6a0c5": {"paper_id": "1f69d28a56246c99e1b7589c55871e1d8ce6a0c5", "abstract": "The regularized SVD model has O(NK + MK) parameters (N users, M movies, K features). We propose two models with O(MK) parameters. The idea is, instead of fitting ui for each user separately, to model ui as a function of a binary vector indicating which movies the user rated. For example uik \u2248 ei \u2211 j\u2208Ji wjk, where Ji is the set of movies rated by user i (possibly including movies for which we do not know ratings, e.g. qualifying.txt) and constant weights ei = (|Ji| + 1)\u22121/2. The first proposed model is:", "title": "Improving regularized singular value decomposition for collaborative filtering"}, "6c007a258b8e35ee7408e9babd9d842cb576ecc2": {"paper_id": "6c007a258b8e35ee7408e9babd9d842cb576ecc2", "abstract": "Collaborative filtering (CF) is valuable in e-commerce, and for direct recommendations for music, movies, news etc. But today's systems have several disadvantages, including privacy risks. As we move toward ubiquitous computing, there is a great potential for individuals to share all kinds of information about places and things to do, see and buy, but the privacy risks are severe. In this paper we describe a new method for collaborative filtering which protects the privacy of individual data. The method is based on a probabilistic factor analysis model. Privacy protection is provided by a peer-to-peer protocol which is described elsewhere, but outlined in this paper. The factor analysis approach handles missing data without requiring default values for them. We give several experiments that suggest that this is most accurate method for CF to date. The new algorithm has other advantages in speed and storage over previous algorithms. Finally, we suggest applications of the approach to other kinds of statistical analyses of survey or questionaire data.", "title": "Collaborative filtering with privacy via factor analysis"}, "926c7f50510a1f4376651c4d46df5fcc935c3004": {"paper_id": "926c7f50510a1f4376651c4d46df5fcc935c3004", "abstract": "In this paper, we propose a level set method for shape-driven object extraction. We introduce a voxel-wise probabilistic level set formulation to account for prior knowledge. To this end, objects are represented in an implicit form. Constraints on the segmentation process are imposed by seeking a projection to the image plane of the prior model modulo a similarity transformation. The optimization of a statistical metric between the evolving contour and the model leads to motion equations that evolve the contour toward the desired image properties while recovering the pose of the object in the new image. Upon convergence, a solution that is similarity invariant with respect to the model and the corresponding transformation are recovered. Promising experimental results demonstrate the potential of such an approach.", "title": "Prior Knowledge, Level Set Representations & Visual Grouping"}, "704aec7a62416a09a01268b0e94eadc5e681348b": {"paper_id": "704aec7a62416a09a01268b0e94eadc5e681348b", "abstract": "AIM\nThe aim of this study was to determine the treatment outcome of the use of a porcine monolayer collagen matrix (mCM) to augment peri-implant soft tissue in conjunction with immediate implant placement as an alternative to patient's own connective tissue.\n\n\nMATERIALS AND METHODS\nA total of 27 implants were placed immediately in 27 patients (14 males and 13 females, with a mean age of 52.2 years) with simultaneous augmentation of the soft tissue by the use of a mCM. The patients were randomly divided into two groups: Group I: An envelope flap was created and mCM was left coronally uncovered, and group II: A coronally repositioned flap was created and the mCM was covered by the mucosa. Soft-tissue thickness (STTh) was measured at the time of surgery (T0) and 6 months postoperatively (T1) using a customized stent. Cone beam computed tomographies (CBCTs) were taken from 12 representative cases at T1. A stringent plaque control regimen was enforced in all the patients during the 6-month observation period.\n\n\nRESULTS\nMean STTh change was similar in both groups (0.7 \u00b1 0.2 and 0.7 \u00b1 0.1 mm in groups I and II respectively). The comparison of STTh between T0 and T1 showed a statistically significant increase of soft tissue in both groups I and II as well as in the total examined population (p < 0.001). The STTh change as well as matrix thickness loss were comparable in both groups (p > 0.05). The evaluation of the CBCTs did not show any signs of resorption of the buccal bone plate.\n\n\nCONCLUSION\nWithin the limitations of this study, it could be concluded that the collagen matrix used in conjunction with immediate implant placement leads to an increased thickness of peri-implant soft tissue independent of the flap creation technique and could be an alternative to connective tissue graft.\n\n\nCLINICAL SIGNIFICANCE\nThe collagen matrix used seems to be a good alternative to patient's own connective tissue and could be used for the soft tissue augmentation around dental implants.", "title": "Use of Collagen Matrix for Augmentation of the Peri-implant Soft Tissue at the Time of Immediate Implant Placement."}, "550921949605a99adbecd6c911319027a03d0d5a": {"paper_id": "550921949605a99adbecd6c911319027a03d0d5a", "abstract": "Lingual nerve damage complicating oral surgery would sometimes require electrographic exploration. Nevertheless, direct recording of conduction in lingual nerve requires its puncture at the foramen ovale. This method is too dangerous to be practiced routinely in these diagnostic indications. The aim of our study was to assess spatial relationships between lingual nerve and mandibular ramus in the infratemporal fossa using an original technique. Therefore, ten lingual nerves were dissected on five fresh cadavers. All the nerves were catheterized with a 3/0 wire. After meticulous repositioning of the nerve and medial pterygoid muscle reinsertion, CT-scan examinations were performed with planar acquisitions and three-dimensional reconstructions. Localization of lingual nerve in the infratemporal fossa was assessed successively at the level of the sigmoid notch of the mandible, lingula and third molar. At the level of the lingula, lingual nerve was far from the maxillary vessels; mean distance between the nerve and the anterior border of the ramus was 19.6\u00a0mm. The posteriorly opened angle between the medial side of the ramus and the line joining the lingual nerve and the anterior border of the ramus measured 17\u00b0. According to these findings, we suggest that the lingual nerve might be reached through the intra-oral puncture at the intermaxillary commissure; therefore, we modify the inferior alveolar nerve block technique to propose a safe and reproducible protocol likely to be performed routinely as electrographic exploration of the lingual nerve. What is more, this original study protocol provided interesting educational materials and could be developed for the conception of realistic 3D virtual anatomy supports.", "title": "Spatial relationships between lingual nerve and mandibular ramus: original study method, clinical and educational applications"}, "21dd2790b76a57b42191b19a54505837f3969141": {"paper_id": "21dd2790b76a57b42191b19a54505837f3969141", "abstract": "In massive open-access online courses (MOOCs), peer grading serves as a critical tool for scaling the grading of complex, open-ended assignments to courses with tens or hundreds of thousands of students. But despite promising initial trials, it does not always deliver accurate results compared to human experts. In this paper, we develop algorithms for estimating and correcting for grader biases and reliabilities, showing significant improvement in peer grading accuracy on real data with 63,199 peer grades from Coursera\u2019s HCI course offerings \u2014 the largest peer grading networks analysed to date. We relate grader biases and reliabilities to other student factors such as engagement, performance as well as commenting style. We also show that our model can lead to more intelligent assignment of graders to gradees.", "title": "Tuned Models of Peer Assessment in MOOCs"}, "26196511e307ec89466af06751a66ee2d95b6305": {"paper_id": "26196511e307ec89466af06751a66ee2d95b6305", "abstract": "Human linguistic annotation is crucial for many natural language processing tasks but can be expensive and time-consuming. We explore the use of Amazon\u2019s Mechanical Turk system, a significantly cheaper and faster method for collecting annotations from a broad base of paid non-expert contributors over the Web. We investigate five tasks: affect recognition, word similarity, recognizing textual entailment, event temporal ordering, and word sense disambiguation. For all five, we show high agreement between Mechanical Turk non-expert annotations and existing gold standard labels provided by expert labelers. For the task of affect recognition, we also show that using non-expert labels for training machine learning algorithms can be as effective as using gold standard annotations from experts. We propose a technique for bias correction that significantly improves annotation quality on two tasks. We conclude that many large labeling tasks can be effectively designed and carried out in this method at a fraction of the usual expense.", "title": "Cheap and Fast - But is it Good? Evaluating Non-Expert Annotations for Natural Language Tasks"}, "238c322d010fbc32bf110377045235c589629cba": {"paper_id": "238c322d010fbc32bf110377045235c589629cba", "abstract": "Data generated as a side effect of game play also solves computational problems and trains AI algorithms.", "title": "Designing games with a purpose"}, "ee883506ab14f14ece8048e2ea23a02d4ce2f347": {"paper_id": "ee883506ab14f14ece8048e2ea23a02d4ce2f347", "abstract": "In order to understand the formation and subsequent evolution of galaxies one must first distinguish between the two main morphological classes of massive systems: spirals and early-type systems. This paper introduces a project, Galaxy Zoo, which provides visual morphological classifications for nearly one million galaxies, extracted from the Sloan Digital Sky Survey (SDSS). This achievement was made possible by inviting the general public to visually inspect and classify these galaxies via the internet. The project has obtained more than 4 \u00d7 107 individual classifications made by \u223c105 participants. We discuss the motivation and strategy for this project, and detail how the classifications were performed and processed. We find that Galaxy Zoo results are consistent with those for subsets of SDSS galaxies classified by professional astronomers, thus demonstrating that our data provide a robust morphological catalogue. Obtaining morphologies by direct visual inspection avoids introducing biases associated with proxies for morphology such as colour, concentration or structural parameters. In addition, this catalogue can be used to directly compare SDSS morphologies with older data sets. The colour\u2013magnitude diagrams for each morphological class are shown, and we illustrate how these distributions differ from those inferred using colour alone as a proxy for", "title": "Galaxy Zoo : morphologies derived from visual inspection of galaxies from the Sloan Digital Sky Survey"}, "c0d6eaab60a57a5416490d5026a78770a5da3d57": {"paper_id": "c0d6eaab60a57a5416490d5026a78770a5da3d57", "abstract": "Pathfinder is an expert system that assists surgical pathologists with the diagnosis of lymph-node diseases. The program is one of a growing number of normative expert systems that use probability and decision theory to acquire, represent, manipulate, and explain uncertain medical knowledge. In this article, we describe Pathfinder and our research in uncertain-reasoning paradigms that was stimulated by the development of the program. We discuss limitations with early decision-theoretic methods for reasoning under uncertainty and our initial attempts to use non-decision-theoretic methods. Then, we describe experimental and theoretical results that directed us to return to reasoning methods based in probability and decision theory.", "title": "Toward normative expert systems: Part I. The Pathfinder project."}, "7c319bc8a31bd764a103d19289672f52386c053f": {"paper_id": "7c319bc8a31bd764a103d19289672f52386c053f", "abstract": "Recently several researchers have investigated techniques for using data to learn Bayesian networks containing compact representations for the conditional probability distributions (CPDs) stored at each node. The majority of this work has concentrated on using decision-tree representations for the CPDs. In addition, researchers typically apply non-Bayesian (or asymptotically Bayesian) scoring functions such as MDL to evaluate the goodness-oft of networks to the data. In this paper we investigate a Bayesian approach to learning Bayesian networks that contain the more general decision-graph representations of the CPDs. First, we describe how to evaluate the posterior probability|that is, the Bayesian score|of such a network, given a database of observed cases. Second, we describe various search spaces that can be used, in conjunction with a scoring function and a search procedure, to identify one or more high-scoring networks. Finally, we present an experimental evaluation of the search spaces, using a greedy algorithm and a Bayesian scoring function.", "title": "A Bayesian Approach to Learning Bayesian Networks with Local Structure"}, "6661e57237e4e8739b7a4946c4d3d4875376c068": {"paper_id": "6661e57237e4e8739b7a4946c4d3d4875376c068", "abstract": "For large state-space Markovian Decision Problems MonteCarlo planning is one of the few viable approaches to find near-optimal solutions. In this paper we introduce a new algorithm, UCT, that applies bandit ideas to guide Monte-Carlo planning. In finite-horizon or discounted MDPs the algorithm is shown to be consistent and finite sample bounds are derived on the estimation error due to sampling. Experimental results show that in several domains, UCT is significantly more efficient than its alternatives.", "title": "Bandit Based Monte-Carlo Planning"}, "7bbdb1803788a0e0cf8b814ed12a8f87e544b6ec": {"paper_id": "7bbdb1803788a0e0cf8b814ed12a8f87e544b6ec", "abstract": "Distributing labeling tasks among hundreds or thousands of annotators is an increasingly important method for annotating large datasets. We present a method for estimating the underlying value (e.g. the class) of each image from (noisy) annotations provided by multiple annotators. Our method is based on a model of the image formation and annotation process. Each image has different characteristics that are represented in an abstract Euclidean space. Each annotator is modeled as a multidimensional entity with variables representing competence, expertise and bias. This allows the model to discover and represent groups of annotators that have different sets of skills and knowledge, as well as groups of images that differ qualitatively. We find that our model predicts ground truth labels on both synthetic and real data more accurately than state of the art methods. Experiments also show that our model, starting from a set of binary labels, may discover rich information, such as different \u201cschools of thought\u201d amongst the annotators, and can group together images belonging to separate categories.", "title": "The Multidimensional Wisdom of Crowds"}, "206b204618640917f278e72bd0e2a881d8cec7ad": {"paper_id": "206b204618640917f278e72bd0e2a881d8cec7ad", "abstract": "One of the major obstacles to using Bayesian methods for pattern recognition has been its computational expense. This thesis presents an approximation technique that can perform Bayesian inference faster and more accurately than previously possible. This method, \"Expectation Propagation,\" unifies and generalizes two previous techniques: assumeddensity filtering, an extension of the Kalman filter, and loopy belief propagation, an extension of belief propagation in Bayesian networks. The unification shows how both of these algorithms can be viewed as approximating the true posterior distribution with a simpler distribution, which is close in the sense of KL-divergence. Expectation Propagation exploits the best of both algorithms: the generality of assumed-density filtering and the accuracy of loopy belief propagation. Loopy belief propagation, because it propagates exact belief states, is useful for limited types of belief networks, such as purely discrete networks. Expectation Propagation approximates the belief states with expectations, such as means and variances, giving it much wider scope. Expectation Propagation also extends belief propagation in the opposite direction-propagating richer belief states which incorporate correlations between variables. This framework is demonstrated in a variety of statistical models using synthetic and real-world data. On Gaussian mixture problems, Expectation Propagation is found, for the same amount of computation, to be convincingly better than rival approximation techniques: Monte Carlo, Laplace's method, and variational Bayes. For pattern recognition, Expectation Propagation provides an algorithm for training Bayes Point Machine classifiers that is faster and more accurate than any previously known. The resulting classifiers outperform Support Vector Machines on several standard datasets, in addition to having a comparable training time. Expectation Propagation can also be used to choose an appropriate feature set for classification, via Bayesian model selection. Thesis Supervisor: Rosalind Picard Title: Associate Professor of Media Arts and Sciences", "title": "A family of algorithms for approximate Bayesian inference"}, "3bfa1a988e5211ebd76f702afc7db1db88091273": {"paper_id": "3bfa1a988e5211ebd76f702afc7db1db88091273", "abstract": "The design, prototyping, and characterization of a radiation pattern reconfigurable antenna (RA) targeting 5G communications are presented. The RA is based on a reconfigurable parasitic layer technique in which a driven dipole antenna is located along the central axis of a 3-D parasitic layer structure enclosing it. The reconfigurable parasitic structure is similar to a hexagonal prism, where the top/bottom bases are formed by a hexagonal domed structure. The surfaces of the parasitic structure house electrically small metallic pixels with various geometries. The adjacent pixels are connected by PIN diode switches to change the geometry of the parasitic surface, thus providing reconfigurability in the radiation pattern. This RA is designed to operate over a 4.8\u20135.2 GHz frequency band, producing various radiation patterns with a beam-steering capability in both the azimuth (<inline-formula> <tex-math notation=\"LaTeX\">$0 {^{\\circ }} <\\phi < 360 {^{\\circ }}$ </tex-math></inline-formula>) and elevation planes (<inline-formula> <tex-math notation=\"LaTeX\">$-18 {^{\\circ }} <\\theta < 18 {^{\\circ }}$ </tex-math></inline-formula>). Small-cell access points equipped with RAs are used to investigate the system level performances for 5G heterogeneous networks. The results show that using distributed mode optimization, RA equipped small-cell systems could provide up to 29% capacity gains and 13% coverage improvements as compared to legacy omnidirectional antenna equipped systems.", "title": "Parasitic Layer-Based Radiation Pattern Reconfigurable Antenna for 5G Communications"}, "246b4dec2ed18834f38c0dace03908b1bd0d427e": {"paper_id": "246b4dec2ed18834f38c0dace03908b1bd0d427e", "abstract": "This paper presents an overview of the theory and currently known techniques for multi-cell MIMO (multiple input multiple output) cooperation in wireless networks. In dense networks where interference emerges as the key capacity-limiting factor, multi-cell cooperation can dramatically improve the system performance. Remarkably, such techniques literally exploit inter-cell interference by allowing the user data to be jointly processed by several interfering base stations, thus mimicking the benefits of a large virtual MIMO array. Multi-cell MIMO cooperation concepts are examined from different perspectives, including an examination of the fundamental information-theoretic limits, a review of the coding and signal processing algorithmic developments, and, going beyond that, consideration of very practical issues related to scalability and system-level integration. A few promising and quite fundamental research avenues are also suggested.", "title": "Multi-Cell MIMO Cooperative Networks: A New Look at Interference"}, "63d984f99622a2831d8f15e9a9552bd585ba8e25": {"paper_id": "63d984f99622a2831d8f15e9a9552bd585ba8e25", "abstract": "A Gaussian broadcast channel (GBC) with r single-antenna receivers and t antennas at the transmitter is considered. Both transmitter and receivers have perfect knowledge of the channel. Despite its apparent simplicity, this model is, in general, a nondegraded broadcast channel (BC), for which the capacity region is not fully known. For the two-user case, we find a special case of Marton's (1979) region that achieves optimal sum-rate (throughput). In brief, the transmitter decomposes the channel into two interference channels, where interference is caused by the other user signal. Users are successively encoded, such that encoding of the second user is based on the noncausal knowledge of the interference caused by the first user. The crosstalk parameters are optimized such that the overall throughput is maximum and, surprisingly, this is shown to be optimal over all possible strategies (not only with respect to Marton's achievable region). For the case of r>2 users, we find a somewhat simpler choice of Marton's region based on ordering and successively encoding the users. For each user i in the given ordering, the interference caused by users j>i is eliminated by zero forcing at the transmitter, while interference caused by users j<i is taken into account by coding for noncausally known interference. Under certain mild conditions, this scheme is found to be throughput-wise asymptotically optimal for both high and low signal-to-noise ratio (SNR). We conclude by providing some numerical results for the ergodic throughput of the simplified zero-forcing scheme in independent Rayleigh fading.", "title": "On the achievable throughput of a multiantenna Gaussian broadcast channel"}, "36677dce5302fd0f09f90e46460462724da0a9fb": {"paper_id": "36677dce5302fd0f09f90e46460462724da0a9fb", "abstract": "A multiple-input-multiple-output (MIMO) system equipped with a new class of antenna arrays, henceforth referred to as multifunction reconfigurable antenna arrays (MRAAs), is investigated. The elements of MRAA, i.e., multifunction reconfigurable antennas (MRAs) presented in this work are capable of dynamically changing the sense of polarization of the radiated field thereby providing two reconfigurable modes of operation, i.e., polarization diversity and space diversity. The transmission signaling scheme can also be switched between transmit diversity (TD) and spatial multiplexing (SM). The results show that the reconfigurable modes of operation of an MRAA used in conjunction with adaptive space-time modulation techniques provide additional degrees of freedom to the current adaptive MIMO systems, resulting in more robust system in terms of quality, capacity and reliability. A performance gain up to 30 dB is possible with the proposed system over conventional fixed antenna MIMO systems depending on the channel conditions", "title": "A MIMO System With Multifunctional Reconfigurable Antennas"}, "2a935a6beae90d9f30e1cf1ef5c17b168456e1b0": {"paper_id": "2a935a6beae90d9f30e1cf1ef5c17b168456e1b0", "abstract": null, "title": "Phased Array Antenna Handbook"}, "579d7885093c51fc45a6ecbdaa66647a9ee3ba23": {"paper_id": "579d7885093c51fc45a6ecbdaa66647a9ee3ba23", "abstract": "Lane change maneuver is a complicated maneuver, and incorrect maneuvering is an important reason for expressway accidents and fatalities. In this scenario, automated lane change has great potential to reduce the number of accidents. Previous research in this area, typically, focuses on the generation of an optimal lane change trajectory, while ignoring the human behavior model. To understand the human lane change behavior model, we carried out experiments on Japanese expressways. By analyzing the human-driver lane change data, we propose a two-segment lane change model that mimics the human-driver. We categorize the driving environment based on the observation grid and propose different lane change behaviors to handle the different scenarios. We develop an intuitive method to select the suitable lane change behavior, for a given scenario, using active (accelerate/decelerate) and passive (wait) information derived from the distance and related velocity (dx/dv) graph. Additionally, we also identify the most desirable and safe conditions for doing lane change based on the human driver preference data. We evaluated the proposed model by performing lane change simulations in the PreScan environment, while considering the vehicle motion/control model. The simulation results show the proposed model is able to handle complicated lane change scenarios with human driver-like performance.", "title": "Human Drivers Based Active-Passive Model for Automated Lane Change"}, "59b202ccc01bae85a88ad0699da7a8ae6aa50fef": {"paper_id": "59b202ccc01bae85a88ad0699da7a8ae6aa50fef", "abstract": "This paper provides a review of the literature in on-road vision-based vehicle detection, tracking, and behavior understanding. Over the past decade, vision-based surround perception has progressed from its infancy into maturity. We provide a survey of recent works in the literature, placing vision-based vehicle detection in the context of sensor-based on-road surround analysis. We detail advances in vehicle detection, discussing monocular, stereo vision, and active sensor-vision fusion for on-road vehicle detection. We discuss vision-based vehicle tracking in the monocular and stereo-vision domains, analyzing filtering, estimation, and dynamical models. We discuss the nascent branch of intelligent vehicles research concerned with utilizing spatiotemporal measurements, trajectories, and various features to characterize on-road behavior. We provide a discussion on the state of the art, detail common performance metrics and benchmarks, and provide perspective on future research directions in the field.", "title": "Looking at Vehicles on the Road: A Survey of Vision-Based Vehicle Detection, Tracking, and Behavior Analysis"}, "6ec02fb5bfc307911c26741fb3804f16d8ad299c": {"paper_id": "6ec02fb5bfc307911c26741fb3804f16d8ad299c", "abstract": "In recent years, active learning has emerged as a powerful tool in building robust systems for object detection using computer vision. Indeed, active learning approaches to on-road vehicle detection have achieved impressive results. While active learning approaches for object detection have been explored and presented in the literature, few studies have been performed to comparatively assess costs and merits. In this study, we provide a cost-sensitive analysis of three popular active learning methods for on-road vehicle detection. The generality of active learning findings is demonstrated via learning experiments performed with detectors based on histogram of oriented gradient features and SVM classification (HOG\u2013SVM), and Haar-like features and Adaboost classification (Haar\u2013Adaboost). Experimental evaluation has been performed on static images and real-world on-road vehicle datasets. Learning approaches are assessed in terms of the time spent annotating, data required, recall, and precision.", "title": "Active learning for on-road vehicle detection: a comparative study"}, "d79eeb61bc8960242550234e7a801d6863c58aec": {"paper_id": "d79eeb61bc8960242550234e7a801d6863c58aec", "abstract": "A mathematical tool to build a fuzzy model of a system where fuzzy implications and reasoning are used is presented. The premise of an implication is the description of fuzzy subspace of inputs and its consequence is a linear input-output relation. The method of identification of a system using its input-output data is then shown. Two applications of the method to industrial processes are also discussed: a water cleaning process and a converter in a steel-making process.", "title": "Fuzzy identification of systems and its applications to modeling and control"}, "05393361e6d9e56ee7dbabb1e5ef6c1c212fc34d": {"paper_id": "05393361e6d9e56ee7dbabb1e5ef6c1c212fc34d", "abstract": "The support vector machine (SVM) is a popular classification technique. However, beginners who are not familiar with SVM often get unsatisfactory results since they miss some easy but significant steps. In this guide, we propose a simple procedure which usually gives reasonable results.", "title": "A Practical Guide to Support Vector Classification"}, "6f72eaef816748a148c1793baa7f78dd19b44fc5": {"paper_id": "6f72eaef816748a148c1793baa7f78dd19b44fc5", "abstract": "Industry 4.0 is still in its development phase and it promises to bring remarkable benefits to the manufacturing industry around the world when employing the Smart Factory application in large organizations and their supply chains. However, there is a risk of a miss-match when trying to introduce Industry 4.0 to Small and Medium Enterprises (SME) as the concept is mainly being developed around large manufacturing companies. The purpose of this research is to analyze the readiness level and feasibility of implementing Industry 4.0 technologies for SME\u2019s in the federal state of Brandenburg (Germany). The work is based on the survey of 20 SME\u2019s assessing their current problems emphasizing on automation, Enterprise Resource Planning (ERP), CAD/CAM, factory layout planning and logistics. Five SME\u2019s from different domains out of the 20 surveyed are taken as case studies to evaluate the potential benefits, trade-offs and barriers from an implementation of these integrated technologies. The findings revealed that the companies are still coping with the issues relating to planning, logistics and automation. It was also found that all the concepts of i4.0 may not be necessary or even beneficial to an enterprise in the current scenario and new strategies need to be developed for its realization in SME\u2019s.", "title": "A multi-case study on Industry 4.0 for SME's in Brandenburg, Germany"}, "0cb7a2c3309c15993db73a2ade9eecc7b6dbb9af": {"paper_id": "0cb7a2c3309c15993db73a2ade9eecc7b6dbb9af", "abstract": "Cyber-physical systems (CPS) are physical and engineered systems whose operations are monitored, coordinated, controlled and integrated by a computing and communication core. Just as the internet transformed how humans interact with one another, cyber-physical systems will transform how we interact with the physical world around us. Many grand challenges await in the economically vital domains of transportation, health-care, manufacturing, agriculture, energy, defense, aerospace and buildings. The design, construction and verification of cyber-physical systems pose a multitude of technical challenges that must be addressed by a cross-disciplinary community of researchers and educators.", "title": "Cyber-physical systems: The next computing revolution"}, "60ac4554e9f89f0ca319d3bbcc4828bf4dad5b05": {"paper_id": "60ac4554e9f89f0ca319d3bbcc4828bf4dad5b05", "abstract": "The report of the President's Council of Advisors on Science and Technology (PCAST) has placed CPS on the top of the priority list for federal research investment [6]. This article first reviews some of the challenges and promises of CPS, followed by an articulation of some specific challenges and promises that are more closely related to the sensor networks, ubiquitous and trustworthy computing conference.", "title": "Cyber-Physical Systems: A New Frontier"}, "71f8f95a4b31dff9f88567e089f00d4eadc2c2d4": {"paper_id": "71f8f95a4b31dff9f88567e089f00d4eadc2c2d4", "abstract": "Industry is the part of an economy that produces material goods which are highly mechanized and automatized. Ever since the beginning of industrialization, technological leaps have led to paradigm shifts which today are ex-post named \u201cindustrial revolutions\u201d: in the field of mechanization (the so-called 1st industrial revolution), of the intensive use of electrical energy (the so-called 2nd industrial revolution), and of the widespread digitalization (the so-called 3rd industrial revolution). On the basis of an advanced digitalization within factories, the combination of Internet technologies and future-oriented technologies in the field of \u201csmart\u201d objects (machines and products) seems to result in a new fundamental paradigm shift in industrial production. The vision of future production contains modular and efficient manufacturing systems and characterizes scenarios in which products control their own manufacturing process. This is supposed to realize the manufacturing of individual products in a batch size of one while maintaining the economic conditions of mass production. Tempted by this future expectation, the term \u201cIndustry 4.0\u201d was established exante for a planned \u201c4th industrial revolution\u201d, the term being a reminiscence of software versioning. Decisive for the fast spread was the recommendation for implementation to the German Government, which carried the term in its title and was picked up willingly by the Federal Ministry of Education and Research and has become an eponym for a future project in the context of the high-tech strategy 2020. Currently an industrial platform consisting of three well-known industry associations named \u201cIndustry 4.0\u201d is contributing to the dispersion of the term. Outside of the German-speaking area, the term is not common. In this paper the term \u201cIndustry 4.0\u201d describes a future project that can be defined by two development directions. On the one hand there is a huge applicationpull, which induces a remarkable need for changes due to changing operative framework conditions. Triggers for this are general social, economic, and political changes. Those are in particular: Short development periods: Development periods and innovation periods need to be shortened. High innovation capability is becoming an essential success factor for many enterprises (\u201ctime to market\u201d). Individualization on demand: A change from a seller\u2019s into a buyer\u2019s market has been becoming apparent for decades now, which means buyers can define the conditions of the trade. This trend leads to an increasing individualization of products and in extreme cases to individual products. This is also called \u201cbatch size one\u201d. Flexibility: Due to the new framework requirements, higher flexibility in product development, especially in production, is necessary. Decentralization: To cope with the specified conditions, faster decisionmaking procedures are necessary. For this, organizational hierarchies need to be reduced. Resource efficiency: Increasing shortage and the related increase of prices for resources as well as social change in the context of ecological aspects require a more intensive focus on sustainability in industrial contexts. The aim is an economic and ecological increase in efficiency. On the other hand, there is an exceptional technology-push in industrial practice. This technology-push has already influenced daily routine in private areas. Buzzwords are Web 2.0, Apps,", "title": "Industry 4.0"}, "059e776cacf87b3ed3f6eb9aa87968247fa68be5": {"paper_id": "059e776cacf87b3ed3f6eb9aa87968247fa68be5", "abstract": "Cyber-Physical Systems (CPS) are integrations of computation and physical processes. Embedded computers and networks monitor and control the physical processes, usually with feedback loops where physical processes affect computations and vice versa. The economic and societal potential of such systems is vastly greater than what has been realized, and major investments are being made worldwide to develop the technology. There are considerable challenges, particularly because the physical components of such systems introduce safety and reliability requirements qualitatively different from those in general- purpose computing. Moreover, physical components are qualitatively different from object-oriented software components. Standard abstractions based on method calls and threads do not work. This paper examines the challenges in designing such systems, and in particular raises the question of whether today's computing and networking technologies provide an adequate foundation for CPS. It concludes that it will not be sufficient to improve design processes, raise the level of abstraction, or verify (formally or otherwise) designs that are built on today's abstractions. To realize the full potential of CPS, we will have to rebuild computing and networking abstractions. These abstractions will have to embrace physical dynamics and computation in a unified way.", "title": "Cyber Physical Systems: Design Challenges"}, "99699a39b093133a4e8375b56bbc3fa286ff3fba": {"paper_id": "99699a39b093133a4e8375b56bbc3fa286ff3fba", "abstract": "Science is a cumulative endeavour as new knowledge is often created in the process of interpreting and combining existing knowledge. This is why literature reviews have long played a decisive role in scholarship. The quality of literature reviews is particularly determined by the literature search process. As Sir Isaac Newton eminently put it: \u201cIf I can see further, it is because I am standing on the shoulders of giants.\u201d Drawing on this metaphor, the goal of writing a literature review is to reconstruct the giant of accumulated knowledge in a specific domain. And in doing so, a literature search represents the fundamental first step that makes up the giant\u2019s skeleton and largely determines its reconstruction in the subsequent literature analysis. In this paper, we argue that the process of searching the literature must be comprehensibly described. Only then can readers assess the exhaustiveness of the review and other scholars in the field can more confidently (re)use the results in their own research. We set out to explore the methodological rigour of literature review articles published in ten major information systems (IS) journals and show that many of these reviews do not thoroughly document the process of literature search. The results drawn from our analysis lead us to call for more rigour in documenting the literature search process and to present guidelines for crafting a literature review and search in the IS domain.", "title": "Reconstructing the giant: On the importance of rigour in documenting the literature search process"}, "607efc04ab251c2eabfab6dcd1fba47c9089d714": {"paper_id": "607efc04ab251c2eabfab6dcd1fba47c9089d714", "abstract": "Tightly coupling GNSS pseudorange and Doppler measurements with other sensors is known to increase the accuracy and consistency of positioning information. Nowadays, high-accuracy geo-referenced lane marking maps are seen as key information sources in autonomous vehicle navigation. When an exteroceptive sensor such as a video camera or a lidar is used to detect them, lane markings provide positioning information which can be merged with GNSS data. In this paper, measurements from a forward-looking video camera are merged with raw GNSS pseudoranges and Dopplers on visible satellites. To create a localization system that provides pose estimates with high availability, dead reckoning sensors are also integrated. The data fusion problem is then formulated as sequential filtering. A reduced-order state space modeling of the observation problem is proposed to give a real-time system that is easy to implement. A Kalman filter with measured input and correlated noises is developed using a suitable error model of the GNSS pseudoranges. Our experimental results show that this tightly coupled approach performs better, in terms of accuracy and consistency, than a loosely coupled method using GNSS fixes as inputs.", "title": "Sequential Data Fusion of GNSS Pseudoranges and Dopplers With Map-Based Vision Systems"}, "4d4be6294e5b30cdf985fcc044f44ec9da495af3": {"paper_id": "4d4be6294e5b30cdf985fcc044f44ec9da495af3", "abstract": "Computer-based sensors and actuators such as global positioning systems, machine vision, and laser-based sensors have progressively been incorporated into mobile robots with the aim of configuring autonomous systems capable of shifting operator activities in agricultural tasks. However, the incorporation of many electronic systems into a robot impairs its reliability and increases its cost. Hardware minimization, as well as software minimization and ease of integration, is essential to obtain feasible robotic systems. A step forward in the application of automatic equipment in agriculture is the use of fleets of robots, in which a number of specialized robots collaborate to accomplish one or several agricultural tasks. This paper strives to develop a system architecture for both individual robots and robots working in fleets to improve reliability, decrease complexity and costs, and permit the integration of software from different developers. Several solutions are studied, from a fully distributed to a whole integrated architecture in which a central computer runs all processes. This work also studies diverse topologies for controlling fleets of robots and advances other prospective topologies. The architecture presented in this paper is being successfully applied in the RHEA fleet, which comprises three ground mobile units based on a commercial tractor chassis.", "title": "New Trends in Robotics for Agriculture: Integration and Assessment of a Real Fleet of Robots"}, "71245f9d9ba0317f78151698dc1ddba7583a3afd": {"paper_id": "71245f9d9ba0317f78151698dc1ddba7583a3afd", "abstract": "Knowledge Graph (KG) embedding projects entities and relations into low dimensional vector space, which has been successfully applied in KG completion task. The previous embedding approaches only model entities and their relations, ignoring a large number of entities\u2019 numeric attributes in KGs. In this paper, we propose a new KG embedding model which jointly model entity relations and numeric attributes. Our approach combines an attribute embedding model with a translation-based structure embedding model, which learns the embeddings of entities, relations, and attributes simultaneously. Experiments of link prediction on YAGO and Freebase show that the performance is effectively improved by adding entities\u2019 numeric attributes in the embedding model.", "title": "Knowledge Graph Embedding with Numeric Attributes of Entities"}, "04b52c8230c3f9f4f4032b06458069d81c8f07b2": {"paper_id": "04b52c8230c3f9f4f4032b06458069d81c8f07b2", "abstract": "We consider the problem of embedding entities and relationships of multirelational data in low-dimensional vector spaces. Our objective is to propose a canonical model which is easy to train, contains a reduced number of parameters and can scale up to very large databases. Hence, we propose TransE, a method which models relationships by interpreting them as translations operating on the low-dimensional embeddings of the entities. Despite its simplicity, this assumption proves to be powerful since extensive experiments show that TransE significantly outperforms state-of-the-art methods in link prediction on two knowledge bases. Besides, it can be successfully trained on a large scale data set with 1M entities, 25k relationships and more than 17M training samples.", "title": "Translating Embeddings for Modeling Multi-relational Data"}, "3bc9f8eb5ba303816fd5f642f2e7408f0752d3c4": {"paper_id": "3bc9f8eb5ba303816fd5f642f2e7408f0752d3c4", "abstract": null, "title": "WORDNET: A Lexical Database For English"}, "0cf0bfc09d0083c2a4afff79347f657523ae40b0": {"paper_id": "0cf0bfc09d0083c2a4afff79347f657523ae40b0", "abstract": "Open-text semantic parsers are designed to interpret any statement in natural language by inferring a corresponding meaning representation (MR \u2013 a formal representation of its sense). Unfortunately, large scale systems cannot be easily machine-learned due to a lack of directly supervised data. We propose a method that learns to assign MRs to a wide range of text (using a dictionary of more than 70,000 words mapped to more than 40,000 entities) thanks to a training scheme that combines learning from knowledge bases (e.g. WordNet) with learning from raw text. The model jointly learns representations of words, entities and MRs via a multi-task training process operating on these diverse sources of data. Hence, the system ends up providing methods for knowledge acquisition and wordsense disambiguation within the context of semantic parsing in a single elegant framework. Experiments on these various tasks indicate the promise of the approach.", "title": "Joint Learning of Words and Meaning Representations for Open-Text Semantic Parsing"}, "994afdf0db0cb0456f4f76468380822c2f532726": {"paper_id": "994afdf0db0cb0456f4f76468380822c2f532726", "abstract": "Knowledge graph completion aims to perform link prediction between entities. In this paper, we consider the approach of knowledge graph embeddings. Recently, models such as TransE and TransH build entity and relation embeddings by regarding a relation as translation from head entity to tail entity. We note that these models simply put both entities and relations within the same semantic space. In fact, an entity may have multiple aspects and various relations may focus on different aspects of entities, which makes a common space insufficient for modeling. In this paper, we propose TransR to build entity and relation embeddings in separate entity space and relation spaces. Afterwards, we learn embeddings by first projecting entities from entity space to corresponding relation space and then building translations between projected entities. In experiments, we evaluate our models on three tasks including link prediction, triple classification and relational fact extraction. Experimental results show significant and consistent improvements compared to stateof-the-art baselines including TransE and TransH. The source code of this paper can be obtained from https: //github.com/mrlyk423/relation extraction.", "title": "Learning Entity and Relation Embeddings for Knowledge Graph Completion"}, "68a33a3afac65eb6e0fb3726c1f9c8b727f32a42": {"paper_id": "68a33a3afac65eb6e0fb3726c1f9c8b727f32a42", "abstract": "Relational learning is becoming increasingly important in many areas of application. Here, we present a novel approach to relational learning based on the factorization of a three-way tensor. We show that unlike other tensor approaches, our method is able to perform collective learning via the latent components of the model and provide an efficient algorithm to compute the factorization. We substantiate our theoretical considerations regarding the collective learning capabilities of our model by the means of experiments on both a new dataset and a dataset commonly used in entity resolution. Furthermore, we show on common benchmark datasets that our approach achieves better or on-par results, if compared to current state-of-the-art relational learning solutions, while it is significantly faster to compute.", "title": "A Three-Way Model for Collective Learning on Multi-Relational Data"}, "1976c9eeccc7115d18a04f1e7fb5145db6b96002": {"paper_id": "1976c9eeccc7115d18a04f1e7fb5145db6b96002", "abstract": "Freebase is a practical, scalable tuple database used to structure general human knowledge. The data in Freebase is collaboratively created, structured, and maintained. Freebase currently contains more than 125,000,000 tuples, more than 4000 types, and more than 7000 properties. Public read/write access to Freebase is allowed through an HTTP-based graph-query API using the Metaweb Query Language (MQL) as a data query and manipulation language. MQL provides an easy-to-use object-oriented interface to the tuple data in Freebase and is designed to facilitate the creation of collaborative, Web-based data-oriented applications.", "title": "Freebase: a collaboratively created graph database for structuring human knowledge"}, "57faa0e5f99442d1723d2c5ccb70b1461987a7ed": {"paper_id": "57faa0e5f99442d1723d2c5ccb70b1461987a7ed", "abstract": "Berners-Lee's compelling vision of a Semantic Web is hindered by a chicken-and-egg problem, which can be best solved by a bootstrapping method - creating enough structured data to motivate the development of applications. This paper argues that autonomously \"Semantifying Wikipedia\" is the best way to solve the problem. We choose Wikipedia as an initial data source, because it is comprehensive, not too large, high-quality, and contains enough manually-derived structure to bootstrap an autonomous, self-supervised process. We identify several types of structures which can be automatically enhanced in Wikipedia (e.g., link structure, taxonomic data, infoboxes, etc.), and we describea prototype implementation of a self-supervised, machine learning system which realizes our vision. Preliminary experiments demonstrate the high precision of our system's extracted data - in one case equaling that of humans.", "title": "Autonomously semantifying wikipedia"}, "1c2dbbc5268eff6c78f581b8fc7c649d40b60538": {"paper_id": "1c2dbbc5268eff6c78f581b8fc7c649d40b60538", "abstract": "The Semantic Web has recently seen a rise of large knowledge bases (such as DBpedia) that are freely accessible via SPARQL endpoints. The structured representation of the contained information opens up new possibilities in the way it can be accessed and queried. In this paper, we present an approach that extracts a graph covering relationships between two objects of interest. We show an interactive visualization of this graph that supports the systematic analysis of the found relationships by providing highlighting, previewing, and filtering features.", "title": "RelFinder: Revealing Relationships in RDF Knowledge Bases"}, "20a773041aa5667fbcf5378ac87cad2edbfd28b7": {"paper_id": "20a773041aa5667fbcf5378ac87cad2edbfd28b7", "abstract": "The DBpedia project is a community effort to extract structured information from Wikipedia and to make this information accessible on the Web. The resulting DBpedia knowledge base currently describes over 2.6 million entities. For each of these entities, DBpedia defines a globally unique identifier that can be dereferenced over the Web into a rich RDF description of the entity, including human-readable definitions in 30 languages, relationships to other resources, classifications in four concept hierarchies, various facts as well as data-level links to other Web data sources describing the entity. Over the last year, an increasing number of data publishers have begun to set data-level links to DBpedia resources, making DBpedia a central interlinking hub for the emerging Web of data. Currently, the Web of interlinked data sources around DBpedia provides approximately 4.7 billion pieces of information and covers domains such as geographic information, people, companies, films, music, genes, drugs, books, and scientific publications. This article describes the extraction of the DBpedia knowledge base, the current status of interlinking DBpedia with other data sources on the Web, and gives an overview of applications that facilitate the Web of Data around DBpedia.", "title": "DBpedia - A crystallization point for the Web of Data"}, "057ac29c84084a576da56247bdfd63bf17b5a891": {"paper_id": "057ac29c84084a576da56247bdfd63bf17b5a891", "abstract": "Many Knowledge Bases (KBs) are now readily available and encompass colossal quantities of information thanks to either a long-term funding effort (e.g. WordNet, OpenCyc) or a collaborative process (e.g. Freebase, DBpedia). However, each of them is based on a different rigid symbolic framework which makes it hard to use their data in other systems. It is unfortunate because such rich structured knowledge might lead to a huge leap forward in many other areas of AI like natural language processing (word-sense disambiguation, natural language understanding, ...), vision (scene classification, image semantic annotation, ...) or collaborative filtering. In this paper, we present a learning process based on an innovative neural network architecture designed to embed any of these symbolic representations into a more flexible continuous vector space in which the original knowledge is kept and enhanced. These learnt embeddings would allow data from any KB to be easily used in recent machine learning methods for prediction and information retrieval. We illustrate our method on WordNet and Freebase and also present a way to adapt it to knowledge extraction from raw text.", "title": "Learning Structured Embeddings of Knowledge Bases"}, "04cc04457e09e17897f9256c86b45b92d70a401f": {"paper_id": "04cc04457e09e17897f9256c86b45b92d70a401f", "abstract": "Many data such as social networks, movie preferences or knowledge bases are multi-relational, in that they describe multiple relations between entities. While there is a large body of work focused on modeling these data, modeling these multiple types of relations jointly remains challenging. Further, existing approaches tend to breakdown when the number of these types grows. In this paper, we propose a method for modeling large multi-relational datasets, with possibly thousands of relations. Our model is based on a bilinear structure, which captures various orders of interaction of the data, and also shares sparse latent factors across different relations. We illustrate the performance of our approach on standard tensor-factorization datasets where we attain, or outperform, state-of-the-art results. Finally, a NLP application demonstrates our scalability and the ability of our model to learn efficient and semantically meaningful verb representations.", "title": "A latent factor model for highly multi-relational data"}, "2744288f090192987e980274999065ad2d6e45d6": {"paper_id": "2744288f090192987e980274999065ad2d6e45d6", "abstract": "Relational learning is concerned with predicting unknown values of a relation, given a database of entities and observed relations among entities. An example of relational learning is movie rating prediction, where entities could include users, movies, genres, and actors. Relations encode users' ratings of movies, movies' genres, and actors' roles in movies. A common prediction technique given one pairwise relation, for example a #users x #movies ratings matrix, is low-rank matrix factorization. In domains with multiple relations, represented as multiple matrices, we may improve predictive accuracy by exploiting information from one relation while predicting another. To this end, we propose a collective matrix factorization model: we simultaneously factor several matrices, sharing parameters among factors when an entity participates in multiple relations. Each relation can have a different value type and error distribution; so, we allow nonlinear relationships between the parameters and outputs, using Bregman divergences to measure error. We extend standard alternating projection algorithms to our model, and derive an efficient Newton update for the projection. Furthermore, we propose stochastic optimization methods to deal with large, sparse matrices. Our model generalizes several existing matrix factorization methods, and therefore yields new large-scale optimization algorithms for these problems. Our model can handle any pairwise relational schema and a wide variety of error models. We demonstrate its efficiency, as well as the benefit of sharing parameters among relations.", "title": "Relational learning via collective matrix factorization"}, "8f5450037cba1ba1f5c2f73fa4ffa66558eae5bd": {"paper_id": "8f5450037cba1ba1f5c2f73fa4ffa66558eae5bd", "abstract": "We consider the problem of learning probabilistic models fo r c mplex relational structures between various types of objects. A model can hel p us \u201cunderstand\u201d a dataset of relational facts in at least two ways, by finding in terpretable structure in the data, and by supporting predictions, or inferences ab out whether particular unobserved relations are likely to be true. Often there is a t radeoff between these two aims: cluster-based models yield more easily interpret abl representations, while factorization-based approaches have given better pr edictive performance on large data sets. We introduce the Bayesian Clustered Tensor Factorization (BCTF) model, which embeds a factorized representation of relatio ns in a nonparametric Bayesian clustering framework. Inference is fully Bayesia n but scales well to large data sets. The model simultaneously discovers interp retable clusters and yields predictive performance that matches or beats previo us probabilistic models for relational data.", "title": "Modelling Relational Data using Bayesian Clustered Tensor Factorization"}, "b162c99873c929447bb7ff48d454867aa83f375c": {"paper_id": "b162c99873c929447bb7ff48d454867aa83f375c", "abstract": "This paper presents PATTY: a large resource for textual patterns that denote binary relations between entities. The patterns are semantically typed and organized into a subsumption taxonomy. The PATTY system is based on efficient algorithms for frequent itemset mining and can process Web-scale corpora. It harnesses the rich type system and entity population of large knowledge bases. The PATTY taxonomy comprises 350,569 pattern synsets. Random-sampling-based evaluation shows a pattern accuracy of 84.7%. PATTY has 8,162 subsumptions, with a random-sampling-based precision of 75%. The PATTY resource is freely available for interactive access and download.", "title": "PATTY: A Taxonomy of Relational Patterns with Semantic Types"}, "996263c3ddbb50f0198354827445abd214f83030": {"paper_id": "996263c3ddbb50f0198354827445abd214f83030", "abstract": "A stochastic model is proposed for social networks in which the actors in a network are partItIoned mto subgroups called blocks. The model provides a stochastrc generalization of the blockmodel. Estimation techniques are developed for the special case of a single relation social network, with blocks specified D prrorr. An extension of the model allows for tendencies toward reciprocation of ties beyond those explained by the partition. The extended model prowdes a one degree-of-freedom test of the model. A numerical example from the social network hterature 1s used to illustrate the methods.", "title": "Stochastic blockmodels: First steps"}, "c8188951d7fcdb989b412c9055702996912ae695": {"paper_id": "c8188951d7fcdb989b412c9055702996912ae695", "abstract": "Over the past few years, massive amounts of world knowledge have been accumulated in publicly available knowledge bases, such as Freebase, NELL, and YAGO. Yet despite their seemingly huge size, these knowledge bases are greatly incomplete. For example, over 70% of people included in Freebase have no known place of birth, and 99% have no known ethnicity. In this paper, we propose a way to leverage existing Web-search-based question-answering technology to fill in the gaps in knowledge bases in a targeted way. In particular, for each entity attribute, we learn the best set of queries to ask, such that the answer snippets returned by the search engine are most likely to contain the correct value for that attribute. For example, if we want to find Frank Zappa's mother, we could ask the query `who is the mother of Frank Zappa'. However, this is likely to return `The Mothers of Invention', which was the name of his band. Our system learns that it should (in this case) add disambiguating terms, such as Zappa's place of birth, in order to make it more likely that the search results contain snippets mentioning his mother. Our system also learns how many different queries to ask for each attribute, since in some cases, asking too many can hurt accuracy (by introducing false positives). We discuss how to aggregate candidate answers across multiple queries, ultimately returning probabilistic predictions for possible values for each attribute. Finally, we evaluate our system and show that it is able to extract a large number of facts with high confidence.", "title": "Knowledge base completion via search-based question answering"}, "4f77341f99376290f21eeccd148d544ae2757b0b": {"paper_id": "4f77341f99376290f21eeccd148d544ae2757b0b", "abstract": "Working memory limits are best defined in terms of the complexity of the relations that can be processed in parallel. Complexity is defined as the number of related dimensions or sources of variation. A binary relation has one argument and one source of variation; its argument can be instantiated in only one way at a time. A binary relation has two arguments, two sources of variation, and two instantiations, and so on. Dimensionality is related to the number of chunks, because both attributes on dimensions and chunks are independent units of information of arbitrary size. Studies of working memory limits suggest that there is a soft limit corresponding to the parallel processing of one quaternary relation. More complex concepts are processed by \"segmentation\" or \"conceptual chunking.\" In segmentation, tasks are broken into components that do not exceed processing capacity and can be processed serially. In conceptual chunking, representations are \"collapsed\" to reduce their dimensionality and hence their processing load, but at the cost of making some relational information inaccessible. Neural net models of relational representations show that relations with more arguments have a higher computational cost that coincides with experimental findings on higher processing loads in humans. Relational complexity is related to processing load in reasoning and sentence comprehension and can distinguish between the capacities of higher species. The complexity of relations processed by children increases with age. Implications for neural net models and theories of cognition and cognitive development are discussed.", "title": "Processing capacity defined by relational complexity: implications for comparative, developmental, and cognitive psychology."}, "1cb0954115b1e2350627d9bfcab33cc44b635f15": {"paper_id": "1cb0954115b1e2350627d9bfcab33cc44b635f15", "abstract": "We propose a simple approach to combining first-order logic and probabilistic graphical models in a single representation. A Markov logic network (MLN) is a first-order knowledge base with a weight attached to each formula (or clause). Together with a set of constants representing objects in the domain, it specifies a ground Markov network containing one feature for each possible grounding of a first-order formula in the KB, with the corresponding weight. Inference in MLNs is performed by MCMC over the minimal subset of the ground network required for answering the query. Weights are efficiently learned from relational databases by iteratively optimizing a pseudo-likelihood measure. Optionally, additional clauses are learned using inductive logic programming techniques. Experiments with a real-world database and knowledge base in a university domain illustrate the promise of this approach.", "title": "Markov logic networks"}, "6f36ba105764cc2eab6d111c8abaa6436a0c0340": {"paper_id": "6f36ba105764cc2eab6d111c8abaa6436a0c0340", "abstract": "Wikipedia has grown to a huge, multi-lingual source of encyclopedic knowledge. Apart from textual content, a large and everincreasing number of articles feature so-called infoboxes, which provide factual information about the articles\u2019 subjects. As the different language versions evolve independently, they provide different information on the same topics. Correspondences between infobox attributes in different language editions can be leveraged for several use cases, such as automatic detection and resolution of inconsistencies in infobox data across language versions, or the automatic augmentation of infoboxes in one language with data from other language versions. We present an instance-based schema matching technique that exploits information overlap in infoboxes across different language editions. As a prerequisite we present a graph-based approach to identify articles in different languages representing the same real-world entity using (and correcting) the interlanguage links in Wikipedia. To account for the untyped nature of infobox schemas, we present a robust similarity measure that can reliably quantify the similarity of strings with mixed types of data. The qualitative evaluation on the basis of manually labeled attribute correspondences between infoboxes in four of the largest Wikipedia editions demonstrates the effectiveness of the proposed approach. 1. Entity and Attribute Matching across Wikipedia Languages Wikipedia is a well-known public encyclopedia. While most of the information contained in Wikipedia is in textual form, the so-called infoboxes provide semi-structured, factual information. They are displayed as tables in many Wikipedia articles and state basic facts about the subject. There are different templates for infoboxes, each targeting a specific category of articles and providing fields for properties that are relevant for the respective subject type. For example, in the English Wikipedia, there is a class of infoboxes about companies, one to describe the fundamental facts about countries (such as their capital and population), one for musical artists, etc. However, each of the currently 281 language versions1 defines and maintains its own set of infobox classes with their own set of properties, as well as providing sometimes different values for corresponding attributes. Figure 1 shows extracts of the English and German infoboxes for the city of Berlin. The arrows indicate matches between properties. It is already apparent that matching purely based on property names is futile: The terms Population density and Bev\u00f6lkerungsdichte or Governing parties and Reg. Parteien have no textual similarity. However, their property values are more revealing: <3,857.6/km2> and <3.875 Einw. je km2> or <SPD/Die Linke> and <SPD und Die Linke> have a high textual similarity, respectively. Email addresses: daniel.rinser@alumni.hpi.uni-potsdam.de (Daniel Rinser), dustin.lange@hpi.uni-potsdam.de (Dustin Lange), naumann@hpi.uni-potsdam.de (Felix Naumann) 1as of March 2011 Our overall goal is to automatically find a mapping between attributes of infobox templates across different language versions. Such a mapping can be valuable for several different use cases: First, it can be used to increase the information quality and quantity in Wikipedia infoboxes, or at least help the Wikipedia communities to do so. Inconsistencies among the data provided by different editions for corresponding attributes could be detected automatically. For example, the infobox in the English article about Germany claims that the population is 81,799,600, while the German article specifies a value of 81,768,000 for the same country. Detecting such conflicts can help the Wikipedia communities to increase consistency and information quality across language versions. Further, the detected inconsistencies could be resolved automatically by fusing the data in infoboxes, as proposed by [1]. Finally, the coverage of information in infoboxes could be increased significantly by completing missing attribute values in one Wikipedia edition with data found in other editions. An infobox template does not describe a strict schema, so that we need to collect the infobox template attributes from the template instances. For the purpose of this paper, an infobox template is determined by the set of attributes that are mentioned in any article that reference the template. The task of matching attributes of corresponding infoboxes across language versions is a specific application of schema matching. Automatic schema matching is a highly researched topic and numerous different approaches have been developed for this task as surveyed in [2] and [3]. Among these, schema-level matchers exploit attribute labels, schema constraints, and structural similarities of the schemas. However, in the setting of Wikipedia infoboxes these Preprint submitted to Information Systems October 19, 2012 Figure 1: A mapping between the English and German infoboxes for Berlin techniques are not useful, because infobox definitions only describe a rather loose list of supported properties, as opposed to a strict relational or XML schema. Attribute names in infoboxes are not always sound, often cryptic or abbreviated, and the exact semantics of the attributes are not always clear from their names alone. Moreover, due to our multi-lingual scenario, attributes are labeled in different natural languages. This latter problem might be tackled by employing bilingual dictionaries, if the previously mentioned issues were solved. Due to the flat nature of infoboxes and their lack of constraints or types, other constraint-based matching approaches must fail. On the other hand, there are instance-based matching approaches, which leverage instance data of multiple data sources. Here, the basic assumption is that similarity of the instances of the attributes reflects the similarity of the attributes. To assess this similarity, instance-based approaches usually analyze the attributes of each schema individually, collecting information about value patterns and ranges, amongst others, such as in [4]. A different, duplicate-based approach exploits information overlap across data sources [5]. The idea there is to find two representations of same real-world objects (duplicates) and then suggest mappings between attributes that have the same or similar values. This approach has one important requirement: The data sources need to share a sufficient amount of common instances (or tuples, in a relational setting), i.e., instances describing the same real-world entity. Furthermore, the duplicates either have to be known in advance or have to be discovered despite a lack of knowledge of corresponding attributes. The approach presented in this article is based on such duplicate-based matching. Our approach consists of three steps: Entity matching, template matching, and attribute matching. The process is visualized in Fig. 2. (1) Entity matching: First, we find articles in different language versions that describe the same real-world entity. In particular, we make use of the crosslanguage links that are present for most Wikipedia articles and provide links between same entities across different language versions. We present a graph-based approach to resolve conflicts in the linking information. (2) Template matching: We determine a cross-lingual mapping between infobox templates by analyzing template co-occurrences in the language versions. (3) Attribute matching: The infobox attribute values of the corresponding articles are compared to identify matching attributes across the language versions, assuming that the values of corresponding attributes are highly similar for the majority of article pairs. As a first step we analyze the quality of Wikipedia\u2019s interlanguage links in Sec. 2. We show how to use those links to create clusters of semantically equivalent entities with only one entity from each language in Sec. 3. This entity matching approach is evaluated in Sec. 4. In Sec. 5, we show how a crosslingual mapping between infobox templates can be established. The infobox attribute matching approach is described in Sec. 6 and in turn evaluated in Sec. 7. Related work in the areas of ILLs, concept identification, and infobox attribute matching is discussed in Sec. 8. Finally, Sec. 9 draws conclusions and discusses future work. 2. Interlanguage Links Our basic assumption is that there is a considerable amount of information overlap across the different Wikipedia language editions. Our infobox matching approach presented later requires mappings between articles in different language editions", "title": "Cross-lingual entity matching and infobox alignment in Wikipedia"}, "27d1adf2f7f78286bc28b1d75c568d318399d8d9": {"paper_id": "27d1adf2f7f78286bc28b1d75c568d318399d8d9", "abstract": "YAGO [9, 6] is one of the largest public ontologies constructed by information extraction. In a recent refactoring called YAGO2s, the system has been given a modular and completely transparent architecture. In this demo, users can see how more than 30 individual modules of YAGO work in parallel to extract facts, to check facts for their correctness, to deduce facts, and to merge facts from different sources. A GUI allows users to play with different input files, to trace the provenance of individual facts to their sources, to change deduction rules, and to run individual extractors. Users can see step by step how the extractors work together to combine the individual facts to the coherent whole of the YAGO ontology.", "title": "Inside YAGO2s: a transparent information extraction architecture"}, "69bb5cfd37ebb4da0a4a8ac966c208111f13174d": {"paper_id": "69bb5cfd37ebb4da0a4a8ac966c208111f13174d", "abstract": "Wikipedia is a goldmine of information; not just for its many readers, but also for the growing community of researchers who recognize it as a resource of exceptional scale and utility. It represents a vast investment of manual effort and judgment: a huge, constantly evolving tapestry of concepts and relations that is being applied to a host of tasks. This article provides a comprehensive description of this work. It focuses on research that extracts and makes use of the concepts, relations, facts and descriptions found in Wikipedia, and organizes the work into four broad categories: applying Wikipedia to natural language processing; using it to facilitate information retrieval and information extraction; and as a resource for ontology building. The article addresses how Wikipedia is being used as is, how it is being improved and adapted, and how it is being combined with other structures to create entirely new resources. We identify the research groups and individuals involved, and how their work has developed in the last few years. We provide a comprehensive list of the open-source software they have produced. r 2009 Elsevier Ltd. All rights reserved.", "title": "Mining meaning from Wikipedia"}, "09f6fa1869be4d3d9188d1313061602038cb97d4": {"paper_id": "09f6fa1869be4d3d9188d1313061602038cb97d4", "abstract": "This article presents YAGO, a large ontology with high coverage and precision. YAGO has been automatically derived from Wikipedia and WordNet. It comprises entities and relations, and currently contains more than 1.7 million entities and 15 million facts. These include the taxonomic Is-A hierarchy as well as semantic relations between entities. The facts for YAGO have been extracted from the category system and the infoboxes of Wikipedia and have been combined with taxonomic relations from WordNet. Type Ontologies Information extraction K checking techniques help us keep YAGO\u2019s precision at 95%\u2014as proven by an extensive evaluation study. YAGO is based on a clean logical model with a decidable consistency. Furthermore, it allows representing n-ary relations in a natural way while maintaining compatibility with RDFS. A powerful query model s data", "title": "YAGO: A Large Ontology from Wikipedia and WordNet"}, "00a3f6924f90fcd77e6e7e6534b957a75d0ced07": {"paper_id": "00a3f6924f90fcd77e6e7e6534b957a75d0ced07", "abstract": "We present YAGO, a light-weight and extensible ontology with high coverage and quality. YAGO builds on entities and relations and currently contains more than 1 million entities and 5 million facts. This includes the Is-A hierarchy as well as non-taxonomic relations between entities (such as HASONEPRIZE). The facts have been automatically extracted from Wikipedia and unified with WordNet, using a carefully designed combination of rule-based and heuristic methods described in this paper. The resulting knowledge base is a major step beyond WordNet: in quality by adding knowledge about individuals like persons, organizations, products, etc. with their semantic relationships - and in quantity by increasing the number of facts by more than an order of magnitude. Our empirical evaluation of fact correctness shows an accuracy of about 95%. YAGO is based on a logically clean model, which is decidable, extensible, and compatible with RDFS. Finally, we show how YAGO can be further extended by state-of-the-art information extraction techniques.", "title": "Yago: a core of semantic knowledge"}, "57820e6f974d198bf4bbdf26ae7e1063bac190c3": {"paper_id": "57820e6f974d198bf4bbdf26ae7e1063bac190c3", "abstract": null, "title": "The Semantic Web\" in Scientific American"}, "aab076ea25beeb1fd4a2d2d7e5692a66ca51cec0": {"paper_id": "aab076ea25beeb1fd4a2d2d7e5692a66ca51cec0", "abstract": "Waveguide structures have a constrained bandwidth due to their cutoff frequencies and presence of higher order modes. Ridged substrate integrated waveguides (RSIW) are low profile, low cost and planar that can be designed for wider bandwidth. In this paper, a two-way power divider based on RSIW is designed to extend bandwidth beyond that of conventional guides. The design utilizes a developed low loss GCPW to RSIW transition to feed the structure. Design was successfully extended to eight way splitter as well. The developed divider has almost a 75% bandwidth with better than 15dB return loss from 5 to 11 GHz. In particular, the splitter demonstrated excellent amplitude and phase imbalance of less than \u00b10.35 dB and \u00b17\u00b0 within 5.5 to 9 GHz, respectively. Theoretical and experimental results are in good agreement.", "title": "Design of single-ridge SIW power dividers with over 75% bandwidth"}, "1b07253ea1b0b0b1279769b906234ff615f6cd8c": {"paper_id": "1b07253ea1b0b0b1279769b906234ff615f6cd8c", "abstract": "This paper presents a novel approach for detecting affine invariant interest points. Our method can deal with significant affine transformations including large scale changes. Such transformations introduce significant changes in the point location as well as in the scale and the shape of the neighbourhood of an interest point. Our approach allows to solve for these problems simultaneously. It is based on three key ideas : 1) The second moment matrix computed in a point can be used to normalize a region in an affine invariant way (skew and stretch). 2) The scale of the local structure is indicated by local extrema of normalized derivatives over scale. 3) An affine-adapted Harris detector determines the location of interest points. A multi-scale version of this detector is used for initialization. An iterative algorithm then modifies location, scale and neighbourhood of each point and converges to affine invariant points. For matching and recognition, the image is characterized by a set of affine invariant points ; the affine transformation associated with each point allows the computation of an affine invariant descriptor which is also invariant to affine illumination changes. A quantitative comparison of our detector with existing ones shows a significant improvement in the presence of large affine deformations. Experimental results for wide baseline matching show an excellent performance in the presence of large perspective transformations including significant scale changes. Results for recognition are very good for a database with more than 5000 images.", "title": "An Affine Invariant Interest Point Detector"}, "0a11cd64f46b34fee230840684dae3cc8e1905a8": {"paper_id": "0a11cd64f46b34fee230840684dae3cc8e1905a8", "abstract": "Computer vision is embracing a new research focus in which the aim is to develop visual skills for robots that allow them to interact with a dynamic, realistic environment. To achieve this aim, new kinds of vision algorithms need to be developed which run in real time and subserve the robot's goals. Two fundamental goals are determining the location of a known object. Color can be successfully used for both tasks. This article demonstrates that color histograms of multicolored objects provide a robust, efficient cue for indexing into a large database of models. It shows that color histograms are stable object representations in the presence of occlusion and over change in view, and that they can differentiate among a large number of objects. For solving the identification problem, it introduces a technique calledHistogram Intersection, which matches model and image histograms and a fast incremental version of Histogram Intersection, which allows real-time indexing into a large database of stored models. For solving the location problem it introduces an algorithm calledHistogram Backprojection, which performs this task efficiently in crowded scenes.", "title": "Color indexing"}, "d11bf8b06cf96d90e1ee3dd6467c5c92ac53e9a1": {"paper_id": "d11bf8b06cf96d90e1ee3dd6467c5c92ac53e9a1", "abstract": "Model-based recognition and motion tracking depends upon the ability to solve for projection and model parameters that will best fit a 3-D model to matching 2-D image features. This paper extends current methods of parameter solving to handle objects with arbitrary curved surfaces and with any number of internal parameters representing articulations, variable dimensions, or surface deformations. Numerical stabilization methods are developed that take account of inherent inaccuracies in the image measurements and allow useful solutions to be determined even when there are fewer matches than unknown parameters. The LevenbergMarquardt method is used to always ensure convergence of the solution. These techniques allow model-based vision to be used for a much wider class of problems than was possible with previous methods. Their application is demonstrated for tracking the motion of curved, parameterized objects. This paper has been published in IEEE Transactions on Pattern Analysis and Machine Intelligence, 13, 5 (May 1991), pp. 441\u2013450.", "title": "Fitting Parameterized Three-Dimensional Models to Images"}, "f4c83b78c787e01d3d4e58ae18c8f3ab5933a02b": {"paper_id": "f4c83b78c787e01d3d4e58ae18c8f3ab5933a02b", "abstract": "An explicit three-dimensional representation is constructed from feature-points extracted from a sequence of images taken by a moving camera. The points are tracked through the sequence, and their 3D locations accurately determined by use of Kalman Filters. The ego-motion of the camera is also solved for.", "title": "3D positional integration from image sequences"}, "0e61ce27c39bf57714380bc4f641bbff5a508d64": {"paper_id": "0e61ce27c39bf57714380bc4f641bbff5a508d64", "abstract": "We describe a technique for image encoding in which local operators of many scales but identical shape serve as the basis functions. The representation differs from established techniques in that the code elements are localized in spatial frequency as well as in space. Pixel-to-pixel correlations are first removed by subtracting a lowpass filtered copy of the image from the image itself. The result is a net data compression since the difference, or error, image has low variance and entropy, and the low-pass filtered image may represented at reduced sample density. Further data compression is achieved by quantizing the difference image. These steps are then repeated to compress the low-pass image. Iteration of the process at appropriately expanded scales generates a pyramid data structure. The encoding process is equivalent to sampling the image with Laplacian operators of many scales. Thus, the code tends to enhance salient image features. A further advantage of the present code is that it is well suited for many image analysis tasks as well as for image compression. Fast algorithms are described for coding and decoding.", "title": "The Laplacian Pyramid as a Compact Image Code"}, "8ce9cb9f2e5c7a7e146b176eb82dbcd34b4c764c": {"paper_id": "8ce9cb9f2e5c7a7e146b176eb82dbcd34b4c764c", "abstract": "The Parallel Circuit SIMulator (PCSIM) is a software package for simulation of neural circuits. It is primarily designed for distributed simulation of large scale networks of spiking point neurons. Although its computational core is written in C++, PCSIM's primary interface is implemented in the Python programming language, which is a powerful programming environment and allows the user to easily integrate the neural circuit simulator with data analysis and visualization tools to manage the full neural modeling life cycle. The main focus of this paper is to describe PCSIM's full integration into Python and the benefits thereof. In particular we will investigate how the automatically generated bidirectional interface and PCSIM's object-oriented modular framework enable the user to adopt a hybrid modeling approach: using and extending PCSIM's functionality either employing pure Python or C++ and thus combining the advantages of both worlds. Furthermore, we describe several supplementary PCSIM packages written in pure Python and tailored towards setting up and analyzing neural simulations.", "title": "PCSIM: A Parallel Simulation Environment for Neural Circuits Fully Integrated with Python"}, "709c6007eb00e1aa462b5b24085e4ac59436a8ea": {"paper_id": "709c6007eb00e1aa462b5b24085e4ac59436a8ea", "abstract": "SAGE is a framework for number theory, algebra, and geometry computation that is initially being designed for computing with elliptic curves and modular forms. The current implementation is primarily due to William Stein. It is open source and freely available under the terms of the GNU General Public License (GPL).", "title": "SAGE: system for algebra and geometry experimentation"}, "8430c0b9afa478ae660398704b11dca1221ccf22": {"paper_id": "8430c0b9afa478ae660398704b11dca1221ccf22", "abstract": "The report introduces a constructive learning algorithm for recurrent neural networks, which modifies only the weights to output units in order to achieve the learning task. key words: recurrent neural networks, supervised learning Zusammenfassung. Der Report f\u00fchrt ein konstruktives Lernverfahren f\u00fcr rekurrente neuronale Netze ein, welches zum Erreichen des Lernzieles lediglich die Gewichte der zu den Ausgabeneuronen f\u00fchrenden Verbindungen modifiziert. Stichw\u00f6rter: rekurrente neuronale Netze, \u00fcberwachtes Lernen", "title": "The''echo state''approach to analysing and training recurrent neural networks"}, "8db76ad9b3afc898f40fb4e4564c43c904df47c0": {"paper_id": "8db76ad9b3afc898f40fb4e4564c43c904df47c0", "abstract": "Lack of trust in online transactions has been cited as the main reason for the abhorrence of online shopping. We have tested the mediating role of trust in online transactions to provide empirical evidence that trust in the online store represents the generic mechanism through which the focal independent variables of website design are able to positively influence purchase intention and reduce the perceived risk. We have further demonstrated the moderating effect of the individual\u2019s culture in e-commerce and thereby offered insights into the relative importance of website design factors contributing to trust for customers of different cultural values.", "title": "The effects of website design on purchase intention in online shopping: the mediating role of trust and the moderating role of culture"}, "28b2723d1feb5373e3136d358210efc76f7d6f46": {"paper_id": "28b2723d1feb5373e3136d358210efc76f7d6f46", "abstract": "M oving some Web consumers along to the purchase click is proving to be difficult, despite the impressive recent growth in online shopping. Consumer online shopping revenues and related corporate profits are still meager, though the industry is optimistic, thanks to bullish forecasts of cyberconsumer activity for the new millennium. In 1996, Internet shopping revenues for U.S. users, excluding cars and real estate, were estimated by Jupiter Communications , an e-commerce consulting firm in New York, at approximately $707 million but are expected to How merchants can win back lost consumer trust in the interests of e-commerce sales. reach nearly $37.5 billion by 2002 [1]. Meanwhile, the business-to-business side is taking off with more than $8 billion in revenues for 1997 and $327 billion predicted by 2002 just in the U.S., according to For-rester Research, an information consulting firm in Cambridge, Mass. [4]. On the consumer side, a variety of barriers are invoked to explain the continuing difficulties. There are, to be sure, numerous barriers. Such factors as the lack of standard technologies for secure payment, and the lack of profitable business models play important roles in the relative dearth of commercial activity by businesses and consumers on the Internet compared to what analysts expect in the near future. Granted, the commercial development of the Web is still in its infancy, so few expect these barriers to commercial development to persist. Still, commercial development of the Web faces a far more formidable barrier\u2014consumers' fear of divulging their personal data\u2014to its ultimate com-mercialization. The reason more people have yet to shop online or even provide information to Web providers in exchange for access to information, is the fundamental lack of faith between most businesses and consumers on the Web today. In essence, consumers simply do not trust most Web providers enough to engage in \" relationship exchanges \" involving money and personal information with them. Our research reveals that this lack of trust arises from the fact that cyberconsumers feel they lack control over the access that Web merchants have to their", "title": "Building Consumer Trust Online"}, "f39f1f1b0ecb76b81aeda59cc9849771551c9578": {"paper_id": "f39f1f1b0ecb76b81aeda59cc9849771551c9578", "abstract": "Moving away from simple foot designs of current quadruped robots towards a more bio-inspired approach, a novel foot design was implemented on the quadruped robot Oncilla. These feet mimic soft paw-pads of dogs and cats with high traction and soft underlying tissue. Consisting of a granular medium enclosed in a flexible membrane, they can be set to different pressure/vacuum conditions. Tests of general properties such as friction force, damping and deformation were completed by proof of concept tests on the robot. These included flat ground locomotion as well as ascending a slope with different inclination. Comparison tests with the previous feet were performed as well, showing that the new feet have a high friction and strong damping properties. Additionally, the speed of flat ground locomotion is comparable to the maximum speed of the robot with the previous feet while retaining the desired trotting gait. These are promising aspects for legged locomotion. The jamming of granular media previously has been used to create a universal gripper which in the future also opens up opportunities to use the feet both in locomotion and simple object manipulation (although the manipulation is not tested here).", "title": "Friction and damping of a compliant foot based on granular jamming for legged robots"}, "c90dabab96e47b078c8ad1c9fc3b153db37ffac5": {"paper_id": "c90dabab96e47b078c8ad1c9fc3b153db37ffac5", "abstract": "This paper explores the quadruped running gaits that use the legs in pairs: the trot (diagonal pairs), the pace (lateral pairs), and the bound (front and rear pairs). Rather than study these gaits in quadruped animals, we studied them in a quadruped robot. We found that each of the gaits that use the legs in pairs can be transformed into a common underlying gait, a virtual biped gait. Once transformed, a single set of control algorithms produce all three gaits, with modest parameter variations between them. The control algorithms manipulated rebound height, running speed, and body attitude, while a low-level mechanism coordinated the behavior of the legs in each pair. The approach was tested with laboratory experiments on a four-legged robot. Data are presented that show the details of the running motion for the three gaits and for transitions from one gait to another.", "title": "Trotting, pacing and bounding by a quadruped robot."}, "8a3eabb4c11a9de3367ea8269e407a3763a4eee9": {"paper_id": "8a3eabb4c11a9de3367ea8269e407a3763a4eee9", "abstract": "In this paper we compare models and experiments involving Scout II, an untethered four-legged running robot with only one actuator per compliant leg. Scout II achieves dynamically stable running of up to 1.3 m s\u22121 on flat ground via a bounding gait. Energetics analysis reveals a highly efficient system with a specific resistance of only 1.4. The running controller requires no task-level or body-state feedback, and relies on the passive dynamics of the mechanical system. These results contribute to the increasing evidence that apparently complex dynamically dexterous tasks may be controlled via simple control laws. We discuss general modeling issues for dynamically stable legged robots. Two simulation models are compared with experimental data to test the validity of common simplifying assumptions. The need for including motor saturation and non-rigid torque transmission characteristics in simulation models is demonstrated. Similar issues are likely to be important in other dynamically stable legged robots as well. An extensive suite of experimental results documents the robot\u2019s performance and the validity of the proposed", "title": "Modeling and Experiments of Untethered Quadrupedal Running with a Bounding Gait: The Scout II Robot"}, "3ba9b26ca6113bf807a4648fbe08cd3536a9b5b3": {"paper_id": "3ba9b26ca6113bf807a4648fbe08cd3536a9b5b3", "abstract": "The legged animals are capable of rapid, energy efficient, and adaptive locomotion in a complex environment. Toward a comprehensive understanding of the nature of such ecologically balanced legged locomotion, in this paper, we propose a novel method to achieve a form of bounding gait for a quadruped robot by using a minimalistic approach. Although this method uses a simple sinusoidal position control with no global sensory feedback, it is shown that the rapid bounding is possible in a relatively robust manner by properly exploiting the intrinsic dynamics and the interaction with the environment. The behavioral analyses with the robot experiments show that this relatively complicated dynamic locomotion is achieved even with a simple controller mainly because of a self-stabilization mechanism. Moreover, by exploiting this mechanism of self-stabilization, we propose a unique approach to control the forward velocity of the locomotion.", "title": "\"Cheap\" Rapid Locomotion of a Quadruped Robot: Self-Stabilization of Bounding Gait"}, "a5366f4d0e17dce1cdb59ddcd90e806ef8741fbc": {"paper_id": "a5366f4d0e17dce1cdb59ddcd90e806ef8741fbc", "abstract": null, "title": "Legged Robots That Balance"}, "6bdda860d65b41c093b42a69277d6ac898fe113d": {"paper_id": "6bdda860d65b41c093b42a69277d6ac898fe113d", "abstract": "Scout II, an autonomous four-legged robot with only one actuator per compliant leg is described. We demonstrate the need to model the actuators and the power source of the robot system carefully in order to obtain experimentally valid models for simulation and analysis. We describe a new, simple running controller that requires minimal task level feedback, yet achieves reliable and fast running up to 1.2 m/s. These results contribute to the increasing evidence that apparently complex dynamically dexterous tasks may be controlled via simple control laws. In addition, the simple mechanical design of our robot may be used as a template for the control of higher degree of freedom quadrupeds. An energetics analysis reveals a highly efficient system with a specific resistance of 0.32 when based on mechanical power dissipation and of 1.0 when based on total electrical power dissipation.", "title": "Quadruped Robot Running With a Bounding Gait"}, "ac1d0006706d26135f6c865fcb236fe703e96873": {"paper_id": "ac1d0006706d26135f6c865fcb236fe703e96873", "abstract": "Job boards and professional social networks heavily use recommender systems in order to better support users in exploring job advertisements. Detecting the similarity between job advertisements is important for job recommendation systems as it allows, for example, the application of item-to-item based recommendations. In this work, we research the usage of dense vector representations to enhance a large-scale job recommendation system and to rank German job advertisements regarding their similarity. We follow a two-folded evaluation scheme: (1) we exploit historic user interactions to automatically create a dataset of similar jobs that enables an offline evaluation. (2) In addition, we conduct an online A/B test and evaluate the best performing method on our platform reaching more than 1 million users. We achieve the best results by combining job titles with full-text job descriptions. In particular, this method builds dense document representation using words of the titles to weigh the importance of words of the full-text description. In the online evaluation, this approach allows us to increase the click-through rate on job recommendations for active users by 8.0%.", "title": "Document-based Recommender System for Job Postings using Dense Representations"}, "18768ab2db59e14253c71bd75bf8481554620060": {"paper_id": "18768ab2db59e14253c71bd75bf8481554620060", "abstract": "Dropout and other feature noising schemes control overfitting by artificially corrupting the training data. For generalized linear models, dropout performs a form of adaptive regularization. Using this viewpoint, we show that the dropout regularizer is first-order equivalent to an L2 regularizer applied after scaling the features by an estimate of the inverse diagonal Fisher information matrix. We also establish a connection to AdaGrad, an online learning algorithm, and find that a close relative of AdaGrad operates by repeatedly solving linear dropout-regularized problems. By casting dropout as regularization, we develop a natural semi-supervised algorithm that uses unlabeled data to create a better adaptive regularizer. We apply this idea to document classification tasks, and show that it consistently boosts the performance of dropout training, improving on state-of-the-art results on the IMDB reviews dataset.", "title": "Dropout Training as Adaptive Regularization"}, "5e9fa46f231c59e6573f9a116f77f53703347659": {"paper_id": "5e9fa46f231c59e6573f9a116f77f53703347659", "abstract": "The existence and use of standard test collections in information retrieval experimentation allows results to be compared between research groups and over time. Such comparisons, however, are rarely made. Most researchers only report results from their own experiments, a practice that allows lack of overall improvement to go unnoticed. In this paper, we analyze results achieved on the TREC Ad-Hoc, Web, Terabyte, and Robust collections as reported in SIGIR (1998\u20132008) and CIKM (2004\u20132008). Dozens of individual published experiments report effectiveness improvements, and often claim statistical significance. However, there is little evidence of improvement in ad-hoc retrieval technology over the past decade. Baselines are generally weak, often being below the median original TREC system. And in only a handful of experiments is the score of the best TREC automatic run exceeded. Given this finding, we question the value of achieving even a statistically significant result over a weak baseline. We propose that the community adopt a practice of regular longitudinal comparison to ensure measurable progress, or at least prevent the lack of it from going unnoticed. We describe an online database of retrieval runs that facilitates such a practice.", "title": "Baselines and Bigrams: Simple, Good Sentiment and Topic Classification"}, "47a87c2cbdd928bb081974d308b3d9cf678d257e": {"paper_id": "47a87c2cbdd928bb081974d308b3d9cf678d257e", "abstract": "A new recurrent neural network based language model (RNN LM) with applications to speech recognition is presented. Results indicate that it is possible to obtain around 50% reduction of perplexity by using mixture of several RNN LMs, compared to a state of the art backoff language model. Speech recognition experiments show around 18% reduction of word error rate on the Wall Street Journal task when comparing models trained on the same amount of data, and around 5% on the much harder NIST RT05 task, even when the backoff model is trained on much more data than the RNN LM. We provide ample empirical evidence to suggest that connectionist language models are superior to standard n-gram techniques, except their high computational (training) complexity.", "title": "Recurrent neural network based language model"}, "2063745d08868c928455f422202b72146a1960fb": {"paper_id": "2063745d08868c928455f422202b72146a1960fb", "abstract": "We present a general learning-based approach for phrase-level sentiment analysis that adopts an ordinal sentiment scale and is explicitly compositional in nature. Thus, we can model the compositional effects required for accurate assignment of phrase-level sentiment. For example, combining an adverb (e.g., \u201cvery\u201d) with a positive polar adjective (e.g., \u201cgood\u201d) produces a phrase (\u201cvery good\u201d) with increased polarity over the adjective alone. Inspired by recent work on distributional approaches to compositionality, we model each word as a matrix and combine words using iterated matrix multiplication, which allows for the modeling of both additive and multiplicative semantic effects. Although the multiplication-based matrix-space framework has been shown to be a theoretically elegant way to model composition (Rudolph and Giesbrecht, 2010), training such models has to be done carefully: the optimization is nonconvex and requires a good initial starting point. This paper presents the first such algorithm for learning a matrix-space model for semantic composition. In the context of the phrase-level sentiment analysis task, our experimental results show statistically significant improvements in performance over a bagof-words model.", "title": "Compositional Matrix-Space Models for Sentiment Analysis"}, "046bf6fb90438335eaee07594855efbf541a8aba": {"paper_id": "046bf6fb90438335eaee07594855efbf541a8aba", "abstract": "Urbanization's rapid progress has modernized many people's lives but also engendered big issues, such as traffic congestion, energy consumption, and pollution. Urban computing aims to tackle these issues by using the data that has been generated in cities (e.g., traffic flow, human mobility, and geographical data). Urban computing connects urban sensing, data management, data analytics, and service providing into a recurrent process for an unobtrusive and continuous improvement of people's lives, city operation systems, and the environment. Urban computing is an interdisciplinary field where computer sciences meet conventional city-related fields, like transportation, civil engineering, environment, economy, ecology, and sociology in the context of urban spaces. This article first introduces the concept of urban computing, discussing its general framework and key challenges from the perspective of computer sciences. Second, we classify the applications of urban computing into seven categories, consisting of urban planning, transportation, the environment, energy, social, economy, and public safety and security, presenting representative scenarios in each category. Third, we summarize the typical technologies that are needed in urban computing into four folds, which are about urban sensing, urban data management, knowledge fusion across heterogeneous data, and urban data visualization. Finally, we give an outlook on the future of urban computing, suggesting a few research topics that are somehow missing in the community.", "title": "Urban Computing: Concepts, Methodologies, and Applications"}, "f16841e022038e94a59f7e0a82002102b78d79a4": {"paper_id": "f16841e022038e94a59f7e0a82002102b78d79a4", "abstract": "Abstract: A new graphical display is proposed for partitioning techniques. Each cluster is represented by a so-called silhouette, which is based on the comparison of its tightness and separation. This silhouette shows which objects he well within their cluster, and which ones are merely somewhere in between clusters. The entire clustering is displayed by combining the silhouettes into a single plot, allowing an appreciation of the relative quality of the clusters and an overview of the data configuration. The average silhouette width provides an evaluation of clustering validity, and might be used to select an \u2018appropriate\u2019 number of clusters.", "title": "Silhouettes : a graphical aid to the interpretation and validation of cluster analysis"}, "491aa17e927dba579c1bc67afcbb2507c0019485": {"paper_id": "491aa17e927dba579c1bc67afcbb2507c0019485", "abstract": "Over the last few years, much online volunteered geographic information (VGI) has emerged and has been increasingly analyzed to understand places and cities, as well as human mobility and activity. However, there are concerns about the quality and usability of such VGI. In this study, we demonstrate a complete process that comprises the collection, unification, classification and validation of a type of VGI\u2014online point-of-interest (POI) data\u2014and develop methods to utilize such POI data to estimate disaggregated land use (i.e., employment size by category) at a very high spatial resolution (census block level) using part of the Boston metropolitan area as an example. With recent advances in activity-based land use, transportation, and environment (LUTE) models, such disaggregated land use data become important to allow LUTE models to analyze and simulate a person\u2019s choices of work location and activity destinations and to understand policy impacts on future cities. These data can also be used as alternatives to explore economic activities at the local level, especially as government-published census-based disaggregated employment data have become less available in the recent decade. Our new approach provides opportunities for cities to estimate land use at high resolution with low cost by utilizing VGI while ensuring its quality with a certain accuracy threshold. The automatic classification of POI can also be utilized for other types of analyses on cities. 2014 Elsevier Ltd. All rights reserved.", "title": "Mining point-of-interest data from social networks for urban land use classification and disaggregation"}, "27496a2ee337db705e7c611dea1fd8e6f41437c2": {"paper_id": "27496a2ee337db705e7c611dea1fd8e6f41437c2", "abstract": "We analyze some of the fundamental design challenges and misconceptions that underlie the development of an efficient and robust NER system. In particular, we address issues such as the representation of text chunks, the inference approach needed to combine local NER decisions, the sources of prior knowledge and how to use them within an NER system. In the process of comparing several solutions to these challenges we reach some surprising conclusions, as well as develop an NER system that achieves 90.8 F1 score on the CoNLL-2003 NER shared task, the best reported result for this dataset.", "title": "Design Challenges and Misconceptions in Named Entity Recognition"}, "666b639aadcd2a8a11d24b36bae6a4f07e802b34": {"paper_id": "666b639aadcd2a8a11d24b36bae6a4f07e802b34", "abstract": "Word representations have proven useful for many NLP tasks, e.g., Brown clusters as features in dependency parsing (Koo et al., 2008). In this paper, we investigate the use of continuous word representations as features for dependency parsing. We compare several popular embeddings to Brown clusters, via multiple types of features, in both news and web domains. We find that all embeddings yield significant parsing gains, including some recent ones that can be trained in a fraction of the time of others. Explicitly tailoring the representations for the task leads to further improvements. Moreover, an ensemble of all representations achieves the best results, suggesting their complementarity.", "title": "Tailoring Continuous Word Representations for Dependency Parsing"}, "1be16d8c557b15cdf2db9e7eb4453f2274fd60af": {"paper_id": "1be16d8c557b15cdf2db9e7eb4453f2274fd60af", "abstract": "Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.org.", "title": "Scikit-learn: Machine Learning in Python"}, "865f3b85a2df23e72dbbeb9ffc4095e44fa6636e": {"paper_id": "865f3b85a2df23e72dbbeb9ffc4095e44fa6636e", "abstract": "This paper presents a new framework for human action recognition from a 3D skeleton sequence. Previous studies do not fully utilize the temporal relationships between video segments in a human action. Some studies successfully used very deep Convolutional Neural Network (CNN) models but often suffer from the data insufficiency problem. In this study, we first segment a skeleton sequence into distinct temporal segments in order to exploit the correlations between them. The temporal and spatial features of a skeleton sequence are then extracted simultaneously by utilizing a fine-to-coarse (F2C) CNN architecture optimized for human skeleton sequences. We evaluate our proposed method on NTU RGB+D and SBU Kinect Interaction dataset. It achieves 79.6% and 84.6% of accuracies on NTU RGB+D with cross-object and cross-view protocol, respectively, which are almost identical with the state-of-the-art performance. In addition, our method significantly improves the accuracy of the actions in two-person interactions.", "title": "A Fine-to-Coarse Convolutional Neural Network for 3D Human Action Recognition"}, "5636dca44384240ce9aff2b10b78458cd3c2f450": {"paper_id": "5636dca44384240ce9aff2b10b78458cd3c2f450", "abstract": "Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.\n In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.", "title": "\"Why Should I Trust You?\": Explaining the Predictions of Any Classifier"}, "4f298d6d0c8870acdbf94fe473ebf6814681bd1f": {"paper_id": "4f298d6d0c8870acdbf94fe473ebf6814681bd1f", "abstract": "We provide a detailed review of the work on human action recognition over the past decade. We refer to \u201cactions\u201d as meaningful human motions. Starting with methods that are based on handcrafted representations, we review the impact of revamped deep neural networks on action recognition. We follow a systematic taxonomy of action recognition approaches to present a coherent discussion over their improvements and fall-backs.", "title": "Going Deeper into Action Recognition: A Survey"}, "376b078694f0c183e4832900debda4dfed021a9a": {"paper_id": "376b078694f0c183e4832900debda4dfed021a9a", "abstract": "This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [5], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [13].", "title": "Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps"}, "d59836912966858a0b1de1b03c8e4f24f53659a7": {"paper_id": "d59836912966858a0b1de1b03c8e4f24f53659a7", "abstract": "The ability to identify and temporally segment fine-grained human actions throughout a video is crucial for robotics, surveillance, education, and beyond. Typical approaches decouple this problem by first extracting local spatiotemporal features from video frames and then feeding them into a temporal classifier that captures high-level temporal patterns. We describe a class of temporal models, which we call Temporal Convolutional Networks (TCNs), that use a hierarchy of temporal convolutions to perform fine-grained action segmentation or detection. Our Encoder-Decoder TCN uses pooling and upsampling to efficiently capture long-range temporal patterns whereas our Dilated TCN uses dilated convolutions. We show that TCNs are capable of capturing action compositions, segment durations, and long-range dependencies, and are over a magnitude faster to train than competing LSTM-based Recurrent Neural Networks. We apply these models to three challenging fine-grained datasets and show large improvements over the state of the art.", "title": "Temporal Convolutional Networks for Action Segmentation and Detection"}, "1f76b7b071f3e65c97d09720f88d6b0ad9f07e8f": {"paper_id": "1f76b7b071f3e65c97d09720f88d6b0ad9f07e8f", "abstract": "Deep residual networks [1] have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/ resnet-1k-layers.", "title": "Identity Mappings in Deep Residual Networks"}, "3cdb1364c3e66443e1c2182474d44b2fb01cd584": {"paper_id": "3cdb1364c3e66443e1c2182474d44b2fb01cd584", "abstract": "We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1] . The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN [2] and also with the well known DeepLab-LargeFOV [3] , DeconvNet [4] architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures and can be trained end-to-end using stochastic gradient descent. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. These quantitative assessments show that SegNet provides good performance with competitive inference time and most efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.", "title": "SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation"}, "5ca4abab527f6b0270e50548f0dea30638c9b86e": {"paper_id": "5ca4abab527f6b0270e50548f0dea30638c9b86e", "abstract": "Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed can nevertheless remain finite: for a special class of initial conditions on the weights, very deep networks incur only a finite, depth independent, delay in learning speed relative to shallow networks. We show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, while scaled random Gaussian initializations cannot. We further exhibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos. Deep learning methods have realized impressive performance in a range of applications, from visual object classification [1, 2, 3] to speech recognition [4] and natural language processing [5, 6]. These successes have been achieved despite the noted difficulty of training such deep architectures [7, 8, 9, 10, 11]. Indeed, many explanations for the difficulty of deep learning have been advanced in the literature, including the presence of many local minima, low curvature regions due to saturating nonlinearities, and exponential growth or decay of back-propagated gradients [12, 13, 14, 15]. Furthermore, many neural network simulations have observed 1 ar X iv :1 31 2. 61 20 v3 [ cs .N E ] 1 9 Fe b 20 14 strikingly nonlinear learning dynamics, including long plateaus of little apparent improvement followed by almost stage-like transitions to better performance. However, a quantitative, analytical understanding of the rich dynamics of deep learning remains elusive. For example, what determines the time scales over which deep learning unfolds? How does training speed retard with depth? Under what conditions will greedy unsupervised pretraining speed up learning? And how do the final learned internal representations depend on the statistical regularities inherent in the training data? Here we provide an exact analytical theory of learning in deep linear neural networks that quantitatively answers these questions for this restricted setting. Because of its linearity, the input-output map of a deep linear network can always be rewritten as a shallow network. In this sense, a linear network does not gain expressive power from depth, and hence will underfit and perform poorly on complex real world problems. But while it lacks this important aspect of practical deep learning systems, a deep linear network can nonetheless exhibit highly nonlinear learning dynamics, and these dynamics change with increasing depth. Indeed, the training error, as a function of the network weights, is non-convex, and gradient descent dynamics on this non-convex error surface exhibits a subtle interplay between different weights across multiple layers of the network. Hence deep linear networks provide an important starting point for understanding deep learning dynamics. To answer these questions, we derive and analyze a set of nonlinear coupled differential equations describing learning dynamics on weight space as a function of the statistical structure of the inputs and outputs. We find exact time-dependent solutions to these nonlinear equations, as well as find conserved quantities in the weight dynamics arising from symmetries in the error function. These solutions provide intuition into how a deep network successively builds up information about the statistical structure of the training data and embeds this information into its weights and internal representations. Moreover, we compare our analytical solutions of learning dynamics in deep linear networks to numerical simulations of learning dynamics in deep non-linear networks, and find that our analytical solutions provide a reasonable approximation. Our solutions also reflect nonlinear phenomena seen in simulations, including alternating plateaus and sharp periods of rapid improvement. Indeed, it has been shown previously [16] that this nonlinear learning dynamics in deep linear networks is sufficient to qualitatively capture aspects of the progressive, hierarchical differentiation of conceptual structure seen in infant development. Next, we apply these solutions to investigate the commonly used greedy layer-wise pretraining strategy for training deep networks [17, 18], and recover conditions under which such pretraining speeds learning. We show that these conditions are approximately satisfied for the MNIST dataset, and that unsupervised pretraining therefore confers an optimization advantage for deep linear networks applied to MNIST. Finally, we exhibit a new class of random orthogonal initial conditions on weights that, in linear networks, provide depth independent learning times, and we show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos. In this regime, synaptic gains are tuned so that linear amplification due to propagation of neural activity through weight matrices exactly balances dampening of activity due to saturating nonlinearities. In particular, we show that even in nonlinear networks, operating in this special regime, Jacobians that are involved in backpropagating error signals act like near isometries. 1 General learning dynamics of gradient descent W 21 W 32 x \u2208 R1 h \u2208 R2 y \u2208 R3 Figure 1: The three layer network analyzed in this section. We begin by analyzing learning in a three layer network (input, hidden, and output) with linear activation functions (Fig 1). We letNi be the number of neurons in layer i. The inputoutput map of the network is y = W W x. We wish to train the network to learn a particular input-output map from", "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks"}, "0122e063ca5f0f9fb9d144d44d41421503252010": {"paper_id": "0122e063ca5f0f9fb9d144d44d41421503252010", "abstract": "Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestlysized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm.", "title": "Large Scale Distributed Deep Networks"}, "53698b91709112e5bb71eeeae94607db2aefc57c": {"paper_id": "53698b91709112e5bb71eeeae94607db2aefc57c", "abstract": "We investigate architectures of discriminatively trained deep Convolutional Networks (ConvNets) for action recognition in video. The challenge is to capture the complementary information on appearance from still frames and motion between frames. We also aim to incorporate into the network design aspects of the best performing hand-crafted features. Our contribution is three-fold. First, we propose a two-stream ConvNet architecture which incorporates spatial and temporal networks. Second, we demonstrate that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data. Finally, we show that multitask learning, applied to two different action classification datasets, can be used to increase the amount of training data and improve the performance on both. Our architecture is trained and evaluated on the standard video actions benchmarks of UCF-101 and HMDB-51, where it matches the state of the art. It also exceeds by a large margin previous attempts to use deep nets for video classification.", "title": "Two-Stream Convolutional Networks for Action Recognition in Videos"}, "80d800dfadbe2e6c7b2367d9229cc82912d55889": {"paper_id": "80d800dfadbe2e6c7b2367d9229cc82912d55889", "abstract": "I present a new way to parallelize the training of convolutional neural networks across multiple GPUs. The method scales significantly better than all alternatives when applied to modern convolutional neural", "title": "One weird trick for parallelizing convolutional neural networks"}, "659fc2a483a97dafb8fb110d08369652bbb759f9": {"paper_id": "659fc2a483a97dafb8fb110d08369652bbb759f9", "abstract": "The Fisher kernel (FK) is a generic framework which combines the benefits of generative and discriminative approaches. In the context of image classification the FK was shown to extend the popular bag-of-visual-words (BOV) by going beyond count statistics. However, in practice, this enriched representation has not yet shown its superiority over the BOV. In the first part we show that with several well-motivated modifications over the original framework we can boost the accuracy of the FK. On PASCAL VOC 2007 we increase the Average Precision (AP) from 47.9% to 58.3%. Similarly, we demonstrate state-of-the-art accuracy on CalTech 256. A major advantage is that these results are obtained using only SIFT descriptors and costless linear classifiers. Equipped with this representation, we can now explore image classification on a larger scale. In the second part, as an application, we compare two abundant resources of labeled images to learn classifiers: ImageNet and Flickr groups. In an evaluation involving hundreds of thousands of training images we show that classifiers learned on Flickr groups perform surprisingly well (although they were not intended for this purpose) and that they can complement classifiers learned on more carefully annotated datasets.", "title": "Improving the Fisher Kernel for Large-Scale Image Classification"}, "2dc9b005e936c9c303386caacc8d41cabdb1a0a1": {"paper_id": "2dc9b005e936c9c303386caacc8d41cabdb1a0a1", "abstract": "The latest generation of Convolutional Neural Networks (CNN) have achieved impressive results in challenging benchmarks on image recognition and object detection, significantly raising the interest of the community in these methods. Nevertheless, it is still unclear how different CNN methods compare with each other and with previous state-of-the-art shallow representations such as the Bag-of-Visual-Words and the Improved Fisher Vector. This paper conducts a rigorous evaluation of these new techniques, exploring different deep architectures and comparing them on a common ground, identifying and disclosing important implementation details. We identify several useful properties of CNN-based representations, including the fact that the dimensionality of the CNN output layer can be reduced significantly without having an adverse effect on performance. We also identify aspects of deep and shallow methods that can be successfully shared. In particular, we show that the data augmentation techniques commonly applied to CNN-based methods can also be applied to shallow methods, and result in an analogous performance boost. Source code and models to reproduce the experiments in the paper is made publicly available.", "title": "Return of the Devil in the Details: Delving Deep into Convolutional Nets"}, "aa358f4a0578234e301a305d8c5de8d859083a4c": {"paper_id": "aa358f4a0578234e301a305d8c5de8d859083a4c", "abstract": "This paper presents a novel visual representation, called orderlets, for real-time human action recognition with depth sensors. An orderlet is a middle level feature that captures the ordinal pattern among a group of low level features. For skeletons, an orderlet captures specific spatial relationship among a group of joints. For a depth map, an orderlet characterizes a comparative relationship of the shape information among a group of subregions. The orderlet representation has two nice properties. First, it is insensitive to small noise since an orderlet only depends on the comparative relationship among individual features. Second, it is a frame-level representation thus suitable for real-time online action recognition. Experimental results demonstrate its superior performance on online action recognition and cross-environment action recognition.", "title": "Discriminative Orderlet Mining for Real-Time Recognition of Human-Object Interaction"}, "cc0d17d072c21de5886492c6a23642f15b7f7a25": {"paper_id": "cc0d17d072c21de5886492c6a23642f15b7f7a25", "abstract": "Human action recognition from RGB-D (Red, Green, Blue and Depth) data has attracted increasing attention since the first work reported in 2010. Over this period, many benchmark datasets have been created to facilitate the development and evaluation of new algorithms. This raises the question of which dataset to select and how to use it in providing a fair and objective comparative evaluation against state-of-the-art methods. To address this issue, this paper provides a comprehensive review of the most commonly used action recognition related RGB-D video datasets, including 27 single-view datasets, 10 multi-view datasets, and 7 multi-person datasets. The detailed information and analysis of these datasets is a useful resource in guiding insightful selection of datasets for future research. In addition, the issues with current algorithm evaluation vis-\u00e1-vis limitations of the available datasets and evaluation protocols are also highlighted; resulting in a number of recommendations for collection of new datasets and use of evaluation protocols.", "title": "RGB-D-based Action Recognition Datasets: A Survey"}, "a61f723e2f45fc859841382e9319a94fc64a994e": {"paper_id": "a61f723e2f45fc859841382e9319a94fc64a994e", "abstract": "Understanding human activities and object affordances are two very important skills, especially for personal robots which operate in human environments. In this work, we consider the problem of extracting a descriptive labeling of the sequence of sub-activities being performed by a human, and more importantly, of their interactions with the objects in the form of associated affordances. Given a RGB-D video, we jointly model the human activities and object affordances as a Markov random field where the nodes represent objects and sub-activities, and the edges represent the relationships between object affordances, their relations with sub-activities, and their evolution over time. We formulate the learning problem using a structural support vector machine (SSVM) approach, where labelings over various alternate temporal segmentations are considered as latent variables. We tested our method on a challenging dataset comprising 120 activity videos collected from 4 subjects, and obtained an accuracy of 79.4% for affordance, 63.4% for sub-activity and 75.0% for high-level activity labeling. We then demonstrate the use of such descriptive labeling in performing assistive tasks by a PR2 robot.", "title": "Learning Human Activities and Object Affordances from RGB-D Videos"}, "75cbc0eec23375df69de6c64e2f48689dde417c5": {"paper_id": "75cbc0eec23375df69de6c64e2f48689dde417c5", "abstract": "With the invention of the low-cost Microsoft Kinect sensor, high-resolution depth and visual (RGB) sensing has become available for widespread use. The complementary nature of the depth and visual information provided by the Kinect sensor opens up new opportunities to solve fundamental problems in computer vision. This paper presents a comprehensive review of recent Kinect-based computer vision algorithms and applications. The reviewed approaches are classified according to the type of vision problems that can be addressed or enhanced by means of the Kinect sensor. The covered topics include preprocessing, object tracking and recognition, human activity analysis, hand gesture analysis, and indoor 3-D mapping. For each category of methods, we outline their main algorithmic contributions and summarize their advantages/differences compared to their RGB counterparts. Finally, we give an overview of the challenges in this field and future research trends. This paper is expected to serve as a tutorial and source of references for Kinect-based computer vision researchers.", "title": "Enhanced Computer Vision With Microsoft Kinect Sensor: A Review"}, "30419576d2ed1a2d699683ac24a4b5ec5b93f093": {"paper_id": "30419576d2ed1a2d699683ac24a4b5ec5b93f093", "abstract": "We propose an algorithm which combines the discriminative information from depth images as well as from 3D joint positions to achieve high action recognition accuracy. To avoid the suppression of subtle discriminative information and also to handle local occlusions, we compute a vector of many independent local features. Each feature encodes spatiotemporal variations of depth and depth gradients at a specific space-time location in the action volume. Moreover, we encode the dominant skeleton movements by computing a local 3D joint position difference histogram. For each joint, we compute a 3D space-time motion volume which we use as an importance indicator and incorporate in the feature vector for improved action discrimination. To retain only the discriminant features, we train a random decision forest (RDF). The proposed algorithm is evaluated on three standard datasets and compared with nine state-of-the-art algorithms. Experimental results show that, on the average, the proposed algorithm outperform all other algorithms in accuracy and have a processing speed of over 112 frames/second.", "title": "Real time action recognition using histograms of depth gradients and random decision forests"}, "27937119d3f679e492876cd5843602af7456a475": {"paper_id": "27937119d3f679e492876cd5843602af7456a475", "abstract": "A simple approach to learning invariances in image classification consists in augmenting the training set with transformed versions of the original images. However, given a large set of possible transformations, selecting a compact subset is challenging. Indeed, all transformations are not equally informative and adding uninformative transformations increases training time with no gain in accuracy. We propose a principled algorithm -- Image Transformation Pursuit (ITP) -- for the automatic selection of a compact set of transformations. ITP works in a greedy fashion, by selecting at each iteration the one that yields the highest accuracy gain. ITP also allows to efficiently explore complex transformations, that combine basic transformations. We report results on two public benchmarks: the CUB dataset of bird images and the ImageNet 2010 challenge. Using Fisher Vector representations, we achieve an improvement from 28.2% to 45.2% in top-1 accuracy on CUB, and an improvement from 70.1% to 74.9% in top-5 accuracy on ImageNet. We also show significant improvements for deep convnet features: from 47.3% to 55.4% on CUB and from 77.9% to 81.4% on ImageNet.", "title": "Transformation Pursuit for Image Classification"}, "783bb44092f45573a9fbf47d023786105dde1d83": {"paper_id": "783bb44092f45573a9fbf47d023786105dde1d83", "abstract": "In recent years, heatmap regression based models have shown their effectiveness in face alignment and pose estimation. However, Conventional Heatmap Regression (CHR) is not accurate nor stable when dealing with high-resolution facial videos, since it finds the maximum activated location in heatmaps which are generated from rounding coordinates, and thus leads to quantization errors when scaling back to the original high-resolution space. In this paper, we propose a Fractional Heatmap Regression (FHR) for high-resolution video-based face alignment. The proposed FHR can accurately estimate the fractional part according to the 2D Gaussian function by sampling three points in heatmaps. To further stabilize the landmarks among continuous video frames while maintaining the precise at the same time, we propose a novel stabilization loss that contains two terms to address time delay and non-smooth issues, respectively. Experiments on 300W, 300VW and Talking Face datasets clearly demonstrate that the proposed method is more accurate and stable than the state-ofthe-art models. Introduction Face alignment aims to estimate a set of facial landmarks given a face image or video sequence. It is a classic computer vision problem that has attributed to many advanced machine learning algorithms Fan et al. (2018); Bulat and Tzimiropoulos (2017); Trigeorgis et al. (2016); Peng et al. (2015, 2016); Kowalski, Naruniec, and Trzcinski (2017); Chen et al. (2017); Liu et al. (2017); Hu et al. (2018). Nowadays, with the rapid development of consumer hardwares (e.g., mobile phones, digital cameras), High-Resolution (HR) video sequences can be easily collected. Estimating facial landmarks on such highresolution facial data has tremendous applications, e.g., face makeup Chen, Shen, and Jia (2017), editing with special effects Korshunova et al. (2017) in live broadcast videos. However, most existing face alinement methods work on faces with medium image resolutions Chen et al. (2017); Bulat and Tzimiropoulos (2017); Peng et al. (2016); Liu et al. (2017). Therefore, developing face alignment algorithms for high-resolution videos is at the core of this paper. To this end, we propose an accurate and stable algorithm for high-resolution video-based face alignment, named Fractional Heatmap Regression (FHR). It is well known that \u2217 indicates equal contributions. Conventional Heatmap Regression (CHR) Loss Fractional Heatmap Regression (FHR) Loss 930 744 411", "title": "Towards Highly Accurate and Stable Face Alignment for High-Resolution Videos"}, "7a061e7eab865fc8d2ef00e029b7070719ad2e9a": {"paper_id": "7a061e7eab865fc8d2ef00e029b7070719ad2e9a", "abstract": "We present an extensive three year study on economically annotating video with crowdsourced marketplaces. Our public framework has annotated thousands of real world videos, including massive data sets unprecedented for their size, complexity, and cost. To accomplish this, we designed a state-of-the-art video annotation user interface and demonstrate that, despite common intuition, many contemporary interfaces are sub-optimal. We present several user studies that evaluate different aspects of our system and demonstrate that minimizing the cognitive load of the user is crucial when designing an annotation platform. We then deploy this interface on Amazon Mechanical Turk and discover expert and talented workers who are capable of annotating difficult videos with dense and closely cropped labels. We argue that video annotation requires specialized skill; most workers are poor annotators, mandating robust quality control protocols. We show that traditional crowdsourced micro-tasks are not suitable for video annotation and instead demonstrate that deploying time-consuming macro-tasks on MTurk is effective. Finally, we show that by extracting pixel-based features from manually labeled key frames, we are able to leverage more sophisticated interpolation strategies to maximize performance given a fixed budget. We validate the power of our framework on difficult, real-world data sets and we demonstrate an inherent trade-off between the mix of human and cloud computing used vs. the accuracy and cost of the labeling. We further introduce a novel, cost-based evaluation criteria that compares vision algorithms by the budget required to achieve an acceptable performance. We hope our findings will spur innovation in the creation of massive labeled video data sets and enable novel data-driven computer vision applications.", "title": "Efficiently Scaling up Crowdsourced Video Annotation"}, "080ce01c304d3fd562c9aa17d1b234d5fc4b4555": {"paper_id": "080ce01c304d3fd562c9aa17d1b234d5fc4b4555", "abstract": "While research on articulated human motion and pose estimation has progressed rapidly in the last few years, there has been no systematic quantitative evaluation of competing methods to establish the current state of the art. We present data obtained using a hardware system that is able to capture synchronized video and ground-truth 3D motion. The resulting HumanEva datasets contain multiple subjects performing a set of predefined actions with a number of repetitions. On the order of 40,000 frames of synchronized motion capture and multi-view video (resulting in over one quarter million image frames in total) were collected at 60\u00a0Hz with an additional 37,000 time instants of pure motion capture data. A standard set of error measures is defined for evaluating both 2D and 3D pose estimation and tracking algorithms. We also describe a baseline algorithm for 3D articulated tracking that uses a relatively standard Bayesian framework with optimization in the form of Sequential Importance Resampling and Annealed Particle Filtering. In the context of this baseline algorithm we explore a variety of likelihood functions, prior models of human motion and the effects of algorithm parameters. Our experiments suggest that image observation models and motion priors play important roles in performance, and that in a multi-view laboratory environment, where initialization is available, Bayesian filtering tends to perform well. The datasets and the software are made available to the research community. This infrastructure will support the development of new articulated motion and pose estimation algorithms, will provide a baseline for the evaluation and comparison of new methods, and will help establish the current state of the art in human pose estimation and tracking.", "title": "HumanEva: Synchronized Video and Motion Capture Dataset and Baseline Algorithm for Evaluation of Articulated Human\u00a0Motion"}, "ce2fd92335e904c4111381a8a1afb9bb7a484d1e": {"paper_id": "ce2fd92335e904c4111381a8a1afb9bb7a484d1e", "abstract": "We describe our experience with collecting roughly 250, 000 image annotations on Amazon Mechanical Turk (AMT). The annotations we collected range from location of keypoints and figure ground masks of various object categories, 3D pose estimates of head and torsos of people in images and attributes like gender, race, type of hair, etc. We describe the setup and strategies we adopted to automatically approve and reject the annotations, which becomes important for large scale annotations. These annotations were used to train algorithms for detection, segmentation, pose estimation, action recognition and attribute recognition of people in images.", "title": "Large Scale Image Annotations on Amazon Mechanical Turk"}, "669b9fd79eb39f712527ee616e35e50eea7fd2fa": {"paper_id": "669b9fd79eb39f712527ee616e35e50eea7fd2fa", "abstract": "In this work, we address the problem of estimating 2d human pose from still images. Recent methods that rely on discriminatively trained deformable parts organized in a tree model have shown to be very successful in solving this task. Within such a pictorial structure framework, we address the problem of obtaining good part templates by proposing novel, non-linear joint regressors. In particular, we employ two-layered random forests as joint regressors. The first layer acts as a discriminative, independent body part classifier. The second layer takes the estimated class distributions of the first one into account and is thereby able to predict joint locations by modeling the interdependence and co-occurrence of the parts. This results in a pose estimation framework that takes dependencies between body parts already for joint localization into account and is thus able to circumvent typical ambiguities of tree structures, such as for legs and arms. In the experiments, we demonstrate that our body parts dependent joint regressors achieve a higher joint localization accuracy than tree-based state-of-the-art methods.", "title": "Human Pose Estimation Using Body Parts Dependent Joint Regressors"}, "148a5fa66480afa7744409cde659f79c7c9b3fdc": {"paper_id": "148a5fa66480afa7744409cde659f79c7c9b3fdc", "abstract": "The objective of this paper is to estimate 2D human pose as a spatial configuration of body parts in TV and movie video shots. Such video material is uncontrolled and extremely challenging. We propose an approach that progressively reduces the search space for body parts, to greatly improve the chances that pose estimation will succeed. This involves two contributions: (i) a generic detector using a weak model of pose to substantially reduce the full pose search space; and (ii) employing 'grabcut' initialized on detected regions proposed by the weak model, to further prune the search space. Moreover, we also propose (Hi) an integrated spatio- temporal model covering multiple frames to refine pose estimates from individual frames, with inference using belief propagation. The method is fully automatic and self-initializing, and explains the spatio-temporal volume covered by a person moving in a shot, by soft-labeling every pixel as belonging to a particular body part or to the background. We demonstrate upper-body pose estimation by an extensive evaluation over 70000 frames from four episodes of the TV series Buffy the vampire slayer, and present an application to full- body action recognition on the Weizmann dataset.", "title": "Progressive search space reduction for human pose estimation"}, "270f029b03ee1bdfeae4ff4c5167b450d185a981": {"paper_id": "270f029b03ee1bdfeae4ff4c5167b450d185a981", "abstract": "We propose a new learning-based method for estimating 2D human pose from a single image, using Dual-Source Deep Convolutional Neural Networks (DS-CNN). Recently, many methods have been developed to estimate human pose by using pose priors that are estimated from physiologically inspired graphical models or learned from a holistic perspective. In this paper, we propose to integrate both the local (body) part appearance and the holistic view of each local part for more accurate human pose estimation. Specifically, the proposed DS-CNN takes a set of image patches (category-independent object proposals for training and multi-scale sliding windows for testing) as the input and then learns the appearance of each local part by considering their holistic views in the full body. Using DS-CNN, we achieve both joint detection, which determines whether an image patch contains a body joint, and joint localization, which finds the exact location of the joint in the image patch. Finally, we develop an algorithm to combine these joint detection/localization results from all the image patches for estimating the human pose. The experimental results show the effectiveness of the proposed method by comparing to the state-of-the-art human-pose estimation methods based on pose priors that are estimated from physiologically inspired graphical models or learned from a holistic perspective.", "title": "Combining local appearance and holistic view: Dual-Source Deep Neural Networks for human pose estimation"}, "1550caab8d12c3f0ea19faaaa6bab3bdd092bafd": {"paper_id": "1550caab8d12c3f0ea19faaaa6bab3bdd092bafd", "abstract": "The goal of the scene labeling task is to assign a class label to each pixel in an image. To ensure a good visual coherence and a high class accuracy, it is essential for a model to capture long range (pixel) label dependencies in images. In a feed-forward architecture, this can be achieved simply by considering a sufficiently large input context patch, around each pixel to be labeled. We propose an approach that consists of a recurrent convolutional neural network which allows us to consider a large input context while limiting the capacity of the model. Contrary to most standard approaches, our method does not rely on any segmentation technique nor any taskspecific features. The system is trained in an end-to-end manner over raw pixels, and models complex spatial dependencies with low inference cost. As the context size increases with the built-in recurrence, the system identifies and corrects its own errors. Our approach yields state-ofthe-art performance on both the Stanford Background Dataset and the SIFT Flow Dataset, while remaining very fast at test time.", "title": "Recurrent Convolutional Neural Networks for Scene Labeling"}, "0f701da6af825ea9666a49da8127184ce74cfea8": {"paper_id": "0f701da6af825ea9666a49da8127184ce74cfea8", "abstract": "Contour detection has been a fundamental component in many image segmentation and object detection systems. Most previous work utilizes low-level features such as texture or saliency to detect contours and then use them as cues for a higher-level task such as object detection. However, we claim that recognizing objects and predicting contours are two mutually related tasks. Contrary to traditional approaches, we show that we can invert the commonly established pipeline: instead of detecting contours with low-level cues for a higher-level recognition task, we exploit object-related features as high-level cues for contour detection.", "title": "DeepEdge: A multi-scale bifurcated deep network for top-down contour detection"}, "07f77ad9c58b21588a9c6fa79ca7917dd58cca98": {"paper_id": "07f77ad9c58b21588a9c6fa79ca7917dd58cca98", "abstract": "In this paper we address three different computer vision tasks using a single basic architecture: depth prediction, surface normal estimation, and semantic labeling. We use a multiscale convolutional network that is able to adapt easily to each task using only small modifications, regressing from the input image to the output map directly. Our method progressively refines predictions using a sequence of scales, and captures many image details without any superpixels or low-level segmentation. We achieve state-of-the-art performance on benchmarks for all three tasks.", "title": "Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-scale Convolutional Architecture"}, "575d6a05bb27316ad677f19e79473e314e6c6f94": {"paper_id": "575d6a05bb27316ad677f19e79473e314e6c6f94", "abstract": "We present a novel architecture, the \u201cstacked what-where auto-encoders\u201d (SWWAE), which integrates discriminative and generative pathways and provides a unified approach to supervised, semi-supervised and unsupervised learning without relying on sampling during training. An instantiation of SWWAE uses a convolutional net (Convnet) (LeCun et al. (1998)) to encode the input, and employs a deconvolutional net (Deconvnet) (Zeiler et al. (2010)) to produce the reconstruction. The objective function includes reconstruction terms that induce the hidden states in the Deconvnet to be similar to those of the Convnet. Each pooling layer produces two sets of variables: the \u201cwhat\u201d which are fed to the next layer, and its complementary variable \u201cwhere\u201d that are fed to the corresponding layer in the generative decoder.", "title": "Stacked What-Where Auto-encoders"}, "1824b1ccace464ba275ccc86619feaa89018c0ad": {"paper_id": "1824b1ccace464ba275ccc86619feaa89018c0ad", "abstract": "This paper addresses the problem of Face Alignment for a single image. We show how an ensemble of regression trees can be used to estimate the face's landmark positions directly from a sparse subset of pixel intensities, achieving super-realtime performance with high quality predictions. We present a general framework based on gradient boosting for learning an ensemble of regression trees that optimizes the sum of square error loss and naturally handles missing or partially labelled data. We show how using appropriate priors exploiting the structure of image data helps with efficient feature selection. Different regularization strategies and its importance to combat overfitting are also investigated. In addition, we analyse the effect of the quantity of training data on the accuracy of the predictions and explore the effect of data augmentation using synthesized data.", "title": "One millisecond face alignment with an ensemble of regression trees"}, "0f05650e3274f8b9c9bcce20ca63ee45c1c7a344": {"paper_id": "0f05650e3274f8b9c9bcce20ca63ee45c1c7a344", "abstract": "This paper offers the first variational approach to the problem of dense 3D reconstruction of non-rigid surfaces from a monocular video sequence. We formulate non-rigid structure from motion (nrsfm) as a global variational energy minimization problem to estimate dense low-rank smooth 3D shapes for every frame along with the camera motion matrices, given dense 2D correspondences. Unlike traditional factorization based approaches to nrsfm, which model the low-rank non-rigid shape using a fixed number of basis shapes and corresponding coefficients, we minimize the rank of the matrix of time-varying shapes directly via trace norm minimization. In conjunction with this low-rank constraint, we use an edge preserving total-variation regularization term to obtain spatially smooth shapes for every frame. Thanks to proximal splitting techniques the optimization problem can be decomposed into many point-wise sub-problems and simple linear systems which can be easily solved on GPU hardware. We show results on real sequences of different objects (face, torso, beating heart) where, despite challenges in tracking, illumination changes and occlusions, our method reconstructs highly deforming smooth surfaces densely and accurately directly from video, without the need for any prior models or shape templates.", "title": "Dense Variational Reconstruction of Non-rigid Surfaces from Monocular Video"}, "2ea6a93199c9227fa0c1c7de13725f918c9be3a4": {"paper_id": "2ea6a93199c9227fa0c1c7de13725f918c9be3a4", "abstract": "There are many excellent toolkits which provide support for developing machine learning software in Python, R, Matlab, and similar environments. Dlib-m l is an open source library, targeted at both engineers and research scientists, which aims to pro vide a similarly rich environment for developing machine learning software in the C++ language. T owards this end, dlib-ml contains an extensible linear algebra toolkit with built in BLAS supp ort. It also houses implementations of algorithms for performing inference in Bayesian networks a nd kernel-based methods for classification, regression, clustering, anomaly detection, and fe atur ranking. To enable easy use of these tools, the entire library has been developed with contract p rogramming, which provides complete and precise documentation as well as powerful debugging too ls.", "title": "Dlib-ml: A Machine Learning Toolkit"}, "80bcdd5d82f03e4d2bca28cbc1399424dac138f9": {"paper_id": "80bcdd5d82f03e4d2bca28cbc1399424dac138f9", "abstract": "VLFeat is an open and portable library of computer vision algorithms. It aims at facilitating fast prototyping and reproducible research for computer vision scientists and students. It includes rigorous implementations of common building blocks such as feature detectors, feature extractors, (hierarchical) k-means clustering, randomized kd-tree matching, and super-pixelization. The source code and interfaces are fully documented. The library integrates directly with MATLAB, a popular language for computer vision research.", "title": "Vlfeat: an open and portable library of computer vision algorithms"}, "404abe4a6b47cb210512b7ba10c155dda6331585": {"paper_id": "404abe4a6b47cb210512b7ba10c155dda6331585", "abstract": "Monocytes are circulating blood leukocytes that play important roles in the inflammatory response, which is essential for the innate response to pathogens. But inflammation and monocytes are also involved in the pathogenesis of inflammatory diseases, including atherosclerosis. In adult mice, monocytes originate in the bone marrow in a Csf-1R (MCSF-R, CD115)-dependent manner from a hematopoietic precursor common for monocytes and several subsets of macrophages and dendritic cells (DCs). Monocyte heterogeneity has long been recognized, but in recent years investigators have identified three functional subsets of human monocytes and two subsets of mouse monocytes that exert specific roles in homeostasis and inflammation in vivo, reminiscent of those of the previously described classically and alternatively activated macrophages. Functional characterization of monocytes is in progress in humans and rodents and will provide a better understanding of the pathophysiology of inflammation.", "title": "Blood monocytes: development, heterogeneity, and relationship with dendritic cells."}, "f71defe250d1444195ff3f237a3cc23b7bf01917": {"paper_id": "f71defe250d1444195ff3f237a3cc23b7bf01917", "abstract": "Ghrelin receptors are expressed by key components of the arousal system. Exogenous ghrelin induces behavioral activation, promotes wakefulness and stimulates eating. We hypothesized that ghrelin-sensitive mechanisms play a role in the arousal system. To test this, we investigated the responsiveness of ghrelin receptor knockout (KO) mice to two natural wake-promoting stimuli. Additionally, we assessed the integrity of their homeostatic sleep-promoting system using sleep deprivation. There was no significant difference in the spontaneous sleep-wake activity between ghrelin receptor KO and wild-type (WT) mice. WT mice mounted robust arousal responses to a novel environment and food deprivation. Wakefulness increased for 6 h after cage change accompanied by increases in body temperature and locomotor activity. Ghrelin receptor KO mice completely lacked the wake and body temperature responses to new environment. When subjected to 48 h food deprivation, WT mice showed marked increases in their waking time during the dark periods of both days. Ghrelin receptor KO mice failed to mount an arousal response on the first night and wake increases were attenuated on the second day. The responsiveness to sleep deprivation did not differ between the two genotypes. These results indicate that the ghrelin-receptive mechanisms play an essential role in the function of the arousal system but not in homeostatic sleep-promoting mechanisms.", "title": "Impaired wake-promoting mechanisms in ghrelin receptor-deficient mice."}, "1ee5f9218b05ce6f55ca8660ba4eb1b093f30d19": {"paper_id": "1ee5f9218b05ce6f55ca8660ba4eb1b093f30d19", "abstract": "Although periodontitis is a chronic inflammatory disease but some factors of acute inflammation phase are involved in this disease among which is the C-Reactive protein (CRP). To minimize its effects, anti-inflammatory drugs or non-pharmacological approaches such as oral hygiene is recommended. CRP can also be used for the prediction and early detection of periodontal disease. The aim of the present study was the comparison of the amount of salivary C-Reactive protein (CRP) in healthy subjects and patients with periodontal disease. This case-control study was done on 90 patients referred to the Department of Periodontology of Babol Dentistry School. These subjects were divided into three groups of healthy (n = 30), gingivitis (n = 30), and chronic periodontitis (n = 30), based on Gingival Index (GI) and Clinical Attachment Loss (CAL) indices. 2ml saliva samples were collected from these people and clinical indicators including GI, CAL, Periodontal Pocket Depth (PPD), and Bleeding Index (BI) were assessed. ELISA method was used to evaluate the salivary CRP levels. Collected data were analyzed using SPSS statistical software by non-Parametric Kruskal-Wallis and Mann-Whitney test and Spearman correlation coefficient and P<0.05 was considered significant. The mean salivary CRP levels were 5332.62\u00b15051.63pg/ml in periodontitis patients, 3545.41\u00b13061.38pg/ml in gingivitis group and 3108.51\u00b13574.47pg/ml in healthy subjects. The statistic analysis showed a significant difference in salivary CRP concentrations between the periodontitis patients and healthy subjects (P=0.045). The results indicate that there is a significant association between periodontitis and salivary CRP concentrations.", "title": "C - Reactive Protein Levels in Patients with Periodontal Disease and Normal Subjects"}, "43471090c5b8db10905addbdaede49ebc51b8515": {"paper_id": "43471090c5b8db10905addbdaede49ebc51b8515", "abstract": "Recommender systems apply knowledge discovery techniques to the problem of making personalized product recommendations during a live customer interaction. These systems, especially the k-nearest neighbor collaborative filtering based ones, are achieving widespread success in E-commerce nowadays. The tremendous growth of customers and products in recent years poses some key challenges for recommender systems. These are: producing high quality recommendations and performing many recommendations per second for millions of customers and products. New recommender system technologies are needed that can quickly produce high quality recommendations, even for very large-scale problems. We address the performance issues by scaling up the neighborhood formation process through the use of clustering techniques.", "title": "Recommender Systems for Large-scale E-Commerce : Scalable Neighborhood Formation Using Clustering"}, "e6f4625a3b2c0999174cdfb23d131e022f8545a0": {"paper_id": "e6f4625a3b2c0999174cdfb23d131e022f8545a0", "abstract": "Predicting items a user would like on the basis of other users\u2019 ratings for these items has become a well-established strategy adopted by many recommendation services on the Internet. Although this can be seen as a classification problem, algorithms proposed thus far do not draw on results from the machine learning literature. We propose a representation for collaborative filtering tasks that allows the application of virtually any machine learning algorithm. We identify the shortcomings of current collaborative filtering techniques and propose the use of learning algorithms paired with feature extraction techniques that specifically address the limitations of previous approaches. Our best-performing algorithm is based on the singular value decomposition of an initial matrix of user ratings, exploiting latent structure that essentially eliminates the need for users to rate common items in order to become predictors for one another's preferences. We evaluate the proposed algorithm on a large database of user ratings for motion pictures and find that our approach significantly outperforms current collaborative filtering algorithms.", "title": "Learning Collaborative Information Filters"}, "c864175780adde19099108de66b4636f4c87d44c": {"paper_id": "c864175780adde19099108de66b4636f4c87d44c", "abstract": "When making a choice in the absence of decisive first-hand knowledge, choosing as other like-minded, similarly-situated people have successfully chosen in the past is a good strategy \u2014 in effect, using other people as filters and guides: filters to strain out potentially bad choices and guides to point out potentially good choices. Current human-computer interfaces largely ignore the power of the social strategy. For most choices within an interface, new users are left to fend for themselves and if necessary, to pursue help outside of the interface. We present a general history-of-use method that automates a social method for informing choice and report on how it fares in the context of a fielded test case: the selection of videos from a large set. The positive results show that communal history-of-use data can serve as a powerful resource for use in interfaces.", "title": "Recommending and Evaluating Choices in a Virtual Community of Use"}, "0265769b0fbf86bb0e700573c80e388bb54c3f7a": {"paper_id": "0265769b0fbf86bb0e700573c80e388bb54c3f7a", "abstract": "Currently most approaches to retrieving textual materials from scienti c databases depend on a lexical match between words in users requests and those in or assigned to documents in a database Because of the tremendous diversity in the words people use to describe the same document lexical methods are necessarily incomplete and imprecise Using the singular value decomposition SVD one can take advantage of the implicit higher order structure in the association of terms with documents by determining the SVD of large sparse term by document matrices Terms and documents represented by of the largest singular vectors are then matched against user queries We call this retrieval method Latent Semantic Indexing LSI because the subspace represents important associative relationships between terms and documents that are not evident in individual documents LSI is a completely automatic yet intelligent indexing method widely applicable and a promising way to improve users access to many kinds of textual materials or to documents and services for which textual descriptions are available A survey of the computational requirements for managing LSI encoded databases as well as current and future applications of LSI is presented", "title": "Using Linear Algebra for Intelligent Information Retrieval"}, "3c80a9cc4f9a5f5ea0b458cde677ff8f7b28e797": {"paper_id": "3c80a9cc4f9a5f5ea0b458cde677ff8f7b28e797", "abstract": "We investigate the use of dimensionality reduction to improve performance for a new class of data analysis software called \u201crecommender systems\u201d. Recommender systems apply knowledge discovery techniques to the problem of making product recommendations during a live customer interaction. These systems are achieving widespread success in E-commerce nowadays, especially with the advent of the Internet. The tremendous growth of customers and products poses three key challenges for recommender systems in the E-commerce domain. These are: producing high quality recommendations, performing many recommendations per second for millions of customers and products, and achieving high coverage in the face of data sparsity. One successful recommender system technology is collaborative filtering , which works by matching customer preferences to other customers in making recommendations. Collaborative filtering has been shown to produce high quality recommendations, but the performance degrades with the number of customers and products. New recommender system technologies are needed that can quickly produce high quality recommendations, even for very largescale problems. This paper presents two different experiments where we have explored one technology called Singular Value Decomposition (SVD) to reduce the dimensionality of recommender system databases. Each experiment compares the quality of a recommender system using SVD with the quality of a recommender system using collaborative filtering. The first experiment compares the effectiveness of the two recommender systems at predicting consumer preferences based on a database of explicit ratings of products. The second experiment compares the effectiveness of the two recommender systems at producing Top-N lists based on a real-life customer purchase database from an E-Commerce site. Our experience suggests that SVD has the potential to meet many of the challenges of recommender systems, under certain conditions.", "title": "Application of Dimensionality Reduction in Recommender System - A Case Study"}, "399327022d64bc23ebe3d54d2b11f880a373be27": {"paper_id": "399327022d64bc23ebe3d54d2b11f880a373be27", "abstract": null, "title": "Algorithms for Clustering Data"}, "bc34c28ee40356b4d7bbe7be7d173a2436a89688": {"paper_id": "bc34c28ee40356b4d7bbe7be7d173a2436a89688", "abstract": "Clustering is a powerful technique for large-scale topic discovery from text. It involves two phases: first, feature extraction maps each document or record to a point in high-dimensional space, then clustering algorithms automatically group the points into a hierarchy of clusters. We describe an unsupervised, near-linear time text clustering system that offers a number of algorithm choices for each phase. We introduce a methodology for measuring the quality of a cluster hierarchy in terms of FMeasure, and present the results of experiments comparing different algorithms. The evaluation considers some feature selection parameters (tfidfand feature vector length) but focuses on the clustering algorithms, namely techniques from Scatter/Gather (buckshot, fractionation, and split/join) and kmeans. Our experiments suggest that continuous center adjustment contributes more to cluster quality than seed selection does. It follows that using a simpler seed selection algorithm gives a better time/quality tradeoff. We describe a refinement to center adjustment, \u201cvector average damping,\u201d that further improves cluster quality. We also compare the near-linear time algorithms to a group average greedy agglomerative clustering algorithm to demonstrate the time/quality tradeoff quantitatively.", "title": "Fast and Effective Text Mining Using Linear-Time Document Clustering"}, "49af3e80343eb80c61e727ae0c27541628c7c5e2": {"paper_id": "49af3e80343eb80c61e727ae0c27541628c7c5e2", "abstract": "Come with us to read a new book that is coming recently. Yeah, this is a new coming book that many people really want to read will you be one of them? Of course, you should be. It will not make you feel so hard to enjoy your life. Even some people think that reading is a hard to do, you must be sure that you can do it. Hard will be felt when you have no ideas about what kind of book to read. Or sometimes, your reading material is not interesting enough.", "title": "Introduction to Modern Information Retrieval"}, "45e72081659db8ee7b86623ad0af96db4c43a6da": {"paper_id": "45e72081659db8ee7b86623ad0af96db4c43a6da", "abstract": "Today\u2019s environments of increasing business change require software development methodologies that are more adaptable. This article examines how complex adaptive systems (CAS) theory can be used to increase our understanding of how agile software development practices can be used to develop this capability. A mapping of agile practices to CAS principles and three dimensions (product, process, and people) results in several recommendations for \u201cbest practices\u201d in systems development.", "title": "Agile Software Development: Adaptive Systems Principles and Best Practices"}, "25bc573a4c8b9ea9314f82797bbfafb2ffbd2d3a": {"paper_id": "25bc573a4c8b9ea9314f82797bbfafb2ffbd2d3a", "abstract": "F aced with the conflicting pressures of accelerated product development and users who demand that increasingly vital systems be made ever more dependable, software development has been thrown into turmoil. Traditionalists advocate using extensive planning, codified processes, and rigorous reuse to make development an efficient and predictable activity that gradually matures toward perfection. Meanwhile, a new generation of developers cites the crushing weight of corporate bureaucracy, the rapid pace of information technology change, and the dehumanizing effects of detailed plan-driven development as cause for revolution. In their rallying cry, the Manifesto for Agile Software Development (http://www.agileAlliance.org), and in columns such as those the \u201cOngoing Debate\u201d sidebar describes, these developers call for a revitalized approach to development that dispenses with all but the essentials. Unsurprisingly, many developers who favor plan-driven methods have reacted to the manifesto with scathing criticism. Real-world examples argue for and against agile methods. Responding to change has been cited as the critical technical success factor in the Internet browser battle between Microsoft and Netscape. But overresponding to change has been cited as the source of many software disasters, such as the $3 billion overrun of the US Federal Aviation Administration\u2019s Advanced Automation System for national air traffic control. I believe that both agile and plan-driven approaches have a responsible center and overinterpreting radical fringes. Although each approach has a home ground of project characteristics within which it performs very well, and much better than the other, outside each approach\u2019s home ground, a combined approach is feasible and preferable.", "title": "Get Ready for Agile Methods, with Care"}, "286d368d1e5b0b0847de3b107b7b589c0a37fb89": {"paper_id": "286d368d1e5b0b0847de3b107b7b589c0a37fb89", "abstract": "0 7 4 0 7 4 5 9 / 0 0 / $ 1 0 . 0 0 \u00a9 2 0 0 0 I E E E J u l y / A u g u s t 2 0 0 0 I E E E S O F T W A R E 19 design, algorithm, code, or test\u2014does indeed improve software quality and reduce time to market. Additionally, student and professional programmers consistently find pair programming more enjoyable than working alone. Yet most who have not tried and tested pair programming reject the idea as a redundant, wasteful use of programming resources: \u201cWhy would I put two people on a job that just one can do? I can\u2019t afford to do that!\u201d But we have found, as Larry Constantine wrote, that \u201cTwo programmers in tandem is not redundancy; it\u2019s a direct route to greater efficiency and better quality.\u201d1 Our supportive evidence comes from professional programmers and from advanced undergraduate students who participated in a structured experiment. The experimental results show that programming pairs develop better code faster with only a minimal increase in prerelease programmer hours. These results apply to all levels of programming skill from novice to expert.", "title": "Strengthening the Case for Pair Programming"}, "bafa32b0928974c2d45b0cd0beedad52db30061c": {"paper_id": "bafa32b0928974c2d45b0cd0beedad52db30061c", "abstract": "Time series clustering has become an important topic, particularly for similarity search amongst long time series such as those arising in bioinformatics. Unfortunately, existing methods for time series clustering that rely on the actual time series point values can become impractical since the methods do not scale well for longer time series, and many clustering algorithms do not easily handle high dimensional data. In this paper we propose a scalable method for time series clustering that replaces the time series point values with some global measures of the characteristics of the time series. These global measures are then clustered using a selforganising map, which performs additional dimension reduction. The proposed approach has been tested using some benchmark time series previously reported for time series clustering, and is shown to yield useful and robust clustering. The resulting clusters are similar to those produced by other methods, with some interesting variations that can be intuitively explained with knowledge of the global characteristics of the time series.", "title": "A Scalable Method for Time Series Clustering"}, "3b8a4cc6bb32b50b29943ceb7248f318e589cd79": {"paper_id": "3b8a4cc6bb32b50b29943ceb7248f318e589cd79", "abstract": "We present an efficient indexing method to locate 1-dimensional subsequences within a collection of sequences, such that the subsequences match a given (query) pattern within a specified tolerance. The idea is to map each data sequences into a small set of multidimensional rectangles in feature space. Then, these rectangles can be readily indexed using traditional spatial access methods, like the R*-tree [9]. In more detail, we use a sliding window over the data sequence and extract its features; the result is a trail in feature space. We propose an efficient and effective algorithm to divide such trails into sub-trails, which are subsequently represented by their Minimum Bounding Rectangles (MBRs). We also examine queries of varying lengths, and we show how to handle each case efficiently. We implemented our method and carried out experiments on synthetic and real data (stock price movements). We compared the method to sequential scanning, which is the only obvious competitor. The results were excellent: our method accelerated the search time from 3 times up to 100 times.", "title": "Fast Subsequence Matching in Time-Series Databases"}, "21932f51366bfb4f440133dd02d96451ad0a0a38": {"paper_id": "21932f51366bfb4f440133dd02d96451ad0a0a38", "abstract": "Libraries and other institutions are interested in providing access to scanned versions of their large collections of handwritten historical manuscripts on electronic media. Convenient access to a collection requires an index, which is manually created at great labour and expense. Since current handwriting recognizers do not perform well on historical documents, a technique called word spotting has been developed: clusters with occurrences of the same word in a collection are established using image matching. By annotating \u201cinteresting\u201d clusters, an index can be built automatically. We present an algorithm for matching handwritten words in noisy historical documents. The segmented word images are preprocessed to create sets of 1-dimensional features, which are then compared using dynamic time warping. We present experimental results on two different data sets from the George Washington collection. Our experiments show that this algorithm performs better and is faster than competing matching techniques.", "title": "Word Image Matching Using Dynamic Time Warping"}, "2ef606258486d6c32fd0b9ca54244273c21331b9": {"paper_id": "2ef606258486d6c32fd0b9ca54244273c21331b9", "abstract": "The problem of similarity search in large time series databases has attracted much attention recently. It is a non-trivial problem because of the inherent high dimensionality of the data. The most promising solutions involve first performing dimensionality reduction on the data, and then indexing the reduced data with a spatial access method. Three major dimensionality reduction techniques have been proposed: Singular Value Decomposition (SVD), the Discrete Fourier transform (DFT), and more recently the Discrete Wavelet Transform (DWT). In this work we introduce a new dimensionality reduction technique which we call Piecewise Aggregate Approximation (PAA). We theoretically and empirically compare it to the other techniques and demonstrate its superiority. In addition to being competitive with or faster than the other methods, our approach has numerous other advantages. It is simple to understand and to implement, it allows more flexible distance measures, including weighted Euclidean queries, and the index can be built in linear time.", "title": "Dimensionality Reduction for Fast Similarity Search in Large Time Series Databases"}, "a590a79fb5fabcf7faa451be16cc38abf89bcd35": {"paper_id": "a590a79fb5fabcf7faa451be16cc38abf89bcd35", "abstract": "Malaria remains the leading communicable disease in Ethiopia, with around one million clinical cases of malaria reported annually. The country currently has plans for elimination for specific geographic areas of the country. Human movement may lead to the maintenance of reservoirs of infection, complicating attempts to eliminate malaria. An unmatched case\u2013control study was conducted with 560 adult patients at a Health Centre in central Ethiopia. Patients who received a malaria test were interviewed regarding their recent travel histories. Bivariate and multivariate analyses were conducted to determine if reported travel outside of the home village within the last month was related to malaria infection status. After adjusting for several known confounding factors, travel away from the home village in the last 30\u2009days was a statistically significant risk factor for infection with Plasmodium falciparum (AOR 1.76; p=0.03) but not for infection with Plasmodium vivax (AOR 1.17; p=0.62). Male sex was strongly associated with any malaria infection (AOR 2.00; p=0.001). Given the importance of identifying reservoir infections, consideration of human movement patterns should factor into decisions regarding elimination and disease prevention, especially when targeted areas are limited to regions within a country.", "title": "Travel history and malaria infection risk in a low-transmission setting in Ethiopia: a case control study"}, "1b18d629928897c1ffcaa37bc1539e9dc51e006a": {"paper_id": "1b18d629928897c1ffcaa37bc1539e9dc51e006a", "abstract": "We report on the ongoing development of a research framework for dynamic integration of information from hard (electronic) and soft (human) sensors. We describe this framework, which includes representation of 2nd order uncertainty. We outline current and planned human-in-the-loop experiments in which an ldquoad hoc community of human observersrdquo provides input reports via mobile phones and PDAs. Our overall approach is based on three pillars: traditional sensing resources (ldquoS-spacerdquo), dynamic communities of human observers (ldquoH-spacerdquo) and resources such as archived sensor data, blogs, reports, dynamic news reports from citizen reporters via the Internet (ldquoI-spacerdquo). The sensors in all three of these pillars need to be characterized and calibrated. In H-space and I-space, calibration issues related to motivation, truthfulness, etc. must be considered in addition to the standard physical characterization and calibration issues that need to be considered in S-space.", "title": "A framework for dynamic hard/soft fusion"}, "65972808a3bdb983f17dea45c3f76762e55845fc": {"paper_id": "65972808a3bdb983f17dea45c3f76762e55845fc", "abstract": "BACKGROUND\nSerious games are motivating and provide a safe environment for students to learn from their mistakes without experiencing any negative consequences from their actions. However, little is known about students' gaming preferences and the types of serious games they like to play for education.\n\n\nOBJECTIVE\nThis study aims to determine the types of gaming aspects that students would like to play in a pharmacy-related serious game.\n\n\nMETHODS\nA cross-sectional study was conducted using a self-administered survey, which obtained students' responses on their preferences regarding various gaming aspects (reward systems, game settings, storylines, viewing perspectives, and gaming styles) and for a hypothetical gaming scenario (authentic simulation or post-apocalyptic fantasy). Descriptive statistics, chi-square, and Fisher's exact tests were used for statistical analyses.\n\n\nRESULTS\nResponse rate was 72.7% (497/684 undergraduates). The most popular game reward systems were unlocking mechanisms (112/497, 22.5%) and experience points (90/497, 18.1%). Most students preferred fantasy/medieval/mythic (253/497, 50.9%) and modern (117/497, 23.5%) settings, but lower year undergraduates preferred modern settings less than upper year seniors (47/236, 19.9% vs 70/242, 28.9%, P=.022). Almost one-third (147/497, 29.6%) preferred an adventurer storyline or an authentic pharmacy-related plot (119/497, 23.9%), and a collaborative game style was most preferred by the students (182/497, 36.6%). Three-dimensional game perspectives (270/497, 54.3%) were more popular than two-dimensional perspectives (221/497, 44.5%), especially among males than females (126/185, 68.1% vs 142/303, 46.9%, P<.001). In terms of choice for a pharmacy-related serious game, a post-apocalyptic fantasy game (scenario B, 287/497, 57.7%) was more popular than an authentic simulation game (scenario A, 209/497, 42.1%). More males preferred the post-apocalyptic fantasy scenario than females (129/187, 69.0% vs 155/306, 50.7%, P<.001).\n\n\nCONCLUSIONS\nIn general, students want a three-dimensional, fantasy/medieval/mythic post-apocalyptic game, based on an adventurer storyline with an unlocking mechanism reward system. A balance between real-life and fantasy elements needs to be struck in order for the game to cater students towards health care practices.", "title": "Student Preferences on Gaming Aspects for a Serious Game in Pharmacy Practice Education: A Cross-Sectional Study"}, "ff1764920c3a523fe4219f490f247669e9703110": {"paper_id": "ff1764920c3a523fe4219f490f247669e9703110", "abstract": "Good computer and video games like System Shock 2, Deus Ex, Pikmin, Rise of Nations, Neverwinter Nights, and Xenosaga: Episode 1 are learning machines. They get themselves learned and learned well, so that they get played long and hard by a great many people. This is how they and their designers survive and perpetuate themselves. If a game cannot be learned and even mastered at a certain level, it won't get played by enough people, and the company that makes it will go broke. Good learning in games is a capitalist-driven Darwinian process of selection of the fittest. Of course, game designers could have solved their learning problems by making games shorter and easier, by dumbing them down, so to speak. But most gamers don't want short and easy games. Thus, designers face and largely solve an intriguing educational dilemma, one also faced by schools and workplaces: how to get people, often young people, to learn and master something that is long and challenging--and enjoy it, to boot.", "title": "What video games have to teach us about learning and literacy"}, "b45509d75e56ecd57658df011bd3675dfd2aa8e3": {"paper_id": "b45509d75e56ecd57658df011bd3675dfd2aa8e3", "abstract": "Now, we come to offer you the right catalogues of book to open. hackers heroes of the computer revolution is one of the literary work in this world in suitable to be reading material. That's not only this book gives reference, but also it will show you the amazing benefits of reading a book. Developing your countless minds is needed; moreover you are kind of people with great curiosity. So, the book is very appropriate for you.", "title": "Hackers - Heroes of the Computer Revolution"}, "6a01ca68b5d5a989004e4690cab36a0c8de0b50a": {"paper_id": "6a01ca68b5d5a989004e4690cab36a0c8de0b50a", "abstract": "The olfactory system (accessory) implicated in reproductive physiology and behavior in mammals is sexually dimorphic. These brain sex differences present two main characteristics: they are seen in neural circuits related to sexual behavior and sexual physiology and they take one of two opposite morphological patterns (male>female or female>male). The present work reports sex differences in the olfactory system in a large homogeneous sample of men (40) and women (51) using of voxel-based morphology. Gray matter concentration showed sexual dimorphism in several olfactory regions. Women have a higher concentration in the orbitofrontal cortex involving Brodmann's areas 10, 11 and 25 and temporomedial cortex (bilateral hippocampus and right amygdala), as well as their left basal insular cortex. In contrast, men show a higher gray matter concentration in the left entorhinal cortex (Brodmann's area 28), right ventral pallidum, dorsal left insular cortex and a region of the orbitofrontal cortex (Brodmann's area 25). This study supports the hypothesis that the mammalian olfactory system is a sexually dimorphic network and provides a theoretical framework for the morphofunctional approach to sex differences in the human brain.", "title": "Sex differences in the human olfactory system"}, "3c06869f8d486923daca993d6096f8a6f203c56b": {"paper_id": "3c06869f8d486923daca993d6096f8a6f203c56b", "abstract": "Statistical parametric maps are spatially extended statistical processes that are used to test hypotheses about regionally specific effects in neuroimaging data. The most established sorts of statistical parametric maps (e.g., Friston et al. [1991]: J Cereb Blood Flow Metab 11:690-699; Worsley et al. 119921: J Cereb Blood Flow Metab 12:YOO-918) are based on linear models, for example ANCOVA, correlation coefficients and t tests. In the sense that these examples are all special cases of the general linear model it should be possible to implement them (and many others) within a unified framework. We present here a general approach that accommodates most forms of experimental layout and ensuing analysis (designed experiments with fixed effects for factors, covariates and interaction of factors). This approach brings together two well established bodies of theory (the general linear model and the theory of Gaussian fields) to provide a complete and simple framework for the analysis of imaging data. The importance of this framework is twofold: (i) Conceptual and mathematical simplicity, in that the same small number of operational equations is used irrespective of the complexity of the experiment or nature of the statistical model and (ii) the generality of the framework provides for great latitude in experimental design and analysis.", "title": "Statistical Parametric Maps in Functional Imaging : A General Linear Approach"}, "b60291cc819e70fe075cd98c0820d33cf8dce7f4": {"paper_id": "b60291cc819e70fe075cd98c0820d33cf8dce7f4", "abstract": "This paper mainly focuses on the development of a high-efficiency power conversion system for kilowatt-level stand-alone generation units with a low output voltage, such as photovoltaic modules, fuel cells, and small-scale wind generators, and it aims at having the same output ac voltage, i.e., 110&nbsp;Vrms/ 60&nbsp;Hz as the utility power for the utilization of a stand-alone power supply. This high-efficiency power conversion system includes one high-efficiency high-step-up dc-dc converter and one soft-switching dc-ac current-source inverter. This dc-dc converter is capable of solving the voltage spike problem while the switch is turned off, and it can achieve the objectives of high efficiency and high voltage gain. Because the techniques of soft switching and voltage clamping are used in the dc-ac current-source inverter, the conversion efficiency could greatly be improved. The effectiveness of the designed circuits is verified by experimentation, and the maximum efficiency of the entire high-efficiency power conversion system is over 91% based on the experimental measurements.", "title": "High-Efficiency Power Conversion System for Kilowatt-Level Stand-Alone Generation Unit With Low Input Voltage"}, "153a72f0b6e65a71c4788502a4cabb9f9284b3a7": {"paper_id": "153a72f0b6e65a71c4788502a4cabb9f9284b3a7", "abstract": "Many grid connected power electronic systems, such as STATCOMs, UPFCs, and distributed generation system interfaces, use a voltage source inverter (VSI) connected to the supply network through a filter. This filter, typically a series inductance, acts to reduce the switching harmonics entering the distribution network. An alternative filter is a LCL network, which can achieve reduced levels of harmonic distortion at lower switching frequencies and with less inductance, and therefore has potential benefits for higher power applications. However, systems incorporating LCL filters require more complex control strategies and are not commonly presented in literature. This paper proposes a robust strategy for regulating the grid current entering a distribution network from a three-phase VSI system connected via a LCL filter. The strategy integrates an outer loop grid current regulator with inner capacitor current regulation to stabilize the system. A synchronous frame PI current regulation strategy is used for the outer grid current control loop. Linear analysis, simulation, and experimental results are used to verify the stability of the control algorithm across a range of operating conditions. Finally, expressions for \u201charmonic impedance\u201d of the system are derived to study the effects of supply voltage distortion on the harmonic performance of the system.", "title": "Grid Current Regulation of a Three-Phase Voltage Source Inverter With an LCL Input Filter"}, "5c7c62854406caf441d3ec4e6bc67c9638b0a10c": {"paper_id": "5c7c62854406caf441d3ec4e6bc67c9638b0a10c", "abstract": "This paper presents a new design of high frequency DC/AC inverter for home applications using fuel cells or photovoltaic array sources. A battery bank parallel to the DC link is provided to take care of the slow dynamic response of the source. The design is based on a push-pull DC/DC converter followed by a full-bridge PWM inverter topology. The nominal power rating is 10 kW. Actual design parameters, procedure and experimental results of a 1.5 kW prototype are provided. The objective of this paper is to explore the possibility of making renewable sources of energy utility interactive by means of low cost power electronic interface.", "title": "High frequency low cost DC-AC inverter design with fuel cell source for home applications"}, "6d913ed7ba72a4b248ae1061120e2ecc7a821449": {"paper_id": "6d913ed7ba72a4b248ae1061120e2ecc7a821449", "abstract": "Building automation systems (BAS) are widely deployed in modern buildings. They are typically engineered adhering to the classical, hierarchical 3-layer model, which has served well in the past but is reaching its limits in complex BAS. The move to more integrated building services also requires tighter integration of the mostly heterogeneous technologies. Vertical integration promises seamless communication from the individual sensor up to IT systems. The hierarchy levels are eliminated in favor of a service-oriented model. Several approaches promise to accomplish this goal: integration of network infrastructure, convergence of different protocols, distribution of services and integration to IT systems.", "title": "Vertical Integration in Building Automation Systems"}, "15fadb6a350f6db6bcefffaa2f8ccf4836b5317f": {"paper_id": "15fadb6a350f6db6bcefffaa2f8ccf4836b5317f", "abstract": "Bluetooth Low Energy (BLE) is ideally suited to exchange information between mobile devices and Internet-of-Things (IoT) sensors. It is supported by most recent consumer mobile devices and can be integrated into sensors enabling them to exchange information in an energy-efficient manner. However, when BLE is used to access or modify sensitive sensor parameters, exchanged messages need to be suitably protected, which may not be possible with the security mechanisms defined in the BLE specification. Consequently we contribute BALSA, a set of cryptographic protocols, a BLE service and a suggested usage architecture aiming to provide a suitable level of security. In this paper we define and analyze these components and describe our proof-of-concept, which demonstrates the feasibility and benefits of BALSA.", "title": "BALSA: Bluetooth Low Energy Application Layer Security Add-on"}, "a7f46ae35116f4c0b3aaa1c9b46d6e79e63b56c9": {"paper_id": "a7f46ae35116f4c0b3aaa1c9b46d6e79e63b56c9", "abstract": "A few years ago it was recognized that the vision of a truly low-cost, low-power radio-based cable replacement was feasible. Such a ubiquitous link would provide the basis for portable devices to communicate together in an ad hoc fashion by creating personal area networks which have similar advantages to their office environment counterpart, the local area network. Bluetooth/sup TM/ is an effort by a consortium of companies to design a royalty-free technology specification enabling this vision. This article describes the radio system behind the Bluetooth concept. Designing an ad hoc radio system for worldwide usage poses several challenges. The article describes the critical system characteristics and motivates the design choices that have been made.", "title": "The Bluetooth radio system"}, "25ba55ea70fd86b40d239cb0401940e0d552905f": {"paper_id": "25ba55ea70fd86b40d239cb0401940e0d552905f", "abstract": "Wireless home automation networks comprise wireless embedded sensors and actuators that enable monitoring and control applications for home user comfort and efficient home management. This article surveys the main current and emerging solutions that are suitable for WHANs, including ZigBee, Z-Wave, INSTEON, Wavenis, and IP-based technology.", "title": "Wireless home automation networks: A survey of architectures and technologies"}, "248e7c9b9f60868f95accdd2fe90053edd84ce6c": {"paper_id": "248e7c9b9f60868f95accdd2fe90053edd84ce6c", "abstract": "Hard disk encryption is known to be vulnerable to a number of attacks that aim to directly extract cryptographic key material from system memory. Several approaches to preventing this class of attacks have been proposed, including Tresor [18] and LoopAmnesia [25]. The common goal of these systems is to confine the encryption key and encryption process itself to the CPU, such that sensitive key material is never released into system memory where it could be accessed by a DMA attack.\n In this work, we demonstrate that these systems are nevertheless vulnerable to such DMA attacks. Our attack, which we call Tresor-Hunt, relies on the insight that DMA-capable adversaries are not restricted to simply reading physical memory, but can write arbitrary values to memory as well. Tresor-Hunt leverages this insight to inject a ring 0 attack payload that extracts disk encryption keys from the CPU into the target system's memory, from which it can be retrieved using a normal DMA transfer.\n Our implementation of this attack demonstrates that it can be constructed in a reliable and OS-independent manner that is applicable to any CPU-bound encryption technique, IA32-based system, and DMA-capable peripheral bus. Furthermore, it does not crash the target system or otherwise significantly compromise its integrity. Our evaluation supports the OS-independent nature of the attack, as well as its feasibility in real-world scenarios. Finally, we discuss several countermeasures that might be adopted to mitigate this attack and render CPU-bound encryption systems viable.", "title": "TRESOR-HUNT: attacking CPU-bound encryption"}, "64b71ec1e51cd1d4582beb06ce0767dcecd5dc2f": {"paper_id": "64b71ec1e51cd1d4582beb06ce0767dcecd5dc2f", "abstract": "Disk encryption has become an important security measure for a multitude of clients, including governments, corporations, activists, security-conscious professionals, and privacy-conscious individuals. Unfortunately, recent research has discovered an effective side channel attack against any disk mounted by a running machine [23]. This attack, known as the cold boot attack, is effective against any mounted volume using state-of-the-art disk encryption, is relatively simple to perform for an attacker with even rudimentary technical knowledge and training, and is applicable to exactly the scenario against which disk encryption is primarily supposed to defend: an adversary with physical access.\n While there has been some previous work in defending against this attack [27], the only currently available solution suffers from the twin problems of disabling access to the SSE registers and supporting only a single encrypted volume, hindering its usefulness for such common encryption scenarios as data and swap partitions encrypted with different keys (the swap key being a randomly generated throw-away key). We present Loop-Amnesia, a kernel-based disk encryption mechanism implementing a novel technique to eliminate vulnerability to the cold boot attack. We contribute a novel technique for shielding multiple encryption keys from RAM and a mechanism for storing encryption keys inside the CPU that does not interfere with the use of SSE. We offer theoretical justification of Loop-Amnesia's invulnerability to the attack, verify that our implementation is not vulnerable in practice, and present measurements showing our impact on I/O accesses to the encrypted disk is limited to a slowdown of approximately 2x. Loop-Amnesia is written for x86-64, but our technique is applicable to other register-based architectures. We base our work on loop-AES, a state-of-the-art open source disk encryption package for Linux.", "title": "Security through amnesia: a software-based solution to the cold boot attack on disk encryption"}, "2076ea658f23f76715f0f770b40ffb83969109bd": {"paper_id": "2076ea658f23f76715f0f770b40ffb83969109bd", "abstract": "Copilot is a coprocessor-based kernel integrity monitor for commodity systems. Copilot is designed to detect malicious modifications to a host\u2019s kernel and has correctly detected the presence of 12 real-world rootkits, each within 30 seconds of their installation with less than a 1% penalty to the host\u2019s performance. Copilot requires no modifications to the protected host\u2019s software and can be expected to operate correctly even when the host kernel is thoroughly compromised \u2013 an advantage over traditional monitors designed to run on the host itself.", "title": "Copilot - a Coprocessor-based Kernel Runtime Integrity Monitor"}, "6c0562ffc00ba7a4d2734ac039ffd181afe2008d": {"paper_id": "6c0562ffc00ba7a4d2734ac039ffd181afe2008d", "abstract": "We propose SecVisor, a tiny hypervisor that ensures code integrity for commodity OS kernels. In particular, SecVisor ensures that only user-approved code can execute in kernel mode over the entire system lifetime. This protects the kernel against code injection attacks, such as kernel rootkits. SecVisor can achieve this propertyeven against an attacker who controls everything but the CPU, the memory controller, and system memory chips. Further, SecVisor can even defend against attackers with knowledge of zero-day kernel exploits.\n Our goal is to make SecVisor amenable to formal verificationand manual audit, thereby making it possible to rule out known classes of vulnerabilities. To this end, SecVisor offers small code size and small external interface. We rely on memory virtualization to build SecVisor and implement two versions, one using software memory virtualization and the other using CPU-supported memory virtualization. The code sizes of the runtime portions of these versions are 1739 and 1112 lines, respectively. The size of the external interface for both versions of SecVisor is 2 hypercalls. It is easy to port OS kernels to SecVisor. We port the Linux kernel version 2.6.20 by adding 12 lines and deleting 81 lines, out of a total of approximately 4.3 million lines of code in the kernel.", "title": "SecVisor: a tiny hypervisor to provide lifetime kernel code integrity for commodity OSes"}, "1355972384f2458f32d339c0304862ac24259aa1": {"paper_id": "1355972384f2458f32d339c0304862ac24259aa1", "abstract": "Current commercial virus detectors are based on The problem of protecting computer systems can be viewed generally as the problem of learning to distinguish relf from other. We describe a method for change detection which is based on the generation of T cella in the immune syetem. Mathematical analysis reveals computational costs of the system, and preliminary experiments illustrate how the method might be applied to the problem of computer viruses.", "title": "Self-nonself discrimination in a computer"}, "65cba5719e3980502116073b82620db8a0ebe406": {"paper_id": "65cba5719e3980502116073b82620db8a0ebe406", "abstract": "The past decade has seen a significant interest on the problem of inducing decision trees that take account of costs of misclassification and costs of acquiring the features used for decision making. This survey identifies over 50 algorithms including approaches that are direct adaptations of accuracy-based methods, use genetic algorithms, use anytime methods and utilize boosting and bagging. The survey brings together these different studies and novel approaches to cost-sensitive decision tree learning, provides a useful taxonomy, a historical timeline of how the field has developed and should provide a useful reference point for future research in this field.", "title": "A survey of cost-sensitive decision tree induction algorithms"}, "508571db5d2f77c17d2829878bb1dc645010dda8": {"paper_id": "508571db5d2f77c17d2829878bb1dc645010dda8", "abstract": "In this paper, we address the problem of retrospectively pruning decision trees induced from data, according to a topdown approach. This problem has received considerable attention in the areas of pattern recognition and machine learning, and many distinct methods have been proposed in literature. We make a comparative study of six well-known pruning methods with the aim of understanding their theoretical foundations, their computational complexity, and the strengths and weaknesses of their formulation. Comments on the characteristics of each method are empirically supported. In particular, a wide experimentation performed on several data sets leads us to opposite conclusions on the predictive accuracy of simplified trees from some drawn in the literature. We attribute this divergence to differences in experimental designs. Finally, we prove and make use of a property of the reduced error pruning method to obtain an objective evaluation of the tendency to overprune/underprune observed in each method. Index Terms \u2014Decision trees, top-down induction of decision trees, simplification of decision trees, pruning and grafting operators, optimal pruning, comparative studies. \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 \u2726 \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014", "title": "A Comparative Analysis of Methods for Pruning Decision Trees"}, "0c668ee24d58ecca165f788d40765e79ed615471": {"paper_id": "0c668ee24d58ecca165f788d40765e79ed615471", "abstract": null, "title": "Classification and Regression Trees"}, "b04db132c033b31010281baa44ce547463367453": {"paper_id": "b04db132c033b31010281baa44ce547463367453", "abstract": "The area under the ROC curve, or the equivalent Gini index, is a widely used measure of performance of supervised classification rules. It has the attractive property that it side-steps the need to specify the costs of the different kinds of misclassification. However, the simple form is only applicable to the case of two classes. We extend the definition to the case of more than two classes by averaging pairwise comparisons. This measure reduces to the standard form in the two class case. We compare its properties with the standard measure of proportion correct and an alternative definition of proportion correct based on pairwise comparison of classes for a simple artificial case and illustrate its application on eight data sets. On the data sets we examined, the measures produced similar, but not identical results, reflecting the different aspects of performance that they were measuring. Like the area under the ROC curve, the measure we propose is useful in those many situations where it is impossible to give costs for the different kinds of misclassification.", "title": "A Simple Generalisation of the Area Under the ROC Curve for Multiple Class Classification Problems"}, "04a20cd0199d0a24fea8e6bf0e0cc61b26c1f3ac": {"paper_id": "04a20cd0199d0a24fea8e6bf0e0cc61b26c1f3ac", "abstract": "One of the surprising recurring phenomena observed in experiments with boosting is that the test error of the generated classifier usually does not increase as its size becomes very large, and often is observed to decrease even after the training error reaches zero. In this paper, we show that this phenomenon is related to the distribution of margins of the training examples with respect to the generated voting classification rule, where the margin of an example is simply the difference between the number of correct votes and the maximum number of votes received by any incorrect label. We show that techniques used in the analysis of Vapnik\u2019s support vector classifiers and of neural networks with small weights can be applied to voting methods to relate the margin distribution to the test error. We also show theoretically and experimentally that boosting is especially effective at increasing the margins of the training examples. Finally, we compare our explanation to those based on the bias-variance decomposition.", "title": "Boosting the margin: A new explanation for the effectiveness of voting methods"}, "c9c9b50b51dc677ff83f58f1a5433b2a41321ec3": {"paper_id": "c9c9b50b51dc677ff83f58f1a5433b2a41321ec3", "abstract": "The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data. High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition.", "title": "Support-Vector Networks"}, "25406e6733a698bfc4ac836f8e74f458e75dad4f": {"paper_id": "25406e6733a698bfc4ac836f8e74f458e75dad4f", "abstract": "We address the question of when a network can be expected to generalize from m random training examples chosen from some arbitrary probability distribution, assuming that future test examples are drawn from the same distribution. Among our results are the following bounds on appropriate sample vs. network size. Assume 0 < \u220a 1/8. We show that if m O(W/\u220a log N/\u220a) random examples can be loaded on a feedforward network of linear threshold functions with N nodes and W weights, so that at least a fraction 1 \u220a/2 of the examples are correctly classified, then one has confidence approaching certainty that the network will correctly classify a fraction 1 \u220a of future test examples drawn from the same distribution. Conversely, for fully-connected feedforward nets with one hidden layer, any learning algorithm using fewer than (W/\u220a) random training examples will, for some distributions of examples consistent with an appropriate weight choice, fail at least some fixed fraction of the time to find a weight choice that will correctly classify more than a 1 \u220a fraction of the future test examples.", "title": "What Size Net Gives Valid Generalization?"}, "0dcdc50ffd9f68e1d0f2878552329920022cfe99": {"paper_id": "0dcdc50ffd9f68e1d0f2878552329920022cfe99", "abstract": "The problem of combining preferences arises in several applications, such as combining the results of different search engines. This work describes an efficient algorithm for combining multiple preferences. We first give a formal framework for the problem. We then describe and analyze a new boosting algorithm for combining preferences called RankBoost. We also describe an efficient implementation of the algorithm for certain natural cases. We discuss two experiments we carried out to assess the performance of RankBoost. In the first experiment, we used the algorithm to combine different WWW search strategies, each of which is a query expansion for a given domain. For this task, we compare the performance of RankBoost to the individual search strategies. The second experiment is a collaborative-filtering task for making movie recommendations. Here, we present results comparing RankBoost to nearest-neighbor and regression algorithms. Thesis Supervisor: David R. Karger Title: Associate Professor", "title": "An Efficient Boosting Algorithm for Combining Preferences"}, "d063402667686896829d4da5cb50d68f74fd3fc2": {"paper_id": "d063402667686896829d4da5cb50d68f74fd3fc2", "abstract": "-This article describes\" computer simulation of the dynamics of a distributed model of the ol)actory system that is aimed at understanding the role of chaos in biological pattern recognition. The model is governed bv coupled nonlinear differential equations with many variables and parameters', which allow multiple highdimensional chaotic states'. An appropriate set c~[ the parameters is identified by computer experiments with the guidance of biological measurements, through which this\" model of the olfactory system maintains a low dimensional global chaotic attractor with multiple ~\" wings,'\" The central part c~[ the attractor is its basal chaotic activity, which simulates the electroencephalographic (EEG) aetivi O' of the olfactory system under zero signal input (exhah~tion). It provides the system with a ready state so that it is unnecessary for the system to \"wake up\" front or return to a \"'dormant\" equilibrium state every time that an input is\" given (by inhalation). Each of the wings may be either a near-limit cycle (a narrow band chaos') or a broad band chaos. The reproducible spatial pattern 0[ each near-limit cycle is\" determined by a template made in the system. A novel input with no template activates the system to either a nonreproducible near-limit cycle wing or a broad band chaotic wing. Pattern recognition in the system may be considered as the transition from one wing to another, as demonstrated by the computer simulation. The time series of the manifestations\" of the attractor are EEG-like waveforms with fractal dimensions that reflect which wing the system is placed in by input or lack of input. The computer simulation also shows that the adaptive behavior of the system is scaling invariant, and it is independent of the initial conditions at the transition ,f?om one wing to another. These properties\" enable the system to classify an uninterrupted sequence Of stimtdi. Keywords--Chaos, Neural networks, Olfaction modeling, Pattern classification, Phase coherence, Scaling invariance, Serial recognition. I. I N T R O D U C T I O N The study of neural networks has opened a new direction in the investigation of novel computer design principles through understanding complex dynamics in biological brains. The marriage of nonlinear dynamics and neurobiology may be very helpful to the field of artifical intelligence by capturing the speed and power of brains during pattern recognition. Acknowledgments: Support from grants AFOSR-87-0317 and MH06686 from NIMH is gratefully acknowledged. The authors thank Harold Szu, physicist; Bill Baird, biophysicist; Barry Rhoades, physiologist; Morris Hirsch, mathematician: and Marjorie Honzik, psychologist, for their careful reading of the original manuscript and their valuable suggestions. The current revised version has benefited from helpful comments by one anonymous action editor and two anonymous referees. Requests tk~r reprints should be sent to Walter J. Freeman, Department of Molecular and Cell Biology, University of California, Berkeley. CA 94720. 1.1 Biological Phenomena Physiological experiments suggest that the main components of neural activity in olfactory systems are chaotic (Skarda & Freeman, 1987). It has been found that distributions of intervals between pulses in trains are typically Poisson; histograms of electroencephalographic (EEG) amplitudes are close to Gaussian curves; and spectra of brain waves are broad with low and variable peaks consistent with 1 / f noise (Freeman, 1975). Fractal dimensions are estimated to be low (Freeman, 1987, 1988). An equilibrium occurs only under deep anesthesia, in coma, or in areas of cortex that have been isolated from the rest of the brain. There is no evidence that pure limit cycle activity appears in olfactory systems, although at times the activities of neurons may come close to a limit cycle. Our physiological experiments tell us: (a) The EEG manifests a particular kind of Chaos that is spatially coherent across the cortex when sampled with an array of electrodes; (b) the adaptive", "title": "Model of biological pattern recognition with spatially chaotic dynamics"}, "6601d921eb76b183a7f1196219f3d475ff920a24": {"paper_id": "6601d921eb76b183a7f1196219f3d475ff920a24", "abstract": "Caulerpa racemosa, a common and opportunistic species widely distributed in tropical and warm-temperate regions, is known to form monospecific stands outside its native range (Verlaque et al. 2003). In October 2011, we observed an alteration in benthic community due to a widespread overgrowth of C. racemosa around the inhabited island of Magoodhoo (3 04\u00a2N; 72 57\u00a2E, Republic of Maldives). The algal mats formed a continuous dense meadow (Fig. 1a) that occupied an area of 95 \u00b7 120 m (~11,000 m) previously dominated by the branching coral Acropora muricata. Partial mortality and total mortality (Fig. 1b, c) were recorded on 45 and 30% of A. muricata colonies, respectively. The total area of influence of C. racemosa was, however, much larger (~25,000 m) including smaller coral patches near to the meadow, where mortality in contact with the algae was also observed on colonies of Isopora palifera, Lobophyllia corymbosa, Pavona varians, Pocillopora damicornis, and Porites solida. Although species of the genus Caulerpa are not usually abundant on oligotrophic coral reefs, nutrient enrichment from natural and/or anthropogenic sources is known to promote green algal blooms (Lapointe and Bedford 2009). Considering the current state of regression of many reefs in the Maldives (Lasagna et al. 2010), we report an unusual phenomenon that could possibly become more common.", "title": "Acropora muricata mortality associated with extensive growth of Caulerpa racemosa in Magoodhoo Island, Republic of Maldives"}, "4e5326b0c248246b88f786907edb4e295eae9928": {"paper_id": "4e5326b0c248246b88f786907edb4e295eae9928", "abstract": "Diabetic macular edema (DME) is an advanced symptom of diabetic retinopathy and can lead to irreversible vision loss. In this paper, a two-stage methodology for the detection and classification of DME severity from color fundus images is proposed. DME detection is carried out via a supervised learning approach using the normal fundus images. A feature extraction technique is introduced to capture the global characteristics of the fundus images and discriminate the normal from DME images. Disease severity is assessed using a rotational asymmetry metric by examining the symmetry of macular region. The performance of the proposed methodology and features are evaluated against several publicly available datasets. The detection performance has a sensitivity of 100% with specificity between 74% and 90%. Cases needing immediate referral are detected with a sensitivity of 100% and specificity of 97%. The severity classification accuracy is 81% for the moderate case and 100% for severe cases. These results establish the effectiveness of the proposed solution.", "title": "Automatic assessment of macular edema from color retinal images"}, "811477e0eec230c34d37eacbe72ff8577052b57f": {"paper_id": "811477e0eec230c34d37eacbe72ff8577052b57f", "abstract": "This meta-analysis (k = 48) investigated two relationships in competitive sport: (1) state cognitive anxiety with performance and (2) state self-confidence with performance. The cognitive anxiety mean effect size was r = -0.10 (P < 0.05). The self-confidence mean effect size was r = 0.24 (P < 0.001). A paired-samples t-test revealed that the magnitude of the self-confidence mean effect size was significantly greater than that of the cognitive anxiety mean effect size. The moderator variables for the cognitive anxiety-performance relationship were sex and standard of competition. The mean effect size for men (r = -0.22) was significantly greater than the mean effect size for women (r = -0.03). The mean effect size for high-standard competition (r = -0.27) was significantly greater than that for comparatively low-standard competition (r = -0.06). The significant moderator variables for the self-confidence-performance relationship were sex, standard of competition and measurement. The mean effect size for men (r = 0.29) was significantly greater than that for women (r = 0.04) and the mean effect size for high-standard competition (r = 0.33) was significantly greater than that for low-standard competition (r = 0.16). The mean effect size derived from studies employing the Competitive State Anxiety Inventory-2 (r = 0.19) was significantly smaller than the mean effect size derived from studies using other measures of self-confidence (r = 0.38). Measurement issues are discussed and future research directions are offered in light of the results.", "title": "The relative impact of cognitive anxiety and self-confidence upon sport performance: a meta-analysis."}, "c5be23d326e6cee1644806c5092be8e5828ab8ea": {"paper_id": "c5be23d326e6cee1644806c5092be8e5828ab8ea", "abstract": "Decimation filter has wide application in both the analog and digital system for data rate conversion as well as filtering. In this paper, we have discussed about efficient structure of a decimation filter. We have three class of filters FIR, IIR and CIC filters. IIR filters are simpler in structure but do not satisfy linear phase requirements which are required in time sensitive features like a video or a speech. FIR filters have a well defined frequency response but they require lot of hardware to store the filter coefficients. CIC filters don\u2019t have this drawback they are coefficient less so hardware requirement is much reduced but as they don\u2019t have well defined frequency response. So another structure is proposed which takes advantage of good feature of both the structures and thus have a cascade of CIC and FIR filters. They exhibit both the advantage of FIR and CIC filters and hence more efficient over all in terms of hardware and frequency response requirements.", "title": "Analysis of Cascaded Integrator Comb ( CIC ) Decimation Filter in Efficient Compensation"}, "55647abc885e4a62aaa46765a89bbd46d4ebdef2": {"paper_id": "55647abc885e4a62aaa46765a89bbd46d4ebdef2", "abstract": "In this article we consider the problem of mapping a noisy estimate of a user's current location to a semantically meaningful point of interest, such as a home, restaurant, or store. Despite the poor accuracy of GPS on current mobile devices and the relatively high density of places in urban areas, it is possible to predict a user's location with considerable precision by explicitly modeling both places and users and by combining a variety of signals about a user's current context. Places are often simply modeled as a single latitude and longitude when in fact they are complex entities existing in both space and time and shaped by the millions of people that interact with them. Similarly, models of users reveal complex but predictable patterns of mobility that can be exploited for this task. We propose a novel spatial search algorithm that infers a user's location by combining aggregate signals mined from billions of foursquare check-ins with real-time contextual information. We evaluate a variety of techniques and demonstrate that machine learning algorithms for ranking and spatiotemporal models of places and users offer significant improvement over common methods for location search based on distance and popularity.", "title": "Learning to rank for spatiotemporal search"}, "86f089e8a560b174d17d9a401338053f4fe84f7f": {"paper_id": "86f089e8a560b174d17d9a401338053f4fe84f7f", "abstract": "We live in a \"small world,\" where two arbitrary people are likely connected by a short chain of intermediate friends. With scant information about a target individual, people can successively forward a message along such a chain. Experimental studies have verified this property in real social networks, and theoretical models have been advanced to explain it. However, existing theoretical models have not been shown to capture behavior in real-world social networks. Here, we introduce a richer model relating geography and social-network friendship, in which the probability of befriending a particular person is inversely proportional to the number of closer people. In a large social network, we show that one-third of the friendships are independent of geography and the remainder exhibit the proposed relationship. Further, we prove analytically that short chains can be discovered in every network exhibiting the relationship.", "title": "Geographic routing in social networks."}, "12d6cf6346f6d693b6dc3b88d176a8a7b192355c": {"paper_id": "12d6cf6346f6d693b6dc3b88d176a8a7b192355c", "abstract": "To build systems shielding users from fraudulent (or phishing) websites, designers need to know which attack strategies work and why. This paper provides the first empirical evidence about which malicious strategies are successful at deceiving general users. We first analyzed a large set of captured phishing attacks and developed a set of hypotheses about why these strategies might work. We then assessed these hypotheses with a usability study in which 22 participants were shown 20 web sites and asked to determine which ones were fraudulent. We found that 23% of the participants did not look at browser-based cues such as the address bar, status bar and the security indicators, leading to incorrect choices 40% of the time. We also found that some visual deception attacks can fool even the most sophisticated users. These results illustrate that standard security indicators are not effective for a substantial fraction of users, and suggest that alternative approaches are needed.", "title": "Why phishing works"}, "57a809faecdeb6c97160be4cab0d0b2f42ed3c6f": {"paper_id": "57a809faecdeb6c97160be4cab0d0b2f42ed3c6f", "abstract": "We investigate how to organize a large collection of geotagged photos, working with a dataset of about 35 million images collected from Flickr. Our approach combines content analysis based on text tags and image data with structural analysis based on geospatial data. We use the spatial distribution of where people take photos to define a relational structure between the photos that are taken at popular places. We then study the interplay between this structure and the content, using classification methods for predicting such locations from visual, textual and temporal features of the photos. We find that visual and temporal features improve the ability to estimate the location of a photo, compared to using just textual features. We illustrate using these techniques to organize a large photo collection, while also revealing various interesting properties about popular cities and landmarks at a global scale.", "title": "Mapping the world's photos"}, "cfed87559dcba4f06742e091fa97041588562aa9": {"paper_id": "cfed87559dcba4f06742e091fa97041588562aa9", "abstract": "The small-world phenomenon \u2014 the principle that most of us are linked by short chains of acquaintances \u2014 was first investigated as a question in sociology and is a feature of a range of networks arising in nature and technology. Experimental study of the phenomenon revealed that it has two fundamental components: first, such short chains are ubiquitous, and second, individuals operating with purely local information are very adept at finding these chains. The first issue has been analysed, and here I investigate the second by modelling how individuals can find short chains in a large social network.", "title": "Navigation in a small world"}, "ce6ed169d817280d02c04ee3442568cd325c1c0b": {"paper_id": "ce6ed169d817280d02c04ee3442568cd325c1c0b", "abstract": "In this paper, we consider the evolution of structure within large online social networks. We present a series of measurements of two such networks, together comprising in excess of five million people and ten million friendship links, annotated with metadata capturing the time of every event in the life of the network. Our measurements expose a surprising segmentation of these networks into three regions: singletons who do not participate in the network; isolated communities which overwhelmingly display star structure; and a giant component anchored by a well-connected core region which persists even in the absence of stars.We present a simple model of network growth which captures these aspects of component structure. The model follows our experimental results, characterizing users as either passive members of the network; inviters who encourage offline friends and acquaintances to migrate online; and linkers who fully participate in the social evolution of the network.", "title": "Structure and evolution of online social networks"}, "9ac34c7040d08a27e7dc75cfa46eb0144de3a284": {"paper_id": "9ac34c7040d08a27e7dc75cfa46eb0144de3a284", "abstract": "In this paper, we present Google, a prototype of a large-scale search engine which makes heavy use of the structure present in hypertext. Google is designed to crawl and index the Web efficiently and produce much more satisfying search results than existing systems. The prototype with a full text and hyperlink database of at least 24 million pages is available at http://google.stanford.edu/ To engineer a search engine is a challenging task. Search engines index tens to hundreds of millions of web pages involving a comparable number of distinct terms. They answer tens of millions of queries every day. Despite the importance of large-scale search engines on the web, very little academic research has been done on them. Furthermore, due to rapid advance in technology and web proliferation, creating a web search engine today is very different from three years ago. This paper provides an in-depth description of our large-scale web search engine -the first such detailed public description we know of to date. Apart from the problems of scaling traditional search techniques to data of this magnitude, there are new technical challenges involved with using the additional information present in hypertext to produce better search results. This paper addresses this question of how to build a practical large-scale system which can exploit the additional information present in hypertext. Also we look at the problem of how to effectively deal with uncontrolled hypertext collections where anyone can publish anything they want.", "title": "The Anatomy of a Large-Scale Hypertextual Web Search Engine"}, "0bf8527d093600c50208faca0b32eef2372ec0d4": {"paper_id": "0bf8527d093600c50208faca0b32eef2372ec0d4", "abstract": "The PageRank algorithm, used in the Google search e ngin , greatly improves the results of Web search by taking into a ccount the link structure of the Web. PageRank assigns to a page a score proportional to the number of times a random surfer would visit that page, if it surfed indefinitely from page to page, follow ing all outlinks from a page with equal probability. We propose to i mprove PageRank by using a more intelligent surfer, one that i s guided by a probabilistic model of the relevance of a page to a query. Efficient execution of our algorithm at query time is made po ssible by precomputing at crawl time (and thus once for all quer ies) the necessary terms. Experiments on two large subsets of the Web indicate that our algorithm significantly outperforms PageRa nk in the (human-rated) quality of the pages returned, while rem aining efficient enough to be used in today\u2019s large search engines.", "title": "The Intelligent surfer: Probabilistic Combination of Link and Content Information in PageRank"}, "2dcdea0a8765c6a4e888aed91d0174f8d45a7a9f": {"paper_id": "2dcdea0a8765c6a4e888aed91d0174f8d45a7a9f", "abstract": "This paper describes the museum wearable: a wearable computer which orchestrates an audiovisual narration as a function of the visitor\u2019s interests gathered from his/her physical path in the museum and length of stops. The wearable is made by a lightweight and small computer that people carry inside a shoulder pack. It offers an audiovisual augmentation of the surrounding environment using a small, lightweight eye-piece display (often called private-eye) attached to conventional headphones. Using custom built infrared location sensors distributed in the museum space, and statistical mathematical modeling, the museum wearable builds a progressively refined user model and uses it to deliver a personalized audiovisual narration to the visitor. This device will enrich and personalize the museum visit as a visual and auditory storyteller that is able to adapt its story to the audience\u2019s interests and guide the public through the path of the exhibit.", "title": "The Museum Wearable : real-time sensor-driven understanding of visitors \u2019 interests for personalized visually-augmented museum experiences"}, "8c9f8805e5463e92011ea32ad7cc3f35f812e522": {"paper_id": "8c9f8805e5463e92011ea32ad7cc3f35f812e522", "abstract": "Multi-relation Question Answering is a challenging task, due to the requirement of elaborated analysis on questions and reasoning over multiple fact triples in knowledge base. In this paper, we present a novel model called Interpretable Reasoning Network that employs an interpretable, hop-by-hop reasoning process for question answering. The model dynamically decides which part of an input question should be analyzed at each hop; predicts a relation that corresponds to the current parsed results; utilizes the predicted relation to update the question representation and the state of the reasoning process; and then drives the next-hop reasoning. Experiments show that our model yields state-of-the-art results on two datasets. More interestingly, the model can offer traceable and observable intermediate predictions for reasoning analysis and failure diagnosis, thereby allowing manual manipulation in predicting the final answer.", "title": "An Interpretable Reasoning Network for Multi-Relation Question Answering"}, "42dec19543930bffec09ab74441440fdec4c94b2": {"paper_id": "42dec19543930bffec09ab74441440fdec4c94b2", "abstract": "With the recent advancement of multilayer convolutional neural networks (CNN), deep learning has achieved amazing success in many areas, especially in visual content understanding and classification. To improve the performance and energy-efficiency of the computation-demanding CNN, the FPGA-based acceleration emerges as one of the most attractive alternatives.\n In this paper we design and implement Caffeine, a hardware/software co-designed library to efficiently accelerate the entire CNN on FPGAs. First, we propose a uniformed convolutional matrix-multiplication representation for both computation-intensive convolutional layers and communication-intensive fully connected (FCN) layers. Second, we design Caffeine with the goal to maximize the underlying FPGA computing and bandwidth resource utilization, with a key focus on the bandwidth optimization by the memory access reorganization not studied in prior work. Moreover, we implement Caffeine in the portable high-level synthesis and provide various hardware/software definable parameters for user configurations. Finally, we also integrate Caffeine into the industry-standard software deep learning framework Caffe. We evaluate Caffeine and its integration with Caffe by implementing VGG16 and AlexNet network on multiple FPGA platforms. Caffeine achieves a peak performance of 365 GOPS on Xilinx KU060 FPGA and 636 GOPS on Virtex7 690t FPGA. This is the best published result to our best knowledge. We achieve more than 100x speedup on FCN layers over previous FPGA accelerators. An end-to-end evaluation with Caffe integration shows up to 7.3x and 43.5x performance and energy gains over Caffe on a 12-core Xeon server, and 1.5x better energy-efficiency over the GPU implementation on a medium-sized FPGA (KU060). Performance projections to a system with a high-end FPGA (Virtex7 690t) shows even higher gains.", "title": "Caffeine: Towards uniformed representation and acceleration for deep convolutional neural networks"}, "1e511a36cd6c793189c544a6f935958a2d98a737": {"paper_id": "1e511a36cd6c793189c544a6f935958a2d98a737", "abstract": "Datasets are an integral part of contemporary object recognition research. They have been the chief reason for the considerable progress in the field, not just as source of large amounts of training data, but also as means of measuring and comparing performance of competing algorithms. At the same time, datasets have often been blamed for narrowing the focus of object recognition research, reducing it to a single benchmark performance number. Indeed, some datasets, that started out as data capture efforts aimed at representing the visual world, have become closed worlds unto themselves (e.g. the Corel world, the Caltech-101 world, the PASCAL VOC world). With the focus on beating the latest benchmark numbers on the latest dataset, have we perhaps lost sight of the original purpose? The goal of this paper is to take stock of the current state of recognition datasets. We present a comparison study using a set of popular datasets, evaluated based on a number of criteria including: relative data bias, cross-dataset generalization, effects of closed-world assumption, and sample value. The experimental results, some rather surprising, suggest directions that can improve dataset collection as well as algorithm evaluation protocols. But more broadly, the hope is to stimulate discussion in the community regarding this very important, but largely neglected issue.", "title": "Unbiased look at dataset bias"}, "03cb609fcfce6c60cbe3eb0dd8254069bf6d7573": {"paper_id": "03cb609fcfce6c60cbe3eb0dd8254069bf6d7573", "abstract": "Deep multi-layer neural networks have many levels of non-linearities, which allows them to potentially represent very compactly highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization appears to often get stuck in poor solutions. Hinton et al. recently introduced a greedy layer-wise unsupervised learning algorithm for Deep Belief Networks (DBN), a generative model with many layers of hidden causal variables. In the context of the above optimization problem, we study this algorithm empirically and explore variants to better understand its success and extend it to cases where the inputs are continuous or where the structure of the input distribution is not revealing enough about the variable to be predicted in a supervised task.", "title": "Greedy Layer-Wise Training of Deep Networks"}, "a2c1d14f22c79dd656cbd3b99953aa301c6bbd74": {"paper_id": "a2c1d14f22c79dd656cbd3b99953aa301c6bbd74", "abstract": "We present a hierarchical model that learns image decompositions via alternating layers of convolutional sparse coding and max pooling. When trained on natural images, the layers of our model capture image information in a variety of forms: low-level edges, mid-level edge junctions, high-level object parts and complete objects. To build our model we rely on a novel inference scheme that ensures each layer reconstructs the input, rather than just the output of the layer directly beneath, as is common with existing hierarchical approaches. This makes it possible to learn multiple layers of representation and we show models with 4 layers, trained on images from the Caltech-101 and 256 datasets. When combined with a standard classifier, features extracted from these models outperform SIFT, as well as representations from other feature learning methods.", "title": "Adaptive deconvolutional networks for mid and high level feature learning"}, "5b9534442f91a87022427b74bca9fd95dd045383": {"paper_id": "5b9534442f91a87022427b74bca9fd95dd045383", "abstract": "Latent semantic models, such as LSA, intend to map a query to its relevant documents at the semantic level where keyword-based matching often fails. In this study we strive to develop a series of new latent semantic models with a deep structure that project queries and documents into a common low-dimensional space where the relevance of a document given a query is readily computed as the distance between them. The proposed deep structured semantic models are discriminatively trained by maximizing the conditional likelihood of the clicked documents given a query using the clickthrough data. To make our models applicable to large-scale Web search applications, we also use a technique called word hashing, which is shown to effectively scale up our semantic models to handle large vocabularies which are common in such tasks. The new models are evaluated on a Web document ranking task using a real-world data set. Results show that our best model significantly outperforms other latent semantic models, which were considered state-of-the-art in the performance prior to the work presented in this paper.", "title": "Learning deep structured semantic models for web search using clickthrough data"}, "a718b85520bea702533ca9a5954c33576fd162b0": {"paper_id": "a718b85520bea702533ca9a5954c33576fd162b0", "abstract": "The main purpose of this paper is to describe a process for partitioning an N-dimensional population into k sets on the basis of a sample. The process, which is called 'k-means,' appears to give partitions which are reasonably efficient in the sense of within-class variance. That is, if p is the probability mass function for the population, S = {S1, S2, * *, Sk} is a partition of EN, and ui, i = 1, 2, * , k, is the conditional mean of p over the set Si, then W2(S) = ff=ISi f z u42 dp(z) tends to be low for the partitions S generated by the method. We say 'tends to be low,' primarily because of intuitive considerations, corroborated to some extent by mathematical analysis and practical computational experience. Also, the k-means procedure is easily programmed and is computationally economical, so that it is feasible to process very large samples on a digital computer. Possible applications include methods for similarity grouping, nonlinear prediction, approximating multivariate distributions, and nonparametric tests for independence among several variables. In addition to suggesting practical classification methods, the study of k-means has proved to be theoretically interesting. The k-means concept represents a generalization of the ordinary sample mean, and one is naturally led to study the pertinent asymptotic behavior, the object being to establish some sort of law of large numbers for the k-means. This problem is sufficiently interesting, in fact, for us to devote a good portion of this paper to it. The k-means are defined in section 2.1, and the main results which have been obtained on the asymptotic behavior are given there. The rest of section 2 is devoted to the proofs of these results. Section 3 describes several specific possible applications, and reports some preliminary results from computer experiments conducted to explore the possibilities inherent in the k-means idea. The extension to general metric spaces is indicated briefly in section 4. The original point of departure for the work described here was a series of problems in optimal classification (MacQueen [9]) which represented special", "title": "SOME METHODS FOR CLASSIFICATION AND ANALYSIS OF MULTIVARIATE OBSERVATIONS"}, "29a3fa469ef3e3132000545bf2c6432c0ec14c76": {"paper_id": "29a3fa469ef3e3132000545bf2c6432c0ec14c76", "abstract": "We propose a new learning method, \"Generalized Learning Vector Quantization (GLVQ),\" in which reference vectors are updated based on the steepest descent method in order to minimize the cost function . The cost function is determined so that the obtained learning rule satisfies the convergence condition. We prove that Kohonen's rule as used in LVQ does not satisfy the convergence condition and thus degrades recognition ability. Experimental results for printed Chinese character recognition reveal that GLVQ is superior to LVQ in recognition ability.", "title": "Generalized Learning Vector Quantization"}, "586d7b215d1174f01a1dc2f6abf6b2eb0f740ab6": {"paper_id": "586d7b215d1174f01a1dc2f6abf6b2eb0f740ab6", "abstract": "We present an unsupervised method for learning a hierarchy of sparse feature detectors that are invariant to small shifts and distortions. The resulting feature extractor consists of multiple convolution filters, followed by a feature-pooling layer that computes the max of each filter output within adjacent windows, and a point-wise sigmoid non-linearity. A second level of larger and more invariant features is obtained by training the same algorithm on patches of features from the first level. Training a supervised classifier on these features yields 0.64% error on MNIST, and 54% average recognition rate on Caltech 101 with 30 training samples per category. While the resulting architecture is similar to convolutional networks, the layer-wise unsupervised training procedure alleviates the over-parameterization problems that plague purely supervised learning procedures, and yields good performance with very few labeled training samples.", "title": "Unsupervised Learning of Invariant Feature Hierarchies with Applications to Object Recognition"}, "abbfc9cfdd5fcd15f592e49838816556c5e7935b": {"paper_id": "abbfc9cfdd5fcd15f592e49838816556c5e7935b", "abstract": "We apply a biologically inspired model of visual object recognition to the multiclass object categorization problem. Our model modifies that of Serre, Wolf, and Poggio. As in that work, we first apply Gabor filters at all positions and scales; feature complexity and position/scale invariance are then built up by alternating template matching and max pooling operations. We refine the approach in several biologically plausible ways, using simple versions of sparsification and lateral inhibition. We demonstrate the value of retaining some position and scale information above the intermediate feature level. Using feature selection we arrive at a model that performs better with fewer features. Our final model is tested on the Caltech 101 object categories and the UIUC car localization task, in both cases achieving state-of-the-art performance. The results strengthen the case for using this class of model in computer vision.", "title": "Multiclass Object Recognition with Sparse, Localized Features"}, "82795cf04f5631e82413b1952625a5a6f21b68ea": {"paper_id": "82795cf04f5631e82413b1952625a5a6f21b68ea", "abstract": "| The architecture, implementation, and applications of a special purpose neural network processor are described. The chip performs over 2000 multiplications and additions simultaneously. Its datapath is particularly suitable for the convolutional topologies that are typical in classi cation networks, but can also be con gured for fully connected or feedback topologies. Resources can be multiplexed to permit implementation of networks with several hundreds of thousands of connections on a single chip. Computations are performed with 6Bits accuracy for the weights and 3Bits for the neuron states. Analog processing is used internally for reduced power dissipation and higher density, but all input/output is digital to simplify system integration. The practicality of the chip is demonstrated with an implementation of a neural network for optical character recognition. This network contains over 130,000 connections and is evaluated in 1ms.", "title": "An analog neural network processor with programmable topology"}, "0f5e08f0db7676b3147e91403cadb4888d0d0aa5": {"paper_id": "0f5e08f0db7676b3147e91403cadb4888d0d0aa5", "abstract": "Parallel sorting networks are widely employed in hardware implementations for sorting due to their high data parallelism and low control overhead. In this paper, we propose an energy and memory efficient mapping methodology for implementing bitonic sorting network on FPGA. Using this methodology, the proposed sorting architecture can be built for a given data parallelism while supporting continuous data streams. We propose a streaming permutation network (SPN) by \"folding\" the classic Clos network. We prove that the SPN is programmable to realize all the interconnection patterns in the bitonic sorting network. A low cost design for sorting with minimal resource usage is obtained by reusing one SPN . We also demonstrate a high throughput design by trading off area for performance. With a data parallelism of p (2 \u2264 p \u2264 N/ log2 N), the high throughput design sorts an N-key sequence with latency O(N/p), throughput (# of keys sorted per cycle) O(p) and uses O(N) memory. This achieves optimal memory efficiency (defined as the ratio of throughput to the amount of on-chip memory used by the design) of O(p/N). Another noteworthy feature of the high throughput design is that only single-port memory rather than dual-port memory is required for processing continuous data streams. This results in 50% reduction in memory consumption. Post place-and-route results show that our architecture demonstrates 1.3x \u223c1.6x improvment in energy efficiency and 1.5x \u223c 5.3x better memory efficiency compared with the state-of-the-art designs.", "title": "Energy and Memory Efficient Mapping of Bitonic Sorting on FPGA"}, "73a58ebe0a18d1d9d4eb2c9aa2a39dd0038616e9": {"paper_id": "73a58ebe0a18d1d9d4eb2c9aa2a39dd0038616e9", "abstract": "This paper aims to accelerate the test-time computation of deep convolutional neural networks (CNNs). Unlike existing methods that are designed for approximating linear filters or linear responses, our method takes the nonlinear units into account. We minimize the reconstruction error of the nonlinear responses, subject to a low-rank constraint which helps to reduce the complexity of filters. We develop an effective solution to this constrained nonlinear optimization problem. An algorithm is also presented for reducing the accumulated error when multiple layers are approximated. A whole-model speedup ratio of 4\u00d7 is demonstrated on a large network trained for ImageNet, while the top-5 error rate is only increased by 0.9%. Our accelerated model has a comparably fast speed as the \u201cAlexNet\u201d [11], but is 4.7% more accurate.", "title": "Efficient and accurate approximations of nonlinear convolutional networks"}, "2ffc74bec88d8762a613256589891ff323123e99": {"paper_id": "2ffc74bec88d8762a613256589891ff323123e99", "abstract": "Convolutional neural network (CNN) has been widely employed for image recognition because it can achieve high accuracy by emulating behavior of optic nerves in living creatures. Recently, rapid growth of modern applications based on deep learning algorithms has further improved research and implementations. Especially, various accelerators for deep CNN have been proposed based on FPGA platform because it has advantages of high performance, reconfigurability, and fast development round, etc. Although current FPGA accelerators have demonstrated better performance over generic processors, the accelerator design space has not been well exploited. One critical problem is that the computation throughput may not well match the memory bandwidth provided an FPGA platform. Consequently, existing approaches cannot achieve best performance due to under-utilization of either logic resource or memory bandwidth. At the same time, the increasing complexity and scalability of deep learning applications aggravate this problem. In order to overcome this problem, we propose an analytical design scheme using the roofline model. For any solution of a CNN design, we quantitatively analyze its computing throughput and required memory bandwidth using various optimization techniques, such as loop tiling and transformation. Then, with the help of rooine model, we can identify the solution with best performance and lowest FPGA resource requirement. As a case study, we implement a CNN accelerator on a VC707 FPGA board and compare it to previous approaches. Our implementation achieves a peak performance of 61.62 GFLOPS under 100MHz working frequency, which outperform previous approaches significantly.", "title": "Optimizing FPGA-based Accelerator Design for Deep Convolutional Neural Networks"}, "a7621b4ec18719b08f3a2a444b6d37a2e20227b7": {"paper_id": "a7621b4ec18719b08f3a2a444b6d37a2e20227b7", "abstract": "Convolutional networks are one of the most widely employed architectures in computer vision and machine learning. In order to leverage their ability to learn complex functions, large amounts of data are required for training. Training a large convolutional network to produce state-of-the-art results can take weeks, even when using modern GPUs. Producing labels using a trained network can also be costly when dealing with web-scale datasets. In this work, we present a simple algorithm which accelerates training and inference by a significant factor, and can yield improvements of over an order of magnitude compared to existing state-of-the-art implementations. This is done by computing convolutions as pointwise products in the Fourier domain while reusing the same transformed feature map many times. The algorithm is implemented on a GPU architecture and addresses a number of related challenges.", "title": "Fast Training of Convolutional Networks through FFTs"}, "69e5845c67f2390aa08899a7dbf8b6daee49313d": {"paper_id": "69e5845c67f2390aa08899a7dbf8b6daee49313d", "abstract": "Goal: Limited-channel EEG research in neonates is hindered by lack of open, accessible analytic tools. To overcome this limitation, we have created the Washington University- Neonatal EEG Analysis Toolbox (WU-NEAT), containing two of the most commonly used tools, provided in an open-source, clinically-validated package running within MATLAB. Methods: The first algorithm is the amplitude-integrated EEG (aEEG), which is generated by filtering, rectifying and time-compressing the original EEG recording, with subsequent semi-logarithmic display. The second algorithm is the spectral edge frequency (SEF), calculated as the critical frequency below which a user- defined proportion of the EEG spectral power is located. The aEEG algorithm was validated by three experienced reviewers. Reviewers evaluated aEEG recordings of fourteen preterm/term infants, displayed twice in random order, once using a reference algorithm and again using the WU-NEAT aEEG algorithm. Using standard methodology, reviewers assigned a background pattern classification. Inter/intra-rater reliability was assessed. For the SEF, calculations were made using the same fourteen recordings, first with the reference and then with the WU-NEAT algorithm. Results were compared using Pearson's correlation coefficient. Results: For the aEEG algorithm, intra- and inter-rater reliability was 100% and 98%, respectively. For the SEF, the mean (SD) Pearson correlation coefficient between algorithms was 0.96 (0.04). Conclusion: We have demonstrated a clinically-validated toolbox for generating the aEEG as well as calculating the SEF from EEG data. Open-source access will enable widespread use of common analytic algorithms which are device-independent and not subject to obsolescence, thereby facilitating future collaborative research in neonatal EEG.", "title": "WU-NEAT: A clinically validated, open- source MATLAB toolbox for limited-channel neonatal EEG analysis"}, "165002bc6d763cb4fe4d5db3cd59a0049711a9af": {"paper_id": "165002bc6d763cb4fe4d5db3cd59a0049711a9af", "abstract": "Monitoring-Oriented Programming (MOP1) [21, 18, 22, 19] is a formal framework for software development and analysis, in which the developer specifies desired properties using definable specification formalisms, along with code to execute when properties are violated or validated. The MOP framework automatically generates monitors from the specified properties and then integrates them together with the user-defined code into the original system.\n The previous design of MOP only allowed specifications without parameters, so it could not be used to state and monitor safety properties referring to two or more related objects. In this paper we propose a parametric specification formalism-independent extension of MOP, together with an implementation of JavaMOP that supports parameters. In our current implementation, parametric specifications are translated into AspectJ code and then weaved into the application using off-the-shelf AspectJ compilers; hence, MOP specifications can be seen as formal or logical aspects.\n Our JavaMOP implementation was extensively evaluated on two benchmarks, Dacapo [14] and Tracematches [8], showing that runtime verification in general and MOP in particular are feasible. In some of the examples, millions of monitor instances are generated, each observing a set of related objects. To keep the runtime overhead of monitoring and event observation low, we devised and implemented a decentralized indexing optimization. Less than 8% of the experiments showed more than 10% runtime overhead; in most cases our tool generates monitoring code as efficient as the hand-optimized code. Despite its genericity, JavaMOP is empirically shown to be more efficient than runtime verification systems specialized and optimized for particular specification formalisms. Many property violations were detected during our experiments; some of them are benign, others indicate defects in programs. Many of these are subtle and hard to find by ordinary testing.", "title": "Mop: an efficient and generic runtime verification framework"}, "d87f3e4aab7633814111a0837bf41141aeb2cebd": {"paper_id": "d87f3e4aab7633814111a0837bf41141aeb2cebd", "abstract": "Contingency planning is the first stage in developing a formal set of production planning and control activities for the reuse of products obtained via return flows in a closed-loop supply chain. The paper takes a contingency approach to explore the factors that impact production planning and control for closed-loop supply chains that incorporate product recovery. A series of three cases are presented, and a framework developed that shows the common activities required for all remanufacturing operations. To build on the similarities and illustrate and integrate the differences in closed-loop supply chains, Hayes and Wheelwright\u2019s product\u2013process matrix is used as a foundation to examine the three cases representing Remanufacture-to-Stock (RMTS), Reassemble-to-Order (RATO), and Remanufacture-to-Order (RMTO). These three cases offer end-points and an intermediate point for closed-loop supply operations. Since they represent different positions on the matrix, characteristics such as returns volume, timing, quality, product complexity, test and evaluation complexity, and remanufacturing complexity are explored. With a contingency theory for closed-loop supply chains that incorporate product recovery in place, past cases can now be reexamined and the potential for generalizability of the approach to similar types of other problems and applications can be assessed and determined. \u00a9 2002 Elsevier Science B.V. All rights reserved.", "title": "Building contingency planning for closed-loop supply chains with product recovery"}, "b52c5386f2d0a1adfb48fa209d86559b9f9a6a98": {"paper_id": "b52c5386f2d0a1adfb48fa209d86559b9f9a6a98", "abstract": "Modeling trajectory data is a building block for many smart-mobility initiatives. Existing approaches apply shallow models such as Markov chain and inverse reinforcement learning to model trajectories, which cannot capture the long-term dependencies. On the other hand, deep models such as Recurrent Neural Network (RNN) have demonstrated their strength of modeling variable length sequences. However, directly adopting RNN to model trajectories is not appropriate because of the unique topological constraints faced by trajectories. Motivated by these findings, we design two RNNbased models which can make full advantage of the strength of RNN to capture variable length sequence and meanwhile to address the constraints of topological structure on trajectory modeling. Our experimental study based on real taxi trajectory datasets shows that both of our approaches largely outperform the existing approaches.", "title": "Modeling Trajectories with Recurrent Neural Networks"}, "dfcae80f4d34ac09ba8063c5cfb5be954d0bf5f1": {"paper_id": "dfcae80f4d34ac09ba8063c5cfb5be954d0bf5f1", "abstract": "We develop a framework for learning generic, expressive image priors that capture the statistics of natural scenes and can be used for a variety of machine vision tasks. The approach provides a practical method for learning high-order Markov random field (MRF) models with potential functions that extend over large pixel neighborhoods. These clique potentials are modeled using the Product-of-Experts framework that uses non-linear functions of many linear filter responses. In contrast to previous MRF approaches all parameters, including the linear filters themselves, are learned from training data. We demonstrate the capabilities of this Field-of-Experts model with two example applications, image denoising and image inpainting, which are implemented using a simple, approximate inference scheme. While the model is trained on a generic image database and is not tuned toward a specific application, we obtain results that compete with specialized techniques.", "title": "Fields of Experts"}, "6f20506ce955b7f82f587a14301213c08e79463b": {"paper_id": "6f20506ce955b7f82f587a14301213c08e79463b", "abstract": null, "title": "Algorithms for Inverse Reinforcement Learning"}, "0c196122410cacc1509830e0dbae6a53b6bdfe78": {"paper_id": "0c196122410cacc1509830e0dbae6a53b6bdfe78", "abstract": "Inverse Reinforcement Learning (IRL) is the problem of learning the reward function underlying a Markov Decision Process given the dynamics of the system and the behaviour of an expert. IRL is motivated by situations where knowledge of the rewards is a goal by itself (as in preference elicitation) and by the task of apprenticeship learning (learning policies from an expert). In this paper we show how to combine prior knowledge and evidence from the expert\u2019s actions to derive a probability distribution over the space of reward functions. We present efficient algorithms that find solutions for the reward learning and apprenticeship learning tasks that generalize well over these distributions. Experimental results show strong improvement for our methods over previous heuristic-based approaches.", "title": "Bayesian Inverse Reinforcement Learning"}, "220bdd265e6721e1d7ec1c4252aa41825147e61b": {"paper_id": "220bdd265e6721e1d7ec1c4252aa41825147e61b", "abstract": "We consider learning in a Markov decision process where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform. This setting is useful in applications (such as the task of driving) where it may be difficult to write down an explicit reward function specifying exactly how different desiderata should be traded off. We think of the expert as trying to maximize a reward function that is expressible as a linear combination of known features, and give an algorithm for learning the task demonstrated by the expert. Our algorithm is based on using \"inverse reinforcement learning\" to try to recover the unknown reward function. We show that our algorithm terminates in a small number of iterations, and that even though we may never recover the expert's reward function, the policy output by the algorithm will attain performance close to that of the expert, where here performance is measured with respect to the expert's unknown reward function.", "title": "Apprenticeship learning via inverse reinforcement learning"}, "401b4aadb946a3d9a0e83e6b7db42eadcaa641c8": {"paper_id": "401b4aadb946a3d9a0e83e6b7db42eadcaa641c8", "abstract": "In this paper we propose a novel gradient algorithm to learn a policy from an expert\u2019s observed behavior assuming that the expert behaves optimally with respect to some unknown reward function of a Markovian Decision Problem. The algorithm\u2019s aim is to find a reward function such that the resulting optimal policy matches well the expert\u2019s observed behavior. The main difficulty is that the mapping from the parameters to policies is both nonsmooth and highly redundant. Resorting to subdifferentials solves the first difficulty, while the second one is overcome by computing natural gradients. We tested the proposed method in two artificial domains and found it to be more reliable and efficient than some previous methods.", "title": "Apprenticeship Learning using Inverse Reinforcement Learning and Gradient Methods"}, "08b67692bc037eada8d3d7ce76cc70994e7c8116": {"paper_id": "08b67692bc037eada8d3d7ce76cc70994e7c8116", "abstract": "Information theory provides a constructive criterion for setting up probability distributions on the basis of partial knowledge, and leads to a type of statistical inference which is called the maximum.entropy estimate. It is the least biased estimate possible on the given information; i.e., it is maximally noncom-mittal with regard to missing information. If one considers statistical mechanics as a form of statistical inference rather than as a physical theory, it is found that the usual computational rules, starting with the determination of the partition function, are an immediate consequence of the maximum-entropy principle. In the resulting \"subjective statistical mechanics,\" the usual rules are thus justified independently of any physical argument, and in particular independently of experimental verification; whether", "title": "Information Theory and Statistical Mechanics"}, "117a50fbdfd473e43e550c6103733e6cb4aecb4c": {"paper_id": "117a50fbdfd473e43e550c6103733e6cb4aecb4c", "abstract": "Imitation learning of sequential, goal-directed behavior by standard supervised techniques is often difficult. We frame learning such behaviors as a maximum margin structured prediction problem over a space of policies. In this approach, we learn mappings from features to cost so an optimal policy in an MDP with these cost mimics the expert's behavior. Further, we demonstrate a simple, provably efficient approach to structured maximum margin learning, based on the subgradient method, that leverages existing fast algorithms for inference. Although the technique is general, it is particularly relevant in problems where A* and dynamic programming approaches make learning policies tractable in problems beyond the limitations of a QP formulation. We demonstrate our approach applied to route planning for outdoor mobile robots, where the behavior a designer wishes a planner to execute is often clear, while specifying cost functions that engender this behavior is a much more difficult task.", "title": "Maximum margin planning"}, "4902805fe1e2f292f6beed7593154e686d7f6dc2": {"paper_id": "4902805fe1e2f292f6beed7593154e686d7f6dc2", "abstract": "Recognizing lines of unconstrained handwritten text is a challenging task. The difficulty of segmenting cursive or overlapping characters, combined with the need to exploit surrounding context, has led to low recognition rates for even the best current recognizers. Most recent progress in the field has been made either through improved preprocessing or through advances in language modeling. Relatively little work has been done on the basic recognition algorithms. Indeed, most systems rely on the same hidden Markov models that have been used for decades in speech and handwriting recognition, despite their well-known shortcomings. This paper proposes an alternative approach based on a novel type of recurrent neural network, specifically designed for sequence labeling tasks where the data is hard to segment and contains long-range bidirectional interdependencies. In experiments on two large unconstrained handwriting databases, our approach achieves word recognition accuracies of 79.7 percent on online data and 74.1 percent on offline data, significantly outperforming a state-of-the-art HMM-based system. In addition, we demonstrate the network's robustness to lexicon size, measure the individual influence of its hidden layers, and analyze its use of context. Last, we provide an in-depth discussion of the differences between the network and HMMs, suggesting reasons for the network's superior performance.", "title": "A Novel Connectionist System for Unconstrained Handwriting Recognition"}, "b118e5ca88647661f27438a76c324db5f04a1b21": {"paper_id": "b118e5ca88647661f27438a76c324db5f04a1b21", "abstract": "Preventing feature co-adaptation by encouraging independent contributions from different features often improves classification and regression performance. Dropout training (Hinton et al., 2012) does this by randomly dropping out (zeroing) hidden units and input features during training of neural networks. However, repeatedly sampling a random subset of input features makes training much slower. Based on an examination of the implied objective function of dropout training, we show how to do fast dropout training by sampling from or integrating a Gaussian approximation, instead of doing Monte Carlo optimization of this objective. This approximation, justified by the central limit theorem and empirical evidence, gives an order of magnitude speedup and more stability. We show how to do fast dropout training for classification, regression, and multilayer neural networks. Beyond dropout, our technique is extended to integrate out other types of noise and small image transformations.", "title": "Fast dropout training"}, "0894b06cff1cd0903574acaa7fcf071b144ae775": {"paper_id": "0894b06cff1cd0903574acaa7fcf071b144ae775", "abstract": "Recent work has shown success in using neural network language models (NNLMs) as features in MT systems. Here, we present a novel formulation for a neural network joint model (NNJM), which augments the NNLM with a source context window. Our model is purely lexicalized and can be integrated into any MT decoder. We also present several variations of the NNJM which provide significant additive improvements. Although the model is quite simple, it yields strong empirical results. On the NIST OpenMT12 Arabic-English condition, the NNJM features produce a gain of +3.0 BLEU on top of a powerful, featurerich baseline which already includes a target-only NNLM. The NNJM features also produce a gain of +6.3 BLEU on top of a simpler baseline equivalent to Chiang\u2019s (2007) original Hiero implementation. Additionally, we describe two novel techniques for overcoming the historically high cost of using NNLM-style models in MT decoding. These techniques speed up NNJM computation by a factor of 10,000x, making the model as fast as a standard back-off LM. This work was supported by DARPA/I2O Contract No. HR0011-12-C-0014 under the BOLT program (Approved for Public Release, Distribution Unlimited). The views, opinions, and/or findings contained in this article are those of the author and should not be interpreted as representing the official views or policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the Department of Defense.", "title": "Fast and Robust Neural Network Joint Models for Statistical Machine Translation"}, "965c9aec5e68d49142c5af6a9f0a984f6c2c743a": {"paper_id": "965c9aec5e68d49142c5af6a9f0a984f6c2c743a", "abstract": "We recently showed that Long Short-Term Memory (LSTM) recurrent neural networks (RNNs) outperform state-of-the-art deep neural networks (DNNs) for large scale acoustic modeling where the models were trained with the cross-entropy (CE) criterion. It has also been shown that sequence discriminative training of DNNs initially trained with the CE criterion gives significant improvements. In this paper, we investigate sequence discriminative training of LSTM RNNs in a large scale acoustic modeling task. We train the models in a distributed manner using asynchronous stochastic gradient descent optimization technique. We compare two sequence discriminative criteria \u2013 maximum mutual information and state-level minimum Bayes risk, and we investigate a number of variations of the basic training strategy to better understand issues raised by both the sequential model, and the objective function. We obtain significant gains over the CE trained LSTM RNN model using sequence discriminative training techniques.", "title": "Sequence discriminative distributed training of long short-term memory recurrent neural networks"}, "6e739fa7167b0661b86a26019ee011fd21bba4b3": {"paper_id": "6e739fa7167b0661b86a26019ee011fd21bba4b3", "abstract": "According to Earnshaw's theorem, the ratio between axial and radial stiffness is always -2 for pure permanent magnetic configurations with rotational symmetry. Using highly permeable material increases the force and stiffness of permanent magnetic bearings. However, the stiffness in the unstable direction increases more than the stiffness in the stable direction. This paper presents an analytical approach to calculating the axial force and the axial and radial stiffnesses of attractive passive magnetic bearings (PMBs) with back iron. The investigations are based on the method of image charges and show in which magnet geometries lead to reasonable axial to radial stiffness ratios. Furthermore, the magnet dimensions achieving maximum force and stiffness per magnet volume are outlined. Finally, the calculation method was applied to the PMB of a magnetically levitated fan, and the analytical results were compared with a finite element analysis.", "title": "Analytical Stiffness Calculation for Permanent Magnetic Bearings With Soft Magnetic Materials"}, "5bcc3db430dd110f6aa2cc1437f4c068b2e238b2": {"paper_id": "5bcc3db430dd110f6aa2cc1437f4c068b2e238b2", "abstract": "This paper presents a novel bidirectional current-fed dual inductor push-pull DC-DC converter with galvanic isolation. The converter features active voltage doubler rectifier, which is controlled by the switching sequence synchronous to that of the input-side switches. The control algorithm proposed enables full-soft-switching of all switches in a wide range of the input voltage and power without requirement of snubbers or resonant switching to be employed. Operation principle for the energy transfer in the both directions is described. Experimental results as well as basic design guidelines are presented.", "title": "Full soft-switching bidirectional isolated current-fed dual inductor push-pull DC-DC converter for battery energy storage applications"}, "cbcab9d6d375b19cd2c79dd6a5b79ffd89128b44": {"paper_id": "cbcab9d6d375b19cd2c79dd6a5b79ffd89128b44", "abstract": "Many real-world tasks require making decisions that involve multiple possibly conflicting objectives. To succeed in such tasks, intelligent systems need planning or learning algorithms that can e ciently find di\u21b5erent ways of balancing the trade-o\u21b5s that such objectives present. In this tutorial, we provide an introduction to decision-theoretic approaches to coping with multiple objectives. We first present an overview of multi-objective decision problems, with real-world examples. Then, we show that di\u21b5erent assumptions about these problems lead to di\u21b5erent solution concepts such as the convex hull and the Pareto front. Next, we provide an overview of state-of-the-art algorithms for tackling them, such as multi-objective variants of dynamic programming. Finally, we highlight some applications of multiobjective methods and discuss some of the most important open questions.", "title": "Multi-Objective Decision Making"}, "b6a37231a24677991a4e169b0e61600232368989": {"paper_id": "b6a37231a24677991a4e169b0e61600232368989", "abstract": "Exercise interventions in individuals with Parkinson's disease incorporate goal-based motor skill training to engage cognitive circuitry important in motor learning. With this exercise approach, physical therapy helps with learning through instruction and feedback (reinforcement) and encouragement to perform beyond self-perceived capability. Individuals with Parkinson's disease become more cognitively engaged with the practice and learning of movements and skills that were previously automatic and unconscious. Aerobic exercise, regarded as important for improvement of blood flow and facilitation of neuroplasticity in elderly people, might also have a role in improvement of behavioural function in individuals with Parkinson's disease. Exercises that incorporate goal-based training and aerobic activity have the potential to improve both cognitive and automatic components of motor control in individuals with mild to moderate disease through experience-dependent neuroplasticity. Basic research in animal models of Parkinson's disease is beginning to show exercise-induced neuroplastic effects at the level of synaptic connections and circuits.", "title": "Exercise-enhanced neuroplasticity targeting motor and cognitive circuitry in Parkinson's disease"}, "8b9f556e85889947c83cb89f6af89e94572822d9": {"paper_id": "8b9f556e85889947c83cb89f6af89e94572822d9", "abstract": "Recommender systems typically leverage two types of signals to effectively recommend items to users: user activities and content matching between user and item profiles, and recommendation models in literature are usually categorized into collaborative filtering models, content-based models and hybrid models. In practice, when rich profiles about users and items are available, and user activities are sparse (cold-start), effective content matching signals become much more important in the relevance of the recommendation. The de-facto method to measure similarity between two pieces of text is computing the cosine similarity of the two bags of words, and each word is weighted by TF (term frequency within the document) \u00d7 IDF (inverted document frequency of the word within the corpus). In general sense, TF can represent any local weighting scheme of the word within each document, and IDF can represent any global weighting scheme of the word across the corpus. In this paper, we focus on the latter, i.e., optimizing the global term weights, for a particular recommendation domain by leveraging supervised approaches. The intuition is that some frequent words (lower IDF, e.g. \u201cdatabase\u201d) can be essential and predictive for relevant recommendation, while some rare words (higher IDF, e.g. the name of a small company) could have less predictive power. Given plenty of observed activities between users and items as training data, we should be able to learn better domain-specific global term weights, which can further improve the relevance of recommendation. We propose a unified method that can simultaneously learn the weights of multiple content matching signals, as well as global term weights for specific recommendation tasks. Our method is efficient to handle large-scale training data \u2217This work was conducted during an internship at LinkedIn. Copyright is held by the International World Wide Web Conference Committee (IW3C2). IW3C2 reserves the right to provide a hyperlink to the author\u2019s site if the Material is used in electronic media. WWW 2016, April 11\u201315, 2016, Montr\u00e9al, Qu\u00e9bec, Canada. ACM 978-1-4503-4143-1/16/04. http://dx.doi.org/10.1145/2872427.2883069 . generated by production recommender systems. And experiments on LinkedIn job recommendation data justify the effectiveness of our approach.", "title": "Learning Global Term Weights for Content-based Recommender Systems"}, "178286f3640f9c5c8c129799d6b00f313481d13a": {"paper_id": "178286f3640f9c5c8c129799d6b00f313481d13a", "abstract": "This paper reports a controlled study with statistical signi cance tests on ve text categorization methods: the Support Vector Machines (SVM), a k-Nearest Neighbor (kNN) classi er, a neural network (NNet) approach, the Linear Leastsquares Fit (LLSF) mapping and a Naive Bayes (NB) classier. We focus on the robustness of these methods in dealing with a skewed category distribution, and their performance as function of the training-set category frequency. Our results show that SVM, kNN and LLSF signi cantly outperform NNet and NB when the number of positive training instances per category are small (less than ten), and that all the methods perform comparably when the categories are su ciently common (over 300 instances).", "title": "A Re-Examination of Text Categorization Methods"}, "73bc87477e45d49f7254fccd5f34d23b4ae5f254": {"paper_id": "73bc87477e45d49f7254fccd5f34d23b4ae5f254", "abstract": "This paper explores feature scoring and selection based on weights from linear classification models. It investigates how these methods combine with various learning models. Our comparative analysis includes three learning algorithms: Na\u00efve Bayes, Perceptron, and Support Vector Machines (SVM) in combination with three feature weighting methods: Odds Ratio, Information Gain, and weights from linear models, the linear SVM and Perceptron. Experiments show that feature selection using weights from linear SVMs yields better classification performance than other feature weighting methods when combined with the three explored learning algorithms. The results support the conjecture that it is the sophistication of the feature weighting method rather than its apparent compatibility with the learning algorithm that improves classification performance.", "title": "Feature selection using linear classifier weights: interaction with classification models"}, "3eae360c6ee52950f27f577aedd5f9934a04e137": {"paper_id": "3eae360c6ee52950f27f577aedd5f9934a04e137", "abstract": "This paper develops a general, formal framework for modeling term dependencies via Markov random fields. The model allows for arbitrary text features to be incorporated as evidence. In particular, we make use of features based on occurrences of single terms, ordered phrases, and unordered phrases. We explore full independence, sequential dependence, and full dependence variants of the model. A novel approach is developed to train the model that directly maximizes the mean average precision rather than maximizing the likelihood of the training data. Ad hoc retrieval experiments are presented on several newswire and web collections, including the GOV2 collection used at the TREC 2004 Terabyte Track. The results show significant improvements are possible by modeling dependencies, especially on the larger web collections.", "title": "A Markov random field model for term dependencies"}, "2538e3eb24d26f31482c479d95d2e26c0e79b990": {"paper_id": "2538e3eb24d26f31482c479d95d2e26c0e79b990", "abstract": "We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.", "title": "Natural Language Processing (almost) from Scratch"}, "5ad41f9ebbc6b6b02d61932a15ae254ba33a687f": {"paper_id": "5ad41f9ebbc6b6b02d61932a15ae254ba33a687f", "abstract": "All rights reserved. No part of this book may be reproduced in any form by any electronic or mechanical means (including photocopying, recording, or information storage and retrieval) without permission in writing from the publisher.", "title": "Introduction to Statistical Relational Learning (Adaptive Computation and Machine Learning)"}, "64c871cd7e0af7e1aeb94d98c6214eb8d8e8b989": {"paper_id": "64c871cd7e0af7e1aeb94d98c6214eb8d8e8b989", "abstract": "Many existing approaches to collaborative filtering can neither handle very large datasets nor easily deal with users who have very few ratings. In this paper we present the Probabilistic Matrix Factorization (PMF) model which scales linearly with the number of observations and, more importantly, performs well on the large, sparse, and very imbalanced Netflix dataset. We further extend the PMF model to include an adaptive prior on the model parameters and show how the model capacity can be controlled automatically. Finally, we introduce a constrained version of the PMF model that is based on the assumption that users who have rated similar sets of movies are likely to have similar preferences. The resulting model is able to generalize considerably better for users with very few ratings. When the predictions of multiple PMF models are linearly combined with the predictions of Restricted Boltzmann Machines models, we achieve an error rate of 0.8861, that is nearly 7% better than the score of Netflix\u2019s own system.", "title": "Probabilistic Matrix Factorization"}, "6cd49dd5d26d1e8e33891f8e64ad3b5012e90ba6": {"paper_id": "6cd49dd5d26d1e8e33891f8e64ad3b5012e90ba6", "abstract": "Maximum Margin Matrix Factorization (MMMF) was recently suggested (Srebro et al., 2005) as a convex, infinite dimensional alternative to low-rank approximations and standard factor models. MMMF can be formulated as a semi-definite programming (SDP) and learned using standard SDP solvers. However, current SDP solvers can only handle MMMF problems on matrices of dimensionality up to a few hundred. Here, we investigate a direct gradient-based optimization method for MMMF and demonstrate it on large collaborative prediction problems. We compare against results obtained by Marlin (2004) and find that MMMF substantially outperforms all nine methods he tested.", "title": "Fast maximum margin matrix factorization for collaborative prediction"}, "d85767a45fc84b83579701147cc7b47ec483dae1": {"paper_id": "d85767a45fc84b83579701147cc7b47ec483dae1", "abstract": "In this work we propose Pixel Content Encoders (PCE), a lightweight image inpainting model, capable of generating novel content for large missing regions in images. Unlike previously presented convolutional neural network based models, our PCE model has an order of magnitude fewer trainable parameters. Moreover, by incorporating dilated convolutions we are able to preserve fine grained spatial information, achieving state-of-the-art performance on benchmark datasets of natural images and paintings. Besides image inpainting, we show that without changing the architecture, PCE can be used for image extrapolation, generating novel content beyond existing image boundaries.", "title": "Light-weight pixel context encoders for image inpainting"}, "edd5771531fe1f29a2ac60d8b5388e2a50944453": {"paper_id": "edd5771531fe1f29a2ac60d8b5388e2a50944453", "abstract": "What can you do with a million images? In this paper, we present a new image completion algorithm powered by a huge database of photographs gathered from the Web. The algorithm patches up holes in images by finding similar image regions in the database that are not only seamless, but also semantically valid. Our chief insight is that while the space of images is effectively infinite, the space of semantically differentiable scenes is actually not that large. For many image completion tasks, we are able to find similar scenes which contain image fragments that will convincingly complete the image. Our algorithm is entirely data driven, requiring no annotations or labeling by the user. Unlike existing image completion methods, our algorithm can generate a diverse set of image completions and we allow users to select among them. We demonstrate the superiority of our algorithm over existing image completion approaches.", "title": "Scene completion using millions of photographs"}, "0823b293d13a5efaf9c3f37109a4a6018d05d074": {"paper_id": "0823b293d13a5efaf9c3f37109a4a6018d05d074", "abstract": "To describe the log-likelihood computation in our model, let us consider a two scale pyramid for the moment. Given a (vectorized) j \u00d7 j image I , denote by l = d(I) the coarsened image, and h = I \u2212 u(d(I)) to be the high pass. In this section, to simplify the computations, we use a slightly different u operator than the one used to generate the images displayed in Figure 3 of the paper. Namely, here we take d(I) to be the mean over each disjoint block of 2\u00d7 2 pixels, and take u to be the operator that removes the mean from each 2\u00d7 2 block. Since u has rank 3d/4, in this section, we write h in an orthonormal basis of the range of u, then the (linear) mapping from I to (l, h) is unitary. We now build a probability density p on Rd2 by p(I) = q0(l, h)q1(l) = q0(d(I), h(I))q1(d(I)); in a moment we will carefully define the functions qi. For now, suppose that qi \u2265 0, \u222b q1(l) dl = 1, and for each fixed l, \u222b q0(l, h) dh = 1. Then we can check that p has unit integral: \u222b", "title": "Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks"}, "6de2b1058c5b717878cce4e7e50d3a372cc4aaa6": {"paper_id": "6de2b1058c5b717878cce4e7e50d3a372cc4aaa6", "abstract": "We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1 2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.", "title": "Generative Adversarial Nets"}, "6d677914cb9c01a340faa0a76e90f30681529026": {"paper_id": "6d677914cb9c01a340faa0a76e90f30681529026", "abstract": "Abstract. Inspired by the recent work of Bertalmio et al. on digital inpaintings [SIGGRAPH 2000], we develop general mathematical models for local inpaintings of nontexture images. On smooth regions, inpaintings are connected to the harmonic and biharmonic extensions, and inpainting orders are analyzed. For inpaintings involving the recovery of edges, we study a variational model that is closely connected to the classical total variation (TV) denoising model of Rudin, Osher, and Fatemi [Phys. D, 60 (1992), pp. 259\u2013268]. Other models are also discussed based on the Mumford\u2013Shah regularity [Comm. Pure Appl. Math., XLII (1989), pp. 577\u2013685] and curvature driven diffusions (CDD) of Chan and Shen [J. Visual Comm. Image Rep., 12 (2001)]. The broad applications of the inpainting models are demonstrated through restoring scratched old photos, disocclusion in vision analysis, text removal, digital zooming, and edge-based image coding.", "title": "Mathematical Models for Local Nontexture Inpaintings"}, "5d73cb400be99ad966a87bee04a4d43954b4beb3": {"paper_id": "5d73cb400be99ad966a87bee04a4d43954b4beb3", "abstract": "This paper studies a combination of generative Markov random field (MRF) models and discriminatively trained deep convolutional neural networks (dCNNs) for synthesizing 2D images. The generative MRF acts on higher-levels of a dCNN feature pyramid, controlling the image layout at an abstract level. We apply the method to both photographic and non-photo-realistic (artwork) synthesis tasks. The MRF regularizer prevents over-excitation artifacts and reduces implausible feature mixtures common to previous dCNN inversion approaches, permitting synthesizing photographic content with increased visual plausibility. Unlike standard MRF-based texture synthesis, the combined system can both match and adapt local features with considerable variability, yielding results far out of reach of classic generative MRF methods.", "title": "Combining Markov Random Fields and Convolutional Neural Networks for Image Synthesis"}, "37bd01a032fa101e9ae38dc4379742f66883a7d7": {"paper_id": "37bd01a032fa101e9ae38dc4379742f66883a7d7", "abstract": "Discriminator architecture is shown in Figure 1 . In our setup the job of the discriminator is to analyze the local statistics of images. Therefore, after five convolutional layers with occasional stride we perform global average pooling. The result is processed by two fully connected layers, followed by a 2-way softmax. We perform 50% dropout after the global average pooling layer and the first fully connected layer.", "title": "Generating Images with Perceptual Similarity Metrics based on Deep Networks"}, "250b1eb62ef3188e7172b63b64b7c9b133b370f9": {"paper_id": "250b1eb62ef3188e7172b63b64b7c9b133b370f9", "abstract": "In this paper we address the problem of minimizing a large class of energy functions that occur in early vision. The major restriction is that the energy function\u2019s smoothness term must only involve pairs of pixels. We propose two algorithms that use graph cuts to compute a local minimum even when very large moves are allowed. The first move we consider is an \u03b1-\u03b2swap: for a pair of labels \u03b1, \u03b2, this move exchanges the labels between an arbitrary set of pixels labeled \u03b1 and another arbitrary set labeled \u03b2. Our first algorithm generates a labeling such that there is no swap move that decreases the energy. The second move we consider is an \u03b1-expansion: for a label \u03b1, this move assigns an arbitrary set of pixels the label \u03b1. Our second algorithm, which requires the smoothness term to be a metric, generates a labeling such that there is no expansion move that decreases the energy. Moreover, this solution is within a known factor of the global minimum. We experimentally demonstrate the effectiveness of our approach on image restoration, stereo and motion. 1 Energy minimization in early vision Many early vision problems require estimating some spatially varying quantity (such as intensity or disparity) from noisy measurements. Such quantities tend to be piecewise smooth; they vary smoothly at most points, but change dramatically at object boundaries. Every pixel p \u2208 P must be assigned a label in some set L; for motion or stereo, the labels are disparities, while for image restoration they represent intensities. The goal is to find a labeling f that assigns each pixel p \u2208 P a label fp \u2208 L, where f is both piecewise smooth and consistent with the observed data. These vision problems can be naturally formulated in terms of energy minimization. In this framework, one seeks the labeling f that minimizes the energy E(f) = Esmooth(f) + Edata(f). Here Esmooth measures the extent to which f is not piecewise smooth, while Edata measures the disagreement between f and the observed data. Many different energy functions have been proposed in the literature. The form of Edata is typically", "title": "Fast Approximate Energy Minimization via Graph Cuts"}, "371b240bebcaa68921aa87db4cd3a5d4e2a3a36b": {"paper_id": "371b240bebcaa68921aa87db4cd3a5d4e2a3a36b", "abstract": "It is a striking fact that in humans the greatest learning occurs precisely at that point in time--childhood--when the most dramatic maturational changes also occur. This report describes possible synergistic interactions between maturational change and the ability to learn a complex domain (language), as investigated in connectionist networks. The networks are trained to process complex sentences involving relative clauses, number agreement, and several types of verb argument structure. Training fails in the case of networks which are fully formed and 'adultlike' in their capacity. Training succeeds only when networks begin with limited working memory and gradually 'mature' to the adult state. This result suggests that rather than being a limitation, developmental restrictions on resources may constitute a necessary prerequisite for mastering certain complex domains. Specifically, successful learning may depend on starting small.", "title": "Learning and development in neural networks: the importance of starting small"}, "4fd69173cabb3d4377432d70488938ac533a5ac3": {"paper_id": "4fd69173cabb3d4377432d70488938ac533a5ac3", "abstract": "In order to simultaneously sharpen image details and attenuate noise, we propose to combine the recent blockmatching and 3D \u00deltering (BM3D) denoising approach, based on 3D transform-domain collaborative \u00deltering, with alpha-rooting, a transform-domain sharpening technique. The BM3D exploits grouping of similar image blocks into 3D arrays (groups) on which collaborative \u00deltering (by hard-thresholding) is applied. We propose two approaches of sharpening by alpha-rooting; the \u00derst applies alpharooting individually on the 2D transform spectra of each grouped block; the second applies alpha-rooting on the 3D-transform spectra of each 3D array in order to sharpen \u00dene image details shared by all grouped blocks and further enhance the interblock differences. The conducted experiments with the proposed method show that it can preserve and sharpen \u00dene image details and effectively attenuate noise.", "title": "JOINT IMAGE SHARPENING AND DENOISING BY 3 D TRANSFORM-DOMAIN COLLABORATIVE FILTERING"}, "9b2b657cbe08d67459c60cc2fbb9f21ed5a3800a": {"paper_id": "9b2b657cbe08d67459c60cc2fbb9f21ed5a3800a", "abstract": "Latent variable models are a powerful tool for addressing several tasks in machine learning. However, the algorithms for learning the parameters of latent variable models are prone to getting stuck in a bad local optimum. To alleviate this problem, we build on the intuition that, rather than considering all samples simultaneously, the algorithm should be presented with the training data in a meaningful order that facilitates learning. The order of the samples is determined by how easy they are. The main challenge is that typically we are not provided with a readily computable measure of the easiness of samples. We address this issue by proposing a novel, iterative self-paced learning algorithm where each iteration simultaneously selects easy samples and learns a new parameter vector. The number of samples selected is governed by a weight that is annealed until the entire training data has been considered. We empirically demonstrate that the self-paced learning algorithm outperforms the state of the art method for learning a latent structural SVM on four applications: object localization, noun phrase coreference, motif finding and handwritten digit recognition.", "title": "Self-Paced Learning for Latent Variable Models"}, "1523de0dd449961051d352349872a53b7436555e": {"paper_id": "1523de0dd449961051d352349872a53b7436555e", "abstract": "In order to work well, many computer vision algorithms require that their parameters be adjusted according to the image noise level, making it an important quantity to estimate. We show how to estimate an upper bound on the noise level from a single image based on a piecewise smooth image prior model and measured CCD camera response functions. We also learn the space of noise level functions how noise level changes with respect to brightness and use Bayesian MAP inference to infer the noise level function from a single image. We illustrate the utility of this noise estimation for two algorithms: edge detection and featurepreserving smoothing through bilateral filtering. For a variety of different noise levels, we obtain good results for both these algorithms with no user-specified inputs.", "title": "Noise Estimation from a Single Image"}, "2c03df8b48bf3fa39054345bafabfeff15bfd11d": {"paper_id": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.", "title": "Deep Residual Learning for Image Recognition"}, "eb04068416ade86de63cf9d9939e14d0bc9b96f9": {"paper_id": "eb04068416ade86de63cf9d9939e14d0bc9b96f9", "abstract": "Image restoration is a long-standing problem in low-level computer vision with many interesting applications. We describe a flexible learning framework based on the concept of nonlinear reaction diffusion models for various image restoration problems. By embodying recent improvements in nonlinear diffusion models, we propose a dynamic nonlinear reaction diffusion model with time-dependent parameters (i.e., linear filters and influence functions). In contrast to previous nonlinear diffusion models, all the parameters, including the filters and the influence functions, are simultaneously learned from training data through a loss based approach. We call this approach TNRD\u2014Trainable Nonlinear Reaction Diffusion. The TNRD approach is applicable for a variety of image restoration tasks by incorporating appropriate reaction force. We demonstrate its capabilities with three representative applications, Gaussian image denoising, single image super resolution and JPEG deblocking. Experiments show that our trained nonlinear diffusion models largely benefit from the training of the parameters and finally lead to the best reported performance on common test datasets for the tested applications. Our trained models preserve the structural simplicity of diffusion models and take only a small number of diffusion steps, thus are highly efficient. Moreover, they are also well-suited for parallel computation on GPUs, which makes the inference procedure extremely fast.", "title": "Trainable Nonlinear Reaction Diffusion: A Flexible Framework for Fast and Effective Image Restoration"}, "559302b64d41868ba7cca91eaedfdb3bfada8592": {"paper_id": "559302b64d41868ba7cca91eaedfdb3bfada8592", "abstract": "An automatic road sign recognition system first locates road signs within images captured by an imaging sensor on-board of a vehicle, and then identifies the detected road signs. This paper presents an automatic neural-network-based road sign recognition system. First, a study of the existing road sign recognition research is presented. In this study, the issues associated with automatic road sign recognition are described, the existing methods developed to tackle the road sign recognition problem are reviewed, and a comparison of the features of these methods is given. Second, the developed road sign recognition system is described. The system is capable of analysing live colour road scene images, detecting multiple road signs within each image, and classifying the type of road signs detected. The system consists of two modules: detection and classification. The detection module segments the input image in the hue-saturation-intensity colour space, and then detects road signs using a Multi-layer Perceptron neural-network. The classification module determines the type of detected road signs using a series of one to one architectural Multi-layer Perceptron neural networks. Two sets of classifiers are trained using the Resillient-Backpropagation and Scaled-Conjugate-Gradient algorithms. The two modules of the system are evaluated individually first. Then the system is tested as a whole. The experimental results demonstrate that the system is capable of achieving an average recognition hit-rate of 95.96% using the scaled-conjugate-gradient trained classifiers.", "title": "Detection and classification of road signs in natural environments"}, "92e4849c02ff5dd16ecdc87385b363741c8ac03a": {"paper_id": "92e4849c02ff5dd16ecdc87385b363741c8ac03a", "abstract": "In this paper we present a scalable hardware architecture to implement large-scale convolutional neural networks and state-of-the-art multi-layered artificial vision systems. This system is fully digital and is a modular vision engine with the goal of performing real-time detection, recognition and segmentation of mega-pixel images. We present a performance comparison between a software, FPGA and ASIC implementation that shows a speed up in custom hardware implementations.", "title": "Hardware accelerated convolutional neural networks for synthetic vision systems"}, "5677e80cd3f3924ff4bbade111d012c313b15d86": {"paper_id": "5677e80cd3f3924ff4bbade111d012c313b15d86", "abstract": "In this paper, we treat tracking as a learning problem of estimating the location and the scale of an object given its previous location, scale, as well as current and previous image frames. Given a set of examples, we train convolutional neural networks (CNNs) to perform the above estimation task. Different from other learning methods, the CNNs learn both spatial and temporal features jointly from image pairs of two adjacent frames. We introduce multiple path ways in CNN to better fuse local and global information. A creative shift-variant CNN architecture is designed so as to alleviate the drift problem when the distracting objects are similar to the target in cluttered environment. Furthermore, we employ CNNs to estimate the scale through the accurate localization of some key points. These techniques are object-independent so that the proposed method can be applied to track other types of object. The capability of the tracker of handling complex situations is demonstrated in many testing sequences.", "title": "Human Tracking Using Convolutional Neural Networks"}, "a71743e9ffa554c25b572c3bda9ac9676fca1061": {"paper_id": "a71743e9ffa554c25b572c3bda9ac9676fca1061", "abstract": "A vision-based vehicle guidance system for road vehicles can have three main roles: 1) road detection; 2) obstacle detection; and 3) sign recognition. The first two have been studied for many years and with many good results, but traffi c sign recognition is a less-studied field. Traffi c signs provide drivers with very valuable informatio n about the road, in order to make drivin g safer and easier. We think that traffi c signs must play the same role for autonomous vehicles. They are designed to be easily recognized by human driver s mainly because their color and shapes are very different from natural environments. The algorithm described in this paper takes advantage of these features. I t has two main parts. The first one, for the detection, uses color thresholding to segment the image and shape analysis to detect the signs. The second one, for the classification, uses a neural network. Some results from natural scenes are shown. On the other hand, the algorithm is valid to detect other kinds of marks that would tell the mobile robot to perform some task at that place.", "title": "Road traffic sign detection and classification"}, "a0d1f3078208fb101e66d54765f86aeb8d606678": {"paper_id": "a0d1f3078208fb101e66d54765f86aeb8d606678", "abstract": "The robust and general method for the recognition of traffic devices like road signs in traffic scene images is necessary for the creation of Driver Support System. Color may be used as a useful attribute for the decomposition of classification problem into several apriori defined road sign groups/subproblems. In this paper, the colorless method for the road sign classification is presented working on gray-level images and allowing the same problem decomposition as its color-based counterpart. The method may be used in combination with the color-independent sign detection algorithms. The road sign recognition system then works entirely without the color which may be used as an alternative procedure when the input traffic scene images lacks good color information.", "title": "Road Sign Classification without Color Information"}, "1b4fba54272e3d5431ba54ebc111247d111d2458": {"paper_id": "1b4fba54272e3d5431ba54ebc111247d111d2458", "abstract": "The posterior femoral cutaneous nerve (PFCN) is a branch of the sacral plexus. It needs to be implemented as a complementary block for anesthesia or in the surgeries necessitating tourniquet in the suitable cases. We consider target oriented block concept within the PFCN block in the anesthesia implementations with the emergence of ultrasonic regional anesthesia in the practice and with the better understanding of sonoanatomy.", "title": "Ultrasound guided posterior femoral cutaneous nerve block."}, "7c7463cfb5aca55ca1907faad46dc1022c77d86e": {"paper_id": "7c7463cfb5aca55ca1907faad46dc1022c77d86e", "abstract": "Outsourced storage has become more and more practical in recent years. Users can now store large amounts of data in multiple servers at a relatively low price. An important issue for outsourced storage systems is to design an efficient scheme to assure users that their data stored at remote servers has not been tampered with. This paper presents a general method and a practical prototype application for verifying the integrity of files in an untrusted network storage service. The verification process is managed by an application running in a trusted environment (typically on the client) that stores just one cryptographic hash value of constant size, corresponding to the \"digest\" of an authenticated data structure. The proposed integrity verification service can work with any storage service since it is transparent to the storage technology used. Experimental results show that our integrity verification method is efficient and practical for network storage systems.", "title": "Efficient integrity checking of untrusted network storage"}, "becddb754d41fa2055be5d50eac0cade5b4004ce": {"paper_id": "becddb754d41fa2055be5d50eac0cade5b4004ce", "abstract": "We give cryptographic schemes that help trace the source of leaks when sensitive or proprietary data is made available to a large set of parties. A very relevant application is in the context of pay television, where only paying customers should be able to view certain programs. In this application, the programs are normally encrypted, and then the sensitive data is the decryption keys that are given to paying customers. If a pirate decoder is found, it is desirable to reveal the source of its decryption keys. We describe fully resilient schemes which can be used against any decoder which decrypts with nonnegligible probability. Since there is typically little demand for decoders which decrypt only a small fraction of the transmissions (even if it is nonnegligible), we further introduce threshold tracing schemes which can only be used against decoders which succeed in decryption with probability greater than some threshold. Threshold schemes are considerably more efficient than fully resilient schemes.", "title": "Tracing traitors"}, "3b6911dc5d98faeb79d3d3e60bcdc40cfd7c9273": {"paper_id": "3b6911dc5d98faeb79d3d3e60bcdc40cfd7c9273", "abstract": "An aggregate signature scheme is a digital signature that supports aggregation: Given n signatures on n distinct messages from n distinct users, it is possible to aggregate all these signatures into a single short signature. This single signature (and the n original messages) will convince the verifier that the n users did indeed sign the n original messages (i.e., user i signed message Mi for i = 1, . . . , n). In this paper we introduce the concept of an aggregate signature, present security models for such signatures, and give several applications for aggregate signatures. We construct an efficient aggregate signature from a recent short signature scheme based on bilinear maps due to Boneh, Lynn, and Shacham. Aggregate signatures are useful for reducing the size of certificate chains (by aggregating all signatures in the chain) and for reducing message size in secure routing protocols such as SBGP. We also show that aggregate signatures give rise to verifiably encrypted signatures. Such signatures enable the verifier to test that a given ciphertext C is the encryption of a signature on a given message M . Verifiably encrypted signatures are used in contract-signing protocols. Finally, we show that similar ideas can be used to extend the short signature scheme to give simple ring signatures.", "title": "Aggregate and Verifiably Encrypted Signatures from Bilinear Maps"}, "517f519b8dbc5b00ff8b1f8578b73a871a1a0b73": {"paper_id": "517f519b8dbc5b00ff8b1f8578b73a871a1a0b73", "abstract": "We describe an RSA-based signing scheme which combines essentially optimal e ciency with attractive security properties. Signing takes one RSA decryption plus some hashing, veri cation takes one RSA encryption plus some hashing, and the size of the signature is the size of the modulus. Assuming the underlying hash functions are ideal, our schemes are not only provably secure, but are so in a tight way| an ability to forge signatures with a certain amount of computational resources implies the ability to invert RSA (on the same size modulus) with about the same computational e ort. Furthermore, we provide a second scheme which maintains all of the above features and in addition provides message recovery. These ideas extend to provide schemes for Rabin signatures with analogous properties; in particular their security can be tightly related to the hardness of factoring. Department of Computer Science and Engineering, Mail Code 0114, University of California at San Diego, 9500 Gilman Drive, La Jolla, CA 92093, USA. E-mail: mihir@cs.ucsd.edu ; Web page: http://www-cse.ucsd.edu/users/mihir y Department of Computer Science, University of California at Davis, Davis, CA 95616, USA. Email: rogaway@cs.ucdavis.edu ; Web page: http://wwwcsif.cs.ucdavis.edu/~rogaway/homepage.html", "title": "The Exact Security of Digital Signatures - HOw to Sign with RSA and Rabin"}, "446961b27f6c14413ae6cc2f78ad7d7c53ede26c": {"paper_id": "446961b27f6c14413ae6cc2f78ad7d7c53ede26c", "abstract": "In this paper, we define and explore proofs of retrievability (PORs). A POR scheme enables an archive or back-up service (prover) to produce a concise proof that a user (verifier) can retrieve a target file F, that is, that the archive retains and reliably transmits file data sufficient for the user to recover F in its entirety.\n A POR may be viewed as a kind of cryptographic proof of knowledge (POK), but one specially designed to handle a large file (or bitstring) F. We explore POR protocols here in which the communication costs, number of memory accesses for the prover, and storage requirements of the user (verifier) are small parameters essentially independent of the length of F. In addition to proposing new, practical POR constructions, we explore implementation considerations and optimizations that bear on previously explored, related schemes.\n In a POR, unlike a POK, neither the prover nor the verifier need actually have knowledge of F. PORs give rise to a new and unusual security definition whose formulation is another contribution of our work.\n We view PORs as an important tool for semi-trusted online archives. Existing cryptographic techniques help users ensure the privacy and integrity of files they retrieve. It is also natural, however, for users to want to verify that archives do not delete or modify files prior to retrieval. The goal of a POR is to accomplish these checks without users having to download the files themselves. A POR can also provide quality-of-service guarantees, i.e., show that a file is retrievable within a certain time bound.", "title": "Pors: proofs of retrievability for large files"}, "da8710aff14eafd02c20f5a6c0b1287eb347bba9": {"paper_id": "da8710aff14eafd02c20f5a6c0b1287eb347bba9", "abstract": "Programmable Logic Controllers (PLCs) are the most important components embedded in Industrial Control Systems (ICSs). ICSs have achieved highest standards in terms of efficiency and performance. As a result of that, higher portion of infrastructure in industries has been automated for the comfort of human beings. Therefore, protection of such systems is crucial. It is important to investigate the vulnerabilities of ICSs in order to solve the threats and attacks against critical infrastructure to protect human lives and assets. PLC is the basic building block of an ICS. If PLCs are exploited, overall system will be exposed to the threat. Many believed that PLCs are secured devices due to its isolation from the external networks of the system. The attacks such as Stuxnet have proven the incorrectness of such thoughts. In this paper we have revealed the vulnerabilities of PLCs through a variety of attack vectors which could affect the related critical infrastructure. Furthermore, we have proposed solutions for such weaknesses in PLC based systems.", "title": "PLC security and critical infrastructure protection"}, "bdf67ee2a13931ca2d5eac458714ed98148d1b34": {"paper_id": "bdf67ee2a13931ca2d5eac458714ed98148d1b34", "abstract": "A model of a real-time intrusion-detection expert system capable of detecting break-ins, penetrations, and other forms of computer abuse is described. The model is based on the hypothesis that security violations can be detected by monitoring a system's audit records for abnormal patterns of system usage. The model includes profiles for representing the behavior of subjects with respect to objects in terms of metrics and statistical models, and rules for acquiring knowledge about this behavior from audit records and for detecting anomalous behavior. The model is independent of any particular system, application environment, system vulnerability, or type of intrusion, thereby providing a framework for a general-purpose intrusion-detection expert system.", "title": "An Intrusion-Detection Model"}, "a7eb3cfc27ac190def5083e20ca7d0ac64f7a575": {"paper_id": "a7eb3cfc27ac190def5083e20ca7d0ac64f7a575", "abstract": "Process control and SCADA systems, with their reliance on proprietary networks and hardware, have long been considered immune to the network attacks that have wreaked so much havoc on corporate information systems. Unfortunately, new research indicates this complacency is misplaced \u2013 the move to open standards such as Ethernet, TCP/IP and web technologies is letting hackers take advantage of the control industry\u2019s ignorance. This paper summarizes the incident information collected in the BCIT Industrial Security Incident Database (ISID), describes a number of events that directly impacted process control systems and identifies the lessons that can be learned from these security events.", "title": "The Myths and Facts behind Cyber Security Risks for Industrial Control Systems"}, "11efa6998c2cfd3de59cf0ec0321a9e17418915d": {"paper_id": "11efa6998c2cfd3de59cf0ec0321a9e17418915d", "abstract": "Malware is notoriously difficult to combat because it appears and spreads so quickly. In this article, we describe the design and implementation of CWSandbox, a malware analysis tool that fulfills our three design criteria of automation, effectiveness, and correctness for the Win32 family of operating systems", "title": "Toward Automated Dynamic Malware Analysis Using CWSandbox"}, "ce89199008685660d96d75a13f4c9cf3a44c23bd": {"paper_id": "ce89199008685660d96d75a13f4c9cf3a44c23bd", "abstract": "The Modbus protocol and its variants are widely used in industrial control applications, especially for pipeline operations in the oil and gas sector. This paper describes the principal attacks on the Modbus Serial and Modbus TCP protocols and presents the corresponding attack taxonomies. The attacks are summarized according to their threat categories, targets and impact on control system assets. The attack taxonomies facilitate formal risk analysis efforts by clarifying the nature and scope of the security threats on Modbus control systems and networks. Also, they provide insights into potential mitigation strategies and the relative costs and benefits of implementing these strategies. c \u00a9 2008 Elsevier B.V. All rights reserved.", "title": "Attack taxonomies for the Modbus protocols"}, "2960c89331eb7afa86584792e2e11dbf6a125820": {"paper_id": "2960c89331eb7afa86584792e2e11dbf6a125820", "abstract": "We present the internals of QEMU, a fast machine emulator using an original portable dynamic translator. It emulates several CPUs (x86, PowerPC, ARM and Sparc) on several hosts (x86, PowerPC, ARM, Sparc, Alpha and MIPS). QEMU supports full system emulation in which a complete and unmodified operating system is run in a virtual machine and Linux user mode emulation where a Linux process compiled for one target CPU can be run on another CPU.", "title": "QEMU, a Fast and Portable Dynamic Translator"}, "a4a24d7d7bb3ec1c65809d56c4ce4563e0ca5c27": {"paper_id": "a4a24d7d7bb3ec1c65809d56c4ce4563e0ca5c27", "abstract": "We present an integrated methodology for detecting, segmenting and classifying breast masses from mammograms with minimal user intervention. This is a long standing problem due to low signal-to-noise ratio in the visualisation of breast masses, combined with their large variability in terms of shape, size, appearance and location. We break the problem down into three stages: mass detection, mass segmentation, and mass classification. For the detection, we propose a cascade of deep learning methods to select hypotheses that are refined based on Bayesian optimisation. For the segmentation, we propose the use of deep structured output learning that is subsequently refined by a level set method. Finally, for the classification, we propose the use of a deep learning classifier, which is pre-trained with a regression to hand-crafted feature values and fine-tuned based on the annotations of the breast mass classification dataset. We test our proposed system on the publicly available INbreast dataset and compare the results with the current state-of-the-art methodologies. This evaluation shows that our system detects 90% of masses at 1 false positive per image, has a segmentation accuracy of around 0.85 (Dice index) on the correctly detected masses, and overall classifies masses as malignant or benign with sensitivity (Se) of 0.98 and specificity (Sp) of 0.7.", "title": "A deep learning approach for the analysis of masses in mammograms with minimal user intervention"}, "759a3b3821d9f0e08e0b0a62c8b693230afc3f8d": {"paper_id": "759a3b3821d9f0e08e0b0a62c8b693230afc3f8d", "abstract": "We present two novel methods for face verification. Our first method - \u201cattribute\u201d classifiers - uses binary classifiers trained to recognize the presence or absence of describable aspects of visual appearance (e.g., gender, race, and age). Our second method - \u201csimile\u201d classifiers - removes the manual labeling required for attribute classification and instead learns the similarity of faces, or regions of faces, to specific reference people. Neither method requires costly, often brittle, alignment between image pairs; yet, both methods produce compact visual descriptions, and work on real-world images. Furthermore, both the attribute and simile classifiers improve on the current state-of-the-art for the LFW data set, reducing the error rates compared to the current best by 23.92% and 26.34%, respectively, and 31.68% when combined. For further testing across pose, illumination, and expression, we introduce a new data set - termed PubFig - of real-world images of public figures (celebrities and politicians) acquired from the internet. This data set is both larger (60,000 images) and deeper (300 images per individual) than existing data sets of its kind. Finally, we present an evaluation of human performance.", "title": "Attribute and simile classifiers for face verification"}, "1f88427d7aa8225e47f946ac41a0667d7b69ac52": {"paper_id": "1f88427d7aa8225e47f946ac41a0667d7b69ac52", "abstract": "In many recent object recognition systems, feature extraction stages are generally composed of a filter bank, a non-linear transformation, and some sort of feature pooling layer. Most systems use only one stage of feature extraction in which the filters are hard-wired, or two stages where the filters in one or both stages are learned in supervised or unsupervised mode. This paper addresses three questions: 1. How does the non-linearities that follow the filter banks influence the recognition accuracy? 2. does learning the filter banks in an unsupervised or supervised manner improve the performance over random filters or hardwired filters? 3. Is there any advantage to using an architecture with two stages of feature extraction, rather than one? We show that using non-linearities that include rectification and local contrast normalization is the single most important ingredient for good accuracy on object recognition benchmarks. We show that two stages of feature extraction yield better accuracy than one. Most surprisingly, we show that a two-stage system with random filters can yield almost 63% recognition rate on Caltech-101, provided that the proper non-linearities and pooling layers are used. Finally, we show that with supervised refinement, the system achieves state-of-the-art performance on NORB dataset (5.6%) and unsupervised pre-training followed by supervised refinement produces good accuracy on Caltech-101 (\u226b 65%), and the lowest known error rate on the undistorted, unprocessed MNIST dataset (0.53%).", "title": "What is the best multi-stage architecture for object recognition?"}, "31e362dee2355e9fef8b8b5dbb14dc74abebb80e": {"paper_id": "31e362dee2355e9fef8b8b5dbb14dc74abebb80e", "abstract": "We introduce a two-layer undirected graphical model, calle d a \u201cReplicated Softmax\u201d, that can be used to model and automatically extract low -dimensional latent semantic representations from a large unstructured collec ti n of documents. We present efficient learning and inference algorithms for thi s model, and show how a Monte-Carlo based method, Annealed Importance Sampling, c an be used to produce an accurate estimate of the log-probability the model a ssigns to test data. This allows us to demonstrate that the proposed model is able to g neralize much better compared to Latent Dirichlet Allocation in terms of b th the log-probability of held-out documents and the retrieval accuracy.", "title": "Replicated Softmax: an Undirected Topic Model"}, "497a80b2813cffb17f46af50e621a71505094528": {"paper_id": "497a80b2813cffb17f46af50e621a71505094528", "abstract": "We propose a non-linear generative model for human motion data that uses an undirected model with binary latent variables and real-valued \u201cvisible\u201d variables that represent joint angles. The latent and visible variables at each time step receive directed connections from the visible variables at the last few time-steps. Such an architecture makes on-line inference efficient and allows us to use a simple approximate learning procedure. After training, the model finds a single set of parameters that simultaneously capture several different kinds of motion. We demonstrate the power of our approach by synthesizing various motion sequences and by performing on-line filling in of data lost during motion capture. Website: http://www.cs.toronto.edu/ \u223cgwtaylor/publications/nips2006mhmublv/", "title": "Modeling Human Motion Using Binary Latent Variables"}, "4e9498322979ee4aa286b7aed222240d789efd02": {"paper_id": "4e9498322979ee4aa286b7aed222240d789efd02", "abstract": "A novel method for finding active contours, or snakes as developed by Xu and Prince [1] is presented in this paper. The approach uses a regularization based technique and calculus of variations to find what the authors call a Gradient Vector Field or GVF in binary-values or grayscale images. The GVF is in turn applied to \u2019pull\u2019 the snake towards the required feature. The approach presented here differs from other snake algorithms in its ability to extend into object concavities and its robust initialization technique. Although their algorithm works better than existing active contour algorithms, it suffers from computational complexity and associated costs in execution, resulting in slow execution time.", "title": "Snakes , Shapes and Gradient Vector Flow"}, "54205667c1f65a320f667d73c354ed8e86f1b9d9": {"paper_id": "54205667c1f65a320f667d73c354ed8e86f1b9d9", "abstract": "A constrained optimization type of numerical algorithm for removing noise from images is presented. The total variation of the image is minimized subject to constraints involving the statistics of the noise. The constraints are imposed using Lagrange multipliers. The solution is obtained using the gradient-projection method. This amounts to solving a time dependent partial differential equation on a manifold determined by the constraints. As t \u2192 \u221e the solution converges to a steady state which is the denoised image. The numerical algorithm is simple and relatively fast. The results appear to be state-of-the-art for very noisy images. The method is noninvasive, yielding sharp edges in the image. The technique could be interpreted as a first step of moving each level set of the image normal to itself with velocity equal to the curvature of the level set divided by the magnitude of the gradient of the image, and a second step which projects the image back onto the constraint set.", "title": "Nonlinear total variation based noise removal algorithms"}, "e7d53f538f5239739d1f943c81d17e4a167c65c6": {"paper_id": "e7d53f538f5239739d1f943c81d17e4a167c65c6", "abstract": "We discuss the video recommendation system in use at YouTube, the world's most popular online video community. The system recommends personalized sets of videos to users based on their activity on the site. We discuss some of the unique challenges that the system faces and how we address them. In addition, we provide details on the experimentation and evaluation framework used to test and tune new algorithms. We also present some of the findings from these experiments.", "title": "The YouTube video recommendation system"}, "92cc12f272ff55795c29cd97dc8ee17a5554308e": {"paper_id": "92cc12f272ff55795c29cd97dc8ee17a5554308e", "abstract": "The problem of recommending items from some fixed database has been studied extensively, and two main paradigms have emerged. In content-based recommendation one tries to recommend items similar to those a given user has liked in the past, whereas in collaborative recommendation one identifies users whose tastes are similar to those of the given user and recommends items they have liked. Our approach in Fab has been to combine these two methods. Here, we explain how a hybrid system can incorporate the advantages of both methods while inheriting the disadvantages of neither. In addition to what one might call the \u201cgeneric advantages\u201d inherent in any hybrid system, the particular design of the Fab architecture brings two additional benefits. First, two scaling problems common to all Web services are addressed\u2014an increasing number of users and an increasing number of documents. Second, the system automatically identifies emergent communities of interest in the user population, enabling enhanced group awareness and communications. Here we describe the two approaches for contentbased and collaborative recommendation, explain how a hybrid system can be created, and then describe Fab, an implementation of such a system. For more details on both the implemented architecture and the experimental design the reader is referred to [1]. The content-based approach to recommendation has its roots in the information retrieval (IR) community, and employs many of the same techniques. Text documents are recommended based on a comparison between their content and a user profile. Data", "title": "Content-Based, Collaborative Recommendation"}, "1f9ede76dbbd6caf7e3877918fae0d421c6f180c": {"paper_id": "1f9ede76dbbd6caf7e3877918fae0d421c6f180c", "abstract": null, "title": "Fast algorithms for mining association rules"}, "0fcc45600283abca12ea2f422e3fb2575f4c7fc0": {"paper_id": "0fcc45600283abca12ea2f422e3fb2575f4c7fc0", "abstract": "Collaborative ltering or recommender systems use a database about user preferences to predict additional topics or products a new user might like. In this paper we describe several algorithms designed for this task, including techniques based on correlation coefcients, vector-based similarity calculations, and statistical Bayesian methods. We compare the predictive accuracy of the various methods in a set of representative problem domains. We use two basic classes of evaluation metrics. The rst characterizes accuracy over a set of individual predictions in terms of average absolute deviation. The second estimates the utility of a ranked list of suggested items. This metric uses an estimate of the probability that a user will see a recommendation in an ordered list. Experiments were run for datasets associated with 3 application areas, 4 experimental protocols, and the 2 evaluation metrics for the various algorithms. Results indicate that for a wide range of conditions, Bayesian networks with decision trees at each node and correlation methods outperform Bayesian-clustering and vectorsimilarity methods. Between correlation and Bayesian networks, the preferred method depends on the nature of the dataset, nature of the application (ranked versus one-by-one presentation), and the availability of votes with which to make predictions. Other considerations include the size of database, speed of predictions, and learning time. Appears in Proceedings of the Fourteenth Conference on Uncertainty in Arti cial Intelligence, Madison, WI, July, 1998. Morgan Kaufmann Publisher.", "title": "Empirical Analysis of Predictive Algorithms for Collaborative Filtering"}, "d0859a9e421a5c2283ffdef5c394f23ef63ecffd": {"paper_id": "d0859a9e421a5c2283ffdef5c394f23ef63ecffd", "abstract": "Improvements have been made throughout the history of medicine, causing physicians to abandon a technique or medications clearly shown to be suboptimal. Unfortunately, this has not happened with rejuvenative surgery. Conventional lower eyelid procedures continue to include removal of orbital fat in most cases, and facelift procedures remain primarily a lateral vector pull. The unfortunate results of these traditional procedures are becoming easy to recognize. Optimal rejuvenation of the lower eyelid complex should be based on the principle that the contour changes characterizing aging involve not only prolapse of orbital fat, but also descent of the cheek tissues, resulting in accentuation of the orbital rim and tear trough groove. Although the necessity of preserving fat and repositioning the soft tissues of the midface has been widely accepted, there still is wide disagreement among authors as to the best approach and surgical technique. This report describes a surgical technique for lower lid midfacial rejuvenation that is a composite of several previously published approaches with some modifications, particularly in the way the Sub-Superficial Musculo Aponeurotic System (SMAS) fat pad is plicated and the midfacial tissues suspended. The technique is simple and safe, resulting in a pleasing natural midface contour.", "title": "Combined Arcus Marginalis Release, Preseptal Orbicularis Muscle Sling, and SOOF Plication for Midfacial Rejuvenation"}, "67f0dda6ac2c74b7a239aaa40de5c7e4bcf24ea1": {"paper_id": "67f0dda6ac2c74b7a239aaa40de5c7e4bcf24ea1", "abstract": "Dynamic Difficulty Adjustment (DDA) consists in an alternative to the static game balancing performed in game design. DDA is done during execution, tracking the player's performance and adjusting the game to present proper challenges to the player. This approach seems appropriate to increase the player entertainment, since it provides balanced challenges, avoiding boredom or frustration during the gameplay. This paper presents a mechanism to perform the dynamic difficulty adjustment during a game match. The idea is to dynamically change the game AI, adapting it to the player skills. We implemented three different AIs to match player behaviors: beginner, regular and experienced in the game Defense of the Ancient (DotA), a modification (MOD) of the game Warcraft III. We performed a series of experiments and, after comparing all results, the presented mechanism was able to keep up with the player's abilities on 85% of all experiments. The remaining 15% failed to suit the player's need because the adjustment did not occur on the right moment.", "title": "Dynamic Difficulty Adjustment through an Adaptive AI"}, "89243fee14b52128727a7ab18fb74b0b2a5967f3": {"paper_id": "89243fee14b52128727a7ab18fb74b0b2a5967f3", "abstract": "Emotions are action dispositions--states of vigilant readiness that vary widely in reported affect, physiology, and behavior. They are driven, however, by only 2 opponent motivational systems, appetitive and aversive--subcortical circuits that mediate reactions to primary reinforcers. Using a large emotional picture library, reliable affective psychophysiologies are shown, defined by the judged valence (appetitive/pleasant or aversive/unpleasant) and arousal of picture percepts. Picture-evoked affects also modulate responses to independently presented startle probe stimuli. In other words, they potentiate startle reflexes during unpleasant pictures and inhibit them during pleasant pictures, and both effects are augmented by high picture arousal. Implications are elucidated for research in basic emotions, psychopathology, and theories of orienting and defense. Conclusions highlight both the approach's constraints and promising paths for future study.", "title": "The emotion probe. Studies of motivation and attention."}, "281919e199e285c02bf92aab07d3d637d3c0b060": {"paper_id": "281919e199e285c02bf92aab07d3d637d3c0b060", "abstract": "This co-authored paper is based on research that originated in 2003 when our team started a series of extensive field studies into the character of gameplay experiences. Originally within the Children as the Actors of Game Cultures research project, our aim was to better understand why particularly young people enjoy playing games, while also asking their parents how they perceive gaming as playing partners or as close observers. Gradually our in-depth interviews started to reveal a complex picture of more general relevance, where personal experiences, social contexts and cultural practices all came together to frame gameplay within something we called game cultures. Culture was the keyword, since we were not interested in studying games and play experiences in isolation, but rather as part of the rich meaning-making practices of lived reality.", "title": "Fundamental Components of the Gameplay Experience: Analysing Immersion"}, "6703ffc5d34f0fd4c574a9a72bb16127237e83ed": {"paper_id": "6703ffc5d34f0fd4c574a9a72bb16127237e83ed", "abstract": "One of the aims of modern First-Person Shooter (FPS ) design is to provide an immersive experience to the player. This paper examines the role of sound in enabling s uch immersion and argues that, even in \u2018realism\u2019 FPS ga mes, it may be achieved sonically through a focus on carica ture rather than realism. The paper utilizes and develo ps previous work in which both a conceptual framework for the d sign and analysis of run and gun FPS sound is developed and the notion of the relationship between player and FPS soundscape as an acoustic ecology is put forward (G rimshaw and Schott 2007a; Grimshaw and Schott 2007b). Some problems of sound practice and sound reproduction i n the game are highlighted and a conceptual solution is p roposed.", "title": "Sound And Immersion In The First-Person Shooter"}, "8216673632b897ec50db06358b77f13ddd432c47": {"paper_id": "8216673632b897ec50db06358b77f13ddd432c47", "abstract": null, "title": "Guidelines for human electromyographic research."}, "188847872834a63fb435cf3a51eef72046464317": {"paper_id": "188847872834a63fb435cf3a51eef72046464317", "abstract": "This paper presents a systematic solution to the persistent problem of buffer overflow attacks. Buffer overflow attacks gained notoriety in 1988 as part of the Morris Worm incident on the Internet. While it is fairly simple to fix individual buffer overflow vulnerabilities, buffer overflow attacks continue to this day. Hundreds of attacks have been discovered, and while most of the obvious vulnerabilities have now been patched, more sophisticated buffer overflow attacks continue to emerge. We describe StackGuard: a simple compiler technique that virtually eliminates buffer overflow vulnerabilities with only modest performance penalties. Privileged programs that are recompiled with the StackGuard compiler extension no longer yield control to the attacker, but rather enter a fail-safe state. These programs require no source code changes at all, and are binary-compatible with existing operating systems and libraries. We describe the compiler technique (a simple patch to gcc), as well as a set of variations on the technique that tradeoff between penetration resistance and performance. We present experimental results of both the penetration resistance and the performance impact of this technique. This research is partially supported by DARPA contracts F3060296-1-0331 and F30602-96-1-0302. Ryerson Polytechnic University", "title": "StackGuard: Automatic Adaptive Detection and Prevention of Buffer-Overflow Attacks"}, "e2ce8da30737835941ed8f71a044c3c1ce7bb48a": {"paper_id": "e2ce8da30737835941ed8f71a044c3c1ce7bb48a", "abstract": "Organizations in virtually every industry are facing unprecedented pressures from many external forces. In an environment characterized by more regulatory mandates, more customer demands for better products and services, and an accelerated pace of technological change, some executive teams are turning to enterprise architecture (EA) to help their organizations better leverage their IT investments. The results of our study show there is a positive relationship between the stage of EA maturity and three areas of IT value: (1) ability to manage external relationships, (2) ability to lower operational costs, and (3) strategic agility. We also found positive relationships between EA maturity and improved business-IT alignment and risk management. Although these findings are based on responses from 140 CIOs working in a single industry that has been slower than others to leverage IT (U.S. hospitals), we believe they provide useful guidelines to help organizations in all industries increase the value from their IT investments.", "title": "The Role of Enterprise Architecture in the Quest for IT Value"}, "7957fad0ddbe7323da83d00b094caedb8eb1a473": {"paper_id": "7957fad0ddbe7323da83d00b094caedb8eb1a473", "abstract": "From half a million hectares at the turn of the century, Philippine mangroves have declined to only 120,000\u00a0ha while fish/shrimp culture ponds have increased to 232,000\u00a0ha. Mangrove replanting programs have thus been popular, from community initiatives (1930s\u20131950s) to government-sponsored projects (1970s) to large-scale international development assistance programs (1980s to present). Planting costs escalated from less than US$100 to over $500/ha, with half of the latter amount allocated to administration, supervision and project management. Despite heavy funds for massive rehabilitation of mangrove forests over the last two decades, the long-term survival rates of mangroves are generally low at 10\u201320%. Poor survival can be mainly traced to two factors: inappropriate species and site selection. The favored but unsuitable Rhizophora are planted in sandy substrates of exposed coastlines instead of the natural colonizers Avicennia and Sonneratia. More significantly, planting sites are generally in the lower intertidal to subtidal zones where mangroves do not thrive rather than the optimal middle to upper intertidal levels, for a simple reason. Such ideal sites have long been converted to brackishwater fishponds whereas the former are open access areas with no ownership problems. The issue of pond ownership may be complex and difficult, but such should not outweigh ecological requirements: mangroves should be planted where fishponds are, not on seagrass beds and tidal flats where they never existed. This paper reviews eight mangrove initiatives in the Philippines and evaluates the biophysical and institutional factors behind success or failure. The authors recommend specific protocols (among them pushing for a 4:1 mangrove to pond ratio recommended for a healthy ecosystem) and wider policy directions to make mangrove rehabilitation in the country more effective.", "title": "A review of mangrove rehabilitation in the Philippines: successes, failures and future prospects"}, "706e5a336d3e3caf5e3b3551ee3cadcbf5d7d999": {"paper_id": "706e5a336d3e3caf5e3b3551ee3cadcbf5d7d999", "abstract": "Design of permanent magnet rotor for LSPMSM with inherent starting torque is the prime consideration for the designer. Presence of short circuiting flux for any kind of the PMSM reduces the starting as well as the steady state performances by reducing the air gap flux density. A new Permanent Magnet rotor with high reluctance flux barrier has been proposed in this paper. The model has been developed and simulated using Maxwell 2D. Two dimensional Finite Element analysis was done for magnetostatics and the transient analysis. The experimental prototype has developed, tested and the results thus obtained have been validated with the simulated results.", "title": "Improvement of the performances of line start permanent magnet synchronous motor with flux barrier in the rotor"}, "a651f9901ce55418f95b579f97a316ba61a395cc": {"paper_id": "a651f9901ce55418f95b579f97a316ba61a395cc", "abstract": "Interior permanent magnet motors equipped with a squirrel-cage rotor are receiving lately an increased interest. Defined as line-start, line-fed or hybrid synchronous-induction motors, such machines combine the advantage of the brushless permanent magnet motors, i.e. high efficiency, constant torque for variable speed, with the high starting capability of the induction motors connected directly to the supply system. This paper proposes a unified analysis of these motors, with an emphasis on how any possible configuration may be described by using symmetrical components and two equivalent fictitious machines: positive and negative sequences. The analysis is validated on a single-phase unbalanced and on a three-phase balanced line-fed interior permanent magnet motors.", "title": "A Unified Approach to the Synchronous Performance Analysis of Single and Poly-Phase Line-Fed Interior Permanent Magnet Motors"}, "48ea25ff18f8bf1477bbfde70b8a4e97a5f05b6d": {"paper_id": "48ea25ff18f8bf1477bbfde70b8a4e97a5f05b6d", "abstract": "This paper presents a successful design of a high-efficiency small but novel interior permanent-magnet motor using NdBFe magnets. It is designed to operate both at line and variable frequencies. Line start with high inertia load was a special consideration. Time-stepping finite-element analysis has been used to successfully predict the dynamic and transient performances of the prototype motors. It has been found that the proposed design has yielded successful simulation and experimental results. The maximum load inertia corresponding to the rotor-bar depth has been determined from the simulation results.", "title": "High-efficiency line-start interior permanent-magnet synchronous motors"}, "71622e179271983ea66e8e00b0405be4e4de5b51": {"paper_id": "71622e179271983ea66e8e00b0405be4e4de5b51", "abstract": "Fractional-slot concentrated-winding (FSCW) synchronous permanent magnet (PM) machines have been gaining interest over the last few years. This is mainly due to the several advantages that this type of windings provides. These include high-power density, high efficiency, short end turns, high slot fill factor particularly when coupled with segmented stator structures, low cogging torque, flux-weakening capability, and fault tolerance. This paper is going to provide a thorough analysis of FSCW synchronous PM machines in terms of opportunities and challenges. This paper will cover the theory and design of FSCW synchronous PM machines, achieving high-power density, flux-weakening capability, comparison of single- versus double-layer windings, fault-tolerance rotor losses, parasitic effects, comparison of interior versus surface PM machines, and various types of machines. This paper will also provide a summary of the commercial applications that involve FSCW synchronous PM machines.", "title": "Fractional-Slot Concentrated-Windings Synchronous Permanent Magnet Machines: Opportunities and Challenges"}, "26d04cdc404b5ed310fe119efe295d217188fe85": {"paper_id": "26d04cdc404b5ed310fe119efe295d217188fe85", "abstract": "In this paper, a quasi-millimeter-wave wideband bandpass filter (BPF) is designed by using a microstrip dual-mode ring resonator and two folded half-wavelength resonators. Based on the transmission line equivalent circuit of the filter, variations of the frequency response of the filter versus the circuit parameters are investigated first by using the derived formulas and circuit simulators. Then a BPF with a 3dB fractional bandwidth (FBW) of 20% at 25.5 GHz is designed, which realizes the desired wide passband, sharp skirt property, and very wide stopband. Finally, the designed BPF is fabricated, and its measured frequency response is found agree well with the simulated result.", "title": "A compact quasi-millimeter-wave microstrip wideband bandpass filter"}, "49edf7f0dbad8b8c101af9ef95c72f62f545591e": {"paper_id": "49edf7f0dbad8b8c101af9ef95c72f62f545591e", "abstract": "Correlated topic modeling has been limited to small model and problem sizes due to their high computational cost and poor scaling. In this paper, we propose a new model which learns compact topic embeddings and captures topic correlations through the closeness between the topic vectors. Our method enables efficient inference in the low-dimensional embedding space, reducing previous cubic or quadratic time complexity to linear w.r.t the topic size. We further speedup variational inference with a fast sampler to exploit sparsity of topic occurrence. Extensive experiments show that our approach is capable of handling model and data scales which are several orders of magnitude larger than existing correlation results, without sacrificing modeling quality by providing competitive or superior performance in document classification and retrieval.", "title": "Efficient Correlated Topic Modeling with Topic Embedding"}, "58513e5043c8a8fb61dbe83ab58225e7f60575af": {"paper_id": "58513e5043c8a8fb61dbe83ab58225e7f60575af", "abstract": "The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.", "title": "Semi-Supervised Learning with Deep Generative Models"}, "0217fb2a54a4f324ddf82babc6ec6692a3f6194f": {"paper_id": "0217fb2a54a4f324ddf82babc6ec6692a3f6194f", "abstract": "This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound of the mutual information objective that can be optimized efficiently. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing supervised methods. For an up-to-date version of this paper, please see https://arxiv.org/abs/1606.03657.", "title": "InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets"}, "77c512cbb832436e1a35ad434e6bb3d763799763": {"paper_id": "77c512cbb832436e1a35ad434e6bb3d763799763", "abstract": "Werner Reichardt Centre for Integrative Neuroscience and Institute of Theoretical Physics, University of T\u00fcbingen, Germany Bernstein Center for Computational Neuroscience, T\u00fcbingen, Germany Graduate School for Neural Information Processing, T\u00fcbingen, Germany Max Planck Institute for Biological Cybernetics, T\u00fcbingen, Germany Department of Neuroscience, Baylor College of Medicine, Houston, TX, USA \u2217To whom correspondence should be addressed; E-mail: leon.gatys@bethgelab.org", "title": "A Neural Algorithm of Artistic Style"}, "24e2ac56c810f773bd4b2d03e7e9bc1a4519ed7a": {"paper_id": "24e2ac56c810f773bd4b2d03e7e9bc1a4519ed7a", "abstract": "Recommendation and review sites offer a wealth of information beyond ratings. For instance, on IMDb users leave reviews, commenting on different aspects of a movie (e.g. actors, plot, visual effects), and expressing their sentiments (positive or negative) on these aspects in their reviews. This suggests that uncovering aspects and sentiments will allow us to gain a better understanding of users, movies, and the process involved in generating ratings.\n The ability to answer questions such as \"Does this user care more about the plot or about the special effects?\" or \"What is the quality of the movie in terms of acting?\" helps us to understand why certain ratings are generated. This can be used to provide more meaningful recommendations.\n In this work we propose a probabilistic model based on collaborative filtering and topic modeling. It allows us to capture the interest distribution of users and the content distribution for movies; it provides a link between interest and relevance on a per-aspect basis and it allows us to differentiate between positive and negative sentiments on a per-aspect basis. Unlike prior work our approach is entirely unsupervised and does not require knowledge of the aspect specific ratings or genres for inference.\n We evaluate our model on a live copy crawled from IMDb. Our model offers superior performance by joint modeling. Moreover, we are able to address the cold start problem -- by utilizing the information inherent in reviews our model demonstrates improvement for new users and movies.", "title": "Jointly modeling aspects, ratings and sentiments for movie recommendation (JMARS)"}, "11de3f9770b08484b28597deb17714fb107caafe": {"paper_id": "11de3f9770b08484b28597deb17714fb107caafe", "abstract": "Several large scale data mining applications, such as text c ategorization and gene expression analysis, involve high-dimensional data that is also inherentl y directional in nature. Often such data is L2 normalized so that it lies on the surface of a unit hyperspher e. Popular models such as (mixtures of) multi-variate Gaussians are inadequate for characteri zing such data. This paper proposes a generative mixture-model approach to clustering directional data based on the von Mises-Fisher (vMF) distribution, which arises naturally for data distributed on the unit hypersphere. In particular, we derive and analyze two variants of the Expectation Maximiza tion (EM) framework for estimating the mean and concentration parameters of this mixture. Nume rical estimation of the concentration parameters is non-trivial in high dimensions since it i nvolves functional inversion of ratios of Bessel functions. We also formulate two clustering algorit hms corresponding to the variants of EM that we derive. Our approach provides a theoretical basis fo r the use of cosine similarity that has been widely employed by the information retrieval communit y, and obtains the spherical kmeans algorithm (kmeans with cosine similarity) as a special case of both variants. Empirical results on clustering of high-dimensional text and gene-expression d ata based on a mixture of vMF distributions show that the ability to estimate the concentration pa rameter for each vMF component, which is not present in existing approaches, yields superior resu lts, especially for difficult clustering tasks in high-dimensional spaces.", "title": "Clustering on the Unit Hypersphere using von Mises-Fisher Distributions"}, "9208ecbd7244040ba6ee59a067b527c8b095fe0a": {"paper_id": "9208ecbd7244040ba6ee59a067b527c8b095fe0a", "abstract": "Continuous space word embeddings learned from large, unstructured corpora have been shown to be effective at capturing semantic regularities in language. In this paper we replace LDA\u2019s parameterization of \u201ctopics\u201d as categorical distributions over opaque word types with multivariate Gaussian distributions on the embedding space. This encourages the model to group words that are a priori known to be semantically related into topics. To perform inference, we introduce a fast collapsed Gibbs sampling algorithm based on Cholesky decompositions of covariance matrices of the posterior predictive distributions. We further derive a scalable algorithm that draws samples from stale posterior predictive distributions and corrects them with a Metropolis\u2013Hastings step. Using vectors learned from a domain-general corpus (English Wikipedia), we report results on two document collections (20-newsgroups and NIPS). Qualitatively, Gaussian LDA infers different (but still very sensible) topics relative to standard LDA. Quantitatively, our technique outperforms existing models at dealing with OOV words in held-out documents.", "title": "Gaussian LDA for Topic Models with Word Embeddings"}, "18bbdf8dfd13c227d68f9630c89c6cc66be66c0f": {"paper_id": "18bbdf8dfd13c227d68f9630c89c6cc66be66c0f", "abstract": "We develop a nested hierarchical Dirichlet process (nHDP) for hierarchical topic modeling. The nHDP generalizes the nested Chinese restaurant process (nCRP) to allow each word to follow its own path to a topic node according to a per-document distribution over the paths on a shared tree. This alleviates the rigid, single-path formulation assumed by the nCRP, allowing documents to easily express complex thematic borrowings. We derive a stochastic variational inference algorithm for the model, which enables efficient inference for massive collections of text documents. We demonstrate our algorithm on 1.8 million documents from The New York Times and 2.7 million documents from Wikipedia.", "title": "Nested Hierarchical Dirichlet Processes"}, "243e7d9e326f06103a703cf951999a97b2e5ec49": {"paper_id": "243e7d9e326f06103a703cf951999a97b2e5ec49", "abstract": "This paper proposes a suite of models for clustering high-dimensional data on a unit sphere based on von Mises-Fisher (vMF) distribution and for discovering more intuitive clusters than existing approaches. The proposed models include a) A Bayesian formulation of vMF mixture that enables information sharing among clusters, b) a Hierarchical vMF mixture that provides multiscale shrinkage and tree structured view of the data and c) a Temporal vMF mixture that captures evolution of clusters in temporal data. For posterior inference, we develop fast variational methods as well as collapsed Gibbs sampling techniques for all three models. Our experiments on six datasets provide strong empirical support in favour of vMF based clustering models over other popular tools such as K-means, Multinomial Mixtures and Latent Dirichlet Allocation.", "title": "Von Mises-Fisher Clustering Models"}, "882b4fccf6faa8487561d4b729744a886da0affb": {"paper_id": "882b4fccf6faa8487561d4b729744a886da0affb", "abstract": "We present WHATSUP, a collaborative filtering system for disseminating news items in a large-scale dynamic setting with no central authority. WHATSUP constructs an implicit social network based on user profiles that express the opinions of users about the news items they receive (like-dislike). Users with similar tastes are clustered using a similarity metric reflecting long-standing and emerging (dis)interests. News items are disseminated through a novel heterogeneous gossip protocol that (1) biases the orientation of its targets towards those with similar interests, and (2) amplifies dissemination based on the level of interest in every news item. We report on an extensive evaluation of WHATSUP through (a) simulations, (b) a ModelNet emulation on a cluster, and (c) a PlanetLab deployment based on real datasets. We show that WHATSUP outperforms various alternatives in terms of accurate and complete delivery of relevant news items while preserving the fundamental advantages of standard gossip: namely, simplicity of deployment and robustness.", "title": "WHATSUP: A Decentralized Instant News Recommender"}, "3219f5733e87afbe1481f7f99c0d06c7c9752c31": {"paper_id": "3219f5733e87afbe1481f7f99c0d06c7c9752c31", "abstract": ".-0/01 2323-04657-98 :<;7:<=>-023: ? @ @ AB;C:<=!? =>DE:<=>DE/F? AG? 465IHJ4 1FK AE-F57L -M5 DE:<N /91 O -98P;Q=>-0/!R 4 D+SUT -0: =>1V=>R @78>1 W AE-02X1 Y#2Z? HJDE4 L[@78>1J5 T /\\=]8>-0/01 2QN 23-9465 ?^=>DE1 4 : 57T 8>DE4 L_?`AEDEO /0T :<=>1 23-98 D(4U=>-98!? /9=>DE1 4a? 4653=>R -9;b?^8>? /\\R DE-0OJD(4 LQK D(5 -0:P@78>-F? 5 :PT /0/0-0:P: DE4acdN<e]1 2323-98>/0-[4 1^K ? 5 ?F;7:0f gh4 =>R DE:`@6? @i-98FjdK -ZDE4JO -0:<=>DEL ?^=>-C:P-9O -98!? Ak=>-0/\\R 4 D(SUT -0:[Yl1 83? 46? AE;7m0DE4 L A(?^8>L -9Nn:P/F? A(-`@ T 8>/!R6? :P-M? 465 @78>-9Yo-\\8>-04 /0-M5 ? =!?QYo1 8p=>R -V@ T 8>@i1 :P-V1 Y @78>1J5 T /0DE4 L%T :P-\\YoT A 8>-0/01 2323-0465 ? =>DE1 4 :3=>1q/0T :<=>1 23-98>:0frg 4s@ ? 8PN =>DE/0T A(? 8Fj]K]? @ @ AB;t?u/01 AEAE-0/9=>DE1 4s1 Y ? AEL 1 8>DB=>R 23:Q:PT /\\Rv? :`=P8!? 5 DBN =>DE1 46? Aw5 ? =!?Q23DE4 D(4 L j 4 -F?^8>-0:<=PNn4 -0DEL R7Wi1 8p/91 AEA(? Wi1 8!?^=>DEO -_x AE=>-\\8>D(4 L j ? 465V5 DE23-04 :PDE1 46? AEDE= ;[8>-F5 T /9=>DE1 4V1 4[= K 1y5 DBz -98>-04U=d5 ? =!? :P-9=>:0fG{ R x 8>:<=.5 ?^=!?`:P-9= K ? :p57-98>DEO -F53Yl8>1 2|=>R -pK]-0W Nn@ T78>/\\R ? :PDE4 LM=P8!? 4 :>? /9N =>DE1 4}1 Y_?qA(? 8>L -~cdN /91 2323-98>/0-a/91 23@6? 4U;tK R -98>-F? :C=>R -u:P-0/91 465 5 ? =!?`:P-\\=.K ? : /01 A(AE-0/\\=>-F5IYl8>1 2\u0080\u007fI1^OJDE-F\u0081#-94 : 231^O7DE-p8>-9/01 2323-04 5 ?^N =>DE1 4~:PDB=>f \u0082 1 8y=>R -M-9\u00837@i-98>DE23-04U=!? A @ T 8>@i1 :PjiK -`5 DEOJD+57-[=>R -V8>-0/9N 1 2323-0465 ? =>DE1 4\u0084L -94 -98!?^=>D(1 4%@ 8>1U/0-9:P:MDE4U=>1I=>R78>-0-Q:PT W~@ 8>1U/0-9:P:P-0:h\u0085 8>-9@ 8>-0:P-04U=!?^=>D(1 4\u00861 Y DE4 @ T =[5 ? =!?Jj 4 -0DEL RJWi1 8>R 1U1J5uYl1 8>2Z? =>DE1 4#j ? 465 8>-9/01 2323-04 5 ?^=>D(1 4ZL -04 -98!? =>DE1 4 fG\u0087q-y5 -0OJDE:P57DEz -\\8>-04U= =>-0/!R 4 D(SUT -9: Yl1 8 5 DBz -98>-04U= :PT WI@ 8>1U/0-0:P:P-9: ? 4 5 ? @ @ AB;b=>R -0DB8./01 2_W DE46?^=>DE1 4 :.1 4 1 T 8`5 ?^=!? :P-9=>:_=>1a/01 23@6?^8>-QYo1 8_8>-0/01 2323-0465 ? =>DE1 4%SUT ? AEDB=h;\u0086? 465 @i-\\8PYo1 8>2Z? 4 /0f 1. INTRODUCTION { R A(? 8>L -0:<=CcGNn/01 2323-98>/0-I:PDB=>-0:Z1 z -98Z23DEA(AEDE1 4 :31 Y[@78>1J5 T /9=>: Yl1 8b:>? AEf}e]R 1U1 :PD(4 L\u0088? 231 4 L\u0084:P1\u00862Z? 4U;%1 @ =>DE1 4 :3DE:b/!R6? AEA(-94 L DE4 L Yl1 8V/01 4 :PT 23-98>:0f[,.-0/01 2323-9465 -\\8p:<;7:<=>-023:yR6?FO -M-023-98>L -05uDE4u8>-9N :P@i1 4 :P-3=>1I=>R D(:M@ 8>1 W AE-02 fC\u0089\u008a8>-9/01 2323-04 5 -98V:<;7:<=>-02\u008bYl1 8`? 4%cdN /91 2323-98>/0-V:PDB=>-V8>-0/0-0DEO -0:yDE4 Yo1 8>2Z? =>DE1 4aY+8>1 2\u008c?Z/91 4 :PT 23-98p? Wi1 T7= K R DE/\\Ra@78>1J5 T /\\=>:y:PR -MDE:.DE4J=>-\\8>-0:<=>-F5 DE4#j ? 465C8>-9/01 2323-04 5 : @78>1J57N T /9=>:V=>R6? =`? 8>-ZAEDEH -0AB;\u0086=>1 x =`R -98_4 -0-F57:0fu{ 1J5 ?F;Uj 8>-0/01 2323-9465 -\\8 :<;7:<=>-023:.?^8>-V5 -0@ A(1F;U-05C1 4IR7T 465J8>-F5 : 1 Y 5 DBz -98>-04U= :PDB=>-0:0j6:P-\\8>O7DE4 L 23DEAEAEDE1 4 :.1 Yw/01 4 :PT 23-98>:0f \u008d 4 -k1 Y =>R -]-F?^8>A(DE-0:<= ? 465V231 :<= :PT /0/0-9:P:<YoT AU8>-0/91 2323-04 5 -98#=>-0/\\R7N 4 1 AE1 L DE-0:pDE:M\u008e>\u008f \u0090o\u0090E\u0091 \u0092>\u008f \u0093<\u0091 \u0094\u0096\u0095o\u0097 \u0098]\u0099 \u0090 \u0094 \u0098\\\u0093\\\u0095o\u009aU\u009bu\u009c \u009dU\u009e j \u009d \u009f7j#\u009e^ Jj \u009e^\u00a1F\u00a2\u00a3f e]1 A(A(? Wi1 8!? N =>DEO x6AB=>-98>DE4 LVK]1 8>HJ:]WU;`W T DEA(5 DE4 L_?V5 ? =!? W ? :P-y1 Y @ 8>-9Yl-98>-04 /0-0:]Yl1 8 @78>1J5 T /9=>:pWU; /91 4 :PT 23-98>:0fy\u0089\u00a44 -9K\u00a5/01 4 :PT 23-98Fji\u00a6y-01 j DE:y2Z? =>/!R -F5 ? L ? D(4 :<=p=>R -V5 ? =!? W6? :P-M=>1357DE:P/01^O -98M\u009a \u0098\\\u0095(\u009b^\u00a7 \u0092>\u008f \u0093! \u0308>jiK R DE/!Ru? 8>-V1 =>R -98 /91 4 :PT 23-98>:VK R 1aR6?FO -ZR D(:<=>1 8>DE/F? AEAB;%R6? 5q:PDE23D(A(?^8`=!? :<=>-3=>1a\u00a6y-01 f Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. EC\u201900, October 17-20, 2000, Minneapolis, Minnesota. Copyright 2000 ACM 1-58113-272-7/00/0010 .. \u00a9 5.00 ad8>1J5 T /\\=>:p=>R6?^=p=>R -M4 -9D(L RJWi1 8>:pAEDEH -`? 8>-_=>R -04 8>-0/01 2323-04657-F5C=>1 \u00a6p-91 jG? :_R -ZK DEAEA]@78>1 W ? W AB;q? AE:P1aAED(H -Z=>R -02 fIe]1 AEA(? Wi1 8!? =>DEO -Cx ABN =>-98>DE4 LIR6? :VWi-0-94qO -98P;\u0084:PT /9/0-0:P:<YoT A]DE4\u0086Wi1 =>R\u00868>-0:P-F?^8>/\\Rq? 465~@ 8!? /\\N =>DE/0f%\u00aby1FK -0O -\\8Fjk=>R -98>-b8>-92Z? DE4\u0088D(23@i1 8P=!? 4U=M8>-0:P-F? 8>/!RvSJT -0:<=>DE1 4 : DE4 1^O -98>/01 23DE4 L`= K 1QYoT 465 ? 23-04U=!? A /!R6? A(AE-04 L -0:.Yo1 8p/01 AEA+? Wi1 8!?^=>D(O x6AB=>-98>DE4 L`8>-0/01 2323-04657-98 :<;7:<=>-023:0f { R x 8>:<=]/\\R ? AEAE-04 L -pDE:k=>1VDE23@ 8>1^O =>R -y:P/F? A(? W DEAEDB=h;Z1 Y#=>R -./91 ABN A(? Wi1 8!? =>DEO -Ix AE=>-\\8>D(4 Lq? A(L 1 8>DB=>R 23:0fs{ R -9:P-a? AEL 1 8>DB=>R 23:3?^8>-a? W A(=>1a:P-F?^8>/\\Rq=>-04 :_1 Y =>R 1 T :>? 4 5 :V1 Y.@i1 =>-94J=>D(? A]4 -9D(L RJWi1 8>:MD(4q8>-0? ABN =>DE23j6W T = =>R -M57-02Z? 4 5 : 1 Y 231J5 -\\8>4acGNn/01 2323-98>/0-[:<;7:<=>-023:.? 8>=>1u:P-F? 8>/!Rq=>-04 :`1 Y.23DEAEAED(1 4 :_1 Y.@i1 =>-04U=>D(? A]4 -0DEL RJWi1 8>:0f\u0086\u0082 T 8P=>R -\\8Fj -9\u00837DE:<=>DE4 Lq? AEL 1 8>DB=>R 23:3R6?FO -I@i-\\8PYo1 8>2Z? 4 /0-I@ 8>1 W AE-023:`K DB=>RsDE4657DBN OJD(5 T6? A /01 4 :PT 23-98>:dYo1 8 K R 1 2\u00a4=>R :PDB=>R6? : A(? 8>L -p? 231 T 4U=>:G1 Y DE47N Yo1 8>2Z? =>DE1 4 fk\u0082 1 8.DE4 :<=!? 4 /0j DBYG?`:PDB=>-[DE: T :PDE4 L_W 8>1FK :PDE4 L3@6?^=P=>-98>4 : ? : DE4 5 DE/F? =>DE1 4 : 1 Y @ 8>1J57T /9= @78>-9Yo-\\8>-04 /0j DB= 2Z?0;3R ?^O -y=>R 1 T :>? 465 : 1 Yd5 ? =!?M@i1 DE4U=>: Yo1 8yDB=>: 231 :<= O ? A(T ? W A(-[/9T :<=>1 23-\\8>:0f { R -0:P\u00ac AE1 4 L /0T :<=>1 23-98d8>1FK :P\u00ad :PAE1FKv5 1FK 4_=>R 4JT 2`Wi-\\8w1 Y 4 -0DEL R7Wi1 8>:d=>R6? =w/F? 4 Wi-[:P-F?^8>/\\R -F5I@i-98 :P-0/91 465 j YlT 8P=>R -98 8>-F5 T /0DE4 LQ:P/F? A(? W DEAEDB=h;Uf { R -y:P-0/01 465Z/\\R ? AEAE-04 L DE: =>1_DE23@ 8>1^O =>R -pSUT6? AEDB=h;Q1 Y =>R -y8>-0/\\N 1 2323-9465 ?^=>DE1 4 :.Yo1 8[=>R -`/91 4 :PT 23-98>:0f`e]1 4 :PT 23-98>: 4 -0-F5a8>-0/01 2QN 23-04 5 ? =>DE1 4 : =>R -9;b/F? 4I=P8>T :<= =>13R -0AE@I=>R -02Xx6465b@ 8>1J57T /9=>: =>R -\\; K DEAEAdA(DEH fygnYk?Z/91 4 :PT 23-98 =P8>T :<=>:p?38>-9/01 2323-04 5 -98 :<;7:<=>-02 j6@ T 8PN /!R6? :P-9:_?b@ 8>1J57T /9=Fj ? 4 5\u0084x64 5 : 1 T =VR -35 1U-0:[4 1 =VAEDEH -`=>R -Q@ 8>1J5JN T /\\=Fj6=>R -M/01 4 :PT 23-98.K DEAEAdWi-_T 4 AEDEH -0AB;I=>1ZT :P-V=>R -[8>-0/91 2323-04 5 -98 :<;7:<=>-02\u00ae? L ? DE4 fk,.-0/01 2323-04657-98 :<;7:<=>-023:0j7AEDEH 1 =>R -98 :P-F?^8>/\\RC:<;7:<N =>-023:0j R6?FO -k=hK 1.=h;7@i-0:w1 Y6/!R6?^8!? /9=>-98>DE:<=>DE/ -98P8>1 8>:0 \u0304 \u00b00\u0091 \u0090  \u03089\u0098]\u009a \u0098\u00a3\u009b \u0091 \u0094\u0096\u0095o\u0097 \u0098\\ \u0308>j K R DE/!R\u0084? 8>-`@78>1J5 T /9=>:y=>R ? =[?^8>-_4 1 =y8>-0/01 2323-9465 -05 j =>R 1 T L Ra=>R /01 4 :PT 23-\\8bK]1 T A+5\u00b1AEDEH -a=>R -92 j ? 4 5%\u00b00\u0091 \u0090  \u03089\u0098327\u008f  \u0308!\u0095l\u0094\u0096\u0095o\u0097 \u0098\\ \u0308>jyK R D(/!R3? 8>@ 8>1J57T /9=>:k=>R6? = ? 8>-y8>-9/01 2323-04 5 -F5 j =>R 1 T L Rb=>R -p/01 4 :PT 23-\\8 5 1U-9: 4 1 =.A(DEH -[=>R -02 f gh4C=>R -VcGNn/01 2323-98>/0-[571 2Z? D(4C=>R -[231 :<= DE23@i1 8PN =!? 4U= -98P8>1 8>:G=>1 ?^O 1 D(5Q? 8>Yo? AE:P@i1 :PDB=>DEO -0:0jU:PDE4 /9=>R -0:P-98P8>1 8>: K DEAEA AE-F? 5 =>1b? 4 L 8P;a/91 4 :PT 23-98>:0j ? 465 :PDE4 /0-M=>R -98>-M?^8>-_T :PT6? AEAB;a2Z? 4J; @ 8>1J57T /9=>:V1 4%? 4\u0088cdNn/01 2323-98>/9-Q:PDB=>-Z=>R ? =_?I/01 4 :PT 23-98VK D(AEAkA(DEH =>1V@ T78>/\\R ? :PjJ:P1V=>R -98>-.DE: 4 1[8>-F? :P1 4Z=>1[8>D(:PHQ8>-9/01 2323-04 5 DE4 Lp1 4 :PR K DEAEA 4 1 = AEDEH f gh4V:P1 23-]K ?0;7: =>R -9:P-]=hK]1y/!R6? A(AE-04 L -0: ?^8>DE4_/01 4  \u0301 D(/\\=Fj :PDE4 /9-]=>R AE-0:P:V=>D(23-Q? 4q? AEL 1 8>DB=>R 2\u03bc:P@i-04 5 :[:P-F?^8>/\\R DE4 L Yo1 8M4 -0DEL RJWi1 8>:0jw=>R 231 8>-V:P/F? A(? W AE-_DE=.K DEAEAdWij ? 4 5I=>R -[K 1 8>:P-`DB=>:ySJT ? AEDB=h;Uf.\u0082 1 8y=>R DE: 8>-F? :P1 4#j DB=bDE:ZD(23@i1 8P=!? 4U=3=>1\u0086=P8>-F?^=3=>R = K 1q/!R6? A(AE-04 L -0:b:PDE2`T AEN =!? 4 -01 T :PAB;s:P1\u0086=>R :P1 AET7=>D(1 4 :b5 DE:P/01^O -\\8>-F5\u00b1? 8>-aWi1 =>RsT :P-9YoT Ap? 465 @ 8!? /9=>DE/F? A\u0096f 1.1 Problem Statement gh4a=>R D(:y@6? @i-98Fj K -V8>-0:P-F? 8>/!R~=>R -0:P-V=hK 1b/!R6? AEAE-04 L -0: =>1 L -9=>R -98Fj WU;I:<=>T 57;7DE4 L34 -9K\u00a4? 465 -9\u00837DE:<=>DE4 Lb? AEL 1 8>DB=>R 23:y=>R ? =yR6?FO -[=>R -V@i1 N =>-04U=>D(? A =>1\u0088DE23@ 8>1^O -uWi1 =>R3:P/F? A(? W DEAEDB=h;*? 4653SUT6? AEDB=h;s1 YM8>-0/01 2QN 23-04 5 -98.:<;7:<=>-023:0f { R -98>-_R ? :.Wi-0-04aAEDB=P=>AE-MK]1 8>HI1 4a-9\u00837@i-98>DE23-04 N =!? A O ? AED(5 ?^=>D(1 4I1 Y 8>-0/01 2323-04657-98 :<;7:<=>-023: ? L ? DE4 :<=.?_:P-\\= 1 Y 8>-0? ABN K]1 8>A(5b5 ? =!? :P-9=>: K DB=>R3=>R 4 1 =!? W AE-p-9\u00837/0-0@7=>DE1 431 Yk\u009c \u00a1^\u00a2 \\fk\u007fI1 8>-y-9\u0083JN @i-\\8>D(23-94J=!? A O ? A(D(5 ? =>DE1 4 DE: 4 -9-F5 -F5C? L ? DE4 :<= 8>-F? AEN\u00a3K]1 8>A(5a5 ?^=!? :P-9=>:0j ? 465ZDB= D(:]D(23@i1 8P=!? 4U= =>R ? = =>R -0:P5 ? =!? :P-\\=>: D(4 /0AET657cdN /91 2323-98>/05 ? =!?Q? :.K]-0AEA ? :y/91 4U=>-04U=.5 ?^=!?7f { R -3Yo1U/0T :`1 Y =>R DE:_@ ? @i-98MDE:_= K 1 N\u00a3Yl1 A(5 fu\u0082wDB8>:<=FjdK]-b@78>1^O7D(57-Z? :<;7:<=>-02Z?^=>DE/ -9\u00837@i-98>DE23-04U=!? A -0O ? AET ? =>DE1 4v1 Y[57DBz -98>-04U=3=>-0/!R 4 D(SUT -9: Yl1 8_8>-0/01 2323-9465 -\\8V:<; :<=>-923:0jG? 4 5q:P-0/01 4 5 jdK]-b@78>-0:P-04U=M4 -9K|? ABN L 1 8>DB=>R 23:_=>R6? =Q? 8>@ ? 8P=>DE/0T A+?^8>AB;\u0088:PT DB=>-F5%Yo1 8Q:P@6? 8>:P5 ? =!?u:P-9=>:0j :PT /\\RI? : =>R 1 :P-p=>R6?^=.? 8>/91 23231 4bDE4 cdNn/01 2323-98>/9-p? @ @ AEDE/F?^=>D(1 4 : 1 Yd8>-0/01 2323-04657-98 =>-0/!R 4 1 AE1 L ;Uf { R -0:P-M? AEL 1 8>DE=>R 23: R6?FO -[/!R6? 8!? /9N =>-\\8>D(:<=>DE/0:Q=>R ? =`2Z? H -Z=>R -02 AEDEH -0AB;%=>1uWi-CY\u0096? :<=>-98ZDE4\u00881 4 AEDE4 -C@i-98PN Yl1 8>2Z? 4 /0=>R ? 4I2Z? 4U;Z@ 8>-0OJDE1 T :PAE;b:<=>T 5 DE-F5I? AEL 1 8>DB=>R 23:0j6? 465bK]:P-9-0Ht=>1\u0086D(4JO -9:<=>D(L ? =>-uR 1FK =>R -uSUT ? AEDB=h;t1 Y[=>R -0DB8b8>-9/01 2323-04 5 ?^N =>DE1 4 : /01 23@6?^8>-0:k=>1V1 =>R -98 ? AEL 1 8>DB=>R 23:]T 4 5 -98]5 DBz -98>-94J=]@ 8!? /9=>DE/F? A /9DE8>/9T 23:<=!? 4 /0-0:0f g 43@i-98PYo1 8>23D(4 L\u00881 T 8I-9\u00837@i-98>DE23-04U=!? AyO ? A(D(5 ? =>DE1 4 j K -\u0084T :P-u=hK]1 5 ? =!? :P-9=>:0f \u0082wDB8>:<=Fj K -[T :P-V5 ? =!?_Y+8>1 2 ?`A(? 8>L -McGNn/01 2323-98>/0/01 2QN @ ? 4U;Uj k\u0095o\u009aJ\u009bU\u0098\\\u0093P\u00a7 J\u0094 \u008f \u0093 27\u008f \u0093<\u0091 \u0094\u0096\u0095\u00a3\u008f \u009a  \u0308>fZ\u0082wDE4 L -\\8>R7T7=V:P-0AEAE:_?ZK D(5 -QO ? 8>DBN -\\=h;Q1 Y R -9=>-\\8>1 L -94 -01 T :]@78>1J5 T /9=>:0j 8!? 4 L DE4 L[DE43@78>D(/9Y+8>1 2X? 8>1 T 465 =>-94u5 1 AEA(? 8>: =>13:P-0O -\\8!? AwRJT 4 578>-F5b5 1 AEA+?^8>:0f 7-0/91 465 j K]-MT :P-M5 ? =!? Y+8>1 2 1 T78 1^K 4_8>-0/01 2323-9465 -\\8G:<", "title": "Analysis of recommendation algorithms for e-commerce"}, "599ebeef9c9d92224bc5969f3e8e8c45bff3b072": {"paper_id": "599ebeef9c9d92224bc5969f3e8e8c45bff3b072", "abstract": "The explosive growth of the world-wide-web and the emergence of e-commerce has led to the development of recommender systems---a personalized information filtering technology used to identify a set of items that will be of interest to a certain user. User-based collaborative filtering is the most successful technology for building recommender systems to date and is extensively used in many commercial recommender systems. Unfortunately, the computational complexity of these methods grows linearly with the number of customers, which in typical commercial applications can be several millions. To address these scalability concerns model-based recommendation techniques have been developed. These techniques analyze the user--item matrix to discover relations between the different items and use these relations to compute the list of recommendations.In this article, we present one such class of model-based recommendation algorithms that first determines the similarities between the various items and then uses them to identify the set of items to be recommended. The key steps in this class of algorithms are (i) the method used to compute the similarity between the items, and (ii) the method used to combine these similarities in order to compute the similarity between a basket of items and a candidate recommender item. Our experimental evaluation on eight real datasets shows that these item-based algorithms are up to two orders of magnitude faster than the traditional user-neighborhood based recommender systems and provide recommendations with comparable or better quality.", "title": "Item-based top-N recommendation algorithms"}, "2d08ee24619ef41164e076fee2b268009aedd0f0": {"paper_id": "2d08ee24619ef41164e076fee2b268009aedd0f0", "abstract": "With the huge amount of information available online, the World Wide Web is a fertile area for data mining research. The Web mining research is at the cross road of research from several research communities, such as database, information retrieval, and within AI, especially the sub-areas of machine learning and natural language processing. However, there is a lot of confusions when comparing research efforts from different point of views. In this paper, we survey the research in the area of Web mining, point out some confusions regarded the usage of the term Web mining and suggest three Web mining categories. Then we situate some of the research with respect to these three categories. We also explore the connection between the Web mining categories and the related agent paradigm. For the survey, we focus on representation issues, on the process, on the learning algorithm, and on the application of the recent works as the criteria. We conclude the paper with some research issues.", "title": "Web Mining Research: A Survey"}, "610c1a091ca16229c8a13984c4f15723178ec89c": {"paper_id": "610c1a091ca16229c8a13984c4f15723178ec89c", "abstract": "Clustering data generally involves some input parameters or heuristics that are usually unknown at the time they are needed. We discuss the general problem of parameters in clustering and present a new approach, TURN, based on boundary detection and apply it to the clustering of web log data. We also present the use of di erent lters on the web log data to focus the clustering results and discuss di erent coe\u00c6cients for de ning similarity in a non-Euclidean space.", "title": "A non-parametric approach to web log analysis"}, "5895fa34605757bb40a5a07bcd8a997290ad100e": {"paper_id": "5895fa34605757bb40a5a07bcd8a997290ad100e", "abstract": "Web traffic is increasingly trending towards mobile devices driving developers to tailor web content to small screens and customize web apps using mobile-only capabilities such as geo-location, accelerometers, offline storage, and camera features. Hybrid apps provide a cross-platform, device independent, means for developers to utilize these features. They work by wrapping web-based code, i.e., HTML5, CSS, and JavaScript, in thin native containers that expose device features. This design pattern encourages re-use of existing code, reduces development time, and leverages existing web development talent that doesn't depend on platform specific languages. Despite these advantages, the newness of hybrid apps raises new security challenges associated with integrating code designed for a web browser with features native to a mobile device. This paper explores these security concerns and defines three forms of attack that can specifically target and exploit hybrid apps connected to web services. Contributions of the paper include a high level process for discovering hybrid app attacks and vulnerabilities, definitions of emerging hybrid attack vectors, and a test bed platform for analyzing vulnerabilities. As an evaluation, hybrid attacks are analyzed in the test bed showing that it provides insight into vulnerabilities and helps assess risk.", "title": "A Testbed and Process for Analyzing Attack Vectors and Vulnerabilities in Hybrid Mobile Apps Connected to Restful Web Services"}, "0b437786ccb91292fa956266ce54c956268dbe59": {"paper_id": "0b437786ccb91292fa956266ce54c956268dbe59", "abstract": "We have been developing robotic exoskeletons to assist motion of physically weak persons such as elderly, disabled, and injured persons. The robotic exoskeleton is controlled basically based on the electromyogram (EMG) signals, since the EMG signals of human muscles are important signals to understand how the user intends to move. Even though the EMG signals contain very important information, however, it is not very easy to predict the user's upper-limb motion (elbow and shoulder motion) based on the EMG signals in real-time because of the difficulty in using the EMG signals as the controller input signals. In this paper, we propose a robotic exoskeleton for human upper-limb motion assist, a hierarchical neuro-fuzzy controller for the robotic exoskeleton, and its adaptation method.", "title": "Neuro-fuzzy control of a robotic exoskeleton with EMG signals"}, "8c366344669769983f0c238a5f0548cac2afcc43": {"paper_id": "8c366344669769983f0c238a5f0548cac2afcc43", "abstract": "This paper describes the work carried out on off-line paper based scribbles such that they can be incorporated into a sketch-based interface without forcing designers to change their natural drawing habits. In this work, the scribbled drawings are converted into a vectorial format which can be recognized by a CAD system. This is achieved by using pattern analysis techniques, namely the Gabor filter to simplify the scribbled drawing. Vector line are then extracted from the resulting drawing by means of Kalman filtering.", "title": "Scribbles to Vectors: Preparation of Scribble Drawings for CAD Interpretation"}, "14cbdd5a0b5ed72695b2b6665c1d07db0919f9ab": {"paper_id": "14cbdd5a0b5ed72695b2b6665c1d07db0919f9ab", "abstract": "This paper reports the results of a numerical comparison of two versions of the fuzzy c-means (FCM) clustering algorithms. In particular, we propose and exemplify an approximate fuzzy c-means (AFCM) implementation based upon replacing the necessary ``exact'' variates in the FCM equation with integer-valued or real-valued estimates. This approximation enables AFCM to exploit a lookup table approach for computing Euclidean distances and for exponentiation. The net effect of the proposed implementation is that CPU time during each iteration is reduced to approximately one sixth of the time required for a literal implementation of the algorithm, while apparently preserving the overall quality of terminal clusters produced. The two implementations are tested numerically on a nine-band digital image, and a pseudocode subroutine is given for the convenience of applications-oriented readers. Our results suggest that AFCM may be used to accelerate FCM processing whenever the feature space is comprised of tuples having a finite number of integer-valued coordinates.", "title": "Efficient Implementation of the Fuzzy c-Means Clustering Algorithms"}, "b5bf45c3ade41289a40f319c782bda2897c07bd7": {"paper_id": "b5bf45c3ade41289a40f319c782bda2897c07bd7", "abstract": "A CMOS transmit-receive (T/R) switch design for ultrawideband (UWB) wireless applications is presented. A shunt inductor is adopted in the T/R switch signal path to improve the insertion loss (IL) and work as electrostatic discharge (ESD) protection device. Two bypassing switches at the gate of the main switching transistors provide additional isolation. Control voltage as low as 1.2V is applied to handle the T/R switch operation. Implemented in UMC 0.13mum MMRF CMOS technology, the measured results show an IL of 0.78-0.99dB in the receive (RX) mode and an isolation from TX port to RX port higher than 30.8dB in the transmit (TX) mode in the 3.1-4.8GHz frequency range. The measured ESD rating is 3kV HBM. The T/R switch is designed for monolithic integration with UWB RF front-end building blocks. The on-chip inductor eliminates the need of an off-chip one for the wideband matching network", "title": "A 0.13/spl mu/m CMOS T/R switch design for ultrawideband wireless applications"}, "1d200f2f428598b79fc1df8da5576dcd35a00d8f": {"paper_id": "1d200f2f428598b79fc1df8da5576dcd35a00d8f", "abstract": "High voltage gain dc-dc converters are required in many industrial applications such as photovoltaic and fuel cell energy systems, high-intensity discharge lamp (HID), dc back-up energy systems, and electric vehicles. This paper presents a novel input-parallel output-series boost converter with dual coupled inductors and a voltage multiplier module. On the one hand, the primary windings of two coupled inductors are connected in parallel to share the input current and reduce the current ripple at the input. On the other hand, the proposed converter inherits the merits of interleaved series-connected output capacitors for high voltage gain, low output voltage ripple, and low switch voltage stress. Moreover, the secondary sides of two coupled inductors are connected in series to a regenerative capacitor by a diode for extending the voltage gain and balancing the primary-parallel currents. In addition, the active switches are turned on at zero current and the reverse recovery problem of diodes is alleviated by reasonable leakage inductances of the coupled inductors. Besides, the energy of leakage inductances can be recycled. A prototype circuit rated 500-W output power is implemented in the laboratory, and the experimental results shows satisfactory agreement with the theoretical analysis.", "title": "A High Gain Input-Parallel Output-Series DC/DC Converter With Dual Coupled Inductors"}, "6dc54f5b3bdabfbbae1a377e2becfa70adcc0803": {"paper_id": "6dc54f5b3bdabfbbae1a377e2becfa70adcc0803", "abstract": "Smart grids heavily depend on communication in order to coordinate the generation, distribution, and consumption of energy-even more so if distributed power plants based on renewable energies are taken into account. Given the variety of communication partners, a heterogeneous network infrastructure consisting of IP-based and suitable field-level networks is the most appropriate solution. This paper investigates such a two-tier infrastructure and possible field-level networks with particular attention to metering and supervisory control and data acquisition applications. For the problem of network integration, a combination of gateway and tunneling solutions is proposed which allows a semitransparent end-to-end connection between application servers and field nodes. The feasibility of the approach and implementation details are discussed at the example of powerline communication and IP-based networks investigated in the European research project on real-time energy management via powerlines and internet. Nevertheless, it is shown that the communication architecture is versatile enough to serve as a generic solution for smart grids.", "title": "End-to-End Communication Architecture for Smart Grids"}, "24ab79f38cb17dc0a62b98ed23122a4062f8f682": {"paper_id": "24ab79f38cb17dc0a62b98ed23122a4062f8f682", "abstract": "Converters for photovoltaic (PV) systems usually consist of two stages: a dc/dc booster and a pulsewidth modulated (PWM) inverter. This cascade of converters presents efficiency issues, interactions between its stages, and problems with the maximum power point tracking. Therefore, only part of the produced electrical energy is utilized. In this paper, the authors propose a single-phase H-bridge multilevel converter for PV systems governed by a new integrated fuzzy logic controller (FLC)/modulator. The novelties of the proposed system are the use of a fully FLC (not requiring any optimal PWM switching-angle generator and proportional-integral controller) and the use of an H-bridge power-sharing algorithm. Most of the required signal processing is performed by a mixed-mode field-programmable gate array, resulting in a fully integrated System-on-Chip controller. The general architecture of the system and its main performance in a large spectrum of practical situations are presented and discussed. The proposed system offers improved performance over two-level inverters, particularly at low-medium power.", "title": "A Multilevel Inverter for Photovoltaic Systems With Fuzzy Logic Control"}, "d326685222ae369483662ea1dfe0b711a9628cef": {"paper_id": "d326685222ae369483662ea1dfe0b711a9628cef", "abstract": "Renewable energy sources based on photovoltaic (PV) along with battery-based energy storage necessitate power conditioning to meet load requirements and/or be connected to the electrical grid. The power conditioning is achieved via a dc-dc converter and a DC-AC inverter stages to produce the desired AC source. This is also the case even when the load is of dc type, such as the typical portable electronic devices that require AC adaptors to be powered from the AC mains. The letter presents a hybrid PV-battery-powered dc bus system that eliminates the DC-AC conversion stage, resulting in lower cost and improved overall energy conversion efficiency. It is also shown experimentally that the switching ac adaptors associated with the various commonly used portable electronic devices can be reused with the proposed dc bus system. A novel high-gain hybrid boost-flyback converter is also introduced with several times higher voltage conversion ratio than the conventional boost converter topology. This arrangement results in higher DC bus levels and lower cable conduction losses. Moreover, the voltage stress on the hybrid boost-flyback converter power switch is within half the output voltage. Experimental results taken from a laboratory prototype are presented to confirm the effectiveness of the proposed converter/system.", "title": "Photovoltaic-Battery-Powered DC Bus System for Common Portable Electronic Devices"}, "c7dbac54ea2b0038f58305f7ad7ee95a84874567": {"paper_id": "c7dbac54ea2b0038f58305f7ad7ee95a84874567", "abstract": "In this paper, a novel high step-up converter is proposed for fuel-cell system applications. As an illustration, a two-phase version configuration is given for demonstration. First, an interleaved structure is adapted for reducing input and output ripples. Then, a C\u00bfuk-type converter is integrated to the first phase to achieve a much higher voltage conversion ratio and avoid operating at extreme duty ratio. In addition, additional capacitors are added as voltage dividers for the two phases for reducing the voltage stress of active switches and diodes, which enables one to adopt lower voltage rating devices to further reduce both switching and conduction losses. Furthermore, the corresponding model is also derived, and analysis of the steady-state characteristic is made to show the merits of the proposed converter. Finally, a 200-W rating prototype system is also constructed to verify the effectiveness of the proposed converter. It is seen that an efficiency of 93.3% can be achieved when the output power is 150-W and the output voltage is 200-V with 0.56 duty ratio.", "title": "A High-Efficiency High Step-Up Converter With Low Switch Voltage Stress for Fuel-Cell System Applications"}, "99814824bfb47340af720445efef0e1dc3b0e35c": {"paper_id": "99814824bfb47340af720445efef0e1dc3b0e35c", "abstract": "We analyzed heart rate variability (HRV) taken by ECG and photoplethysmography (PPG) to assess their agreement. We also analyzed the sensitivity and specificity of PPG to identify subjects with low HRV as an example of its potential use for clinical applications. The HRV parameters: mean heart rate (HR), amplitude, and ratio of heart rate oscillation (E\u2013I difference, E/I ratio), RMSSD, SDNN, and Power LF, were measured during 1-min deep breathing tests (DBT) in 343 individuals, followed by a 5-min short-term HRV (s-HRV), where the HRV parameters: HR, SD1, SD2, SDNN, Stress Index, Power HF, Power LF, Power VLF, and Total Power, were determined as well. Parameters were compared through correlation analysis and agreement analysis by Bland\u2013Altman plots. PPG derived parameters HR and SD2 in s-HRV showed better agreement than SD1, Power HF, and stress index, whereas in DBT HR, E/I ratio and SDNN were superior to Power LF and RMSSD. DBT yielded stronger agreement than s-HRV. A slight overestimation of PPG HRV over HCG HRV was found. HR, Total Power, and SD2 in the s-HRV, HR, Power LF, and SDNN in the DBT showed high sensitivity and specificity to detect individuals with poor HRV. Cutoff percentiles are given for the future development of PPG-based devices. HRV measured by PPG shows good agreement with ECG HRV when appropriate parameters are used, and PPG-based devices can be employed as an easy screening tool to detect individuals with poor HRV, especially in the 1-min DBT test.", "title": "Heart rate variability (HRV) in deep breathing tests and 5-min short-term recordings: agreement of ear photoplethysmography with ECG measurements, in 343 subjects"}, "fd6d5e9587791094fd759990b4898c745322d7e0": {"paper_id": "fd6d5e9587791094fd759990b4898c745322d7e0", "abstract": "Modeling of photovoltaic (PV) systems is essential for the designers of solar generation plants to do a yield analysis that accurately predicts the expected power output under changing environmental conditions. This paper presents a comparative analysis of PV module modeling methods based on the single-diode model with series and shunt resistances. Parameter estimation techniques within a modeling method are used to estimate the five unknown parameters in the single diode model. Two sets of estimated parameters were used to plot the I-V characteristics of two PV modules, i.e., SQ80 and KC200GT, for the different sets of modeling equations, which are classified into models 1 to 5 in this study. Each model is based on the different combinations of diode saturation current and photogenerated current plotted under varying irradiance and temperature. Modeling was done using MATLAB/Simulink software, and the results from each model were first verified for correctness against the results produced by their respective authors. Then, a comparison was made among the different models (models 1 to 5) with respect to experimentally measured and datasheet I-V curves. The resultant plots were used to draw conclusions on which combination of parameter estimation technique and modeling method best emulates the manufacturer specified characteristics.", "title": "Comparative Analysis of Different Single-Diode PV Modeling Methods"}, "12f9db9d7b63694a3ce60df9e9f01d8fc3208cba": {"paper_id": "12f9db9d7b63694a3ce60df9e9f01d8fc3208cba", "abstract": "This work presents the construction of a model for a PV panel using the single-diode five-parameters model, based exclusively on data-sheet parameters. The model takes into account the series and parallel (shunt) resistance of the panel. The equivalent circuit and the basic equations of the PV cell/panel in Standard Test Conditions (STC)1 are shown, as well as the parameters extraction from the data-sheet values. The temperature dependence of the cell dark saturation current is expressed with an alternative formula, which gives better correlation with the datasheet values of the power temperature dependence. Based on these equations, a PV panel model, which is able to predict the panel behavior in different temperature and irradiance conditions, is built and tested.", "title": "PV panel model based on datasheet values"}, "7352c3f955d4cc992342686dce4395d253df6443": {"paper_id": "7352c3f955d4cc992342686dce4395d253df6443", "abstract": "Neural networks, as powerful tools for data mining and knowledge engineering, can learn from data to build feature-based classifiers and nonlinear predictive models. Training neural networks involves the optimization of nonconvex objective functions, and usually, the learning process is costly and infeasible for applications associated with data streams. A possible, albeit counterintuitive, alternative is to randomly assign a subset of the networks\u2019 weights so that the resulting optimization task can be formulated as a linear least-squares problem. This methodology can be applied to both feedforward and recurrent networks, and similar techniques can be used to approximate kernel functions. Many experimental results indicate that such randomized models can reach sound performance compared to fully adaptable ones, with a number of favorable benefits, including (1) simplicity of implementation, (2) faster learning with less intervention from human beings, and (3) possibility of leveraging overall linear regression and classification algorithms (e.g., l1 norm minimization for obtaining sparse formulations). This class of neural networks attractive and valuable to the data mining community, particularly for handling large scale data mining in real-time. However, the literature in the field is extremely vast and fragmented, with many results being reintroduced multiple times under different names. This overview aims to provide a self-contained, uniform introduction to the different ways in which randomization can be applied to the design of neural networks and kernel functions. A clear exposition of the basic framework underlying all these approaches helps to clarify innovative lines of research, open problems, and most importantly, foster the exchanges of well-known results throughout different communities. \u00a9 2017 John Wiley & Sons, Ltd", "title": "Randomness in neural networks: an overview"}, "c892ac0384b8cf449c74c29ee8ecb30ede9ee852": {"paper_id": "c892ac0384b8cf449c74c29ee8ecb30ede9ee852", "abstract": "We present a method for learning nonlinear systems, echo state networks (ESNs). ESNs employ artificial recurrent neural networks in a way that has recently been proposed independently as a learning mechanism in biological brains. The learning method is computationally efficient and easy to use. On a benchmark task of predicting a chaotic time series, accuracy is improved by a factor of 2400 over previous techniques. The potential for engineering applications is illustrated by equalizing a communication channel, where the signal error rate is improved by two orders of magnitude.", "title": "Harnessing nonlinearity: predicting chaotic systems and saving energy in wireless communication."}, "6bdabcdcde21d4d71321935e2e0332e32eda5366": {"paper_id": "6bdabcdcde21d4d71321935e2e0332e32eda5366", "abstract": "How to efficiently train recurrent networks remains a challenging and active research topic. Most of the proposed training approaches are based on computational ways to efficiently obtain the gradient of the error function, and can be generally grouped into five major groups. In this study we present a derivation that unifies these approaches. We demonstrate that the approaches are only five different ways of solving a particular matrix equation. The second goal of this paper is develop a new algorithm based on the insights gained from the novel formulation. The new algorithm, which is based on approximating the error gradient, has lower computational complexity in computing the weight update than the competing techniques for most typical problems. In addition, it reaches the error minimum in a much smaller number of iterations. A desirable characteristic of recurrent network training algorithms is to be able to update the weights in an on-line fashion. We have also developed an on-line version of the proposed algorithm, that is based on updating the error gradient approximation in a recursive manner.", "title": "New results on recurrent network training: unifying the algorithms and accelerating convergence"}, "45499bae1643e5af75b7ce82378c993f818c9d44": {"paper_id": "45499bae1643e5af75b7ce82378c993f818c9d44", "abstract": "Estimating the flows of rivers can have significant economic impact, as this can help in agricultural water management and in protection from water shortages and possible flood damage. The first goal of this paper is to apply neural networks to the problem of forecasting the flow of the River Nile in Egypt. The second goal of the paper is to utilize the time series as a benchmark to compare between several neural-network forecasting methods.We compare between four different methods to preprocess the inputs and outputs, including a novel method proposed here based on the discrete Fourier series. We also compare between three different methods for the multistep ahead forecast problem: the direct method, the recursive method, and the recursive method trained using a backpropagation through time scheme. We also include a theoretical comparison between these three methods. The final comparison is between different methods to perform longer horizon forecast, and that includes ways to partition the problem into the several subproblems of forecasting K steps ahead.", "title": "A comparison between neural-network forecasting techniques-case study: river flow forecasting"}, "8da1dda34ecc96263102181448c94ec7d645d085": {"paper_id": "8da1dda34ecc96263102181448c94ec7d645d085", "abstract": "Abstr,,ct. In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set ofaffine functionals can uniformly approximate any continuous function of n real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single bidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.", "title": "Approximation by superpositions of a sigmoidal function"}, "5c905b298c074d17be3fdbc47705fe5cd0797c2e": {"paper_id": "5c905b298c074d17be3fdbc47705fe5cd0797c2e", "abstract": "We describe a generalization of the PAC learning model that is based on statistical decision theory. In this model the learner receives randomly drawn examples, each example consisting of an instance x E X and an outcome J E Y, and tries to find a decision rule h: X + A, where h E X, that specifies the appropriate action a E A to take for each instance x in order to minimize the expectation of a loss I( y, a). Here X, Y, and A are arbitrary sets. I is a real-valued function, and examples are generated according to an arbitrary joint distribution on Xx Y. Special cases include the problem of learning a function from X into Y, the problem of learning the conditional probability distribution on Y given X (regression), and the problem of learning a distribution on X (density estimation). We give theorems on the uniform convergence of empirical loss estimates to true expected loss rates for certain decision rule spaces 2, and show how this implies learnability with bounded sample size, disregarding computational complexity. As an application, we give distribution-independent upper bounds on the sample size needed for learning with feedforward neural networks, Our theorems use a generalized notion of VC dimension that applies to classes of real-valued functions, adapted from Vapnik and Pollard\u2019s work, and a notion of capacity and metric dimension for classes of functions that map into a bounded metric space. \u2018(\u20181 1992 Academic Press, Inc.", "title": "Decision Theoretic Generalizations of the PAC Model for Neural Net and Other Learning Applications"}, "82566f380f61e835292e483cda84eb3d22e32cd4": {"paper_id": "82566f380f61e835292e483cda84eb3d22e32cd4", "abstract": "-Taking advantage of techniques developed by Kolmogorov, we give a direct proof of the universal approximation capabilities of perceptron type networks with two hidden layers. From our proof we derive estimates of numbers of hidden units based on properties of the function being approximated and the accuracy of its approximation. Keywords--Feedforward neural networks, Multilayer perceptron type networks, Sigmoidal activation function, Approximations of continuous functions, Uniform approximation, Universal approximation capabilities, Estimates of number of hidden units, Modulus of continuity.", "title": "Kolmogorov's theorem and multilayer neural networks"}, "46a6e5dc1a9331ae1a3475df0cae957b58f95f36": {"paper_id": "46a6e5dc1a9331ae1a3475df0cae957b58f95f36", "abstract": "We present a new algorithm for finding low-complexity neural networks with high generalization capability. The algorithm searches for a flat minimum of the error function. A flat minimum is a large connected region in weight space where the error remains approximately constant. An MDL-based, Bayesian argument suggests that flat minima correspond to simple networks and low expected overfitting. The argument is based on a Gibbs algorithm variant and a novel way of splitting generalization error into underfitting and overfitting error. Unlike many previous approaches, ours does not require gaussian assumptions and does not depend on a good weight prior. Instead we have a prior over input output functions, thus taking into account net architecture and training set. Although our algorithm requires the computation of second-order derivatives, it has backpropagation's order of complexity. Automatically, it effectively prunes units, weights, and input lines. Various experiments with feedforward and recurrent nets are described. In an application to stock market prediction, flat minimum search outperforms conventional backprop, weight decay, and optimal brain surgeon/optimal brain damage.", "title": "Flat Minima"}, "eefcc7bcc05436dac9881acb4ff4e4a0b730e175": {"paper_id": "eefcc7bcc05436dac9881acb4ff4e4a0b730e175", "abstract": "We address image classification on a large-scale, i.e. when a large number of images and classes are involved. First, we study classification accuracy as a function of the image signature dimensionality and the training set size. We show experimentally that the larger the training set, the higher the impact of the dimensionality on the accuracy. In other words, high-dimensional signatures are important to obtain state-of-the-art results on large datasets. Second, we tackle the problem of data compression on very large signatures (on the order of 105 dimensions) using two lossy compression strategies: a dimensionality reduction technique known as the hash kernel and an encoding technique based on product quantizers. We explain how the gain in storage can be traded against a loss in accuracy and/or an increase in CPU cost. We report results on two large databases \u2014 ImageNet and a dataset of lM Flickr images \u2014 showing that we can reduce the storage of our signatures by a factor 64 to 128 with little loss in accuracy. Integrating the decompression in the classifier learning yields an efficient and scalable training algorithm. On ILSVRC2010 we report a 74.3% accuracy at top-5, which corresponds to a 2.5% absolute improvement with respect to the state-of-the-art. On a subset of 10K classes of ImageNet we report a top-1 accuracy of 16.7%, a relative improvement of 160% with respect to the state-of-the-art.", "title": "High-dimensional signature compression for large-scale image classification"}, "1224f4ed2a064a0a19ae8c7b9801064bda932c07": {"paper_id": "1224f4ed2a064a0a19ae8c7b9801064bda932c07", "abstract": "Gaussian mixture models are currently the dominant technique for modeling the emission distribution of hidden Markov models for speech recognition. We show that better phone recognition on the TIMIT dataset can be achieved by replacing Gaussian mixture models by deep neural networks that contain many layers of features and a very large number of parameters. These networks are first pre-trained as a multi-layer generative model of a window of spectral feature vectors without making use of any discriminative information. Once the generative pre-training has designed the features, we perform discriminative fine-tuning using backpropagation to adjust the features slightly to make them better at predicting a probability distribution over the states of monophone hidden Markov models.", "title": "Acoustic Modeling Using Deep Belief Networks"}, "1b65af0b2847cf6edb1461eda659f08be27bc76d": {"paper_id": "1b65af0b2847cf6edb1461eda659f08be27bc76d", "abstract": "We propose a new method for estimation in linear models. The 'lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients hat are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.", "title": "Regression Shrinkage and Selection via the Lasso"}, "1001c09821f6910b5b8038a3c5993456ba966946": {"paper_id": "1001c09821f6910b5b8038a3c5993456ba966946", "abstract": "The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a \u201cblack art\u201d requiring expert experience, rules of thumb, or sometimes bruteforce search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm\u2019s generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expertlevel performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.", "title": "Practical Bayesian Optimization of Machine Learning Algorithms"}, "749076c5d579ef469e65bff964a1f2bffe0cc202": {"paper_id": "749076c5d579ef469e65bff964a1f2bffe0cc202", "abstract": "We classify digits of real-world house numbers using convolutional neural networks (ConvNets). Con-vNets are hierarchical feature learning neural networks whose structure is biologically inspired. Unlike many popular vision approaches that are hand-designed, ConvNets can automatically learn a unique set of features optimized for a given task. We augmented the traditional ConvNet architecture by learning multi-stage features and by using Lp pooling and establish a new state-of-the-art of 95.10% accuracy on the SVHN dataset (48% error improvement). Furthermore, we analyze the benefits of different pooling methods and multi-stage features in ConvNets. The source code and a tutorial are available at eblearn.sf.net.", "title": "Convolutional neural networks applied to house numbers digit classification"}, "8db26a22942404bd435909a16bb3a50cd67b4318": {"paper_id": "8db26a22942404bd435909a16bb3a50cd67b4318", "abstract": "Stacked denoising autoencoders (SDAs) have been successfully used to learn new representations for domain adaptation. Recently, they have attained record accuracy on standard benchmark tasks of sentiment analysis across different text domains. SDAs learn robust data representations by reconstruction, recovering original features from data that are artificially corrupted with noise. In this paper, we propose marginalized SDA (mSDA) that addresses two crucial limitations of SDAs: high computational cost and lack of scalability to high-dimensional features. In contrast to SDAs, our approach of mSDA marginalizes noise and thus does not require stochastic gradient descent or other optimization algorithms to learn parameters \u2014 in fact, they are computed in closed-form. Consequently, mSDA, which can be implemented in only 20 lines of MATLAB, significantly speeds up SDAs by two orders of magnitude. Furthermore, the representations learnt by mSDA are as effective as the traditional SDAs, attaining almost identical accuracies in benchmark tasks.", "title": "Marginalized Denoising Autoencoders for Domain Adaptation"}, "e0d33260912c9c345f4c22398adceb6d4ded1eec": {"paper_id": "e0d33260912c9c345f4c22398adceb6d4ded1eec", "abstract": "The objective of this paper is to control the speed of Permanent Magnet Synchronous Motor (PMSM) over wide range of speed by consuming minimum time and low cost. Therefore, comparative performance analysis of PMSM on basis of speed regulation has been done in this study. Comparison of two control strategies i.e. Field oriented control (FOC) without sensor less Model Reference Adaptive System (MRAS) and FOC with sensor less MRAS has been carried out. Sensor less speed control of PMSM is achieved by using estimated speed deviation as feedback signal for the PI controller. Performance of the both control strategies has been evaluated in in MATLAB Simulink software. Simulation studies show the response of PMSM speed during various conditions of load and speed variations. Obtained results reveal that the proposed MRAS technique can effectively estimate the speed of rotor with high exactness and torque response is significantly quick as compared to the system without MRAS control system.", "title": "Performance analysis of PMSM drive based on FOC technique with and without MRAS method"}, "106ce02a1c651f3cf34a12eb9aa14ddd5e203090": {"paper_id": "106ce02a1c651f3cf34a12eb9aa14ddd5e203090", "abstract": "The doubling constant of a metric space(X; d) is the smallest value such that every ball inX can be covered by balls of half the radius. Thedoubling dimensionof X is then defined as dim(X) = log2 . A metric (or sequence of metrics) is calleddoublingprecisely when its doubling dimension is bounded. This is a robust class of metric spaces which contains many families of metrics that occur in applied settings. We give tight bounds for embedding doubling metrics into (low-dimensional) normed spaces. We consider both general doubling metrics, as well as more restricted families such as those arising from trees, from graphs excluding a fixed minor, and from snowflaked metrics. Our techniques include decomposition theorems for doubling metrics, and an analysis of a fractal in the plane due to Laakso [21]. Finally, we discuss some applications and point out a central open question regarding dimensionality reduction in L2.", "title": "Bounded Geometries, Fractals, and Low-Distortion Embeddings"}, "81a5f154d379cfd608a87ccd8f02ff865b5b70c7": {"paper_id": "81a5f154d379cfd608a87ccd8f02ff865b5b70c7", "abstract": "Importance\nThe use of palliative care programs and the number of trials assessing their effectiveness have increased.\n\n\nObjective\nTo determine the association of palliative care with quality of life (QOL), symptom burden, survival, and other outcomes for people with life-limiting illness and for their caregivers.\n\n\nData Sources\nMEDLINE, EMBASE, CINAHL, and Cochrane CENTRAL to July 2016.\n\n\nStudy Selection\nRandomized clinical trials of palliative care interventions in adults with life-limiting illness.\n\n\nData Extraction and Synthesis\nTwo reviewers independently extracted data. Narrative synthesis was conducted for all trials. Quality of life, symptom burden, and survival were analyzed using random-effects meta-analysis, with estimates of QOL translated to units of the Functional Assessment of Chronic Illness Therapy-palliative care scale (FACIT-Pal) instrument (range, 0-184 [worst-best]; minimal clinically important difference [MCID], 9 points); and symptom burden translated to the Edmonton Symptom Assessment Scale (ESAS) (range, 0-90 [best-worst]; MCID, 5.7 points).\n\n\nMain Outcomes and Measures\nQuality of life, symptom burden, survival, mood, advance care planning, site of death, health care satisfaction, resource utilization, and health care expenditures.\n\n\nResults\nForty-three RCTs provided data on 12\u202f731 patients (mean age, 67 years) and 2479 caregivers. Thirty-five trials used usual care as the control, and 14 took place in the ambulatory setting. In the meta-analysis, palliative care was associated with statistically and clinically significant improvements in patient QOL at the 1- to 3-month follow-up (standardized mean difference, 0.46; 95% CI, 0.08 to 0.83; FACIT-Pal mean difference, 11.36] and symptom burden at the 1- to 3-month follow-up (standardized mean difference, -0.66; 95% CI, -1.25 to -0.07; ESAS mean difference, -10.30). When analyses were limited to trials at low risk of bias (n\u2009=\u20095), the association between palliative care and QOL was attenuated but remained statistically significant (standardized mean difference, 0.20; 95% CI, 0.06 to 0.34; FACIT-Pal mean difference, 4.94), whereas the association with symptom burden was not statistically significant (standardized mean difference, -0.21; 95% CI, -0.42 to 0.00; ESAS mean difference, -3.28). There was no association between palliative care and survival (hazard ratio, 0.90; 95% CI, 0.69 to 1.17). Palliative care was associated consistently with improvements in advance care planning, patient and caregiver satisfaction, and lower health care utilization. Evidence of associations with other outcomes was mixed.\n\n\nConclusions and Relevance\nIn this meta-analysis, palliative care interventions were associated with improvements in patient QOL and symptom burden. Findings for caregiver outcomes were inconsistent. However, many associations were no longer significant when limited to trials at low risk of bias, and there was no significant association between palliative care and survival.", "title": "Association Between Palliative Care and Patient and Caregiver Outcomes: A Systematic Review and Meta-analysis."}, "645161110b2f06e7e9ea7277be85fe4e4d74a2a2": {"paper_id": "645161110b2f06e7e9ea7277be85fe4e4d74a2a2", "abstract": "Ofer Arazy University of Alberta ofer.arazy@ualberta.ca Nanda Kumar City University of New York Nanda.Kumar@baruch.cuny.edu Bracha Shapira Ben-Gurion University of the Negev bshapira@bgumail.bgu.ac.il Social recommender systems utilize data regarding users\u2019 social relationships in filtering relevant information to users. To date, results show that incorporating social relationship data \u2013 beyond consumption profile similarity \u2013 is beneficial only in a very limited set of cases. The main conjecture of this study is that the inconclusive results are, at least to some extent, due to an under-specification of the nature of the social relations. To date, there exist no clear guidelines for using behavioral theory to guide systems design. Our primary objective is to propose a methodology for theory-driven design. We enhance Walls et al.\u2019s (1992) IS Design Theory by introducing the notion of \u201capplied behavioral theory,\u201d as a means of better linking theory and system design. Our second objective is to apply our theory-driven design methodology to social recommender systems, with the aim of improving prediction accuracy. A behavioral study found that some social relationships (e.g., competence, benevolence) are most likely to affect a recipient\u2019s advice-taking decision. We designed, developed, and tested a recommender system based on these principles, and found that the same types of relationships yield the best recommendation accuracy. This striking correspondence highlights the importance of behavioral theory in guiding system design. We discuss implications for design science and for research on recommender systems.", "title": "A Theory-Driven Design Framework for Social Recommender Systems"}, "8010f187f8d5a382203634b049cb26e545957b11": {"paper_id": "8010f187f8d5a382203634b049cb26e545957b11", "abstract": "Ontology mapping is seen as a solution provider in today\u2019s landscape of ontology research. As the number of ontologies that are made publicly available and accessible on the Web increases steadily, so does the need for applications to use them. A single ontology is no longer enough to support the tasks envisaged by a distributed environment like the Semantic Web. Multiple ontologies need to be accessed from several applications. Mapping could provide a common layer from which several ontologies could be accessed and hence could exchange information in semantically sound manners. Developing such mappings has been the focus of a variety of works originating from diverse communities over a number of years. In this article we comprehensively review and present these works. We also provide insights on the pragmatics of ontology mapping and elaborate on a theoretical approach for defining ontology mapping.", "title": "Ontology mapping : the state of the art"}, "9ac43a98fe6fde668afb4fcc115e4ee353a6732d": {"paper_id": "9ac43a98fe6fde668afb4fcc115e4ee353a6732d", "abstract": "Face detection is a well-explored problem. Many challenges on face detectors like extreme pose, illumination, low resolution and small scales are studied in the previous work. However, previous proposed models are mostly trained and tested on good-quality images which are not always the case for practical applications like surveillance systems. In this paper, we first review the current state-of-the-art face detectors and their performance on benchmark dataset FDDB, and compare the design protocols of the algorithms. Secondly, we investigate their performance degradation while testing on low-quality images with different levels of blur, noise, and contrast. Our results demonstrate that both hand-crafted and deep-learning based face detectors are not robust enough for low-quality images. It inspires researchers to produce more robust design for face detection in the wild.", "title": "Survey of Face Detection on Low-Quality Images"}, "76b2675a276cd0bc8d14aa57400e8b5d20f7d973": {"paper_id": "76b2675a276cd0bc8d14aa57400e8b5d20f7d973", "abstract": "Today thanks to low cost and high performance DSP's, Kalman filtering (KF) becomes an efficient candidate to avoid mechanical sensors in motor control. We present in this work experimental results by using a steady state KF method to estimate the speed and rotor position for hybrid stepper motor. With this method the computing time is reduced. The Kalman gain is pre-computed from numerical simulation and introduced as a constant in the real time algorithm. The load torque is also on-line estimated by the same algorithm. At start-up the initial rotor position is detected by the impulse current method.", "title": "Sensorless control of hybrid stepper motor"}, "398550ee21f0977183e11dd39887d223b1c9d89f": {"paper_id": "398550ee21f0977183e11dd39887d223b1c9d89f", "abstract": "Organic light-emitting diodes (OLEDs) are competitive candidates for the next generation flat-panel displays and solid state lighting sources. Efficient blue-emitting materials have been one of the most important prerequisites to kick off the commercialization of OLEDs. This tutorial review focuses on the design of blue fluorescent emitters and their applications in OLEDs. At first, some typical blue fluorescent materials as dopants are briefly introduced. Then nondoped blue emitters of hydrocarbon compounds are presented. Finally, the nondoped blue emitters endowed with hole-, electron- and bipolar-transporting abilities are comprehensively reviewed. The key issues on suppressing close-packing, achieving pure blue chromaticity, improving thermal and morphological stabilities, manipulating charge transporting abilities, simplifying device structures and the applications in panchromatic OLEDs are discussed.", "title": "Blue fluorescent emitters: design tactics and applications in organic light-emitting diodes."}, "6e14c7aa3385d4db338e97199d357bc2f53d35e5": {"paper_id": "6e14c7aa3385d4db338e97199d357bc2f53d35e5", "abstract": "The International Academy of Education (IAE) is a not-for-profit scientific association that promotes educational research, and its dissemination and implementation. Founded in 1986, the Academy is dedicated to strengthening the contributions of research, solving critical educational problems throughout the world, and providing better communication among policy makers, researchers, and practitioners. The general aim of the IAE is to foster scholarly excellence in all fields of education. Towards this end, the Academy provides timely syntheses of research-based evidence of international importance. The Academy also provides critiques of research and of its evidentiary basis and its application to policy. This booklet about teacher professional learning and development has been prepared for inclusion in the Educational Practices Series developed by the International Academy of Education and distributed by the International Bureau of Education and the Academy. As part of its mission, the Academy provides timely syntheses of research on educational topics of international importance. This is the eighteenth in a series of booklets on educational practices that generally improve learning. This particular booklet is based on a synthesis of research evidence produced for the New Zealand Ministry of Education's Iterative Best Evidence Synthesis (BES) Programme, which is designed to be a catalyst for systemic improvement and sustainable development in education. This synthesis, and others in the series, are available electronically at www.educationcounts.govt.nz/themes/BES. All BESs are written using a collaborative approach that involves the writers, teacher unions, principal groups, teacher educators, academics, researchers, policy advisers, and other interested parties. To ensure its rigour and usefulness, each BES follows national guidelines developed by the Ministry of Education. Professor Helen Timperley was lead writer for the Teacher Professional Learning and Development: Best Evidence Synthesis Iteration [BES], assisted by teacher educators Aaron Wilson and Heather Barrar and research assistant Irene Fung, all of the University of Auckland. The BES is an analysis of 97 studies of professional development that led to improved outcomes for the students of the participating teachers. Most of these studies came from the United States, New Zealand, the Netherlands, the United Kingdom, Canada, and Israel. Dr Lorna Earl provided formative quality assurance for the synthesis; Professor John Hattie and Dr Gavin Brown oversaw the analysis of effect sizes. Helen Timperley is Professor of Education at the University of Auckland. The primary focus of her research is promotion of professional and organizational learning in schools for the purpose of improving student learning. She has \u2026", "title": "Teacher professional learning and development"}, "b7f72b9048a9af22830f11c96c4ec781d42c84c0": {"paper_id": "b7f72b9048a9af22830f11c96c4ec781d42c84c0", "abstract": "Generating novel graph structures that optimize given objectives while obeying some given underlying rules is fundamental for chemistry, biology and social science research. This is especially important in the task of molecular graph generation, whose goal is to discover novel molecules with desired properties such as drug-likeness and synthetic accessibility, while obeying physical laws such as chemical valency. However, designing models to find molecules that optimize desired properties while incorporating highly complex and non-differentiable rules remains to be a challenging task. Here we propose Graph Convolutional Policy Network (GCPN), a general graph convolutional network based model for goaldirected graph generation through reinforcement learning. The model is trained to optimize domain-specific rewards and adversarial loss through policy gradient, and acts in an environment that incorporates domain-specific rules. Experimental results show that GCPN can achieve 61% improvement on chemical property optimization over state-of-the-art baselines while resembling known molecules, and achieve 184% improvement on the constrained property optimization task.", "title": "Graph Convolutional Policy Network for Goal-Directed Molecular Graph Generation"}, "959d77f21c6850b25217421129ec10d9cabc1c61": {"paper_id": "959d77f21c6850b25217421129ec10d9cabc1c61", "abstract": "A single line feed stacked microstrip antenna for 4G system is presented. The proposed antenna with two properly square patches are stacked. The top patch can perform as a driven element is design on 2.44 GHz and lower patch is also design on 2.44 GHz. The performance of proposed antenna for 4G band frequency (2400-2500 MHz). Also gating the improvement of bandwidth (15%) and antenna efficiency (95%) are very high compared to conventional antenna. Key word \u2014 Microstrip patch antenna; stacked, 4G, Antenna efficiency.", "title": "Stacked Square Microstrip Antenna for 4G System"}, "1c54061d64168a5cb350b1a577c1dbe654270a2c": {"paper_id": "1c54061d64168a5cb350b1a577c1dbe654270a2c", "abstract": "Discrete Exterior Calculus (DEC) is a discrete version of the smooth exterior calculus. Exterior calculus is calculus on smooth manifolds, and DEC is a calculus for discrete manifolds. It has applications in computational mechanics, computer graphics and other fields. This project has two parts. In the first part, we build a C++ class library to implement some of the objects and operators of DEC. The objects implemented are discrete forms, and the operators implemented are the exterior derivative, Hodge star and wedge product. These objects and operators are implemented for 2D meshes embedded in R. The second part of this project is to extend DEC to include general discrete tensors. As a very preliminary first step, we do this in the context of an application, by proposing the definition of a discrete stress tensor for elasticity. We show how to compute the discrete stress tensor in planar elasticity and propose a discretization for Cauchy\u2019s equation of motion based on that.", "title": "Discrete Exterior Calculus and Its Implementation"}, "4186ac0d5780bd693d195c0f9583c9dc277a4cb9": {"paper_id": "4186ac0d5780bd693d195c0f9583c9dc277a4cb9", "abstract": "The objective of this paper is to present an approach to electromagnetic field simulation based on the systematic use of the global (i.e. integral) quantities. In this approach, the equations of electromagnetism are obtained directly in a finite form starting from experimental laws without resorting to the differential formulation. This finite formulation is the natural extension of the network theory to electromagnetic field and it is suitable for computational electromagnetics.", "title": "FINITE FORMULATION OF THE ELECTROMAGNETIC FIELD"}, "8a8e348040de838c670a8213c317ccfdf50d5c01": {"paper_id": "8a8e348040de838c670a8213c317ccfdf50d5c01", "abstract": "This paper proposes a unified and consistent set of flexible tools to approximate important geometric attributes, including normal vectors and curvatures on arbitrary triangle meshes. We present a consistent derivation of these first and second order differential properties using averaging Voronoi cells and the mixed Finite-Element/Finite-Volume method, and compare them to existing formulations. Building upon previous work in discrete geometry, these operators are closely related to the continuous case, guaranteeing an appropriate extension from the continuous to the discrete setting: they respect most intrinsic properties of the continuous differential operators. We show that these estimates are optimal in accuracy under mild smoothness conditions, and demonstrate their numerical quality. We also present applications of these operators, such as mesh smoothing, enhancement, and quality checking, and show results of denoising in higher dimensions, such as for tensor images.", "title": "Discrete Differential-Geometry Operators for Triangulated 2-Manifolds"}, "d6fdfd8ac50c4b91dcb3acb4032f6acc2511585a": {"paper_id": "d6fdfd8ac50c4b91dcb3acb4032f6acc2511585a", "abstract": "An essential feature of large scale free graphs, such as the Web, protein-to-protein interaction, brain connectivity, and social media graphs, is that they tend to form recursive communities. The latter are densely connected vertex clusters exhibiting quick local information dissemination and processing. Under the fuzzy graph model vertices are fixed while each edge exists with a given probability according to a membership function. This paper presents Fuzzy Walktrap and Fuzzy Newman-Girvan, fuzzy versions of two established community discovery algorithms. The proposed algorithms have been applied to a synthetic graph generated by the Kronecker model with different termination criteria and the results are discussed. Keywords-Fuzzy graphs; Membership function; Community detection; Termination criteria; Walktrap algorithm; NewmanGirvan algorithm; Edge density; Kronecker model; Large graph analytics; Higher order data", "title": "On converting community detection algorithms for fuzzy graphs in Neo4j"}, "10b13d1d7ce00845058a04fea59dc55f1db412d5": {"paper_id": "10b13d1d7ce00845058a04fea59dc55f1db412d5", "abstract": "Query processing over graph-structured data is enjoying a growing number of applications. A top-k keyword search query on a graph finds the top k answers according to some ranking criteria, where each answer is a substructure of the graph containing all query keywords. Current techniques for supporting such queries on general graphs suffer from several drawbacks, e.g., poor worst-case performance, not taking full advantage of indexes, and high memory requirements. To address these problems, we propose BLINKS, a bi-level indexing and query processing scheme for top-k keyword search on graphs. BLINKS follows a search strategy with provable performance bounds, while additionally exploiting a bi-level index for pruning and accelerating the search. To reduce the index space, BLINKS partitions a data graph into blocks: The bi-level index stores summary information at the block level to initiate and guide search among blocks, and more detailed information for each block to accelerate search within blocks. Our experiments show that BLINKS offers orders-of-magnitude performance improvement over existing approaches.", "title": "BLINKS: ranked keyword searches on graphs"}, "6ae5d06fd366d44107752b903d0ec22b41321e3d": {"paper_id": "6ae5d06fd366d44107752b903d0ec22b41321e3d", "abstract": "In this paper we introduce a novel collapsed Gibbs sampling method for the widely used latent Dirichlet allocation (LDA) model. Our new method results in significant speedups on real world text corpora. Conventional Gibbs sampling schemes for LDA require O(K) operations per sample where K is the number of topics in the model. Our proposed method draws equivalent samples but requires on average significantly less then K operations per sample. On real-word corpora FastLDA can be as much as 8 times faster than the standard collapsed Gibbs sampler for LDA. No approximations are necessary, and we show that our fast sampling scheme produces exactly the same results as the standard (but slower) sampling scheme. Experiments on four real world data sets demonstrate speedups for a wide range of collection sizes. For the PubMed collection of over 8 million documents with a required computation time of 6 CPU months for LDA, our speedup of 5.7 can save 5 CPU months of computation.", "title": "Fast collapsed gibbs sampling for latent dirichlet allocation"}, "0250b5c0f7a414dec8c7a0aa7be20c9637eeb6ec": {"paper_id": "0250b5c0f7a414dec8c7a0aa7be20c9637eeb6ec", "abstract": "We consider the problem of detecting communities or modules in networks, groups of vertices with a higher-than-average density of edges connecting them. Previous work indicates that a robust approach to this problem is the maximization of the benefit function known as \"modularity\" over possible divisions of a network. Here we show that this maximization process can be written in terms of the eigenspectrum of a matrix we call the modularity matrix, which plays a role in community detection similar to that played by the graph Laplacian in graph partitioning calculations. This result leads us to a number of possible algorithms for detecting community structure, as well as several other results, including a spectral measure of bipartite structure in networks and a centrality measure that identifies vertices that occupy central positions within the communities to which they belong. The algorithms and measures proposed are illustrated with applications to a variety of real-world complex networks.", "title": "Finding community structure in networks using the eigenvectors of matrices."}, "71e5a6c95ff476c303c3adeae0c1a4387485f733": {"paper_id": "71e5a6c95ff476c303c3adeae0c1a4387485f733", "abstract": "The discovery and analysis of community structure in networks is a topic of considerable recent interest within the physics community, but most methods proposed so far are unsuitable for very large networks because of their computational cost. Here we present a hierarchical agglomeration algorithm for detecting community structure which is faster than many competing algorithms: its running time on a network with n vertices and m edges is O (md log n) where d is the depth of the dendrogram describing the community structure. Many real-world networks are sparse and hierarchical, with m approximately n and d approximately log n, in which case our algorithm runs in essentially linear time, O (n log(2) n). As an example of the application of this algorithm we use it to analyze a network of items for sale on the web site of a large on-line retailer, items in the network being linked if they are frequently purchased by the same buyer. The network has more than 400 000 vertices and 2 x 10(6) edges. We show that our algorithm can extract meaningful communities from this network, revealing large-scale patterns present in the purchasing habits of customers.", "title": "Finding community structure in very large networks."}, "2a005868b79511cf8c924cd5990e2497527a0527": {"paper_id": "2a005868b79511cf8c924cd5990e2497527a0527", "abstract": "A number of recent studies have focused on the statistical properties of networked systems such as social networks and the Worldwide Web. Researchers have concentrated particularly on a few properties that seem to be common to many networks: the small-world property, power-law degree distributions, and network transitivity. In this article, we highlight another property that is found in many networks, the property of community structure, in which network nodes are joined together in tightly knit groups, between which there are only looser connections. We propose a method for detecting such communities, built around the idea of using centrality indices to find community boundaries. We test our method on computer-generated and real-world graphs whose community structure is already known and find that the method detects this known structure with high sensitivity and reliability. We also apply the method to two networks whose community structure is not well known--a collaboration network and a food web--and find that it detects significant and informative community divisions in both cases.", "title": "Community structure in social and biological networks."}, "4af182338ee63754d4569c26cb6a5c3bbdd8cf2a": {"paper_id": "4af182338ee63754d4569c26cb6a5c3bbdd8cf2a", "abstract": "The concept of maximum entropy can be traced back along multiple threads to Biblical times Only recently however have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition In this paper we describe a method for statistical modeling based on maximum entropy We present a maximum likelihood approach for automatically con structing maximum entropy models and describe how to implement this approach e ciently using as examples several problems in natural language processing", "title": "A Maximum Entropy Approach to Natural Language Processing"}, "48caac2f65bce47f6d27400ae4f60d8395cec2f3": {"paper_id": "48caac2f65bce47f6d27400ae4f60d8395cec2f3", "abstract": "Gradient boosting constructs additive regression models by sequentially tting a simple parameterized function (base learner) to current \\pseudo\"{residuals by least{squares at each iteration. The pseudo{residuals are the gradient of the loss functional being minimized, with respect to the model values at each training data point, evaluated at the current step. It is shown that both the approximation accuracy and execution speed of gradient boosting can be substantially improved by incorporating randomization into the procedure. Speci cally, at each iteration a subsample of the training data is drawn at random (without replacement) from the full training data set. This randomly selected subsample is then used in place of the full sample to t the base learner and compute the model update for the current iteration. This randomized approach also increases robustness against overcapacity of the base learner. 1 Gradient Boosting In the function estimation problem one has a system consisting of a random \\output\" or \\response\" variable y and a set of random \\input\" or \\explanatory\" variables x = fx1; ; xng. Given a \\training\" sample fyi;xig N 1 of known (y;x){values, the goal is to nd a function F (x) that maps x to y, such that over the joint distribution of all (y;x){values, the expected value of some speci ed loss function (y; F (x)) is minimized F (x) = argmin F (x) Ey;x (y; F (x)): (1) Boosting approximates F (x) by an \\additive\" expansion of the form", "title": "Stochastic Gradient Boosting"}, "46dbcee23edc8385921749f6710f480440a61375": {"paper_id": "46dbcee23edc8385921749f6710f480440a61375", "abstract": "New types of document collections are being developed by various web services. The service providers keep track of non-textual features such as click counts. In this paper, we present a framework to use non-textual features to predict the quality of documents. We also show our quality measure can be successfully incorporated into the language modeling-based retrieval model. We test our approach on a collection of question and answer pairs gathered from a community based question answering service where people ask and answer questions. Experimental results using our quality measure show a significant improvement over our baseline.", "title": "A framework to predict the quality of answers with non-textual features"}, "3b73ec58211d54350788c7063c8bae7767cfe7f7": {"paper_id": "3b73ec58211d54350788c7063c8bae7767cfe7f7", "abstract": "Passive Bistatic Radar (PBR), also known as Passive Coherent Location (PCL), uses illuminators of opportunity. Passive radar using signals in a single frequency network modulated according to the Digital Audio/Video Broadcasting (DAB/DVB) standards using orthogonal frequency division multiplexing (OFDM) has recently been of increasing interest. There has been considerable research to develop tracking systems addressing its inherent difficulties [3]\u2014[7], [10]\u2014[13]; the poor quality\u2013or absence\u2013of angular information, and the lack of label of the transmitter on top of the usual target/measurement association concerns. First, there are algorithms using the Multi-Hypothesis Tracker (MHT) [12], [13] addressing the complexity problem from association ambiguities between measurement, targets and illuminators by initially forming two dimensional (measurement-target) hypotheses in the two-dimensional range/Doppler domain. Tracking is thence performed directly on target parameters by the MHT without considering the association between measurements and illuminators: the range/Doppler MHT extracts measurements and removes false alarms. Then, de-ghosting is performed by evaluating likelihood probabilities of possible data associations. When a Cartesian track is confirmed, the remaining tracks from other possible associations are declared false and tracking starts in the Cartesian domain. This MHT approach is good but but is not without issues. One is the appropriate motion model in range and Doppler space: probably the target dynamics in the Cartesian domain are known, the trajectories are not easily described in a space of target parameters, because the trajectories are related to illuminator/receiver/target geometry and there is association ambiguity among measurements, illuminators, and targets. And that is another concern: the illuminator association is never explicitly addressed. Now, track maintenance algorithms that operate directly in Cartesian coordinates have been explored [4], [5], one using modified Joint Probabilistic Data Association (JPDA) and another a particle filter. For the former, in order to address the large number of threelist hypotheses, a \u201csuper-target\u201d idea was proposed; and the particle filters work under the PMHT measurement model that each measurement\u2019s assignments are independent of others\u2019. These methods have also been examined downstream from an initiation approach (the PMHTI method, suggested in [6]) that initiates tracks in Cartesian coordinates. In fact the PMHT seems to be an effective and natural way to accommodate the data association with the extra list (transmitters). So in this paper, we present it: it is really very simple. This tracker, combined with the initiation algorithm (the modified PMHTI method in [6]), shows excellent performance in comparison with the JPDA filter and particle filter.", "title": "The PMHT for Passive Radar in a DAB/DVB Network"}, "2c6f8da69230b98fc2870ca140eaf1e135ddd932": {"paper_id": "2c6f8da69230b98fc2870ca140eaf1e135ddd932", "abstract": "Existing analytical techniques for functional magnetic resonance imaging (fMRI) data always need some specific assumptions on the time series. In this article, we present a new approach for fMRI activation detection, which can be implemented without any assumptions on the time series. Our method is based on a region growing method, which is very popular for image segmentation. A comparison of performance on fMRI activation detection is made between the proposed method and the deconvolution method and the fuzzy clustering method with receiver operating characteristic (ROC) methodology. In addition, we examine the effectiveness and usefulness of our method on real experimental data. Experimental results show that our method outperforms over the deconvolution method and the fuzzy clustering method on a number of aspects. These results suggest that our region growing method can serve as a reliable analysis of fMRI data.", "title": "Region growing method for the analysis of functional MRI data"}, "f7a0d42044b26be8d509310ba20fd8d665943eba": {"paper_id": "f7a0d42044b26be8d509310ba20fd8d665943eba", "abstract": "We present template attacks, the strongest form of side channel attack possible in an information theoretic sense. These attacks can break implementations and countermeasures whose security is dependent on the assumption that an adversary cannot obtain more than one or a limited number of side channel samples. They require that an adversary has access to an identical experimental device that he can program to his choosing. The success of these attacks in such constraining situations is due manner in which noise within each sample is handled. In contrast to previous approaches which viewed noise as a hindrance that had to be reduced or eliminated, our approach focuses on precisely modeling noise, and using this to fully extract information present in a single sample. We describe in detail how an implementation of RC4, not amenable to techniques such as SPA and DPA, can easily be broken using template attacks with a single sample. Other applications include attacks on certain DES implementations which use DPA\u2013resistant hardware and certain SSL accelerators which can be attacked by monitoring electromagnetic emanations from an RSA operation even from distances of fifteen feet.", "title": "Template Attacks"}, "dc821ab3a1a3b49661639da37e980bfd21d3746a": {"paper_id": "dc821ab3a1a3b49661639da37e980bfd21d3746a", "abstract": null, "title": "Blind Signatures for Untraceable Payments"}, "0dba88589f12be4b7438da48056c44a97844450e": {"paper_id": "0dba88589f12be4b7438da48056c44a97844450e", "abstract": "This document describes the RC encryption algorithm a fast symmetric block cipher suitable for hardware or software imple mentations A novel feature of RC is the heavy use of data dependent rotations RC has a variable word size a variable number of rounds and a variable length secret key The encryption and decryption algorithms are exceptionally simple", "title": "The RC5 Encryption Algorithm"}, "3bf471d6cf1c84b3113e73a0b82dd54118dd2261": {"paper_id": "3bf471d6cf1c84b3113e73a0b82dd54118dd2261", "abstract": "An increasing number of systems, from pay-TV to electronic purses, rely on the tamper resistance of smartcards and other security processors. We describe a number of attacks on such systems \u2014 some old, some new and some that are simply little known outside the chip testing community. We conclude that trusting tamper resistance is problematic; smartcards are broken routinely, and even a device that was described by a government signals agency as \u2018the most secure processor generally available\u2019 turns out to be vulnerable. Designers of secure systems should consider the consequences with care. 1 Tamperproofing of cryptographic", "title": "Tamper Resistance \u2014 a Cautionary Note"}, "68a52ce4829fe5c32d9f7610de0e59a30d4de722": {"paper_id": "68a52ce4829fe5c32d9f7610de0e59a30d4de722", "abstract": "are the property of their respective owners. The information contained in this presentation is provided for illustrative purposes only, and is provided without any guarantee or warranty whatsoever, and does not necessarily represent official opinions of CRI or its partners. Confidential-unauthorized copying, use or redistribution is prohibited.", "title": "Differential Power Analysis"}, "6421f90ab18ab5bec79ad104d11f7af0e7a8ee78": {"paper_id": "6421f90ab18ab5bec79ad104d11f7af0e7a8ee78", "abstract": "The global threat to public health posed by emerging multidrug-resistant bacteria in the past few years necessitates the development of novel approaches to combat bacterial infections. Endolysins encoded by bacterial viruses (or phages) represent one promising avenue of investigation. These enzyme-based antibacterials efficiently kill Gram-positive bacteria upon contact by specific cell wall hydrolysis. However, a major hurdle in their exploitation as antibacterials against Gram-negative pathogens is the impermeable lipopolysaccharide layer surrounding their cell wall. Therefore, we developed and optimized an approach to engineer these enzymes as outer membrane-penetrating endolysins (Artilysins), rendering them highly bactericidal against Gram-negative pathogens, including Pseudomonas aeruginosa and Acinetobacter baumannii. Artilysins combining a polycationic nonapeptide and a modular endolysin are able to kill these (multidrug-resistant) strains in vitro with a 4 to 5 log reduction within 30 min. We show that the activity of Artilysins can be further enhanced by the presence of a linker of increasing length between the peptide and endolysin or by a combination of both polycationic and hydrophobic/amphipathic peptides. Time-lapse microscopy confirmed the mode of action of polycationic Artilysins, showing that they pass the outer membrane to degrade the peptidoglycan with subsequent cell lysis. Artilysins are effective in vitro (human keratinocytes) and in vivo (Caenorhabditis elegans). Importance: Bacterial resistance to most commonly used antibiotics is a major challenge of the 21st century. Infections that cannot be treated by first-line antibiotics lead to increasing morbidity and mortality, while millions of dollars are spent each year by health care systems in trying to control antibiotic-resistant bacteria and to prevent cross-transmission of resistance. Endolysins--enzymes derived from bacterial viruses--represent a completely novel, promising class of antibacterials based on cell wall hydrolysis. Specifically, they are active against Gram-positive species, which lack a protective outer membrane and which have a low probability of resistance development. We modified endolysins by protein engineering to create Artilysins that are able to pass the outer membrane and become active against Pseudomonas aeruginosa and Acinetobacter baumannii, two of the most hazardous drug-resistant Gram-negative pathogens.", "title": "Engineered Endolysin-Based \u201cArtilysins\u201d To Combat Multidrug-Resistant Gram-Negative Pathogens"}, "27637d4de11b3d6baeb9bf26597ec6fd86a51d5a": {"paper_id": "27637d4de11b3d6baeb9bf26597ec6fd86a51d5a", "abstract": "Behaviour trees provide the possibility of improving on existing Artificial Intelligence techniques in games by being simple to implement, scalable, able to handle the complexity of games, and modular to improve reusability. This ultimately improves the development process for designing automated game players. We cover here the use of behaviour trees to design and develop an AI-controlled player for the commercial real-time strategy game DEFCON. In particular, we evolved behaviour trees to develop a competitive player which was able to outperform the game\u2019s original AI-bot more than 50% of the time. We aim to highlight the potential for evolving behaviour trees as a practical approach to developing AI-bots in games.", "title": "Evolving Behaviour Trees for the Commercial Game DEFCON"}, "a52b81b322087ce82e4ec4124b3a47cbb391a0eb": {"paper_id": "a52b81b322087ce82e4ec4124b3a47cbb391a0eb", "abstract": "An important component of routine visual behavior is the ability to find one item in a visual world filled with other, distracting items. This ability to performvisual search has been the subject of a large body of research in the past 15 years. This paper reviews the visual search literature and presents a model of human search behavior. Built upon the work of Neisser, Treisman, Julesz, and others, the model distinguishes between a preattentive, massively parallel stage that processes information about basic visual features (color, motion, various depth cues, etc.) across large portions of the visual field and a subsequent limited-capacity stage that performs other, more complex operations (e.g., face recognition, reading, object identification) over a limited portion of the visual field. The spatial deployment of the limited-capacity process is under attentional control. The heart of the guided search model is the idea that attentional deployment of limited resources isguided by the output of the earlier parallel processes. Guided Search 2.0 (GS2) is a revision of the model in which virtually all aspects of the model have been made more explicit and/or revised in light of new data. The paper is organized into four parts: Part 1 presents the model and the details of its computer simulation. Part 2 reviews the visual search literature on preattentive processing of basic features and shows how the GS2 simulation reproduces those results. Part 3 reviews the literature on the attentional deployment of limited-capacity processes in conjunction and serial searches and shows how the simulation handles those conditions. Finally, Part 4 deals with shortcomings of the model and unresolved issues.", "title": "Guided Search 2.0 A revised model of visual search."}, "73e51b9820e90eb6525fc953c35c9288527cecfd": {"paper_id": "73e51b9820e90eb6525fc953c35c9288527cecfd", "abstract": "Existing neural dependency parsers usually encode each word in a sentence with bi-directional LSTMs, and estimate the score of an arc from the LSTM representations of the head and the modifier, possibly missing relevant context information for the arc being considered. In this study, we propose a neural feature extraction method that learns to extract arcspecific features. We apply a neural network-based attention method to collect evidences for and against each possible head-modifier pair, with which our model computes certainty scores of belief and disbelief, and determines the final arc score by subtracting the score of disbelief from the one of belief. By explicitly introducing two kinds of evidences, the arc candidates can compete against each other based on more relevant information, especially for the cases where they share the same head or modifier. It makes possible to better discriminate two or more competing arcs by presenting their rivals (disbelief evidence). Experiments on various datasets show that our arc-specific feature extraction mechanism significantly improves the performance of bi-directional LSTMbased models by explicitly modeling long-distance dependencies. For both English and Chinese, the proposed model achieve a higher accuracy on dependency parsing task than most existing neural attention-based models.", "title": "Attention-based Belief or Disbelief Feature Extraction for Dependency Parsing"}, "0104063400e6d69294edc95fb14c7e8fac347f6a": {"paper_id": "0104063400e6d69294edc95fb14c7e8fac347f6a", "abstract": "In typical classification tasks, we seek a function which assigns a label to a single object. Kernel-based approaches, such as support vector machines (SVMs), which maximize the margin of confidence of the classifier, are the method of choice for many such tasks. Their popularity stems both from the ability to use high-dimensional feature spaces, and from their strong theoretical guarantees. However, many real-world tasks involve sequential, spatial, or structured data, where multiple labels must be assigned. Existing kernel-based methods ignore structure in the problem, assigning labels independently to each object, losing much useful information. Conversely, probabilistic graphical models, such as Markov networks, can represent correlations between labels, by exploiting problem structure, but cannot handle high-dimensional feature spaces, and lack strong theoretical generalization guarantees. In this paper, we present a new framework that combines the advantages of both approaches: Maximum margin Markov (M ) networks incorporate both kernels, which efficiently deal with highdimensional features, and the ability to capture correlations in structured data. We present an efficient algorithm for learning M networks based on a compact quadratic program formulation. We provide a new theoretical bound for generalization in structured domains. Experiments on the task of handwritten character recognition, demonstrate very significant gains over previous approaches.", "title": "Max-Margin Markov Networks"}, "2d02141fd8c263d9c9a05d704c2d3e38525a7167": {"paper_id": "2d02141fd8c263d9c9a05d704c2d3e38525a7167", "abstract": "We present a new algorithm for learning hypernym (is-a) relations from text, a key problem in machine learning for natural language understanding. This method generalizes earlier work that relied on hand-built lexico-syntactic patterns by introducing a general-purpose formalization of the pattern space based on syntactic dependency paths. We learn these paths automatically by taking hypernym/hyponym word pairs from WordNet, finding sentences containing these words in a large parsed corpus, and automatically extracting these paths. These paths are then used as features in a high-dimensional representation of noun relationships. We use a logistic regression classifier based on these features for the task of corpus-based hypernym pair identification. Our classifier is shown to outperform previous pattern-based methods for identifying hypernym pairs (using WordNet as a gold standard), and is shown to outperform those methods as well as WordNet on an independent test set.", "title": "Learning Syntactic Patterns for Automatic Hypernym Discovery"}, "71e90c4015e3dea597dbd583f3d3d08cdc0077fb": {"paper_id": "71e90c4015e3dea597dbd583f3d3d08cdc0077fb", "abstract": "Recurrent neural networks (RNNs) are connectionist models of sequential data that are naturally applicable to the analysis of natural language. Recently, \u201cdepth in space\u201d \u2014 as an orthogonal notion to \u201cdepth in time\u201d \u2014 in RNNs has been investigated by stacking multiple layers of RNNs and shown empirically to bring a temporal hierarchy to the architecture. In this work we apply these deep RNNs to the task of opinion expression extraction formulated as a token-level sequence-labeling task. Experimental results show that deep, narrow RNNs outperform traditional shallow, wide RNNs with the same number of parameters. Furthermore, our approach outperforms previous CRF-based baselines, including the state-of-the-art semi-Markov CRF model, and does so without access to the powerful opinion lexicons and syntactic features relied upon by the semi-CRF, as well as without the standard layer-by-layer pre-training typically required of RNN architectures.", "title": "Opinion Mining with Deep Recurrent Neural Networks"}, "1594d954abc650bce2db445c52a76e49655efb0c": {"paper_id": "1594d954abc650bce2db445c52a76e49655efb0c", "abstract": "We introduce recurrent neural network grammars, probabilistic models of sentences with explicit phrase structure. We explain efficient inference procedures that allow application to both parsing and language modeling. Experiments show that they provide better parsing in English than any single previously published supervised generative model and better language modeling than state-of-the-art sequential RNNs in English and Chinese.", "title": "Recurrent Neural Network Grammars"}, "d3bbd57899d938e8c4bcafbbda10ceb59638e4db": {"paper_id": "d3bbd57899d938e8c4bcafbbda10ceb59638e4db", "abstract": "We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoderdecoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice. We will make our encoder publicly available.", "title": "Skip-Thought Vectors"}, "c1d96c0f421f8b519cafdbb4e499faa1c797ed9b": {"paper_id": "c1d96c0f421f8b519cafdbb4e499faa1c797ed9b", "abstract": "Transition-based dependency parsers generally use heuristic decoding algorithms but can accommodate arbitrarily rich feature representations. In this paper, we show that we can improve the accuracy of such parsers by considering even richer feature sets than those employed in previous systems. In the standard Penn Treebank setup, our novel features improve attachment score form 91.4% to 92.9%, giving the best results so far for transitionbased parsing and rivaling the best results overall. For the Chinese Treebank, they give a signficant improvement of the state of the art. An open source release of our parser is freely available.", "title": "Transition-based Dependency Parsing with Rich Non-local Features"}, "5752b8dcec5856b7ad6289bbe1177acce535fba4": {"paper_id": "5752b8dcec5856b7ad6289bbe1177acce535fba4", "abstract": "We develop a formal grammatical system called a link grammar, show how English grammar can be encoded in such a system, and give algorithms for efficiently parsing with a link grammar. Although the expressive power of link grammars is equivalent to that of context free grammars, encoding natural language grammars appears to be much easier with the new system. We have written a program for general link parsing and written a link grammar for the English language. The performance of this preliminary system \u2013 both in the breadth of English phenomena that it captures and in the computational resources used \u2013 indicates that the approach may have practical uses as well as linguistic significance. Our program is written in C and may be obtained through the internet. c 1991 Daniel Sleator and Davy Temperley * School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, sleator@cs.cmu.edu. y Music Department, Columbia University, New York, NY 10027, dt3@cunixa.cc.columbia.edu. Research supported in part by the National Science Foundation under grant CCR-8658139, Olin Corporation, and R. R. Donnelley and Sons. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of Olin Corporation, R. R. Donnelley and Sons, or the NSF.", "title": "Parsing English with a Link Grammar"}, "846a3c8833e809810f2b5f8ea7982b95e8bb42d1": {"paper_id": "846a3c8833e809810f2b5f8ea7982b95e8bb42d1", "abstract": "We present an effective training algorithm for linearly-scored dependency parsers that implements online largemargin multi-class training (Crammer and Singer, 2003; Crammer et al., 2003) on top of efficient parsing techniques for dependency trees (Eisner, 1996). The trained parsers achieve a competitive dependency accuracy for both English and Czech with no language specific enhancements.", "title": "Online Large-Margin Training of Dependency Parsers"}, "0d4fe56e306d2d3ed4fa272ecbc5fe018eeb850a": {"paper_id": "0d4fe56e306d2d3ed4fa272ecbc5fe018eeb850a", "abstract": "We present Searn, an algorithm for integrating search and learning to solve complex structured prediction problems such as those that occur in natural language, speech, computational biology, and vision. Searn is a meta-algorithm that transforms these complex problems into simple classification problems to which any binary classifier may be applied. Unlike current algorithms for structured learning that require decomposition of both the loss function and the feature functions over the predicted structure, Searn is able to learn prediction functions for any loss function and any class of features. Moreover, Searn comes with a strong, natural theoretical guarantee: good performance on the derived classification problems implies good performance on the structured prediction problem.", "title": "Search-based structured prediction"}, "15639e3b5dc0c163a06e1432af773b6a2d69baff": {"paper_id": "15639e3b5dc0c163a06e1432af773b6a2d69baff", "abstract": "Wi-Fi based fingerprinting systems, mostly utilize the Received Signal Strength Indicator (RSSI), which is known to be unreliable due to environmental and hardware effects. In this paper, we present a novel Wi-Fi fingerprinting system, exploiting the fine-grained information known as Channel State Information (CSI). The frequency diversity of CSI can be effectively utilized to represent a location in both frequency and spatial domain resulting in more accurate indoor localization. We propose a novel location signature CSI-MIMO that incorporates Multiple Input Multiple Output (MIMO) information and use both the magnitude and the phase of CSI of each sub-carrier. We experimentally evaluate the performance of CSI-MIMO fingerprinting using the k-nearest neighbor and the Bayes algorithm. The accuracy of the proposed CSI-MIMO is compared with Finegrained Indoor Fingerprinting System (FIFS) and a simple CSI-based system. The experimental result shows an accuracy improvement of 57% over FIFS with an accuracy of 0.95 meters.", "title": "CSI-MIMO: Indoor Wi-Fi fingerprinting system"}, "62edb6639dc857ad0f33e5d8ef97af89be7a3bc7": {"paper_id": "62edb6639dc857ad0f33e5d8ef97af89be7a3bc7", "abstract": "A novel system for the location of people in an office environment is described. Members of staff wear badges that transmit signals providing information about their location to a centralized location service, through a network of sensors. The paper also examines alternative location techniques, system design issues and applications, particularly relating to telephone call routing. Location systems raise concerns about the privacy of an individual and these issues are also addressed.", "title": "The Active Badge Location System"}, "774db16a3f25a73ceda9e6ab4d5a8b8f3c40605d": {"paper_id": "774db16a3f25a73ceda9e6ab4d5a8b8f3c40605d", "abstract": "In this paper, we propose a new method for indexing large amounts of point and spatial data in highdimensional space. An analysis shows that index structures such as the R*-tree are not adequate for indexing high-dimensional data sets. The major problem of R-tree-based index structures is the overlap of the bounding boxes in the directory, which increases with growing dimension. To avoid this problem, we introduce a new organization of the directory which uses a split algorithm minimizing overlap and additionally utilizes the concept of supernodes. The basic idea of overlap-minimizing split and supernodes is to keep the directory as hierarchical as possible, and at the same time to avoid splits in the directory that would result in high overlap. Our experiments show that for high-dimensional data, the X-tree outperforms the well-known R*-tree and the TV-tree by up to two orders of magnitude.", "title": "The X-tree : An Index Structure for High-Dimensional Data"}, "4caee41ec5e6194b2cfd1fe64bdac2dc60b929c3": {"paper_id": "4caee41ec5e6194b2cfd1fe64bdac2dc60b929c3", "abstract": "Single-unit recording studies in the macaque have carefully documented the modulatory effects of attention on the response properties of visual cortical neurons. Attention produces qualitatively different effects on firing rate, depending on whether a stimulus appears alone or accompanied by distracters. Studies of contrast gain control in anesthetized mammals have found parallel patterns of results when the luminance contrast of a stimulus increases. This finding suggests that attention has co-opted the circuits that mediate contrast gain control and that it operates by increasing the effective contrast of the attended stimulus. Consistent with this idea, microstimulation of the frontal eye fields, one of several areas that control the allocation of spatial attention, induces spatially local increases in sensitivity both at the behavioral level and among neurons in area V4, where endogenously generated attention increases contrast sensitivity. Studies in the slice have begun to explain how modulatory signals might cause such increases in sensitivity.", "title": "Attentional modulation of visual processing."}, "46df3c069dce6a159d52d381a89368cbf22fb3c2": {"paper_id": "46df3c069dce6a159d52d381a89368cbf22fb3c2", "abstract": "This paper explores the usage of Facebook and YouTube among Malaysian students and the possibility of internet addiction in order to determine the effect of using social media in their social and academic lives. Data was collected from 667 Facebook users and 1056 YouTube users. Examining Young's [1]Internet addiction scale among the students revealed that 18% of Facebook users and 22% of YouTube users are addicted, and they spend more than two hours on Facebook and YouTube per day. They use Facebook for information, maintain relationships, academic learning, product inquiry, and meeting people, while YouTube is used for entertainment, information, academic learning, and product inquiry. These results create awareness for instructors and academic institution using YouTube videos and Facebook as complementary tools for teaching. They should be aware of the potential for compulsive and addicted users to be distracted from prescribed videos to unrelated materials.", "title": "Facebook and YouTube addiction: The usage pattern of Malaysian students"}, "20a0b23741824a17c577376fdd0cf40101af5880": {"paper_id": "20a0b23741824a17c577376fdd0cf40101af5880", "abstract": "We propose an effective approach for spatio-temporal action localization in realistic videos. The approach first detects proposals at the frame-level and scores them with a combination of static and motion CNN features. It then tracks high-scoring proposals throughout the video using a tracking-by-detection approach. Our tracker relies simultaneously on instance-level and class-level detectors. The tracks are scored using a spatio-temporal motion histogram, a descriptor at the track level, in combination with the CNN features. Finally, we perform temporal localization of the action using a sliding-window approach at the track level. We present experimental results for spatio-temporal localization on the UCF-Sports, J-HMDB and UCF-101 action localization datasets, where our approach outperforms the state of the art with a margin of 15%, 7% and 12% respectively in mAP.", "title": "Learning to Track for Spatio-Temporal Action Localization"}, "05c9fbbe4a926c29c1df357ab9205bf252a7118b": {"paper_id": "05c9fbbe4a926c29c1df357ab9205bf252a7118b", "abstract": "We introduce an approach for learning human actions as interactions between persons and objects in realistic videos. Previous work typically represents actions with low-level features such as image gradients or optical flow. In contrast, we explicitly localize in space and track over time both the object and the person, and represent an action as the trajectory of the object w.r.t. to the person position. Our approach relies on state-of-the-art techniques for human detection [32], object detection [10], and tracking [39]. We show that this results in human and object tracks of sufficient quality to model and localize human-object interactions in realistic videos. Our human-object interaction features capture the relative trajectory of the object w.r.t. the human. Experimental results on the Coffee and Cigarettes dataset [25], the video dataset of [19], and the Rochester Daily Activities dataset [29] show that 1) our explicit human-object model is an informative cue for action recognition; 2) it is complementary to traditional low-level descriptors such as 3D--HOG [23] extracted over human tracks. We show that combining our human-object interaction features with 3D-HOG improves compared to their individual performance as well as over the state of the art [23], [29].", "title": "Explicit Modeling of Human-Object Interactions in Realistic Videos"}, "04c5268d7a4e3819344825e72167332240a69717": {"paper_id": "04c5268d7a4e3819344825e72167332240a69717", "abstract": "In this paper we introduce a template-based method for recognizing human actions called action MACH. Our approach is based on a maximum average correlation height (MACH) filter. A common limitation of template-based methods is their inability to generate a single template using a collection of examples. MACH is capable of capturing intra-class variability by synthesizing a single Action MACH filter for a given action class. We generalize the traditional MACH filter to video (3D spatiotemporal volume), and vector valued data. By analyzing the response of the filter in the frequency domain, we avoid the high computational cost commonly incurred in template-based approaches. Vector valued data is analyzed using the Clifford Fourier transform, a generalization of the Fourier transform intended for both scalar and vector-valued data. Finally, we perform an extensive set of experiments and compare our method with some of the most recent approaches in the field by using publicly available datasets, and two new annotated human action datasets which include actions performed in classic feature films and sports broadcast television.", "title": "Action MACH a spatio-temporal Maximum Average Correlation Height filter for action recognition"}, "21d4258394a9c8f0ea15f0792d67f7e645720ff6": {"paper_id": "21d4258394a9c8f0ea15f0792d67f7e645720ff6", "abstract": "We propose a unified approach for bottom-up hierarchical image segmentation and object candidate generation for recognition, called Multiscale Combinatorial Grouping (MCG). For this purpose, we first develop a fast normalized cuts algorithm. We then propose a high-performance hierarchical segmenter that makes effective use of multiscale information. Finally, we propose a grouping strategy that combines our multiscale regions into highly-accurate object candidates by exploring efficiently their combinatorial space. We conduct extensive experiments on both the BSDS500 and on the PASCAL 2012 segmentation datasets, showing that MCG produces state-of-the-art contours, hierarchical regions and object candidates.", "title": "Multiscale Combinatorial Grouping"}, "124d967683544973581f951ee93b3f7c069e3ced": {"paper_id": "124d967683544973581f951ee93b3f7c069e3ced", "abstract": "We present a biologically-motivated system for the recognition of actions from video sequences. The approach builds on recent work on object recognition based on hierarchical feedforward architectures [25, 16, 20] and extends a neurobiological model of motion processing in the visual cortex [10]. The system consists of a hierarchy of spatio-temporal feature detectors of increasing complexity: an input sequence is first analyzed by an array of motion- direction sensitive units which, through a hierarchy of processing stages, lead to position-invariant spatio-temporal feature detectors. We experiment with different types of motion-direction sensitive units as well as different system architectures. As in [16], we find that sparse features in intermediate stages outperform dense ones and that using a simple feature selection approach leads to an efficient system that performs better with far fewer features. We test the approach on different publicly available action datasets, in all cases achieving the highest results reported to date.", "title": "A Biologically Inspired System for Action Recognition"}, "b98c68f01d84ac07dc7fc51af782018070da748f": {"paper_id": "b98c68f01d84ac07dc7fc51af782018070da748f", "abstract": "The objective of this paper is classifying images by the object categories they contain, for example motorbikes or dolphins. There are three areas of novelty. First, we introduce a descriptor that represents local image shape and its spatial layout, together with a spatial pyramid kernel. These are designed so that the shape correspondence between two images can be measured by the distance between their descriptors using the kernel. Second, we generalize the spatial pyramid kernel, and learn its level weighting parameters (on a validation set). This significantly improves classification performance. Third, we show that shape and appearance kernels may be combined (again by learning parameters on a validation set).\n Results are reported for classification on Caltech-101 and retrieval on the TRECVID 2006 data sets. For Caltech-101 it is shown that the class specific optimization that we introduce exceeds the state of the art performance by more than 10%.", "title": "Representing shape with a spatial pyramid kernel"}, "6ad32b70ee21b6fc16ff4caf7b4ada2aaf13cabc": {"paper_id": "6ad32b70ee21b6fc16ff4caf7b4ada2aaf13cabc", "abstract": "Most successful object recognition systems rely on binary classification, deciding only if an object is present or not, but not providing information on the actual object location. To estimate the object's location, one can take a sliding window approach, but this strongly increases the computational cost because the classifier or similarity function has to be evaluated over a large set of candidate subwindows. In this paper, we propose a simple yet powerful branch and bound scheme that allows efficient maximization of a large class of quality functions over all possible subimages. It converges to a globally optimal solution typically in linear or even sublinear time, in contrast to the quadratic scaling of exhaustive or sliding window search. We show how our method is applicable to different object detection and image retrieval scenarios. The achieved speedup allows the use of classifiers for localization that formerly were considered too slow for this task, such as SVMs with a spatial pyramid kernel or nearest-neighbor classifiers based on the lambda2 distance. We demonstrate state-of-the-art localization performance of the resulting systems on the UIUC Cars data set, the PASCAL VOC 2006 data set, and in the PASCAL VOC 2007 competition.", "title": "Efficient Subwindow Search: A Branch and Bound Framework for Object Localization"}, "07f488bf2285b290058eb49cf8c25abfd3a13c7d": {"paper_id": "07f488bf2285b290058eb49cf8c25abfd3a13c7d", "abstract": "We describe an approach to object and scene retrieval which searches for and localizes all the occurrences of a user outlined object in a video. The object is represented by a set of viewpoint invariant region descriptors so that recognition can proceed successfully despite changes in viewpoint, illumination and partial occlusion. The temporal continuity of the video within a shot is used to track the regions in order to reject unstable regions and reduce the effects of noise in the descriptors. The analogy with text retrieval is in the implementation where matches on descriptors are pre-computed (using vector quantization), and inverted file systems and document rankings are used. The result is that retrieval is immediate, returning a ranked list of key frames/shots in the manner of Google. The method is illustrated for matching on two full length feature films.", "title": "Video Google: A Text Retrieval Approach to Object Matching in Videos"}, "fa36e9da13f7a055ce95beb61035c72b6d89e2cb": {"paper_id": "fa36e9da13f7a055ce95beb61035c72b6d89e2cb", "abstract": "In this paper, we present a system that allows to realize frequency-division multiplexing multiple-input multiple-output (MIMO) radars which are based on the frequency-modulated continuous-wave (FMCW) principle. Multiple frequency-shifted FMCW signals, which can be separated in the receiver, are used as transmit (TX) signals. The frequency shifting is implemented using digitally generated sinusoids in conjunction with delta-sigma-based transmitters. The proposed principle requires only little adaptions compared to conventional FMCW-setups. Furthermore a fast Fourier-transform stage, being present in many conventional FMCW radars, can be used to separate the TX signals leading to an efficient usage of existing building blocks. Measurements carried out using a prototype system with 48 MIMO channels demonstrate the feasibility of the proposed approach.", "title": "A frequency-division MIMO FMCW radar system using delta-sigma-based transmitters"}, "1c8203e8b826de9d056dffbe6eaffd4fc245d7e1": {"paper_id": "1c8203e8b826de9d056dffbe6eaffd4fc245d7e1", "abstract": "In this paper a method to realize a heterodyne frequency-modulated continuous-wave (FMCW) radar is presented. The system operates at a frequency of 77 GHz and is based on an in-phase quadrature-phase (IQ) modulator in the transmit (TX)-path of the system. This IQ modulator is used as a single-sideband mixer, which allows the realization of an offset frequency between the radar's TX and receive signal. In this way it is possible to shift the frequency band containing the target information away from dc, which simplifies FMCW range measurement of targets close to the radar.", "title": "An IQ-modulator based heterodyne 77-GHz FMCW radar"}, "8308e7b39d1f556e4041b4630a41aa8435fe1a49": {"paper_id": "8308e7b39d1f556e4041b4630a41aa8435fe1a49", "abstract": "MIMO (multiple-input multiple-output) radar refers to an architecture that employs multiple, spatially distributed transmitters and receivers. While, in a general sense, MIMO radar can be viewed as a type of multistatic radar, the separate nomenclature suggests unique features that set MIMO radar apart from the multistatic radar literature and that have a close relation to MIMO communications. This article reviews some recent work on MIMO radar with widely separated antennas. Widely separated transmit/receive antennas capture the spatial diversity of the target's radar cross section (RCS). Unique features of MIMO radar are explained and illustrated by examples. It is shown that with noncoherent processing, a target's RCS spatial variations can be exploited to obtain a diversity gain for target detection and for estimation of various parameters, such as angle of arrival and Doppler. For target location, it is shown that coherent processing can provide a resolution far exceeding that supported by the radar's waveform.", "title": "MIMO Radar with Widely Separated Antennas"}, "408a8e250316863da94ffb3eab077175d08c01bf": {"paper_id": "408a8e250316863da94ffb3eab077175d08c01bf", "abstract": null, "title": "Multiple Emitter Location and Signal Parameter-- Estimation"}, "883b38649d0f98dc2f0829e94fc1a91e878fe220": {"paper_id": "883b38649d0f98dc2f0829e94fc1a91e878fe220", "abstract": "This contribution shows the successful implementation of a fully-differential single-chip radar transceiver with switchable transmit path. The chip facilitates cascading of multiple modules via daisy-chaining using the integrated LO power splitter and LO output on the chip edge opposite to the LO input (LO feedthrough). An on-chip differential rat-race coupler provides for separation of transmit (TX) and receive (RX) paths, therefore only a single antenna port is required. The circuit performance is demonstrated in on-board measurements.", "title": "A 79-GHz radar transceiver with switchable TX and LO feedthrough in a Silicon-Germanium technology"}, "79d358cad40dca093549264a7af0b8bf0a11121a": {"paper_id": "79d358cad40dca093549264a7af0b8bf0a11121a", "abstract": "In this paper the design and the implementation of a linear, non-uniform antenna array for a 77-GHz MIMO FMCW system that allows for the estimation of both the distance and the angular position of a target are presented. The goal is to achieve a good trade-off between the main beam width and the side lobe level. The non-uniform spacing in addition with the MIMO principle offers a superior performance compared to a classical uniform half-wavelength antenna array with an equal number of elements. However the design becomes more complicated and can not be tackled using analytical methods. Starting with elementary array factor considerations the design is approached using brute force, stepwise brute force, and particle swarm optimization. The particle swarm optimized array was also implemented. Simulation results and measurements are presented and discussed.", "title": "Design of a linear non-uniform antenna array for a 77-GHz MIMO FMCW radar"}, "c4bf850225bdb46b09a3c992d1bc87daf9e66248": {"paper_id": "c4bf850225bdb46b09a3c992d1bc87daf9e66248", "abstract": "This paper makes available a concise review of data windows and their affect on the detection of harmonic signals in the presence of broad-band noise, and in the presence of nearby strong harmonic interference. We also call attention to a number of common errors in the application of windows when used with the fast Fourier transform. This paper includes a comprehensive catalog of data windows along with their significant performance parameters from which the different windows can be compared. Finally, an example demonstrates the use and value of windows to resolve closely spaced harmonic signals characterized by large differences in amplitude.", "title": "On the use of windows for harmonic analysis with the discrete Fourier transform"}, "df1a69782932b8583dfc5cfdff78211404cf5de4": {"paper_id": "df1a69782932b8583dfc5cfdff78211404cf5de4", "abstract": null, "title": "Evaluation of Security Level of Cryptography: RSA-OAEP, RSA-PSS, RSA Signature"}, "683c8f5c60916751bb23f159c86c1f2d4170e43f": {"paper_id": "683c8f5c60916751bb23f159c86c1f2d4170e43f", "abstract": null, "title": "Probabilistic Encryption"}, "1ff107c3230c51ae3cc8e0f14dced3eaebea9a8e": {"paper_id": "1ff107c3230c51ae3cc8e0f14dced3eaebea9a8e", "abstract": "An encryption method is presented with the novel property that publicly revealing an encryption key does not thereby reveal the corresponding decryption key. This has two important consequences: (1) Couriers or other secure means are not needed to transmit keys, since a message can be enciphered using an encryption key publicly revealed by the intented recipient. Only he can decipher the message, since only he knows the corresponding decryption key. (2) A message can be \u201csigned\u201d using a privately held decryption key. Anyone can verify this signature using the corresponding publicly revealed encryption key. Signatures cannot be forged, and a signer cannot later deny the validity of his signature. This has obvious applications in \u201celectronic mail\u201d and \u201celectronic funds transfer\u201d systems. A message is encrypted by representing it as a number M, raising M to a publicly specified power e, and then taking the remainder when the result is divided by the publicly specified product, n, of two large secret primer numbers p and q. Decryption is similar; only a different, secret, power d is used, where e * d \u2261 1(mod (p - 1) * (q - 1)). The security of the system rests in part on the difficulty of factoring the published divisor, n.", "title": "A Method for Obtaining Digital Signatures and Public-Key Cryptosystems"}, "e354ec85b8287bf15ed596be16ef6e422ccc29e7": {"paper_id": "e354ec85b8287bf15ed596be16ef6e422ccc29e7", "abstract": "-We develop a technique to test the hypothesis that multilavered../~'ed@~rward network,~ with [~'w units on the .Drst hidden layer ,~eneralize better than networks with many ttllits in the ~irst laver. Large networks are trained to per/orrn a class![)cation task and the redundant units are removed (\"pruning\") to produce the smallest network capable of'perf'orming the task. A teclmiqtte ,/~r inserting layers u'here /~rtttlitlg has introduced linear inseparability is also described. Two tests Of abilio' to generalize are used--the ability to classiflv training inputs corrupwd hv noise and the ability to classtlflv new patterns/)ore each class. The hypothes'is is f?~ltnd to be ,fa{s'e f~>r networks trained with noisy input.s'. Pruning to the mittitnum nt#nber c~f units in the ./irvt layer produces twtworks which correctly classify the training ,set hut j,,eneralize poor O' compared with lar~er ttetworks. Keywords--Neural Networks, Back-propagation, Pattern recognition, Generalization, t|idden units. Pruning. I N T R O D U C T I O N One of the major strengths of artificial neural networks is their ability to recognize or correctly classify patterns which have never been presented to the network before. Neural networks appear unique in their ability to extract the essential features from a training set and use them to identify new inputs. It is not known how the size or structure of a network affects this quality. This work concerns layered, feed-forward networks learning a classification task by back-propagation. Our desire was to investigate the relationship between network structure and the ability of the network to generalize from the training set. We examined the effect of noise in the training set on network structure and generalization, and we examined the effects of network size. This second could not be done simply by training networks of different sizes, since trained networks are not necessarily making effective use of all their hidden units. To address the question of the effective size of a network, we have been developing a technique of training a network which is known or suspected to be larger than required and then trimming off excess units to obtain the smallest network (Sietsma & Dow, Acknowledgement : The authors wish to thank Mr. David A. Penington for his valuable contributions, both in ideas and in excellent computer programs. Requests for reprints should be sent to J. Sietsma, USD, Material Research Laboratory, P.O. Box 50, Ascot Vale, Victoria 3032, Australia. 1988). This is slower than training the \"right'\" size network from the start, but it proves to have a number of advantages. When we started we had some assumptions about the relationship between size and ability to generalize, which we wished to test. If a network can be trained successfully with a small number of units on the first processing layer, these units must be extracting features of the classes which can be compactly expressed by the units and interpreted by the higher layers. This compact coding might be expected to imply good generalization performance. To have many units in a layer can allow a network to become overspecific, approximating a look-up table, particularly in the extreme where the number of units in the first processing layer is equal to the number of examplars in the training set. It has been suggested that networks with n'.,we layers, and fewer units in the early layers, may generalize better than \"'shallow\" networks with many units in each layer (Rumelhart, 1988). However, narrow networks with many layers are far harder to train than broad networks of one or two layers. Sometimes we found that after rigorous removal of inessential units, more layers were required to perform the task. This suggested a way of producing long, narrow networks. A broad network could be trained, trimmed to the fewest possible units, and then extra layers inserted to enable the network to relearn the solution. This avoids both the training difficulties and the problem that the smallest number of units needed for a task is not generally known. We could then test", "title": "Creating artificial neural networks that generalize"}, "2a1e1da81b535e1bead3fc2ab6af8b07877823b9": {"paper_id": "2a1e1da81b535e1bead3fc2ab6af8b07877823b9", "abstract": "The elements of the Hessian matrix consist of the second derivatives of the error measure with respect to the weights and thresholds in the network. They are needed in Bayesian estimation of network regularization parameters, for estimation of error bars on the network outputs, for network pruning algorithms, and for fast retraining of the network following a small change in the training data. In this paper we present an extended backpropagation algorithm that allows all elements of the Hessian matrix to be evaluated exactly for a feedforward network of arbitrary topology. Software implementation of the algorithm is straightforward.", "title": "Exact Calculation of the Hessian Matrix for the Multilayer Perceptron"}, "5fc8dd97ee6dcf60b474f2f169331a05b0242805": {"paper_id": "5fc8dd97ee6dcf60b474f2f169331a05b0242805", "abstract": "Fast retrieval methods are critical for large-scale and data-driven vision applications. Recent work has explored ways to embed high-dimensional features or complex distance functions into a low-dimensional Hamming space where items can be efficiently searched. However, existing methods do not apply for high-dimensional kernelized data when the underlying feature embedding for the kernel is unknown. We show how to generalize locality-sensitive hashing to accommodate arbitrary kernel functions, making it possible to preserve the algorithm's sub-linear time similarity search guarantees for a wide class of useful similarity functions. Since a number of successful image-based kernels have unknown or incomputable embeddings, this is especially valuable for image retrieval tasks. We validate our technique on several large-scale datasets, and show that it enables accurate and fast performance for example-based object classification, feature matching, and content-based retrieval.", "title": "Kernelized locality-sensitive hashing for scalable image search"}, "37807e97c624fb846df7e559553b32539ba2ea5d": {"paper_id": "37807e97c624fb846df7e559553b32539ba2ea5d", "abstract": "-We give conditions ensuring that multilayer jeedJorward networks with as Jew as a single hidden layer and an appropriately smooth hidden layer activation fimction are capable o f arbitrarily accurate approximation to an arbitrao' function and its derivatives. In fact, these networks can approximate functions that are not dtifferentiable in the classical sense, but possess only a generalized derivative, as is the case jor certain piecewise dtlf[erentiable Junctions. The conditions imposed on the hidden layer activation function are relatively mild: the conditions imposed on the domain o f the fimction to be approximated have practical intplications. Our approximation results provide a previously missing theoretical justification jor the use of multilayer /~'ed/brward networks in applications requiring simultaneous approximation o f a function and its derivatives. Keywords--Approximation, Derivatives, Sobolev space, Feedforward networks. 1. I N T R O D U C T I O N The capability of sufficiently complex multilayer feedforward networks to approximate an unknown mapping./~. R' ----~ R arbitrarily well has been recently investigated by Cybenko (1989), Funahashi (1989), Hecht-Nielsen (1989), Hornik, Stinchcombe, and White (1989) (HSW) (all for sigmoid hidden layer activation functions) and Stinchcombe and White (1989) (SW) (non-sigmoid hidden layer activation functions). In applications, it may be desirable to approximate not only the unknown mapping, but also its unknown derivatives. This is the case in Jordan's (1989) recent investigation of robot learning of smooth movement . Jordan states: The Jacobian matrix 8z/Ox . . . is the matrix that relates small changes in the controller output to small changes in the task space results and cannot be assumed to be available a priori, or provided by the environment. However, all of the derivatives in the matrix are forward derivatives. They are easily obtained by differentiation if a forward model is available. The forward model itself must be learned, but this can be achieved directly by system idenAcknowledgements: We arc indebted to Angelo Melino for pressing us on the issue addressed here and to the referees for numerous helpful suggestions. White's participation was supported by NSF Grant SES-8806990. Requests for reprints should be sent to Halbert White, Department of Economics, D-008, University of California, San Diego. La Jolla, CA 92093. tification. Once the model is accurate over a particular domain, its derivatives provide a learning operator that allows the system to convert errors in task space into errors in articulatory space and thereby change the controller. Thus, learning an adequate approximation to the Jacobian matrix of an unknown mapping is a key component of Jordan 's approach to robot learning of smooth movement . Despite the success of Jordan 's experiments, there is no existing theoretical guarantee that multilayer feedforward networks generally have the capability to approximate an unknown mapping and its derivatives simultaneously. For example, a network with hard limiting hidden layer activations approximates unknown mappings with a piecewise-constant function, the first derivatives of which exist and are zero almost everywhere. Obviously, the derivatives of such a network output function cannot approximate the derivatives of an arbitrary function. Intuition suggests that networks having smooth hidden layer activation functions ought to have output function derivatives that will approximate the derivatives of an unknown mapping. However , the justification for this intuition is not obvious. Consider the class of single hidden layer feedforward networks having network output functions belonging to the set 2. \"(G) -~ {g : x\"-~ R[ g(x) : /:,G(_i';,,):", "title": "Universal approximation of an unknown mapping and its derivatives using multilayer feedforward networks"}, "e706dde7b062300e7efff65cd6dbe0cda5f13e6d": {"paper_id": "e706dde7b062300e7efff65cd6dbe0cda5f13e6d", "abstract": "Two groups of 8-year-old children who were monolingual or bilingual completed a complex classification task in which they made semantic judgments on stimuli that were presented either visually or auditorily. The task requires coordinating a variety of executive control components, specifically working memory, inhibition, and shifting. When each of the visual and auditory tasks was presented alone, performance was comparable for children in the two groups. Combining the two modalities into a dual-task paradigm made the task more difficult, and on this combined task bilingual children maintained better accuracy than monolingual children, especially on the visual task. The results are interpreted in terms of the enhanced ability of bilingual children to coordinate the executive control components required in performing this complex task.", "title": "Coordination of executive functions in monolingual and bilingual children."}, "51a3b91ff58dd31b5e4afbc6e192f0bb484e0ac4": {"paper_id": "51a3b91ff58dd31b5e4afbc6e192f0bb484e0ac4", "abstract": "This study explored the structure of verbal and visuospatial short-term and working memory in children between ages 4 and 11 years. Multiple tasks measuring 4 different memory components were used to capture the cognitive processes underlying working memory. Confirmatory factor analyses indicated that the processing component of working memory tasks was supported by a common resource pool, while storage aspects depend on domain-specific verbal and visuospatial resources. This model is largely stable across this developmental period, although some evidence exists that the links between the domain-specific visuospatial construct and the domain-general processing construct were higher in the 4- to- 6-year age group. The data also suggest that all working memory components are in place by 4 years of age.", "title": "Verbal and visuospatial short-term and working memory in children: are they separable?"}, "49f7a58aaf92ee772306f8413b2187fdb6090a4c": {"paper_id": "49f7a58aaf92ee772306f8413b2187fdb6090a4c", "abstract": "The discovery of premotor and parietal cells known as mirror neurons in the macaque brain that fire not only when the animal is in action, but also when it observes others carrying out the same actions provides a plausible neurophysiological mechanism for a variety of important social behaviours, from imitation to empathy. Recent data also show that dysfunction of the mirror neuron system in humans might be a core deficit in autism, a socially isolating condition. Here, we review the neurophysiology of the mirror neuron system and its role in social cognition and discuss the clinical implications of mirror neuron dysfunction.", "title": "The mirror neuron system and the consequences of its dysfunction"}, "b6da427a7f1a9928539d8a4df819e50ad5c14d98": {"paper_id": "b6da427a7f1a9928539d8a4df819e50ad5c14d98", "abstract": "The structure of working memory and its development across the childhood years were investigated in children 4-15 years of age. The children were given multiple assessments of each component of the A. D. Baddeley and G. Hitch (1974) working memory model. Broadly similar linear functions characterized performance on all measures as a function of age. From 6 years onward, a model consisting of 3 distinct but correlated factors corresponding to the working memory model provided a good fit to the data. The results indicate that the basic modular structure of working memory is present from 6 years of age and possibly earlier, with each component undergoing sizable expansion in functional capacity throughout the early and middle school years to adolescence.", "title": "The structure of working memory from 4 to 15 years of age."}, "b1dd96621eac6a4bf53b0f7a9a23470bf756553c": {"paper_id": "b1dd96621eac6a4bf53b0f7a9a23470bf756553c", "abstract": "Predictions concerning development, interrelations, and possible independence of working memory, inhibition, and cognitive flexibility were tested in 325 participants (roughly 30 per age from 4 to 13 years and young adults; 50% female). All were tested on the same computerized battery, designed to manipulate memory and inhibition independently and together, in steady state (single-task blocks) and during task-switching, and to be appropriate over the lifespan and for neuroimaging (fMRI). This is one of the first studies, in children or adults, to explore: (a) how memory requirements interact with spatial compatibility and (b) spatial incompatibility effects both with stimulus-specific rules (Simon task) and with higher-level, conceptual rules. Even the youngest children could hold information in mind, inhibit a dominant response, and combine those as long as the inhibition required was steady-state and the rules remained constant. Cognitive flexibility (switching between rules), even with memory demands minimized, showed a longer developmental progression, with 13-year-olds still not at adult levels. Effects elicited only in Mixed blocks with adults were found in young children even in single-task blocks; while young children could exercise inhibition in steady state it exacted a cost not seen in adults, who (unlike young children) seemed to re-set their default response when inhibition of the same tendency was required throughout a block. The costs associated with manipulations of inhibition were greater in young children while the costs associated with increasing memory demands were greater in adults. Effects seen only in RT in adults were seen primarily in accuracy in young children. Adults slowed down on difficult trials to preserve accuracy; but the youngest children were impulsive; their RT remained more constant but at an accuracy cost on difficult trials. Contrary to our predictions of independence between memory and inhibition, when matched for difficulty RT correlations between these were as high as 0.8, although accuracy correlations were less than half that. Spatial incompatibility effects and global and local switch costs were evident in children and adults, differing only in size. Other effects (e.g., asymmetric switch costs and the interaction of switching rules and switching response-sites) differed fundamentally over age.", "title": "Development of cognitive control and executive functions from 4 to 13 years: Evidence from manipulations of memory, inhibition, and task switching"}, "29d0086027dcc741b12880787360237e63734d12": {"paper_id": "29d0086027dcc741b12880787360237e63734d12", "abstract": "Two views of bilingualism are presented--the monolingual or fractional view which holds that the bilingual is (or should be) two monolinguals in one person, and the bilingual or wholistic view which states that the coexistence of two languages in the bilingual has produced a unique and specific speaker-hearer. These views affect how we compare monolinguals and bilinguals, study language learning and language forgetting, and examine the speech modes--monolingual and bilingual--that characterize the bilingual's everyday interactions. The implications of the wholistic view on the neurolinguistics of bilingualism, and in particular bilingual aphasia, are discussed.", "title": "Neurolinguists, beware! The bilingual is not two monolinguals in one person"}, "2db0c6e9b5e6c25d0b0340d76ecef4edd7d5fe8f": {"paper_id": "2db0c6e9b5e6c25d0b0340d76ecef4edd7d5fe8f", "abstract": "Ninety-six participants, who were younger (20 years) or older (68 years) adults and either monolingual or bilingual, completed tasks assessing working memory, lexical retrieval, and executive control. Younger participants performed most of the tasks better than older participants, confirming the effect of aging on these processes. The effect of language group was different for each type of task: Monolinguals and bilinguals performed similarly on working memory tasks, monolinguals performed better on lexical retrieval tasks, and bilinguals performed better on executive control tasks, with some evidence for larger language group differences in older participants on the executive control tasks. These results replicate findings from individual studies obtained using only 1 type of task and different participants. The confirmation of this pattern in the same participants is discussed in terms of a suggested explanation of how the need to manage 2 language systems leads to these different outcomes for cognitive and linguistic functions.", "title": "Cognitive control and lexical access in younger and older bilinguals."}, "086f9dbf8d1cf11a5184327b7df73821dd92c590": {"paper_id": "086f9dbf8d1cf11a5184327b7df73821dd92c590", "abstract": "In their first years, children's understanding of mental states seems to improve dramatically, but the mechanisms underlying these changes are still unclear. Such 'theory of mind' (ToM) abilities may arise during development, or have an innate basis, developmental changes reflecting limitations of other abilities involved in ToM tasks (e.g. inhibition). Special circumstances such as early bilingualism may enhance ToM development or other capacities required by ToM tasks. Here we compare 3-year-old bilinguals and monolinguals on a standard ToM task, a modified ToM task and a control task involving physical reasoning. The modified ToM task mimicked a language-switch situation that bilinguals often encounter and that could influence their ToM abilities. If such experience contributes to an early consolidation of ToM in bilinguals, they should be selectively enhanced in the modified task. In contrast, if bilinguals have an advantage due to better executive inhibitory abilities involved in ToM tasks, they should outperform monolinguals on both ToM tasks, inhibitory demands being similar. Bilingual children showed an advantage on the two ToM tasks but not on the control task. The precocious success of bilinguals may be associated with their well-developed control functions formed during monitoring and selecting languages.", "title": "Early bilingualism enhances mechanisms of false-belief reasoning."}, "84d64b31c0640cfe98b74095940f561fca72a536": {"paper_id": "84d64b31c0640cfe98b74095940f561fca72a536", "abstract": "WORKING memory refers to a system for temporary storage and manipulation of information in the brain, a function critical for a wide range of cognitive operations. It has been proposed that working memory includes a central executive system (CES) to control attention and information flow to and from verbal and spatial short-term memory buffers1. Although the prefrontal cortex is activated during both verbal and spatial passive working memory tasks2\u20138, the brain regions involved in the CES component of working memory have not been identified. We have used functional magnetic resonance imaging (fMRI) to examine brain activation during the concurrent performance of two tasks, which is expected to engage the CES. Activation of the prefrontal cortex was observed when both tasks are performed together, but not when they are performed separately. These results support the view that the prefrontal cortex is involved in human working memory.", "title": "The neural basis of the central executive system of working memory"}, "2a6dc23584804d65e5c572881b55dc26abd034c5": {"paper_id": "2a6dc23584804d65e5c572881b55dc26abd034c5", "abstract": "The human frontal cortex helps mediate working memory, a system that is used for temporary storage and manipulation of information and that is involved in many higher cognitive functions. Working memory includes two components: short-term storage (on the order of seconds) and executive processes that operate on the contents of storage. Recently, these two components have been investigated in functional neuroimaging studies. Studies of storage indicate that different frontal regions are activated for different kinds of information: storage for verbal materials activates Broca's area and left-hemisphere supplementary and premotor areas; storage of spatial information activates the right-hemisphere premotor cortex; and storage of object information activates other areas of the prefrontal cortex. Two of the fundamental executive processes are selective attention and task management. Both processes activate the anterior cingulate and dorsolateral prefrontal cortex.", "title": "Storage and executive processes in the frontal lobes."}, "426ec3282590af5d845f6eabfe625080df5d7b25": {"paper_id": "426ec3282590af5d845f6eabfe625080df5d7b25", "abstract": "184 L earning to read is one of the greatest accomplishments in childhood because it is the foundation for learning and academic achievement. Therefore, it is not surprising that debates among educators about how best to help children learn to read have been heated, polarized, and unsettled for many years. The intensity of the debates , coupled with enormous political pressures and commercial interests, has made learning to read a contentious public issue in the United States. What makes the debates at the beginning of the new century different than similar debates during the past 50 years is a greater than ever reliance on scientific evidence to guide educational policies for assessment and instruction. The attention and new credibility given to reading research have been hard won by the academic community and have great promise, but there are also pitfalls to avoid in the rush to use basic research for legislated policies and educational prescriptions (Berliner, 2002; Feuer, Towne, & Shavelson, 2002). The purpose of this article is to reveal flaws in traditional research on developing reading skills that have wide ramifications. My goal is to stimulate researchers to reconsider how to conceptualize, research, and interpret the development of reading skills. This reinterpretation is necessary because traditional reading research has ignored fundamental differences in the developmental trajec-tories of reading skills. These different trajectories are manifested in different times of skill onset, different durations of acquisition, and different asymptotic levels of performance. What is most important is that there are some skills that are more constrained than others; they are learned quickly, mastered entirely, and should not be conceptualized as enduring individual difference variables. Constrained skills have been analyzed with the same research tools and parametric statistical analyses as unconstrained skills. I claim that this is a mistake that can lead to spurious claims about early reading skills. The theoretical focus of this article is on construct validity and the developmental differences among reading skills. The methodologi-cal focus is on the distributions of longitudinal data and the methods to examine 185 THEORIES ABOUT reading have neglected basic differences in the developmental trajectories of skills related to reading. This essay proposes that some reading skills, such as learning the letters of the alphabet, are constrained to small sets of knowledge that are mastered in relatively brief periods of development. In contrast, other skills, such as vocabulary, are unconstrained by the knowledge to \u2026", "title": "Reinterpreting the development of reading skills"}, "aa39a6a7a67f1bfbb7109f783e72d65b26692d21": {"paper_id": "aa39a6a7a67f1bfbb7109f783e72d65b26692d21", "abstract": "A correlational study examined relationships between motivational orientation, self-regulated learning, and classroom academic performance for 173 seventh graders from eight science and seven English classes. A self-report measure of student self-efficacy, intrinsic value, test anxiety, self-regulation, and use of learning strategies was administered, and performance data were obtained from work on classroom assignments. Self-efficacy and intrinsic value were positively related to cognitive engagement and performance. Regression analyses revealed that, depending on the outcome measure, self-regulation, self-efficacy, and test anxiety emerged as the best predictors of performance. Intrinsic value did not have a direct influence on performance but was strongly related to self-regulation and cognitive strategy use, regardless of prior achievement level. The implications of individual differences in motivational orientation for cognitive engagement and self-regulation in the classroom are discussed.", "title": "Motivational and Self-Regulated Learning Components of Classroom Academic Performance"}, "5e22dbd58cbf86619784c805aa87a5456bafd208": {"paper_id": "5e22dbd58cbf86619784c805aa87a5456bafd208", "abstract": "Semantically, objects in unstructured document are related each other to perform a certain entity relation. This certain entity relation such: drug-drug interaction through their compounds, buyer-seller relationship through the goods or services, etc. Motivated by that kind of interaction, this study proposes a method to extract those objects and their interactions. It is presented a general framework of objectinteraction mining of large corpora. The framework is started with the initial step in extracting a single object in the unstructured document. In this study, the initial step is a pattern learning method that is applied to drug-label documents to extract drug-names. We utilize an existing external knowledge to identify a certain regular expressions surrounding the targeted object and the probabilities of that regular expression, to perform the pattern learning process. The performance of this pattern learning approach is promising to apply in this relation extraction area. As presented in the results of this study, the best f-score performance of this method is 0.78 f-score. With adjusting of some parameters and or improving the method, the performance can be potentially improved", "title": "Mining Relation Extraction Based on Pattern Learning Approach"}, "5e214a2af786fadb419e9e169a252c6ca6e7d9f0": {"paper_id": "5e214a2af786fadb419e9e169a252c6ca6e7d9f0", "abstract": "This volume takes a broad view of information extraction as any method for ltering information from large volumes of text. This includes the retrieval of documents from collections and the tagging of particular terms in text. In this paper we shall use a narrower de nition: the identi cation of instances of a particular class of events or relationships in a natural language text, and the extraction of the relevant arguments of the event or relationship. Information extraction therefore involves the creation of a structured representation (such as a data base) of selected information drawn from the text. The idea of reducing the information in a document to a tabular structure is not new. Its feasibility for sublanguage texts was suggested by Zellig Harris in the 1950's, and an early implementation for medical texts was done at New York University by Naomi Sager[20]. However, the speci c notion of information extraction described here has received wide currency over the last decade through the series of Message Understanding Conferences [1, 2, 3, 4, 14]. We shall discuss these Conferences in more detail a bit later, and shall use simpli ed versions of extraction tasks from these Conferences as examples throughout this paper. Figure 1 shows a simpli ed example from one of the earlier MUC's, involving terrorist events (MUC-3) [1]. For each terrorist event, the system had to determine the type of attack (bombing, arson, etc.), the date, location, perpetrator (if stated), targets, and e ects on targets. Other examples of extraction tasks are international joint ventures (where the arguments included the partners, the new venture, its product or service, etc.) and executive succession (indicating who was hired or red by which company for which position). Information extraction is a more limited task than \\full text understanding\". In full text understanding, we aspire to represent in a explicit fashion all the information in a text. In contrast, in information extraction we delimit in advance, as part of the speci cation of the task, the semantic range of the output: the relations we will represent, and the allowable llers in each slot of a relation.", "title": "Information Extraction: Techniques and Challenges"}, "1a0a415178d20754418987e312681f3fb0b40257": {"paper_id": "1a0a415178d20754418987e312681f3fb0b40257", "abstract": "This paper discusses the use of unlabeled examples for the problem of named entity classification. A large number of rules is needed for coverage of the domain, suggesting that a fairly large number of labeled examples should be required to train a classifier. However, we show that the use of unlabeled data can reduce the requirements for supervision to just 7 simple \u201cseed\u201d rules. The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type. We present two algorithms. The first method uses a similar algorithm to that of (Yarowsky 95), with modifications motivated by (Blum and Mitchell 98). The second algorithm extends ideas from boosting algorithms, designed for supervised learning tasks, to the framework suggested by (Blum and Mitchell 98).", "title": "Unsupervised Models for Named Entity Classification"}, "15a3ed7dfc005aa6f84f1e1fb81ef6d54ce3d8aa": {"paper_id": "15a3ed7dfc005aa6f84f1e1fb81ef6d54ce3d8aa", "abstract": "We consider the problem of using a large unla beled sample to boost performance of a learn ing algorithm when only a small set of labeled examples is available In particular we con sider a problem setting motivated by the task of learning to classify web pages in which the description of each example can be partitioned into two distinct views For example the de scription of a web page can be partitioned into the words occurring on that page and the words occurring in hyperlinks that point to that page We assume that either view of the example would be su cient for learning if we had enough labeled data but our goal is to use both views together to allow inexpensive unlabeled data to augment a much smaller set of labeled ex amples Speci cally the presence of two dis tinct views of each example suggests strategies in which two learning algorithms are trained separately on each view and then each algo rithm s predictions on new unlabeled exam ples are used to enlarge the training set of the other Our goal in this paper is to provide a PAC style analysis for this setting and more broadly a PAC style framework for the general problem of learning from both labeled and un labeled data We also provide empirical results on real web page data indicating that this use of unlabeled examples can lead to signi cant improvement of hypotheses in practice This paper is to appear in the Proceedings of the Conference on Computational Learning Theory This research was supported in part by the DARPA HPKB program under contract F and by NSF National Young Investigator grant CCR INTRODUCTION In many machine learning settings unlabeled examples are signi cantly easier to come by than labeled ones One example of this is web page classi cation Suppose that we want a program to electronically visit some web site and download all the web pages of interest to us such as all the CS faculty member pages or all the course home pages at some university To train such a system to automatically classify web pages one would typically rely on hand labeled web pages These labeled examples are fairly expensive to obtain because they require human e ort In contrast the web has hundreds of millions of unlabeled web pages that can be inexpensively gathered using a web crawler Therefore we would like our learning algorithm to be able to take as much advantage of the unlabeled data as possible This web page learning problem has an interesting feature Each example in this domain can naturally be described using several di erent kinds of information One kind of information about a web page is the text appearing on the document itself A second kind of information is the anchor text attached to hyperlinks pointing to this page from other pages on the web The two problem characteristics mentioned above availability of both labeled and unlabeled data and the availability of two di erent kinds of information about examples suggest the following learning strat egy Using an initial small set of labeled examples nd weak predictors based on each kind of information for instance we might nd that the phrase research inter ests on a web page is a weak indicator that the page is a faculty home page and we might nd that the phrase my advisor on a link is an indicator that the page being pointed to is a faculty page Then attempt to bootstrap from these weak predictors using unlabeled data For instance we could search for pages pointed to with links having the phrase my advisor and use them as probably positive examples to further train a learning algorithm based on the words on the text page and vice versa We call this type of bootstrapping co training and it has a close connection to bootstrapping from incomplete data in the Expectation Maximization setting see for instance The question this raises is is there any reason to believe co training will help Our goal is to address this question by developing a PAC style theoretical framework to better understand the issues involved in this approach We also give some preliminary empirical results on classifying university web pages see Section that are encouraging in this context More broadly the general question of how unlabeled examples can be used to augment labeled data seems a slippery one from the point of view of standard PAC as sumptions We address this issue by proposing a notion of compatibility between a data distribution and a target function Section and discuss how this relates to other approaches to combining labeled and unlabeled data Section", "title": "Combining Labeled and Unlabeled Data with Co-Training"}, "acec622ca4fb7e01a56116522d35ded149969d0a": {"paper_id": "acec622ca4fb7e01a56116522d35ded149969d0a", "abstract": "Many corpus-based natural language processing systems rely on text corpora that have been manually annotated with syntactic or semantic tags. In particular, all previous dictionary construction systems for information extraction have used an annotated training corpus or some form of annotated input. We have developed a system called AutoSlog-TS that creates dictionaries of extraction patterns using only untagged text. AutoSlog-TS is based on the AutoSlog system, which generated extraction patterns using annotated text and a set of heuristic rules. By adapting AutoSlog and combining it with statistical techniques, we eliminated its dependency on tagged text. In experiments with the MUG-4 terrorism domain, AutoSlogTS created a dictionary of extraction patterns that performed comparably to a dictionary created by AutoSlog, using only preclassified texts as input.", "title": "Automatically Generating Extraction Patterns from Untagged Text"}, "edc9c380fd142fb7b0c6a8ee5858abf038866cb9": {"paper_id": "edc9c380fd142fb7b0c6a8ee5858abf038866cb9", "abstract": "Several research initiatives have been undertaken to map fishing effort at high spatial resolution using the Vessel Monitoring System (VMS). An alternative to the VMS is represented by the Automatic Identification System (AIS), which in the EU became compulsory in May 2014 for all fishing vessels of length above 15 meters. The aim of this paper is to assess the uptake of the AIS in the EU fishing fleet and the feasibility of producing a map of fishing effort with high spatial and temporal resolution at European scale. After analysing a large AIS dataset for the period January-August 2014 and covering most of the EU waters, we show that AIS was adopted by around 75% of EU fishing vessels above 15 meters of length. Using the Swedish fleet as a case study, we developed a method to identify fishing activity based on the analysis of individual vessels' speed profiles and produce a high resolution map of fishing effort based on AIS data. The method was validated using detailed logbook data and proved to be sufficiently accurate and computationally efficient to identify fishing grounds and effort in the case of trawlers, which represent the largest portion of the EU fishing fleet above 15 meters of length. Issues still to be addressed before extending the exercise to the entire EU fleet are the assessment of coverage levels of the AIS data for all EU waters and the identification of fishing activity in the case of vessels other than trawlers.", "title": "Mapping Fishing Effort through AIS Data"}, "a4d5ff6f1fb8b304c3e6fd5f1a7abd9b5c52955c": {"paper_id": "a4d5ff6f1fb8b304c3e6fd5f1a7abd9b5c52955c", "abstract": "Machine learning is increasingly used to make sense of the physical world yet may suffer from adversarial manipulation. We examine the Viola-Jones 2D face detection algorithm to study whether images can be created that humans do not notice as faces yet the algorithm detects as faces. We show that it is possible to construct images that Viola-Jones recognizes as containing faces yet no human would consider a face. Moreover, we show that it is possible to construct images that fool facial detection even when they are printed and then photographed.", "title": "Spoofing 2D Face Detection: Machines See People Who Aren't There"}, "6bdcedb895256357a6bc8ffef5a0790697403372": {"paper_id": "6bdcedb895256357a6bc8ffef5a0790697403372", "abstract": "Many machine learning models are vulnerable to adversarial examples: inputs that are specially crafted to cause a machine learning model to produce an incorrect output. Adversarial examples that affect one model often affect another model, even if the two models have different architectures or were trained on different training sets, so long as both models were trained to perform the same task. An attacker may therefore train their own substitute model, craft adversarial examples against the substitute, and transfer them to a victim model, with very little information about the victim. Recent work has further developed a technique that uses the victim model as an oracle to label a synthetic training set for the substitute, so the attacker need not even collect a training set to mount the attack. We extend these recent techniques using reservoir sampling to greatly enhance the efficiency of the training procedure for the substitute model. We introduce new transferability attacks between previously unexplored (substitute, victim) pairs of machine learning model classes, most notably SVMs and decision trees. We demonstrate our attacks on two commercial machine learning classification systems from Amazon (96.19% misclassification rate) and Google (88.94%) using only 800 queries of the victim model, thereby showing that existing machine learning approaches are in general vulnerable to systematic black-box attacks regardless of their structure.", "title": "Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples"}, "2cf3fd84f30e5cae30dd46a3d7ecc0d63583b1a6": {"paper_id": "2cf3fd84f30e5cae30dd46a3d7ecc0d63583b1a6", "abstract": "In security-sensitive applications, the success of machine learning depends on a thorough vetting of their resistance to adversarial data. In one pertinent, well-motivated attack scenario, an adversary may attempt to evade a deployed system at test time by carefully manipulating attack samples. In this work, we present a simple but effective gradientbased approach that can be exploited to systematically assess the security of several, widely-used classification algorithms against evasion attacks. Following a recently proposed framework for security evaluation, we simulate attack scenarios that exhibit different risk levels for the classifier by increasing the attacker\u2019s knowledge of the system and her ability to manipulate attack samples. This gives the classifier designer a better picture of the classifier performance under evasion attacks, and allows him to perform a more informed model selection (or parameter setting). We evaluate our approach on the relevant security task of malware detection in PDF files, and show that such systems can be easily evaded. We also sketch some countermeasures suggested by our analysis.", "title": "Evasion Attacks against Machine Learning at Test Time"}, "35734e8724559fb0d494e5cba6a28ad7a3d5dd4d": {"paper_id": "35734e8724559fb0d494e5cba6a28ad7a3d5dd4d", "abstract": "Several machine learning models, including neural networks, consistently misclassify adversarial examples\u2014inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks\u2019 vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.", "title": "Explaining and Harnessing Adversarial Examples"}, "94187ef33e34af2cdb42502083c6f9b4c3f5ba6b": {"paper_id": "94187ef33e34af2cdb42502083c6f9b4c3f5ba6b", "abstract": "Machine learning (ML) models, e.g., state-of-the-art deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modified to yield erroneous model outputs, while appearing unmodified to human observers. Potential attacks include having malicious content like malware identified as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of either the model internals or its training data. We introduce the first practical demonstration of an attacker controlling a remotely hosted DNN with no such knowledge. Indeed, the only capability of our black-box adversary is to observe labels given by the DNN to chosen inputs. Our attack strategy consists in training a local model to substitute for the target DNN, using inputs synthetically generated by an adversary and labeled by the target DNN. We then use the local substitute to craft adversarial examples, and find that they are misclassified by the targeted DNN. To perform a real-world and properly-blinded evaluation, we attack a DNN hosted by MetaMind, an online deep learning API. After labeling 6,400 synthetic inputs to train our substitute (an order of magnitude smaller than the training set used by MetaMind), we find that their DNN misclassifies adversarial examples crafted with our substitute at a rate of 84.24%. We demonstrate that our strategy generalizes to many ML techniques like logistic regression or SVMs, regardless of the ML model chosen for the substitute. We instantiate the same attack against models hosted by Amazon and Google, using logistic regression substitutes trained with only 800 label queries. They yield adversarial examples misclassified by Amazon and Google at rates of 96.19% and 88.94%. We also find that this black-box attack strategy is capable of evading defense strategies previously found to make adversarial example crafting harder. We conclude with experiments exploring why adversarial samples transfer between models.", "title": "Practical Black-Box Attacks against Deep Learning Systems using Adversarial Examples"}, "907ebe1847c904780c8715da11b21a308c94d697": {"paper_id": "907ebe1847c904780c8715da11b21a308c94d697", "abstract": "Widespread deployment of biometric systems supporting consumer transactions is starting to occur. Smart consumer devices, such as tablets and phones, have the potential to act as biometric readers authenticating user transactions. However, the use of these devices in uncontrolled environments is highly susceptible to replay attacks, where these biometric data are captured and replayed at a later time. Current approaches to counter replay attacks in this context are inadequate. In order to show this, we demonstrate a simple replay attack that is 100% effective against a recent state-of-the-art face recognition system; this system was specifically designed to robustly distinguish between live people and spoofing attempts, such as photographs. This paper proposes an approach to counter replay attacks for face recognition on smart consumer devices using a noninvasive challenge and response technique. The image on the screen creates the challenge, and the dynamic reflection from the person's face as they look at the screen forms the response. The sequence of screen images and their associated reflections digitally watermarks the video. By extracting the features from the reflection region, it is possible to determine if the reflection matches the sequence of images that were displayed on the screen. Experiments indicate that the face reflection sequences can be classified under ideal conditions with a high degree of confidence. These encouraging results may pave the way for further studies in the use of video analysis for defeating biometric replay attacks on consumer devices.", "title": "Face Recognition on Consumer Devices: Reflections on Replay Attacks"}, "cfb06ca51d03b7e625678d97d4661db69e2ee534": {"paper_id": "cfb06ca51d03b7e625678d97d4661db69e2ee534", "abstract": "Voice interfaces are becoming more ubiquitous and are now the primary input method for many devices. We explore in this paper how they can be attacked with hidden voice commands that are unintelligible to human listeners but which are interpreted as commands by devices. We evaluate these attacks under two different threat models. In the black-box model, an attacker uses the speech recognition system as an opaque oracle. We show that the adversary can produce difficult to understand commands that are effective against existing systems in the black-box model. Under the white-box model, the attacker has full knowledge of the internals of the speech recognition system and uses it to create attack commands that we demonstrate through user testing are not understandable by humans. We then evaluate several defenses, including notifying the user when a voice command is accepted; a verbal challenge-response protocol; and a machine learning approach that can detect our attacks with 99.8% accuracy.", "title": "Hidden Voice Commands"}, "04e34e689386604ab37780c48797352321f95102": {"paper_id": "04e34e689386604ab37780c48797352321f95102", "abstract": "Signal processing and pattern recognition algorithms make extensive use of convolution. In many cases, computational accuracy is not as important as computational speed. In feature extraction, for instance, the features of interest in a signal are usually quite distorted. This form of noise justi es some level of quantization in order to achieve faster feature extraction. Our approach consists of approximating regions of the signal with low degree polynomials, and then di erentiating the resulting signals in order to obtain impulse functions (or derivatives of impulse functions). With this representation, convolution becomes extremely simple and can be implemented quite e ectively. The true convolution can be recovered by integrating the result of the convolution. This method yields substantial speed up in feature extraction and is applicable to convolutional neural networks.", "title": "Boxlets: A Fast Convolution Algorithm for Signal Processing and Neural Networks"}, "138f8fc3e05509eb9d43d0446fcff21a73cf06ae": {"paper_id": "138f8fc3e05509eb9d43d0446fcff21a73cf06ae", "abstract": "This course will provide an introduction to statistical pattern recognition. The lectures will focus on different techniques including methods for feature extraction, dimensionality reduction, data clustering and pattern classification. State-of-art approaches such as ensemble learning and sparse modelling will be introduced. Selected real-world applications will illustrate how the techniques are applied in practice.", "title": "Statistical Pattern Recognition"}, "81a47b8599c70549a12cb79ef788c69ea1ac2bb0": {"paper_id": "81a47b8599c70549a12cb79ef788c69ea1ac2bb0", "abstract": "As mobile devices are equipped with more memory and computational capability, a novel peer-to-peer communication model for mobile cloud computing is proposed to interconnect nearby mobile devices through various short range radio communication technologies to form mobile cloudlets, where every mobile device works as either a computational service provider or a client of a service requester. Though this kind of computation offloading benefits compute-intensive applications, the corresponding service models and analytics tools are remaining open issues. In this paper we categorize computation offloading into three modes: remote cloud service mode, connected ad hoc cloudlet service mode, and opportunistic ad hoc cloudlet service mode. We also conduct a detailed analytic study for the proposed three modes of computation offloading at ad hoc cloudlet.", "title": "On the computation offloading at ad hoc cloudlet: architecture and service modes"}, "0a4110fda21f0de29824ead1df591d2c5e1da8d0": {"paper_id": "0a4110fda21f0de29824ead1df591d2c5e1da8d0", "abstract": "As cloud services grow to span more and more globally distributed datacenters, there is an increasingly urgent need for automated mechanisms to place application data across these datacenters. This placement must deal with business constraints such as WAN bandwidth costs and datacenter capacity limits, while also minimizing user-perceived latency. The task of placement is further complicated by the issues of shared data, data inter-dependencies, application changes and user mobility. We document these challenges by analyzing month-long traces from Microsoft's Live Messenger and Live Mesh, two large-scale commercial cloud services.", "title": "Volley: Automated Data Placement for Geo-Distributed Cloud Services"}, "3bddba3214fe9e7248a934455a2624aa5b781778": {"paper_id": "3bddba3214fe9e7248a934455a2624aa5b781778", "abstract": "Previous work analyzing social networks has mainly focused on binary friendship relations. However, in online social networks the low cost of link formation can lead to networks with heterogeneous relationship strengths (e.g., acquaintances and best friends mixed together). In this case, the binary friendship indicator provides only a coarse representation of relationship information. In this work, we develop an unsupervised model to estimate relationship strength from interaction activity (e.g., communication, tagging) and user similarity. More specifically, we formulate a link-based latent variable model, along with a coordinate ascent optimization procedure for the inference. We evaluate our approach on real-world data from Facebook and LinkedIn, showing that the estimated link weights result in higher autocorrelation and lead to improved classification accuracy.", "title": "Modeling relationship strength in online social networks"}, "21968ae000669eb4cf03718a0d97e23a6bf75926": {"paper_id": "21968ae000669eb4cf03718a0d97e23a6bf75926", "abstract": "Recently, there has been tremendous interest in the phenomenon of influence propagation in social networks. The studies in this area assume they have as input to their problems a social graph with edges labeled with probabilities of influence between users. However, the question of where these probabilities come from or how they can be computed from real social network data has been largely ignored until now. Thus it is interesting to ask whether from a social graph and a log of actions by its users, one can build models of influence. This is the main problem attacked in this paper. In addition to proposing models and algorithms for learning the model parameters and for testing the learned models to make predictions, we also develop techniques for predicting the time by which a user may be expected to perform an action. We validate our ideas and techniques using the Flickr data set consisting of a social graph with 1.3M nodes, 40M edges, and an action log consisting of 35M tuples referring to 300K distinct actions. Beyond showing that there is genuine influence happening in a real social network, we show that our techniques have excellent prediction performance.", "title": "Learning influence probabilities in social networks"}, "4515d348e13fd31e2e7f3df1ab8c04899dd77d59": {"paper_id": "4515d348e13fd31e2e7f3df1ab8c04899dd77d59", "abstract": "If cloud computing (CC) is to achieve its potential, there needs to be a clear understanding of the various issues involved, both from the perspectives of the providers and the consumers of the technology. There is an equally urgent need for understanding the business-related issues surrounding CC. We interviewed several industry executives who are either involved as developers or are evaluating CC as an enterprise user. We identify the strengths, weaknesses, opportunities and threats for the industry. We also identify the various issues that will affect the different stakeholders of CC. We issue a set of recommendations for the practitioners who will provide and manage this technology. For IS researchers, we outline the different areas of research that need attention so that we are in a position to advise the industry in the years to come. Finally, we outline some of the key issues facing governmental agencies who will be involved in the regulation of cloud computing.", "title": "Cloud Computing - The Business Perspective"}, "6d3e787b471c3328053c581e8f93e41c82dfabab": {"paper_id": "6d3e787b471c3328053c581e8f93e41c82dfabab", "abstract": "We are concerned that the IS research community is making the discipline's central identity ambiguous by, all too frequently, under-investigating phenomena intimately associated with IT-based systems and over-investigating phenomena distantly associated with IT-based systems. In this 1 Ron Weber was the accepting senior editor for this paper. commentary, we begin by discussing why establishing an identity for the IS field is important. We then describe what such an identity may look like by proposing a core set of properties, i.e., concepts and phenomena, that define the IS field. Next, we discuss research by IS scholars that either fails to address this core set of properties (labeled as error of exclusion) or that addresses concepts/phenomena falling outside this core set (labeled as error of inclusion). We conclude by offering suggestions for redirecting IS scholarship toward the concepts and phenomena that we argue define the core of the IS discipline.", "title": "The Identity Crisis Within the IS Discipline: Defining and Communicating the Discipline's Core Properties"}, "be4fb1ad3317228b60938d83aca27b496fbcb5d4": {"paper_id": "be4fb1ad3317228b60938d83aca27b496fbcb5d4", "abstract": "Your use of the JSTOR archive indicates your acceptance of JSTOR's Terms and Conditions of Use, available at . http://www.jstor.org/page/info/about/policies/terms.jsp. JSTOR's Terms and Conditions of Use provides, in part, that unless you have obtained prior permission, you may not download an entire issue of a journal or multiple copies of articles, and you may use content in the JSTOR archive only for your personal, non-commercial use.", "title": "Technological Discontinuities and Organizational Environments"}, "0f44833eb9047158221e7b3128cde1347b58ccd6": {"paper_id": "0f44833eb9047158221e7b3128cde1347b58ccd6", "abstract": "Energy-proportional designs would enable large energy savings in servers, potentially doubling their efficiency in real-life use. Achieving energy proportionality will require significant improvements in the energy usage profile of every system component, particularly the memory and disk subsystems.", "title": "The Case for Energy-Proportional Computing"}, "a256647d65b0c81e60e232f4212abcc9a6730aaf": {"paper_id": "a256647d65b0c81e60e232f4212abcc9a6730aaf", "abstract": "Bigtable is a distributed storage system for managing structured data that is designed to scale to a very large size: petabytes of data across thousands of commodity servers. Many projects at Google store data in Bigtable, including web indexing, Google Earth, and Google Finance. These applications place very different demands on Bigtable, both in terms of data size (from URLs to web pages to satellite imagery) and latency requirements (from backend bulk processing to real-time data serving). Despite these varied demands, Bigtable has successfully provided a flexible, high-performance solution for all of these Google products. In this article, we describe the simple data model provided by Bigtable, which gives clients dynamic control over data layout and format, and we describe the design and implementation of Bigtable.", "title": "Bigtable: A Distributed Storage System for Structured Data"}, "3dde3fec553b8d24a85d7059a3cc629ab33f7578": {"paper_id": "3dde3fec553b8d24a85d7059a3cc629ab33f7578", "abstract": "This whitepaper proposes OpenFlow: a way for researchers to run experimental protocols in the networks they use every day. OpenFlow is based on an Ethernet switch, with an internal flow-table, and a standardized interface to add and remove flow entries. Our goal is to encourage networking vendors to add OpenFlow to their switch products for deployment in college campus backbones and wiring closets. We believe that OpenFlow is a pragmatic compromise: on one hand, it allows researchers to run experiments on heterogeneous switches in a uniform way at line-rate and with high port-density; while on the other hand, vendors do not need to expose the internal workings of their switches. In addition to allowing researchers to evaluate their ideas in real-world traffic settings, OpenFlow could serve as a useful campus component in proposed large-scale testbeds like GENI. Two buildings at Stanford University will soon run OpenFlow networks, using commercial Ethernet switches and routers. We will work to encourage deployment at other schools; and We encourage you to consider deploying OpenFlow in your university network too", "title": "OpenFlow: enabling innovation in campus networks"}, "63af2b296e04f6e5ac51991bcdd04e06687623b2": {"paper_id": "63af2b296e04f6e5ac51991bcdd04e06687623b2", "abstract": "IMSI Catchers are tracking devices that break the privacy of the subscribers of mobile access networks, with disruptive effects to both the communication services and the trust and credibility of mobile network operators. Recently, we verified that IMSI Catcher attacks are really practical for the state-of-the-art 4G/LTE mobile systems too. Our IMSI Catcher device acquires subscription identities (IMSIs) within an area or location within a few seconds of operation and then denies access of subscribers to the commercial network. Moreover, we demonstrate that these attack devices can be easily built and operated using readily available tools and equipment, and without any programming. We describe our experiments and procedures that are based on commercially available hardware and unmodified open source software.", "title": "Easy 4G/LTE IMSI Catchers for Non-Programmers"}, "b7cc1306bd5a905fcbb8219d6984c71a3f7da2b0": {"paper_id": "b7cc1306bd5a905fcbb8219d6984c71a3f7da2b0", "abstract": "The long-term advancement (LTE) is the new mobile communication system, built after a redesigned physical part and predicated on an orthogonal regularity division multiple gain access to (OFDMA) modulation, features solid performance in challenging multipath surroundings and substantially boosts the performance of the cellular channel in conditions of pieces per second per Hertz (bps/Hz). Nevertheless, as all cordless systems, LTE is susceptible to radio jamming episodes. Such dangers have security implications especially regarding next-generation disaster response communication systems predicated on LTE technology. This proof concept paper overviews some new effective attacks (smart jamming) that extend the number and effectiveness of basic radio jamming. Predicated on these new hazards, some new potential security research guidelines are introduced, looking to improve the resiliency of LTE systems against such problems. A spread-spectrum modulation of the key downlink broadcast stations is coupled with a scrambling of the air tool allocation of the uplink control stations and a sophisticated system information subject matter encryption scheme.", "title": "Enhancing the Security of LTE Networks against Jamming Attacks"}, "04df3c9d1061b1627bdacb143ff3d519911ffe00": {"paper_id": "04df3c9d1061b1627bdacb143ff3d519911ffe00", "abstract": "Mobile communication is an essential part of our daily lives. Therefore, it needs to be secure and reliable. In this paper, we study the security of feature phones, the most common type of mobile phone in the world. We built a framework to analyze the security of SMS clients of feature phones. The framework is based on a small GSM base station, which is readily available on the market. Through our analysis we discovered vulnerabilities in the feature phone platforms of all major manufacturers. Using these vulnerabilities we designed attacks against end-users as well as mobile operators. The threat is serious since the attacks can be used to prohibit communication on a large scale and can be carried out from anywhere in the world. Through further analysis we determined that such attacks are amplified by certain configurations of the mobile network. We conclude our research by providing a set of countermeasures.", "title": "SMS of Death: From Analyzing to Attacking Mobile Phones on a Large Scale"}, "6bb277af69021357c405773067bb44dca63315de": {"paper_id": "6bb277af69021357c405773067bb44dca63315de", "abstract": "Mobile telecommunication has become an important part of our daily lives. Yet, industry standards such as GSM often exclude scenarios with active attackers. Devices participating in communication are seen as trusted and non-malicious. By implementing our own baseband firmware based on OsmocomBB, we violate this trust and are able to evaluate the impact of a rogue device with regard to the usage of broadcast information. Through our analysis we show two new attacks based on the paging procedure used in cellular networks. We demonstrate that for at least GSM, it is feasible to hijack the transmission of mobile terminated services such as calls, perform targeted denial of service attacks against single subscribers and as well against large geographical regions within a metropolitan area.", "title": "Let Me Answer That for You: Exploiting Broadcast Information in Cellular Networks"}, "324e4ffdf45c445f96cdb2364ec4e3fa055b20fa": {"paper_id": "324e4ffdf45c445f96cdb2364ec4e3fa055b20fa", "abstract": "According to a study released this July by Juniper Research, more than half the world's largest companies are now researching blockchain technologies with the goal of integrating them into their products. Projects are already under way that will disrupt the management of health care records, property titles, supply chains, and even our online identities. But before we remount the entire digital ecosystem on blockchain technology, it would be wise to take stock of what makes the approach unique and what costs are associated with it. Blockchain technology is, in essence, a novel way to manage data. As such, it competes with the data-management systems we already have. Relational databases, which orient information in updatable tables of columns and rows, are the technical foundation of many services we use today. Decades of market exposure and well-funded research by companies like Oracle Corp. have expanded the functionality and hardened the security of relational databases. However, they suffer from one major constraint: They put the task of storing and updating entries in the hands of one or a few entities, whom you have to trust won't mess with the data or get hacked.", "title": "Blockchain world - Do you need a blockchain? This chart will tell you if the technology can solve your problem"}, "603c8182f0aab344c402f9607bb1d7226494459c": {"paper_id": "603c8182f0aab344c402f9607bb1d7226494459c", "abstract": "In Natural Language Processing (NLP), the quality of a system depends to a great extent on the quality of the linguistic resources it uses. Due to the unpredictable character of valency properties, a reliable source for information about valency is important for syntactic and semantic analysis. With this in mind, we discuss how the Valency Dictionary of English in machine-readable form can be used as a resource for NLP. We will show that the valency data can be integrated into a Left-Associative Grammar and thus can be used for accurately parsing natural language with a rule-based approach.", "title": "Using High-Quality Resources in NLP: The Valency Dictionary of English as a Resource for Left-Associative Grammars"}, "f9a25e0dc776857fc24ebc7115c980312f2719b1": {"paper_id": "f9a25e0dc776857fc24ebc7115c980312f2719b1", "abstract": "A semantic concordance is a textual corpus and a lexicon So combined that every substantive word in the text is linked to its appropriate ~nse in the lexicon. Thus it can be viewed either as a corpus in which words have been tagged syntactically and semantically, or as a lexicon in which example sentences can be found for many definitions. A semantic concordance is being constructed to u s e in studies of sense resolution in context (semantic disambiguation). The Brown Corpus is the text and WordNet is the lexicon. Semantic tags (pointers to WordNet synsets) are inserted in the text manually using an interface, ConText, that was designed to facilitate the task. Another interface supports searches of the tagged text. Some practical uses for semantic concordances are proposed. 1. I N T R O D U C T I O N We wish to propose a new version of an old idea. Lexicographers have traditionally based their work on a corpus of examples taken from approved usage, but considerations of cost usually limit published dictionaries to lexical entries having only a scattering of phrases to illustrate the usages from which definitions were derived. As a consequence of this economic pressure, most dictionaries are relatively weak in providing contextual information: someone learning English as a second language will find in an English dictionary many alternative meanings for a common word, but little or no help in determining the linguistic contexts in which the word can be used to express those different meanings. Today, however, large computer memories are affordable enough that this limitation can be removed; it would now be feasible to publish a dictionary electronically along with all of the citation sentences on which it was based. The resulting combination would be more than a lexicon and more than a corpus; we propose to call it a semantic concordance. If the corpus is some specific text, it is a specific semantic concordance; ff the corpus includes many different texts, it is a universal semantic concordance. We have begun constructing a universal semantic concordance in conjunction with our work on a lexical database. The result can be viewed either as a collection of passages in which words have been tagged syntactically and semantieally, or as a lexicon in which illustrative sentences can be found for many definitions. At the present time, the correlation of a lexical meaning with examples in which a word is used to express that meaning must be done by hand. Manual semantic tagging is tedious; it should be done automatically as soon as it is possible to resolve word senses in context automatically. It is hoped that the manual creation of a semantic concordance will provide an appropriate environment for developing and testing those automatic procedures. 2. W O R D N E T : A L E X I C A L D A T A B A S E The lexical component of the universal semantic concordance that we are constructing is WordNet, an on-line lexical resource inspired by current psycholinguistic theories of haman lexical memory [1, 2]. A standard, handheld dictionary is organized alphabetically; it puts together words that are spelled alike and scatters words with related meanings. Although on-line versions of such standard dictionaries can relieve a user of alphabetical searches, it is clearly inefficient to use a computer merely as a rapid page-turner. WordNet is an example of a more efficient combination of traditional lexicography and modern computer science. The most ambitious feature of WordNet is the attempt to organize lexical information in terms of word meanings, rather than word forms. WordNet is organized by semantic relations (rather than by semantic components) within the open-class categories of noun, verb, adjective, and adverb; closed-class categories of words (pronouns, prepositions, conjunctions, etc.) are not included in WordNet. The semantic relations among open-class words include: synonymy and antonymy (which are semantic relations between words and which are found in all four syntactic categories); hyponymy and hypernymy (which are semantic relations between concepts and which organize nouns into a categorical hierarchy); meronymy and holonymy (which represent part-whole relations among noun concepts); and troponymy (manner relations) and entailment relations between verb concepts. These semantic relations were chosen to be intuitively obvious to nonlinguists and to have broad applicability throughout the lexicon. The basic elements of WordNet are sets of synonyms (or synsets), which are taken to represent lexicalized concepts. A synset is a group of words that are synonymous, in the sense that there are contexts in which they can be interchanged without changing the meaning of the statement. For example, WordNet distinguishes between the synsets:", "title": "A Semantic Concordance"}, "98bbd141e5bd025c825c41798a5bca364ca44b5c": {"paper_id": "98bbd141e5bd025c825c41798a5bca364ca44b5c", "abstract": "We design a general framework named Hdoctor for hard drive failure prediction. Hdoctor leverages the power of big data to achieve a significant improvement comparing to all previous researches that used sophisticated machine learning algorithms. Hdoctor exhibits a series of engineering innovations: (1) constructing time dependent features to characterize the Self-Monitoring, Analysis and Reporting Technology (SMART) value transitions during disk failures, (2) combining features to enable the model to learn the correlation among different SMART attributes, (3) regarding circumstance data such as cluster workload, temperature, humidity, location as related features. Meanwhile, Hdoctor collects/labels samples and updates model automatically, and works well for all kinds of disk failure prediction in our intelligent data center. In this work, we use Hdoctor to collect 74,477,717 training records from our clusters involving 220,022 disks. By training a simple and scalable model, our system achieves a detection rate of 97.82%, with a false alarm rate (FAR) of 0.3%, which hugely outperforms all previous algorithms. In addition, Hdoctor is an excellent indicator for how to predict different hardware failures efficiently under various circumstances.", "title": "Hard Drive Failure Prediction Using Big Data"}, "47b0d6c0c40d6ec07cc63794df13bfcae47668d6": {"paper_id": "47b0d6c0c40d6ec07cc63794df13bfcae47668d6", "abstract": "Quantum computers can (in theory) solve certain problems far faster than a classical computer running any known classical algorithm.While existing technologies for building quantum computers are in their infancy, it is not too early to consider their scalability and reliability in the context of the design of large-scale quantum computers. To architect such systems, one must understand what it takes to design and model a balanced, fault-tolerant quantum computer architecture. The goal of this lecture is to provide architectural abstractions for the design of a quantum computer and to explore the systems-level challenges in achieving scalable, fault-tolerant quantum computation. In this lecture,we provide an engineering-oriented introduction to quantum computation with an overview of the theory behind key quantum algorithms. Next, we look at architectural case studies based upon experimental data and future projections for quantum computation implemented using trapped ions. While we focus here on architectures targeted for realization using trapped ions, the techniques for quantum computer architecture design, quantum fault-tolerance, and compilation described in this lecture are applicable to many other physical technologies that may be viable candidates for building a large-scale quantum computing system. We also discuss general issues involved with programming a quantum computer as well as a discussion of work on quantum architectures based on quantum teleportation. Finally, we consider some of the open issues remaining in the design of quantum computers.", "title": "The Datacenter as a Computer: An Introduction to the Design of Warehouse-Scale Machines"}, "1886edb4e771c1c0aa7bae360d7f3de23ac4ac8e": {"paper_id": "1886edb4e771c1c0aa7bae360d7f3de23ac4ac8e", "abstract": "It is estimated that over 90% of all new information produced in the world is being stored on magnetic media, most of it on hard disk drives. Despite their importance, there is relatively little published work on the failure patterns of disk drives, and the key factors that affect their lifetime. Most available data are either based on extrapolation from accelerated aging experiments or from relatively modest sized field studies. Moreover, larger population studies rarely have the infrastructure in place to collect health signals from components in operation, which is critical information for detailed failure analysis. We present data collected from detailed observations of a large disk drive population in a production Internet services deployment. The population observed is many times larger than that of previous studies. In addition to presenting failure statistics, we analyze the correlation between failures and several parameters generally believed to impact longevity. Our analysis identifies several parameters from the drive\u2019s self monitoring facility (SMART) that correlate highly with failures. Despite this high correlation, we conclude that models based on SMART parameters alone are unlikely to be useful for predicting individual drive failures. Surprisingly, we found that temperature and activity levels were much less correlated with drive failures than previously reported.", "title": "Failure Trends in a Large Disk Drive Population"}, "663e064469ad91e6bda345d216504b4c868f537b": {"paper_id": "663e064469ad91e6bda345d216504b4c868f537b", "abstract": "Today's data centers may contain tens of thousands of computers with significant aggregate bandwidth requirements. The network architecture typically consists of a tree of routing and switching elements with progressively more specialized and expensive equipment moving up the network hierarchy. Unfortunately, even when deploying the highest-end IP switches/routers, resulting topologies may only support 50% of the aggregate bandwidth available at the edge of the network, while still incurring tremendous cost. Non-uniform bandwidth among data center nodes complicates application design and limits overall system performance.\n In this paper, we show how to leverage largely commodity Ethernet switches to support the full aggregate bandwidth of clusters consisting of tens of thousands of elements. Similar to how clusters of commodity computers have largely replaced more specialized SMPs and MPPs, we argue that appropriately architected and interconnected commodity switches may deliver more performance at less cost than available from today's higher-end solutions. Our approach requires no modifications to the end host network interface, operating system, or applications; critically, it is fully backward compatible with Ethernet, IP, and TCP.", "title": "A scalable, commodity data center network architecture"}, "111b0d103542ad8679767e67bbeff8b6504002bc": {"paper_id": "111b0d103542ad8679767e67bbeff8b6504002bc", "abstract": "We have designed and implemented the Google File System, a scalable distributed file system for large distributed data-intensive applications. It provides fault tolerance while running on inexpensive commodity hardware, and it delivers high aggregate performance to a large number of clients. While sharing many of the same goals as previous distributed file systems, our design has been driven by observations of our application workloads and technological environment, both current and anticipated, that reflect a marked departure from some earlier file system assumptions. This has led us to reexamine traditional choices and explore radically different design points. The file system has successfully met our storage needs. It is widely deployed within Google as the storage platform for the generation and processing of data used by our service as well as research and development efforts that require large data sets. The largest cluster to date provides hundreds of terabytes of storage across thousands of disks on over a thousand machines, and it is concurrently accessed by hundreds of clients. In this paper, we present file system interface extensions designed to support distributed applications, discuss many aspects of our design, and report measurements from both micro-benchmarks and real world use.", "title": "The Google file system"}, "760b8efc52271fd453f92132de847e9bebd81636": {"paper_id": "760b8efc52271fd453f92132de847e9bebd81636", "abstract": "L1 regularized logistic regression is now a workhorse of machine learning: it is widely used for many classification problems, particularly ones with many features. L1 regularized logistic regression requires solving a convex optimization problem. However, standard algorithms for solving convex optimization problems do not scale well enough to handle the large datasets encountered in many practical settings. In this paper, we propose an efficient algorithm for L1 regularized logistic regression. Our algorithm iteratively approximates the objective function by a quadratic approximation at the current point, while maintaining the L1 constraint. In each iteration, it uses the efficient LARS (Least Angle Regression) algorithm to solve the resulting L1 constrained quadratic optimization problem. Our theoretical results show that our algorithm is guaranteed to converge to the global optimum. Our experiments show that our algorithm significantly outperforms standard algorithms for solving convex optimization problems. Moreover, our algorithm outperforms four previously published algorithms that were specifically designed to solve the L1 regularized logistic regression problem.", "title": "Efficient L1 Regularized Logistic Regression"}, "14aa73f7aa00fc2309045cff7d65eed60df581b2": {"paper_id": "14aa73f7aa00fc2309045cff7d65eed60df581b2", "abstract": "Conditional maximum entropy (ME) models provide a general purpose machine learning technique which has been successfully applied to fields as diverse as computer vision and econometrics, and which is used for a wide variety of classification problems in natural language processing. However, the flexibility of ME models is not without cost. While parameter estimation for ME models is conceptually straightforward, in practice ME models for typical natural language tasks are very large, and may well contain many thousands of free parameters. In this paper, we consider a number of algorithms for estimating the parameters of ME models, including iterative scaling, gradient ascent, conjugate gradient, and variable metric methods. Surprisingly, the standardly used iterative scaling algorithms perform quite poorly in comparison to the others, and for all of the test problems, a limitedmemory variable metric algorithm outperformed the other choices.", "title": "A Comparison of Algorithms for Maximum Entropy Parameter Estimation"}, "0ecb33ced5b0976accdf13817151f80568b6fdcb": {"paper_id": "0ecb33ced5b0976accdf13817151f80568b6fdcb", "abstract": "Discriminative reranking is one method for constructing high-performance statistical parsers (Collins, 2000). A discriminative reranker requires a source of candidate parses for each sentence. This paper describes a simple yet novel method for constructing sets of 50-best parses based on a coarse-to-fine generative parser (Charniak, 2000). This method generates 50-best lists that are of substantially higher quality than previously obtainable. We used these parses as the input to a MaxEnt reranker (Johnson et al., 1999; Riezler et al., 2002) that selects the best parse from the set of parses for each sentence, obtaining an f-score of 91.0% on sentences of length 100 or less.", "title": "Coarse-to-Fine n-Best Parsing and MaxEnt Discriminative Reranking"}, "123a89a9c28043f478906781bec55822abc6bae0": {"paper_id": "123a89a9c28043f478906781bec55822abc6bae0", "abstract": "In this paper an algorithm based on the concepts of genetic algorithms that uses an estimation of a probability distribution of promising solutions in order to generate new candidate solutions is proposed To esti mate the distribution techniques for model ing multivariate data by Bayesian networks are used The proposed algorithm identi es reproduces and mixes building blocks up to a speci ed order It is independent of the ordering of the variables in the strings rep resenting the solutions Moreover prior in formation about the problem can be incor porated into the algorithm However prior information is not essential Preliminary ex periments show that the BOA outperforms the simple genetic algorithm even on decom posable functions with tight building blocks as a problem size grows", "title": "Bayesian Optimization Algorithm"}, "f1c3683cacc3dc7898f3603753af87565f8ad677": {"paper_id": "f1c3683cacc3dc7898f3603753af87565f8ad677", "abstract": "Fifty years have passed since the publication of the first regression tree algorithm. New techniques have added capabilities that far surpass those of the early methods. Modern classification trees can partition the data with linear splits on subsets of variables and fit nearest neighbor, kernel density, and other models in the partitions. Regression trees can fit almost every kind of traditional statistical model, including least-squares, quantile, logistic, Poisson, and proportional hazards models, as well as models for longitudinal and multiresponse data. Greater availability and affordability of software (much of which is free) have played a significant role in helping the techniques gain acceptance and popularity in the broader scientific community. This article surveys the developments and briefly reviews the key ideas behind some of the major algorithms.", "title": "Fifty Years of Classification and Regression Trees 1"}, "e52c061e94c610424fb8639d754c8d18276403e7": {"paper_id": "e52c061e94c610424fb8639d754c8d18276403e7", "abstract": "We invesrigare new appmaches for frequent graph-based patrem mining in graph darasers andpmpose a novel ofgorirhm called gSpan (graph-based,Tubsmrure parrern mining), which discovers frequenr subsrrucrures z h o u r candidate generorion. &an builds a new lexicographic or. der among graphs, and maps each graph to a unique minimum DFS code as irs canonical label. Based on rhis lexicographic orde,: &an adopts rhe deprh-jrsr search srraregy ro mine frequenr cannecred subgraphs eflciently. Our performance study shows rhar gSpan subsianriolly outperforms previous algorithm, somerimes by an order of magnirude.", "title": "gSpan: Graph-Based Substructure Pattern Mining"}, "b28d7cdbb79f8ae3b561bc9b7c20c48dc3978b3e": {"paper_id": "b28d7cdbb79f8ae3b561bc9b7c20c48dc3978b3e", "abstract": "We combine the spectral (viscosity) method and ensemble averaging to propose an algorithm that computes admissible measure-valued solutions of the incompressible Euler equations. The resulting approximate young measures are proved to converge (with increasing numerical resolution) to a measure-valued solution. We present numerical experiments demonstrating the robustness and efficiency of the proposed algorithm, as well as the appropriateness of measure-valued solutions as a solution framework for the Euler equations. Furthermore, we report an extensive computational study of the twodimensional vortex sheet, which indicates that the computed measure-valued solution is non-atomic and implies possible non-uniqueness of weak solutions constructed by Delort.", "title": "Computation of measure-valued solutions for the incompressible Euler equations"}, "6dc15235e98fa56e9e0846f59cbe5c358bd0404a": {"paper_id": "6dc15235e98fa56e9e0846f59cbe5c358bd0404a", "abstract": "User-oriented research in the game industry is undergoing a change from relying on informal user-testing methods adapted directly from productivity software development to integrating modern approaches to usability- and user experience testing. Gameplay metrics analysis form one of these techniques, being based on instrumentation methods in HCI. Gameplay metrics are instrumentation data about the user behavior and user-game interaction, and can be collected during testing, production and the live period of the lifetime of a digital game. The use of instrumentation data is relatively new to commercial game development, and remains a relatively unexplored method of user research. In this paper, the focus is on utilizing game metrics for informing the analysis of gameplay during commercial game production as well as in research contexts. A series of case studies are presented, focusing on the major commercial game titles Kane & Lynch and Fragile Alliance.", "title": "Towards gameplay analysis via gameplay metrics"}, "fb3274b567c9f6a54dcf215e30ca26c73415a3e5": {"paper_id": "fb3274b567c9f6a54dcf215e30ca26c73415a3e5", "abstract": "is one of the most comprehensively studied ingredients in the food supply. Yet, despite our considerable knowledge of caffeine and centuries of safe consumption in foods and beverages, questions and misperceptions about the potential health effects associated with caffeine persist. This Review provides up-to-date information on caffeine, examines its safety and summarizes the most recent key research conducted on caffeine and health. EXECUTIVE SUMMARY Caffeine is added to soft drinks as a flavoring agent; it imparts a bitterness that modifies the flavors of other components, both sour and sweet. Although there has been controversy as to its effectiveness in this role, a review of the literature suggests that caffeine does, in fact, contribute to the sensory appeal of soft drinks. [Drewnowski, 2001] Moderate intake of 300 mg/day (about three cups of coffee per day) of caffeine does not cause adverse health effects in healthy adults, although some groups, including those with hypertension and the elderly, may be more vulnerable. Also, regular consumers of coffee and other caffeinated beverages may experience some undesirable, but mild, short-lived symptoms if they stop consuming caffeine , particularly if the cessation is abrupt. However, there is little evidence of health risks of caffeine consumption. In fact, some evidence of health benefits exists for adults who consume moderate amounts of caffeine. Caffeine consumption may help reduce the risk of several chronic diseases, including diabetes, Parkinson's disease, liver disease, and colorectal cancer, as well as improve immune function. Large prospective cohort studies in the Netherlands, Finland, Sweden, and the United States have found caffeine consumption is associated with reduced risk of developing type 2 diabetes, although the mechanisms are unclear. Several other cohort studies have found that caffeine consumption from coffee and other beverages decreases the risk of Parkinson's Disease in men, as well as in women who have never used post-menopausal hormone replacement therapy. Epidemiological studies also suggest that coffee consumption may decrease the risk of liver injury, cirrhosis and hepatocellular carcinoma (liver cancer), although the reasons for these results have not been determined. In addition, coffee consumption appears to reduce the risk of colorectal cancer, but this has not generally been confirmed in prospective cohort studies. An anti-inflammatory effect has also been observed in a number of studies on caffeine's impact on the immune system. Most studies have found that caffeine consumption does not significantly increase the risk of coronary heart disease (CHD) or stroke. \u2026", "title": "Caffeine & Health : Clarifying The Controversies IFIC"}, "02cd06a841f287bdeff93c77997e42f4e0aa9d28": {"paper_id": "02cd06a841f287bdeff93c77997e42f4e0aa9d28", "abstract": "Dynamic folds and wrinkles are an important visual cue for creating believably dressed characters in virtual environments. Adding these fine details to real-time cloth visualization is challenging, as the low-quality cloth used for real-time applications often has no reference shape, an extremely low triangle count, and poor temporal and spatial coherence. We introduce a novel real-time method for adding dynamic, believable wrinkles to such coarse cloth animation. We trace spatially and temporally coherent wrinkle paths, overcoming the inaccuracies and noise in low-end cloth animation, by employing a two stage stretch tensor estimation process. We first employ a graph-cut segmentation technique to extract spatially and temporally reliable surface motion patterns, detecting consistent compressing, stable, and stretching patches. We then use the detected motion patterns to compute a per-triangle temporally adaptive reference shape and a stretch tensor based on it. We use this tensor to dynamically generate new wrinkle geometry on the coarse cloth mesh by taking advantage of the GPU tessellation unit. Our algorithm produces plausible fine wrinkles on real-world data sets at real-time frame rates, and is suitable for the current generation of consoles and PC graphics cards.", "title": "Real-time dynamic wrinkling of coarse animated cloth"}, "2f50826e7f17855e0b3598f4eb694aee9ddda91b": {"paper_id": "2f50826e7f17855e0b3598f4eb694aee9ddda91b", "abstract": "Moving garments and other cloth objects exhibit dynamic, complex wrinkles. Generating such wrinkles in a virtual environment currently requires either a time-consuming manual design process, or a computationally expensive simulation, often combined with accurate parameter-tuning requiring specialized animator skills. Our work presents an alternative approach for wrinkle generation which combines coarse cloth animation with a post-processing step for efficient generation of realistic-looking fine dynamic wrinkles. Our method uses the stretch tensor of the coarse animation output as a guide for wrinkle placement. To ensure temporal coherence, the placement mechanism uses a space-time approach allowing not only for smooth wrinkle appearance and disappearance, but also for wrinkle motion, splitting, and merging over time. Our method generates believable wrinkle geometry using specialized curve-based implicit deformers. The method is fully automatic and has a single user control parameter that enables the user to mimic different fabrics.", "title": "Animation wrinkling: augmenting coarse cloth simulations with realistic-looking wrinkles"}, "f862947252fc40b0fb5f8d120bee5706236541f6": {"paper_id": "f862947252fc40b0fb5f8d120bee5706236541f6", "abstract": "Physically based deformable models have been widely embraced by the Computer Graphics community. Many problems outlined in a previous survey by Gibson and Mirtich [GM97] have been addressed, thereby making these models interesting and useful for both offline and real-time applications, such as motion pictures and video games. In this paper, we present the most significant contributions of the past decade, which produce such impressive and perceivably realistic animations and simulations: finite element/difference/volume methods, mass-spring systems, meshfree methods, coupled particle systems and reduced deformable models based on modal analysis. For completeness, we also make a connection to the simulation of other continua, such as fluids, gases and melting objects. Since time integration is inherent to all simulated phenomena, the general notion of time discretization is treated separately, while specifics are left to the respective models. Finally, we discuss areas of application, such as elastoplastic deformation and fracture, cloth and hair animation, virtual surgery simulation, interactive entertainment and fluid/smoke animation, and also suggest areas for future research.", "title": "Physically Based Deformable Models in Computer Graphics"}, "6f2cd87c665a034fdc4e51be11b2185ace398d79": {"paper_id": "6f2cd87c665a034fdc4e51be11b2185ace398d79", "abstract": "Modeling dressed characters is known as a very tedious process. It usually requires specifying 2D fabric patterns, positioning and assembling them in 3D, and then performing a physically-based simulation. The latter accounts for gravity and collisions to compute the rest shape of the garment, with the adequate folds and wrinkles. This paper presents a more intuitive way to design virtual clothing. We start with a 2D sketching system in which the user draws the contours and seam-lines of the garment directly on a virtual mannequin. Our system then converts the sketch into an initial 3D surface using an existing method based on a precomputed distance field around the mannequin. The system then splits the created surface into different panels delimited by the seam-lines. The generated panels are typically not developable. However, the panels of a realistic garment must be developable, since each panel must unfold into a 2D sewing pattern. Therefore our system automatically approximates each panel with a developable surface, while keeping them assembled along the seams. This process allows us to output the corresponding sewing patterns. The last step of our method computes a natural rest shape for the 3D garment, including the folds due to the collisions with the body and gravity. The folds are generated using procedural modeling of the buckling phenomena observed in real fabric. The result of our algorithm consists of a realistic looking 3D mannequin dressed in the designed garment and the 2D patterns which can be used for distortion free texture mapping. As we demonstrate, the patterns we create allow us to sew real replicas of the virtual garments.", "title": "Virtual garments : A Fully Geometric Approach for Clothing Design"}, "413e7a2e72e4c7389499c5f665300ae71060b93f": {"paper_id": "413e7a2e72e4c7389499c5f665300ae71060b93f", "abstract": "Given an arbitrary mesh, we present a method to construct a progressive mesh (PM) such that all meshes in the PM sequence share a common texture parametrization. Our method considers two important goals simultaneously. It minimizes texture stretch (small texture distances mapped onto large surface distances) to balance sampling rates over all locations and directions on the surface. It also minimizes texture deviation (\u201cslippage\u201d error based on parametric correspondence) to obtain accurate textured mesh approximations. The method begins by partitioning the mesh into charts using planarity and compactness heuristics. It creates a stretch-minimizing parametrization within each chart, and resizes the charts based on the resulting stretch. Next, it simplifies the mesh while respecting the chart boundaries. The parametrization is re-optimized to reduce both stretch and deviation over the whole PM sequence. Finally, the charts are packed into a texture atlas. We demonstrate using such atlases to sample color and normal maps over several models.", "title": "Texture mapping progressive meshes"}, "4af908e235f2e74d53166ed0108e909fc84f1717": {"paper_id": "4af908e235f2e74d53166ed0108e909fc84f1717", "abstract": "Following the hierarchical Bayesian framework for blind deconvolution problems, in this paper, we propose the use of simultaneous autoregressions as prior distributions for both the image and blur, and gamma distributions for the unknown parameters (hyperparameters) of the priors and the image formation noise. We show how the gamma distributions on the unknown hyperparameters can be used to prevent the proposed blind deconvolution method from converging to undesirable image and blur estimates and also how these distributions can be inferred in realistic situations. We apply variational methods to approximate the posterior probability of the unknown image, blur, and hyperparameters and propose two different approximations of the posterior distribution. One of these approximations coincides with a classical blind deconvolution method. The proposed algorithms are tested experimentally and compared with existing blind deconvolution methods", "title": "Blind Deconvolution Using a Variational Approach to Parameter, Image, and Blur Estimation"}, "0ba749975aaffee5e8d0e877011414f10fd700c9": {"paper_id": "0ba749975aaffee5e8d0e877011414f10fd700c9", "abstract": "We present the AP16-OL7 database which was released as the training and test data for the oriental language recognition (OLR) challenge on APSIPA 2016. Based on the database, a baseline system was constructed on the basis of the i-vector model. We report the baseline results evaluated in various metrics defined by the AP16-OLR evaluation plan and demonstrate that AP16-OL7 is a reasonable data resource for multilingual research.", "title": "AP16-OL7: A multilingual database for oriental languages and a language recognition baseline"}, "72af0afcf15e4e09832c811e98a827356d4dd16f": {"paper_id": "72af0afcf15e4e09832c811e98a827356d4dd16f", "abstract": "We propose a new approach to the problem of estimating the hyperparameters which define the interspeaker variability model in joint factor analysis. We tested the proposed estimation technique on the NIST 2006 speaker recognition evaluation data and obtained 10%-15% reductions in error rates on the core condition and the extended data condition (as measured both by equal error rates and the NIST detection cost function). We show that when a large joint factor analysis model is trained in this way and tested on the core condition, the extended data condition and the cross-channel condition, it is capable of performing at least as well as fusions of multiple systems of other types. (The comparisons are based on the best results on these tasks that have been reported in the literature.) In the case of the cross-channel condition, a factor analysis model with 300 speaker factors and 200 channel factors can achieve equal error rates of less than 3.0%. This is a substantial improvement over the best results that have previously been reported on this task.", "title": "A Study of Interspeaker Variability in Speaker Verification"}, "a69c34076de67bf891d4f26a4c494f4e71ab2288": {"paper_id": "a69c34076de67bf891d4f26a4c494f4e71ab2288", "abstract": "This paper presents a new speaker verification system architecture based on Joint Factor Analysis (JFA) as feature extractor. In this modeling, the JFA is used to define a new low-dimensional space named the total variability factor space, instead of both channel and speaker variability spaces for the classical JFA. The main contribution in this approach, is the use of the cosine kernel in the new total factor space to design two different systems: the first system is Support Vector Machines based, and the second one uses directly this kernel as a decision score. This last scoring method makes the process faster and less computation complex compared to others classical methods. We tested several intersession compensation methods in total factors, and we found that the combination of Linear Discriminate Analysis and Within Class Covariance Normalization achieved the best performance. We achieved a remarkable results using fast scoring method based only on cosine kernel especially for male trials, we yield an EER of 1.12% and MinDCF of 0.0094 on the English trials of the NIST 2008 SRE dataset.", "title": "Support vector machines versus fast scoring in the low-dimensional total variability space for speaker verification"}, "482dde1c02c79c14d5de010b369961b35fb3bcf8": {"paper_id": "482dde1c02c79c14d5de010b369961b35fb3bcf8", "abstract": "This paper extends the within-class covariance normalizat ion (WCCN) technique described in [1, 2] for training generaliz ed linear kernels. We describe a practical procedure for apply ing WCCN to an SVM-based speaker recognition system where the input feature vectors reside in a high-dimensional space. O ur approach involves using principal component analysis (PCA) t o split the original feature space into two subspaces: a low-dimens ional \u201cPCA space\u201d and a high-dimensional \u201cPCA-complement space. \u201d After performing WCCN in the PCA space, we concatenate the resulting feature vectors with a weighted version of their P CAcomplements. When applied to a state-of-the-art MLLR-SVM speaker recognition system, this approach achieves improv ements of up to 22% in EER and 28% in minimum decision cost function (DCF) over our previous baseline. We also achieve substanti al improvements over an MLLR-SVM system that performs WCCN in the PCA space but discards the PCA-complement.", "title": "Within-class covariance normalization for SVM-based speaker recognition"}, "ca84a0e8a34dffac72775eeeea88ca94ae60717d": {"paper_id": "ca84a0e8a34dffac72775eeeea88ca94ae60717d", "abstract": "ALIZE is an open-source platform for speaker recognition. The ALIZE library implements a low-level statistical engine based on the well-known Gaussian mixture modelling. The toolkit includes a set of high level tools dedicated to speaker recognition based on the latest developments in speaker recognition such as Joint Factor Analysis, Support Vector Machine, i-vector modelling and Probabilistic Linear Discriminant Analysis. Since 2005, the performance of ALIZE has been demonstrated in series of Speaker Recognition Evaluations (SREs) conducted by NIST and has been used by many participants in the last NISTSRE 2012. This paper presents the latest version of the corpus and performance on the NIST-SRE 2010 extended task.", "title": "ALIZE 3.0 - open source toolkit for state-of-the-art speaker recognition"}, "6e0e1a22a43eaf2fd10394ea6abefc32f161b989": {"paper_id": "6e0e1a22a43eaf2fd10394ea6abefc32f161b989", "abstract": "In this paper we describe the major elements of MIT Lincoln Laboratory\u2019s Gaussian mixture model (GMM)-based speaker verification system used successfully in several NIST Speaker Recognition Evaluations (SREs). The system is built around the likelihood ratio test for verification, using simple but effective GMMs for likelihood functions, a universal background model (UBM) for alternative speaker representation, and a form of Bayesian adaptation to derive speaker models from the UBM. The development and use of a handset detector and score normalization to greatly improve verification performance is also described and discussed. Finally, representative performance benchmarks and system behavior experiments on NIST SRE corpora are presented. \uf6d9 2000 Academic Press", "title": "Speaker Verification Using Adapted Gaussian Mixture Models"}, "2d3ab651aa843229932ed63f19986dae0c6aace0": {"paper_id": "2d3ab651aa843229932ed63f19986dae0c6aace0", "abstract": "Your use of the JSTOR archive indicates your acceptance of JSTOR's Terms and Conditions of Use, available at http://www.jstor.org/about/terms.html. JSTOR's Terms and Conditions of Use provides, in part, that unless you have obtained prior permission, you may not download an entire issue of a journal or multiple copies of articles, and you may use content in the JSTOR archive only for your personal, non-commercial use. Please contact the publisher regarding any further use of this work. Publisher contact information may be obtained at http://www.jstor.org/journals/astata.html. Each copy of any part of a JSTOR transmission must contain the same copyright notice that appears on the screen or printed page of such transmission.", "title": "Locally Weighted Regression : An Approach to Regression Analysis by Local Fitting"}, "73fab6de084fc2b0769c074d053ba71b0cad53ac": {"paper_id": "73fab6de084fc2b0769c074d053ba71b0cad53ac", "abstract": "This paper deals with a modified technique for the recognition of single stage and multiple power quality (PQ) disturbances. An algorithm based on Stockwell's transform and artificial neural network-based classifier and a rule-based decision tree is proposed in this paper. The analysis and classification of single stage PQ disturbances consisting of both events and variations such as sag, swell, interruption, harmonics, transients, notch, spike, and flicker are presented. Moreover, the proposed algorithm is also applied on multiple PQ disturbances such as harmonics with sag, swell, flicker, and interruption. A database of these PQ disturbances based on IEEE-1159 standard is generated in MATLAB for simulation studies. The proposed algorithm extracts significant features of various PQ disturbances using S-transform, which are used as input to this hybrid classifier for the classification of PQ disturbances. Satisfactory results of effective recognition and classification of PQ disturbances are obtained with the proposed algorithm. Finally, the proposed method is also implemented on real-time PQ events acquired in a laboratory to confirm the validity of this algorithm in practical conditions.", "title": "Recognition of Power-Quality Disturbances Using S-Transform-Based ANN Classifier and Rule-Based Decision Tree"}, "4146ac831303d3fd241bfbd496a60efd95969d7f": {"paper_id": "4146ac831303d3fd241bfbd496a60efd95969d7f", "abstract": "-By replacing the sigmoid activation function often used in neural networks with an exponential function, a probabilistic neural network ( PNN) that can compute nonlinear decision boundaries which approach the Bayes optimal is formed. Alternate activation functions having similar properties are also discussed. A fourlayer neural network of the type proposed can map any input pattern to any number of classifications. The decision boundaries can be modified in real-time using new data as they become available, and can be implemented using artificial hardware \"'neurons\" that operate entirely in parallel. Provision is also made for estimating the probability and reliability of a classification as well as making the decision. The technique offers a tremendous speed advantage for problems in which the incremental adaptation time of back propagation is a significant fraction of the total computation time. For one application, the PNN paradigm was 200,000 times faster than back-propagation. Keywords--Neural network, Probability density function, Parallel processor, \"Neuron\", Pattern recognition, Parzen window, Bayes strategy, Associative memory.", "title": "Probabilistic neural networks"}, "a82bbcdfedf2945700eec1c209d637eecf84d39f": {"paper_id": "a82bbcdfedf2945700eec1c209d637eecf84d39f", "abstract": "In this study a mission data base based clustering approach that can be used in radar warning receivers for the purpose of deinterleaving is suggested. Cell based deinterleaving technique, which is widely used at the present time, utilizes the information of direction of arrival, frequency and pulse width. In this study, different from this approach used in the literature, frequency, direction of arrival and pulse amplitude parameters are utilized for deinterleaving. With this technique it is shown that accurate results can be obtained by simulation.", "title": "A clustering approach for radar warning receivers"}, "76b6b2be912f263db3be8dca2724fb32a70a0077": {"paper_id": "76b6b2be912f263db3be8dca2724fb32a70a0077", "abstract": "In six unloaded cadaver knees we used MRI to determine the shapes of the articular surfaces and their relative movements. These were confirmed by dissection. Medially, the femoral condyle in sagittal section is composed of the arcs of two circles and that of the tibia of two angled flats. The anterior facets articulate in extension. At about 20 degrees the femur 'rocks' to articulate through the posterior facets. The medial femoral condyle does not move anteroposteriorly with flexion to 110 degrees. Laterally, the femoral condyle is composed entirely, or almost entirely, of a single circular facet similar in radius and arc to the posterior medial facet. The tibia is roughly flat. The femur tends to roll backwards with flexion. The combination during flexion of no anteroposterior movement medially (i.e., sliding) and backward rolling (combined with sliding) laterally equates to internal rotation of the tibia around a medial axis with flexion. About 5 degrees of this rotation may be obligatory from 0 degrees to 10 degrees flexion; thereafter little rotation occurs to at least 45 degrees. Total rotation at 110 degrees is about 20 degrees, most if not all of which can be suppressed by applying external rotation to the tibia at 90 degrees.", "title": "Tibiofemoral movement 1: the shapes and relative movements of the femur and tibia in the unloaded cadaver knee."}, "82548cc43e90017329180dd9accf9363cc1deb88": {"paper_id": "82548cc43e90017329180dd9accf9363cc1deb88", "abstract": "The study aims to compare the efficacy and safety of over-the-counter whitestrips with the American Dental Association (ADA)-recommended home-whitening using the 10\u00a0% carbamide peroxide gel. Randomized controlled trials (RCTs) comparing the clinical efficacy and safety of the whitestrips with the 10\u00a0% carbamide peroxide (10\u00a0% CP) gel applied on tray for tooth whitening in adults were searched at PubMed and Cochrane Central Register of Controlled Trials databases and selected up to October 2014. Efficacy of the whitening techniques was assessed through \u2206E, \u2206L, and \u2206b parameters, while side effects were analyzed as dichotomous variables. Data was extracted independently by two reviewers. Metanalysis was performed using random- and fixed-effect models (RevMan 5.3). Eight studies were included in the metanalysis. The metanalysis revealed no significant difference between the intervention groups for tooth-whitening efficacy measured as \u0394E (mean difference [MD]\u22120.53; 95\u00a0% CI [\u22121.72;0.66]; Z\u00a0=\u00a00.88; p\u00a0=\u00a00.38) and \u0394L (MD\u22120.22; 95\u00a0% CI [\u22120.81;0.36]; z\u00a0=\u00a00.75; p\u00a0=\u00a00.45); reduction of yellowing was higher with the whitestrips (MD\u22120.47; 95\u00a0% CI [\u22120.89; \u22120.06]; Z\u00a0=\u00a02.25; p\u00a0=\u00a00.02). Tooth sensitivity (risk ratio [RR] 1.17; 95\u00a0% CI [0.81\u20131.69]; Z\u00a0=\u00a00.81; p\u00a0=\u00a00.42) and gingival sensitivity (RR 0.76; 95\u00a0% CI [0.53\u20131.10]; Z\u00a0=\u00a01.44; p\u00a0=\u00a00.15) were similar, regardless of the whitening method used. The observed gingival irritation was higher when the 10\u00a0% CP gel was applied on tray (RR 0.43; 95\u00a0% CI [0.20\u20130.93]; Z\u00a0=\u00a02.14; p\u00a0=\u00a00.03). The quality of evidence generated was rated very low for all outcomes. There is no sound evidence to support the use of the whitening strips in detriment of the ADA-recommended technique based on the 10\u00a0% carbamide peroxide gel applied on tray. To the moment, there is no sound evidence in dental literature to suggest that the ADA-recommended whitening technique based on 10\u00a0% carbamide peroxide gel could be substituted by the whitening strips. The existing studies, with their limitations, revealed similar tooth whitening and tooth and gingival sensitivity for both whitening techniques.", "title": "Efficacy and safety of over-the-counter whitening strips as compared to home-whitening with 10\u00a0% carbamide peroxide gel\u2014systematic review of RCTs and metanalysis"}, "9310c9871945334edf40a175add5d820e034dae9": {"paper_id": "9310c9871945334edf40a175add5d820e034dae9", "abstract": "In this paper, we propose and validate a novel design for a double-gate tunnel field-effect transistor (DG tunnel FET), for which the simulations show significant improvements compared with single-gate devices using a gate dielectric. For the first time, DG tunnel FET devices, which are using a high-gate dielectric, are explored using realistic design parameters, showing an on-current as high as 0.23 mA for a gate voltage of 1.8 V, an off-current of less than 1 fA (neglecting gate leakage), an improved average subthreshold swing of 57 mV/dec, and a minimum point slope of 11 mV/dec. The 2D nature of tunnel FET current flow is studied, demonstrating that the current is not confined to a channel at the gate-dielectric surface. When varying temperature, tunnel FETs with a high-kappa gate dielectric have a smaller threshold voltage shift than those using SiO2, while the subthreshold slope for fixed values of Vg remains nearly unchanged, in contrast with the traditional MOSFET. Moreover, an Ion/Ioff ratio of more than 2 times 1011 is shown for simulated devices with a gate length (over the intrinsic region) of 50 nm, which indicates that the tunnel FET is a promising candidate to achieve better-than-ITRS low-standby-power switch performance.", "title": "Double-Gate Tunnel FET With High-$\\kappa$  Gate Dielectric"}, "1e2e07d6b89eb399db74d53a8dcc07decc694bd9": {"paper_id": "1e2e07d6b89eb399db74d53a8dcc07decc694bd9", "abstract": "Fast abnormal events detection in video is important for intelligent analysis of video. This paper proposes a fast anomaly detection algorithm based on sparse optical flow. We improve the efficiency of optical flow computation with foreground mask and spacial sampling and increase the robustness of optical flow with good feature (TK) points selecting and forward-backward filtering. A foreground channel is also added to the feature vector to help detect static or low speed objects. The algorithm is validated on real-life traffic surveillance to prove its effectiveness. It is also evaluated on a benchmark dataset and achieve detection results comparable to state-of-art methods and outperforms them at pixel-level when the false alarm rate is low. The strength of our algorithm is that it runs real-time on the benchmark dataset which is hundreds of times faster than comparative methods.", "title": "Fast anomaly detection in traffic surveillance video based on robust sparse optical flow"}, "308e059b7e0b0ef9faf48b3449214d89403778e2": {"paper_id": "308e059b7e0b0ef9faf48b3449214d89403778e2", "abstract": "We propose a space-time Markov random field (MRF) model to detect abnormal activities in video. The nodes in the MRF graph correspond to a grid of local regions in the video frames, and neighboring nodes in both space and time are associated with links. To learn normal patterns of activity at each local node, we capture the distribution of its typical optical flow with a mixture of probabilistic principal component analyzers. For any new optical flow patterns detected in incoming video clips, we use the learned model and MRF graph to compute a maximum a posteriori estimate of the degree of normality at each local node. Further, we show how to incrementally update the current model parameters as new video observations stream in, so that the model can efficiently adapt to visual context changes over a long period of time. Experimental results on surveillance videos show that our space-time MRF model robustly detects abnormal activities both in a local and global sense: not only does it accurately localize the atomic abnormal activities in a crowded video, but at the same time it captures the global-level abnormalities caused by irregular interactions between local activities.", "title": "Observe locally, infer globally: A space-time MRF for detecting abnormal activities with incremental updates"}, "3ed7cce354b626b5919add701ea37b81b4aa3155": {"paper_id": "3ed7cce354b626b5919add701ea37b81b4aa3155", "abstract": "Compared to other anomalous video event detection approaches that analyze object trajectories only, we propose a context-aware method to detect anomalies. By tracking all moving objects in the video, three different levels of spatiotemporal contexts are considered, i.e., point anomaly of a video object, sequential anomaly of an object trajectory, and co-occurrence anomaly of multiple video objects. A hierarchical data mining approach is proposed. At each level, frequency based analysis is performed to automatically discover regular rules of normal events. Events deviating from these rules are identified as anomalies. The proposed method is computationally efficient and can infer complex rules. Experiments on real traffic video validate that the detected video anomalies are hazardous or illegal according to traffic \u2217Corresponding author (phone: 224-392-2622, fax: 847-491-4455) Email addresses: fanjiang2008@u.northwestern.edu (Fan Jiang), jsyuan@ntu.edu.sg (Junsong Yuan), s-tsaftaris@northwestern.edu (Sotirios A. Tsaftaris), aggk@eecs.northwestern.edu (Aggelos K. Katsaggelos) Preprint submitted to Computer Vision and Image Understanding February 28, 2010 regulations.", "title": "Anomalous video event detection using spatiotemporal context"}, "1790ae874429c5e033677c358cb43a77e4486f55": {"paper_id": "1790ae874429c5e033677c358cb43a77e4486f55", "abstract": "A common method for real-time segmentation of moving regions in image sequences involves \u201cbackground subtraction,\u201d or thresholding the error between an estimate of the image without moving objects and the current image. The numerous approaches to this problem differ in the type of background model used and the procedure used to update the model. This paper discusses modeling each pixel as a mixture of Gaussians and using an on-line approximation to update the model. The Gaussian distributions of the adaptive mixture model are then evaluated to determine which are most likely to result from a background process. Each pixel is classified based on whether the Gaussian distribution which represents it most effectively is considered part of the background model. This results in a stable, real-time outdoor tracker which reliably deals with lighting changes, repetitive motions from clutter, and long-term scene changes. This system has been run almost continuously for 16 months, 24 hours a day, through rain and snow.", "title": "Adaptive Background Mixture Models for Real-Time Tracking"}, "8a916608e81eea5d7494e577c8563cae44a1b8c6": {"paper_id": "8a916608e81eea5d7494e577c8563cae44a1b8c6", "abstract": "We present a novel algorithm for detection of certain types of unusual events. The algorithm is based on multiple local monitors which collect low-level statistics. Each local monitor produces an alert if its current measurement is unusual and these alerts are integrated to a final decision regarding the existence of an unusual event. Our algorithm satisfies a set of requirements that are critical for successful deployment of any large-scale surveillance system. In particular, it requires a minimal setup (taking only a few minutes) and is fully automatic afterwards. Since it is not based on objects' tracks, it is robust and works well in crowded scenes where tracking-based algorithms are likely to fail. The algorithm is effective as soon as sufficient low-level observations representing the routine activity have been collected, which usually happens after a few minutes. Our algorithm runs in real-time. It was tested on a variety of real-life crowded scenes. A ground-truth was extracted for these scenes, with respect to which detection and false-alarm rates are reported.", "title": "Robust Real-Time Unusual Event Detection using Multiple Fixed-Location Monitors"}, "9f6a3844158c36ab9f8cd9be4429da52ec404573": {"paper_id": "9f6a3844158c36ab9f8cd9be4429da52ec404573", "abstract": "We consider the problem of selecting grouped variables (factors) for accurate prediction in regression. Such a problem arises naturally in many practical situations with the multifactor analysis-of-variance problem as the most important and well-known example. Instead of selecting factors by stepwise backward elimination, we focus on the accuracy of estimation and consider extensions of the lasso, the LARS algorithm and the non-negative garrotte for factor selection. The lasso, the LARS algorithm and the non-negative garrotte are recently proposed regression methods that can be used to select individual variables. We study and propose efficient algorithms for the extensions of these methods for factor selection and show that these extensions give superior performance to the traditional stepwise backward elimination method in factor selection problems.We study the similarities and the differences between these methods. Simulations and real examples are used to illustrate the methods.", "title": "Model selection and estimation in regression with grouped variables"}, "332162f79ec80ee092ab122f8aee9370bc3719d4": {"paper_id": "332162f79ec80ee092ab122f8aee9370bc3719d4", "abstract": "Suppose you are given some data set drawn from an underlying probability distribution P and you want to estimate a simple subset S of input space such that the probability that a test point drawn from P lies outside of S equals some a priori specified value between 0 and 1. We propose a method to approach this problem by trying to estimate a function f that is positive on S and negative on the complement. The functional form of f is given by a kernel expansion in terms of a potentially small subset of the training data; it is regularized by controlling the length of the weight vector in an associated feature space. The expansion coefficients are found by solving a quadratic programming problem, which we do by carrying out sequential optimization over pairs of input patterns. We also provide a theoretical analysis of the statistical performance of our algorithm. The algorithm is a natural extension of the support vector algorithm to the case of unlabeled data.", "title": "Estimating the Support of a High-Dimensional Distribution"}, "b49bb7ecd2afd6461c78ff29536839b5ee45cd15": {"paper_id": "b49bb7ecd2afd6461c78ff29536839b5ee45cd15", "abstract": "Well, someone can decide by themselves what they want to do and need to do but sometimes, that kind of person will need some problem solving methods in artificial intelligence references. People with open minded will always try to seek for the new things and information from many sources. On the contrary, people with closed mind will always think that they can do it by their principals. So, what kind of person are you?", "title": "Problem-solving methods in artificial intelligence"}, "9c5af2226a48672c9d12cbd22f41b400717f5a21": {"paper_id": "9c5af2226a48672c9d12cbd22f41b400717f5a21", "abstract": "Designing and implementing efficient, provably correct parallel neural network processing is challenging. Existing high-level parallel abstractions like MapReduce are insufficiently expressive while low-level tools like MPI and Pthreads leave ML experts repeatedly solving the same design challenges. However, the diversity and large-scale data size have posed a significant challenge to construct a flexible and high-performance implementation of deep learning neural networks. To improve the performance and maintain the scalability, we present CNNLab, a novel deep learning framework using GPU and FPGA-based accelerators. CNNLab provides a uniform programming model to users so that the hardware implementation and the scheduling are invisible to the programmers. At runtime, CNNLab leverages the trade-offs between GPU and FPGA before offloading the tasks to the accelerators. Experimental results on the state-of-the-art Nvidia K40 GPU and Altera DE5 FPGA board demonstrate that the CNNLab can provide a universal framework with efficient support for diverse applications without increasing the burden of the programmers. Moreover, we analyze the detailed quantitative performance, throughput, power, energy, and performance density for both approaches. Experimental results leverage the trade-offs between GPU and FPGA and provide useful practical experiences for the deep learning research community.", "title": "CNNLab: a Novel Parallel Framework for Neural Networks using GPU and FPGA-a Practical Study with Trade-off Analysis"}, "1637ac4fed83b8309df2de07fbeb8b2511bb1170": {"paper_id": "1637ac4fed83b8309df2de07fbeb8b2511bb1170", "abstract": "Many large-scale machine learning (ML) applications use iterative algorithms to converge on parameter values that make the chosen model fit the input data. Often, this approach results in the same sequence of accesses to parameters repeating each iteration. This paper shows that these repeating patterns can and should be exploited to improve the efficiency of the parallel and distributed ML applications that will be a mainstay in cloud computing environments. Focusing on the increasingly popular \"parameter server\" approach to sharing model parameters among worker threads, we describe and demonstrate how the repeating patterns can be exploited. Examples include replacing dynamic cache and server structures with static pre-serialized structures, informing prefetch and partitioning decisions, and determining which data should be cached at each thread to avoid both contention and slow accesses to memory banks attached to other sockets. Experiments show that such exploitation reduces per-iteration time by 33--98%, for three real ML workloads, and that these improvements are robust to variation in the patterns over time.", "title": "Exploiting iterative-ness for parallel ML computations"}, "043afbd936c95d0e33c4a391365893bd4102f1a7": {"paper_id": "043afbd936c95d0e33c4a391365893bd4102f1a7", "abstract": "Large deep neural network models have recently demonstrated state-of-the-art accuracy on hard visual recognition tasks. Unfortunately such models are extremely time consuming to train and require large amount of compute cycles. We describe the design and implementation of a distributed system called Adam comprised of commodity server machines to train such models that exhibits world-class performance, scaling and task accuracy on visual recognition tasks. Adam achieves high efficiency and scalability through whole system co-design that optimizes and balances workload computation and communication. We exploit asynchrony throughout the system to improve performance and show that it additionally improves the accuracy of trained models. Adam is significantly more efficient and scalable than was previously thought possible and used 30x fewer machines to train a large 2 billion connection model to 2x higher accuracy in comparable time on the ImageNet 22,000 category image classification task than the system that previously held the record for this benchmark. We also show that task accuracy improves with larger models. Our results provide compelling evidence that a distributed systems-driven approach to deep learning using current training algorithms is worth pursuing.", "title": "Project Adam: Building an Efficient and Scalable Deep Learning Training System"}, "54dd77bd7b904a6a69609c9f3af11b42f654ab5d": {"paper_id": "54dd77bd7b904a6a69609c9f3af11b42f654ab5d", "abstract": null, "title": "ImageNet: A large-scale hierarchical image database"}, "f5f1beada9e269b2a7faed8dfe936919ac0c2397": {"paper_id": "f5f1beada9e269b2a7faed8dfe936919ac0c2397", "abstract": "Convolutional Neural Networks (CNNs) have been successfully used for many computer vision applications. It would be beneficial to these applications if the computational workload of CNNs could be reduced. In this work we analyze the linear algebraic properties of CNNs and propose an algorithmic modification to reduce their computational workload. An up to a 47% reduction can be achieved without any change in the image recognition results or the addition of any hardware accelerators.", "title": "Minimizing Computation in Convolutional Neural Networks"}, "04fd80d4d6c6ac5ef94350e3142580f54a66c093": {"paper_id": "04fd80d4d6c6ac5ef94350e3142580f54a66c093", "abstract": "For learning and classification workloads that operate on large amounts of unstructured data with stringent performance constraints, general purpose processor performance scales poorly with data size. In this paper, we present a programmable accelerator for this workload domain. To architect the accelerator, we profile five representative workloads, and find that their computationally intensive portions can be formulated as matrix or vector operations generating large amounts of intermediate data, which are then reduced by a secondary operation such as array ranking, finding max/min and aggregation. The proposed accelerator, called MAPLE, has hundreds of simple processing elements (PEs) laid out in a two-dimensional grid, with two key features. First, it uses in-memory processing where on-chip memory blocks perform the secondary reduction operations. By doing so, the intermediate data are dynamically processed and never stored or sent off-chip. Second, MAPLE uses banked off-chip memory, and organizes its PEs into independent groups each with its own off-chip memory bank. These two features together allow MAPLE to scale its performance with data size. This paper describes the MAPLE architecture, explores its design space with a simulator, and illustrates how to automatically map application kernels to the hardware. We also implement a 512-PE FPGA prototype of MAPLE and find that it is 1.5-10x faster than a 2.5 GHz quad-core Xeon processor despite running at a modest 125 MHz.", "title": "A programmable parallel accelerator for learning and classification"}, "092217c2267f6e0673590aa151d811e579ff7760": {"paper_id": "092217c2267f6e0673590aa151d811e579ff7760", "abstract": "The Roofline model offers insight on how to improve the performance of software and hardware.", "title": "Roofline: an insightful visual performance model for multicore architectures"}, "4f40ea0248653d4ffb6ef4857cd23f0f713d8c69": {"paper_id": "4f40ea0248653d4ffb6ef4857cd23f0f713d8c69", "abstract": "We present a massively parallel coprocessor for accelerating Convolutional Neural Networks (CNNs), a class of important machine learning algorithms. The coprocessor functional units, consisting of parallel 2D convolution primitives and programmable units performing sub-sampling and non-linear functions specific to CNNs, implement a \u201cmeta-operator\u201d to which a CNN may be compiled to. The coprocessor is serviced by distributed off-chip memory banks with large data bandwidth. As a key feature, we use low precision data and further increase the effective memory bandwidth by packing multiple words in every memory operation, and leverage the algorithm\u2019s simple data access patterns to use off-chip memory as a scratchpad for intermediate data, critical for CNNs. A CNN is mapped to the coprocessor hardware primitives with instructions to transfer data between the memory and coprocessor. We have implemented a prototype of the CNN coprocessor on an off-the-shelf PCI FPGA card with a single Xilinx Virtex5 LX330T FPGA and 4 DDR2 memory banks totaling 1GB. The coprocessor prototype can process at the rate of 3.4 billion multiply accumulates per second (GMACs) for CNN forward propagation, a speed that is 31x faster than a software implementation on a 2.2 GHz AMD Opteron processor. For a complete face recognition application with the CNN on the coprocessor and the rest of the image processing tasks on the host, the prototype is 6-10x faster, depending on the host-coprocessor bandwidth.", "title": "A Massively Parallel Coprocessor for Convolutional Neural Networks"}, "23d14ab0f18fa881a2ac8ae027be6b9f2c91d74d": {"paper_id": "23d14ab0f18fa881a2ac8ae027be6b9f2c91d74d", "abstract": "Many companies are deploying services, either for consumers or industry, which are largely based on machine-learning algorithms for sophisticated processing of large amounts of data. The state-of-the-art and most popular such machine-learning algorithms are Convolutional and Deep Neural Networks (CNNs and DNNs), which are known to be both computationally and memory intensive. A number of neural network accelerators have been recently proposed which can offer high computational capacity/area ratio, but which remain hampered by memory accesses.\n However, unlike the memory wall faced by processors on general-purpose workloads, the CNNs and DNNs memory footprint, while large, is not beyond the capability of the on-chip storage of a multi-chip system. This property, combined with the CNN/DNN algorithmic characteristics, can lead to high internal bandwidth and low external communications, which can in turn enable high-degree parallelism at a reasonable area cost. In this article, we introduce a custom multi-chip machine-learning architecture along those lines. We show that, on a subset of the largest known neural network layers, it is possible to achieve a speedup of 450.65x over a GPU, and reduce the energy by 150.31x on average for a 64-chip system. We implement the node down to the place and route at 28nm, containing a combination of custom storage and computational units, with industry-grade interconnects.", "title": "DaDianNao: A Machine-Learning Supercomputer"}, "58c45859350b7e9fc2dc6676e318e8f526073f5f": {"paper_id": "58c45859350b7e9fc2dc6676e318e8f526073f5f", "abstract": "Recently, the hybrid deep neural network (DNN)- hidden Markov model (HMM) has been shown to significantly improve speech recognition performance over the conventional Gaussian mixture model (GMM)-HMM. The performance improvement is partially attributed to the ability of the DNN to model complex correlations in speech features. In this paper, we show that further error rate reduction can be obtained by using convolutional neural networks (CNNs). We first present a concise description of the basic CNN and explain how it can be used for speech recognition. We further propose a limited-weight-sharing scheme that can better model speech features. The special structure such as local connectivity, weight sharing, and pooling in CNNs exhibits some degree of invariance to small shifts of speech features along the frequency axis, which is important to deal with speaker and environment variations. Experimental results show that CNNs reduce the error rate by 6%-10% compared with DNNs on the TIMIT phone recognition and the voice search large vocabulary speech recognition tasks.", "title": "Convolutional Neural Networks for Speech Recognition"}, "233b1774f28c9972df2dfcf20dfbb0df45792bd0": {"paper_id": "233b1774f28c9972df2dfcf20dfbb0df45792bd0", "abstract": "Deep networks are state-of-the-art models used for understanding the content of images, videos, audio and raw input data. Current computing systems are not able to run deep network models in real-time with low power consumption. In this paper we present nn-X: a scalable, low-power coprocessor for enabling real-time execution of deep neural networks. nn-X is implemented on programmable logic devices and comprises an array of configurable processing elements called collections. These collections perform the most common operations in deep networks: convolution, subsampling and non-linear functions. The nn-X system includes 4 high-speed direct memory access interfaces to DDR3 memory and two ARM Cortex-A9 processors. Each port is capable of a sustained throughput of 950 MB/s in full duplex. nn-X is able to achieve a peak performance of 227 G-ops/s, a measured performance in deep learning applications of up to 200 G-ops/s while consuming less than 4 watts of power. This translates to a performance per power improvement of 10 to 100 times that of conventional mobile and desktop processors.", "title": "A 240 G-ops/s Mobile Coprocessor for Deep Neural Networks"}, "1606b1475e125bba1b2d87bcf1e33b06f42c5f0d": {"paper_id": "1606b1475e125bba1b2d87bcf1e33b06f42c5f0d", "abstract": "In real-world face detection, large visual variations, such as those due to pose, expression, and lighting, demand an advanced discriminative model to accurately differentiate faces from the backgrounds. Consequently, effective models for the problem tend to be computationally prohibitive. To address these two conflicting challenges, we propose a cascade architecture built on convolutional neural networks (CNNs) with very powerful discriminative capability, while maintaining high performance. The proposed CNN cascade operates at multiple resolutions, quickly rejects the background regions in the fast low resolution stages, and carefully evaluates a small number of challenging candidates in the last high resolution stage. To improve localization effectiveness, and reduce the number of candidates at later stages, we introduce a CNN-based calibration stage after each of the detection stages in the cascade. The output of each calibration stage is used to adjust the detection window position for input to the subsequent stage. The proposed method runs at 14 FPS on a single CPU core for VGA-resolution images and 100 FPS using a GPU, and achieves state-of-the-art detection performance on two public face detection benchmarks.", "title": "A convolutional neural network cascade for face detection"}, "6f3b511729fe87658da0d22065688c42bdcd1f4c": {"paper_id": "6f3b511729fe87658da0d22065688c42bdcd1f4c", "abstract": "Many modern visual recognition algorithms incorporate a step of spatial \u2018pooling\u2019, where the outputs of several nearby feature detectors are combined into a local or global \u2018bag of features\u2019, in a way that preserves task-related information while removing irrelevant details. Pooling is used to achieve invariance to image transformations, more compact representations, and better robustness to noise and clutter. Several papers have shown that the details of the pooling operation can greatly influence the performance, but studies have so far been purely empirical. In this paper, we show that the reasons underlying the performance of various pooling methods are obscured by several confounding factors, such as the link between the sample cardinality in a spatial pool and the resolution at which low-level features have been extracted. We provide a detailed theoretical analysis of max pooling and average pooling, and give extensive empirical comparisons for object recognition tasks.", "title": "A Theoretical Analysis of Feature Pooling in Visual Recognition"}, "0f4019244234a074f53f0ed0e008008d284f6780": {"paper_id": "0f4019244234a074f53f0ed0e008008d284f6780", "abstract": "Training a feed-forward network for the fast neural style transfer of images has proven successful, but the naive extension of processing videos frame by frame is prone to producing flickering results. We propose the first end-toend network for online video style transfer, which generates temporally coherent stylized video sequences in near realtime. Two key ideas include an efficient network by incorporating short-term coherence, and propagating short-term coherence to long-term, which ensures consistency over a longer period of time. Our network can incorporate different image stylization networks and clearly outperforms the per-frame baseline both qualitatively and quantitatively. Moreover, it can achieve visually comparable coherence to optimization-based video style transfer, but is three orders of magnitude faster.", "title": "Coherent Online Video Style Transfer"}, "17598f2061549df6a28c2fca5fe6d244fd9b2ad0": {"paper_id": "17598f2061549df6a28c2fca5fe6d244fd9b2ad0", "abstract": "A f?umeworkfor learning parumeterized models oj\u2018optical flow from image sequences is presented. A class of motions is represented by U set of orthogonul basis $ow fields that are computed froni a training set using priiicipal component analysis. Many complex image motions can be represeiited by a linear combiriution of U small number of these basisjows. The learned motion models may be usedfilr optical flow estimation und for model-bused recognition. For optical flow estimation we describe a robust, multi-resolution scheme for directly computing the parameters of the learned flow models from image derivatives. As examples we consider learning motion discontinuities, non-rigid motion of human mouths, und urticulated human motion.", "title": "Learning Parameterized Models of Image Motion"}, "00a7370518a6174e078df1c22ad366a2188313b5": {"paper_id": "00a7370518a6174e078df1c22ad366a2188313b5", "abstract": "Optical flow cannot be computed locally, since only one independent measurement is available from the image sequence at a point, while the flow velocity has two components. A second constraint is needed. A method for finding the optical flow pattern is presented which assumes that the apparent velocity of the brightness pattern varies smoothly almost everywhere in the image. An iterative implementation is shown which successfully computes the optical flow for a number of synthetic image sequences. The algorithm is robust in that it can handle image sequences that are quantized rather coarsely in space and time. It is also insensitive to quantization of brightness levels and additive noise. Examples are included where the assumption of smoothness is violated at singular points or along lines in the image.", "title": "Determining Optical Flow"}, "563e656203f29f0cbabc5cf0611355ba79ae4320": {"paper_id": "563e656203f29f0cbabc5cf0611355ba79ae4320", "abstract": "We study an energy functional for computing optical flow that combines three assumptions: a brightness constancy assumption, a gradient constancy assumption, and a discontinuity-preserving spatio-temporal smoothness constraint. In order to allow for large displacements, linearisations in the two data terms are strictly avoided. We present a consistent numerical scheme based on two nested fixed point iterations. By proving that this scheme implements a coarse-to-fine warping strategy, we give a theoretical foundation for warping which has been used on a mainly experimental basis so far. Our evaluation demonstrates that the novel method gives significantly smaller angular errors than previous techniques for optical flow estimation. We show that it is fairly insensitive to parameter variations, and we demonstrate its excellent robustness under noise. In Proc. 8th European Conference on Computer Vision, Springer LNCS 3024, T. Pajdla and J. Matas (Eds.), vol. 4, pp. 25-36, Prague, Czech Republic, May 2004 c \u00a9 Springer-Verlag Berlin Heidelberg 2004 Received The Longuet-Higgins Best Paper Award.", "title": "High Accuracy Optical Flow Estimation Based on a Theory for Warping"}, "243e681e23e7d1744defd2ee0c83643b05f003d3": {"paper_id": "243e681e23e7d1744defd2ee0c83643b05f003d3", "abstract": "Scene labeling consists of labeling each pixel in an image with the category of the object it belongs to. We propose a method that uses a multiscale convolutional network trained from raw pixels to extract dense feature vectors that encode regions of multiple sizes centered on each pixel. The method alleviates the need for engineered features, and produces a powerful representation that captures texture, shape, and contextual information. We report results using multiple postprocessing methods to produce the final labeling. Among those, we propose a technique to automatically retrieve, from a pool of segmentation components, an optimal set of components that best explain the scene; these components are arbitrary, for example, they can be taken from a segmentation tree or from any family of oversegmentations. The system yields record accuracies on the SIFT Flow dataset (33 classes) and the Barcelona dataset (170 classes) and near-record accuracy on Stanford background dataset (eight classes), while being an order of magnitude faster than competing approaches, producing a 320\u00d7240 image labeling in less than a second, including feature extraction.", "title": "Learning Hierarchical Features for Scene Labeling"}, "8ca53d187f6beb3d1e4fb0d1b68544d578c86c53": {"paper_id": "8ca53d187f6beb3d1e4fb0d1b68544d578c86c53", "abstract": "Ground truth optical flow is difficult to measure in real scenes with natural motion. As a result, optical flow data sets are restricted in terms of size, complexity, and diversity, making optical flow algorithms difficult to train and test on realistic data. We introduce a new optical flow data set derived from the open source 3D animated short film Sintel. This data set has important features not present in the popular Middlebury flow evaluation: long sequences, large motions, specular reflections, motion blur, defocus blur, and atmospheric effects. Because the graphics data that generated the movie is open source, we are able to render scenes under conditions of varying complexity to evaluate where existing flow algorithms fail. We evaluate several recent optical flow algorithms and find that current highly-ranked methods on the Middlebury evaluation have difficulty with this more complex data set suggesting further research on optical flow estimation is needed. To validate the use of synthetic data, we compare the imageand flow-statistics of Sintel to those of real films and videos and show that they are similar. The data set, metrics, and evaluation website are publicly available.", "title": "A Naturalistic Open Source Movie for Optical Flow Evaluation"}, "391e52ac04408d3e6496614ffafd6ac89c1b6c45": {"paper_id": "391e52ac04408d3e6496614ffafd6ac89c1b6c45", "abstract": "This paper poses object category detection in images as a type of 2D-to-3D alignment problem, utilizing the large quantities of 3D CAD models that have been made publicly available online. Using the \"chair\" class as a running example, we propose an exemplar-based 3D category representation, which can explicitly model chairs of different styles as well as the large variation in viewpoint. We develop an approach to establish part-based correspondences between 3D CAD models and real photographs. This is achieved by (i) representing each 3D model using a set of view-dependent mid-level visual elements learned from synthesized views in a discriminative fashion, (ii) carefully calibrating the individual element detectors on a common dataset of negative images, and (iii) matching visual elements to the test image allowing for small mutual deformations but preserving the viewpoint and style constraints. We demonstrate the ability of our system to align 3D models with 2D objects in the challenging PASCAL VOC images, which depict a wide variety of chairs in complex scenes.", "title": "Seeing 3D Chairs: Exemplar Part-Based 2D-3D Alignment Using a Large Dataset of CAD Models"}, "7568d13a82f7afa4be79f09c295940e48ec6db89": {"paper_id": "7568d13a82f7afa4be79f09c295940e48ec6db89", "abstract": "Rendering the semantic content of an image in different styles is a difficult image processing task. Arguably, a major limiting factor for previous approaches has been the lack of image representations that explicitly represent semantic information and, thus, allow to separate image content from style. Here we use image representations derived from Convolutional Neural Networks optimised for object recognition, which make high level image information explicit. We introduce A Neural Algorithm of Artistic Style that can separate and recombine the image content and style of natural images. The algorithm allows us to produce new images of high perceptual quality that combine the content of an arbitrary photograph with the appearance of numerous wellknown artworks. Our results provide new insights into the deep image representations learned by Convolutional Neural Networks and demonstrate their potential for high level image synthesis and manipulation.", "title": "Image Style Transfer Using Convolutional Neural Networks"}, "063c6ae786c34d3722c6d9060df6339e246bbc3b": {"paper_id": "063c6ae786c34d3722c6d9060df6339e246bbc3b", "abstract": "Here we introduce a new model of natural textures based on the feature spaces of convolutional neural networks optimised for object recognition. Samples from the model are of high perceptual quality demonstrating the generative power of neural networks trained in a purely discriminative fashion. Within the model, textures are represented by the correlations between feature maps in several layers of the network. We show that across layers the texture representations increasingly capture the statistical properties of natural images while making object information more and more explicit. The model provides a new tool to generate stimuli for neuroscience and might offer insights into the deep representations learned by convolutional neural networks.", "title": "Texture Synthesis Using Convolutional Neural Networks"}, "102a2096ba2e2947dc252445f764e7583b557680": {"paper_id": "102a2096ba2e2947dc252445f764e7583b557680", "abstract": "This paper proposes Markovian Generative Adversarial Networks (MGANs), a method for training generative neural networks for efficient texture synthesis. While deep neural network approaches have recently demonstrated remarkable results in terms of synthesis quality, they still come at considerable computational costs (minutes of run-time for low-res images). Our paper addresses this efficiency issue. Instead of a numerical deconvolution in previous work, we precompute a feedforward, strided convolutional network that captures the feature statistics of Markovian patches and is able to directly generate outputs of arbitrary dimensions. Such network can directly decode brown noise to realistic texture, or photos to artistic paintings. With adversarial training, we obtain quality comparable to recent neural texture synthesis methods. As no optimization is required any longer at generation time, our run-time performance (0.25M pixel images at 25Hz) surpasses previous neural texture synthesizers by a significant margin (at least 500 times faster). We apply this idea to texture synthesis, style transfer, and video stylization.", "title": "Precomputed Real-Time Texture Synthesis with Markovian Generative Adversarial Networks"}, "6c97cb86ab0e05e496c207892effe6e8f63deec2": {"paper_id": "6c97cb86ab0e05e496c207892effe6e8f63deec2", "abstract": "Example-based texture synthesis algorithms have gained widespread popularity for their ability to take a single input image and create a perceptually similar non-periodic texture. However, previous methods rely on single input exemplars that can capture only a limited band of spatial scales. For example, synthesizing a continent-like appearance at a variety of zoom levels would require an impractically high input resolution. In this paper, we develop a multiscale texture synthesis algorithm. We propose a novel example-based representation, which we call an exemplar graph, that simply requires a few low-resolution input exemplars at different scales. Moreover, by allowing loops in the graph, we can create infinite zooms and infinitely detailed textures that are impossible with current example-based methods. We also introduce a technique that ameliorates inconsistencies in the user's input, and show that the application of this method yields improved interscale coherence and higher visual quality. We demonstrate optimizations for both CPU and GPU implementations of our method, and use them to produce animations with zooming and panning at multiple scales, as well as static gigapixel-sized images with features spanning many spatial scales.", "title": "Multiscale texture synthesis"}, "41f773c0847cd2d65b9fe8dfb2c73b7bee2e3227": {"paper_id": "41f773c0847cd2d65b9fe8dfb2c73b7bee2e3227", "abstract": "Current digital painting tools are primarily targeted at professionals and are often overwhelmingly complex for use by novices. At the same time, simpler tools may not invoke the user creatively, or are limited to plain styles that lack visual sophistication. There are many people who are not art professionals, yet would like to partake in digital creative expression. Challenges and rewards for novices differ greatly from those for professionals. In this paper, we leverage existing works in Creativity and Creativity Support Tools (CST) to formulate design goals specifically for digital art creation tools for novices. We implemented these goals within a digital painting system, called Painting with Bob. We evaluate the efficacy of the design and our prototype with a user study, and we find that users are highly satisfied with the user experience, as well as the paintings created with our system.", "title": "Painting with Bob: assisted creativity for novices"}, "aad7826b952e3f1ab17b1dc181f58f3c13bd7216": {"paper_id": "aad7826b952e3f1ab17b1dc181f58f3c13bd7216", "abstract": "We present an approach to example-based stylization of 3D renderings that better preserves the rich expressiveness of hand-created artwork. Unlike previous techniques, which are mainly guided by colors and normals, our approach is based on light propagation in the scene. This novel type of guidance can distinguish among context-dependent illumination effects, for which artists typically use different stylization techniques, and delivers a look closer to realistic artwork. In addition, we demonstrate that the current state of the art in guided texture synthesis produces artifacts that can significantly decrease the fidelity of the synthesized imagery, and propose an improved algorithm that alleviates them. Finally, we demonstrate our method's effectiveness on a variety of scenes and styles, in applications like interactive shading study or autocompletion.", "title": "StyLit: illumination-guided example-based stylization of 3D renderings"}, "08485bbdfec76168205469ab971cd6b29a0e67ab": {"paper_id": "08485bbdfec76168205469ab971cd6b29a0e67ab", "abstract": "Converging evidence suggests that the primate ventral visual pathway encodes increasingly complex stimulus features in downstream areas. We quantitatively show that there indeed exists an explicit gradient for feature complexity in the ventral pathway of the human brain. This was achieved by mapping thousands of stimulus features of increasing complexity across the cortical sheet using a deep neural network. Our approach also revealed a fine-grained functional specialization of downstream areas of the ventral stream. Furthermore, it allowed decoding of representations from human brain activity at an unsurpassed degree of accuracy, confirming the quality of the developed approach. Stimulus features that successfully explained neural responses indicate that population receptive fields were explicitly tuned for object categorization. This provides strong support for the hypothesis that object categorization is a guiding principle in the functional organization of the primate ventral stream.", "title": "Deep Neural Networks Reveal a Gradient in the Complexity of Neural Representations across the Ventral Stream."}, "4e2c7fe486bf6a613b3a573365a98c53b238824e": {"paper_id": "4e2c7fe486bf6a613b3a573365a98c53b238824e", "abstract": "Perceptual systems routinely separate content from style, classifying familiar words spoken in an unfamiliar accent, identifying a font or handwriting style across letters, or recognizing a familiar face or object seen under unfamiliar viewing conditions. Yet a general and tractable computational model of this ability to untangle the underlying factors of perceptual observations remains elusive (Hofstadter, 1985). Existing factor models (Mardia, Kent, & Bibby, 1979; Hinton & Zemel, 1994; Ghahramani, 1995; Bell & Sejnowski, 1995; Hinton, Dayan, Frey, & Neal, 1995; Dayan, Hinton, Neal, & Zemel, 1995; Hinton & Ghahramani, 1997) are either insufficiently rich to capture the complex interactions of perceptually meaningful factors such as phoneme and speaker accent or letter and font, or do not allow efficient learning algorithms. We present a general framework for learning to solve two-factor tasks using bilinear models, which provide sufficiently expressive representations of factor interactions but can nonetheless be fit to data using efficient algorithms based on the singular value decomposition and expectation-maximization. We report promising results on three different tasks in three different perceptual domains: spoken vowel classification with a benchmark multi-speaker database, extrapolation of fonts to unseen letters, and translation of faces to novel illuminants.", "title": "Separating Style and Content with Bilinear Models"}, "2402312da67861c63a103b92cfac7ee101ff0544": {"paper_id": "2402312da67861c63a103b92cfac7ee101ff0544", "abstract": "As there has been a paradigm shift in the learning load from a human subject to a computer, machine learning has been considered as a useful tool for Brain-Computer Interfaces (BCIs). In this paper, we propose a novel Bayesian framework for discriminative feature extraction for motor imagery classification in an EEG-based BCI in which the class-discriminative frequency bands and the corresponding spatial filters are optimized by means of the probabilistic and information-theoretic approaches. In our framework, the problem of simultaneous spatiospectral filter optimization is formulated as the estimation of an unknown posterior probability density function (pdf) that represents the probability that a single-trial EEG of predefined mental tasks can be discriminated in a state. In order to estimate the posterior pdf, we propose a particle-based approximation method by extending a factored-sampling technique with a diffusion process. An information-theoretic observation model is also devised to measure discriminative power of features between classes. From the viewpoint of classifier design, the proposed method naturally allows us to construct a spectrally weighted label decision rule by linearly combining the outputs from multiple classifiers. We demonstrate the feasibility and effectiveness of the proposed method by analyzing the results and its success on three public databases.", "title": "A Novel Bayesian Framework for Discriminative Feature Extraction in Brain-Computer Interfaces"}, "ab26729bb75d29f9d1ade33f52e226dcd60965e6": {"paper_id": "ab26729bb75d29f9d1ade33f52e226dcd60965e6", "abstract": "We present applications of rough set methods for feature selection in pattern recognition. We emphasize the role of the basic constructs of rough set approach in feature selection, namely reducts and their approximations, including dynamic reducts. In the overview of methods for feature selection we discuss feature selection criteria, including the rough set based methods. Our algorithm for feature selection is based on an application of a rough set method to the result of principal components analysis (PCA) used for feature projection and reduction. Finally, the paper presents numerical results of face and mammogram recognition experiments using neural network, with feature selection based on proposed PCA and rough set methods. 2002 Elsevier Science B.V. All rights reserved.", "title": "Rough set methods in feature selection and recognition"}, "1190ff6c6cc72a038314ae9f9f482d05e49059d2": {"paper_id": "1190ff6c6cc72a038314ae9f9f482d05e49059d2", "abstract": "Noninvasive electroencephalogram (EEG) recordings provide for easy and safe access to human neocortical processes which can be exploited for a brain-computer interface (BCI). At present, however, the use of BCIs is severely limited by low bit-transfer rates. We systematically analyze and develop two recent concepts, both capable of enhancing the information gain from multichannel scalp EEG recordings: 1) the combination of classifiers, each specifically tailored for different physiological phenomena, e.g., slow cortical potential shifts, such as the premovement Bereitschaftspotential or differences in spatio-spectral distributions of brain activity (i.e., focal event-related desynchronizations) and 2) behavioral paradigms inducing the subjects to generate one out of several brain states (multiclass approach) which all bare a distinctive spatio-temporal signature well discriminable in the standard scalp EEG. We derive information-theoretic predictions and demonstrate their relevance in experimental data. We will show that a suitably arranged interaction between these concepts can significantly boost BCI performances.", "title": "Boosting bit rates in noninvasive EEG single-trial classifications by feature combination and multiclass paradigms"}, "2d3878f9bf4ca77737ee09c04d087e3a813094f3": {"paper_id": "2d3878f9bf4ca77737ee09c04d087e3a813094f3", "abstract": "Amyotrophic lateral sclerosis, or ALS, is a degenerative disease of the motor neurons that eventually leads to complete paralysis. We are developing a wheelchair system that can help ALS patients, and others who can't use physical interfaces such as joysticks or gaze tracking, regain some autonomy. The system must be usable in hospitals and homes with minimal infrastructure modification. It must be safe and relatively low cost and must provide optimal interaction between the user and the wheelchair within the constraints of the brain-computer interface. To this end, we have built the first working prototype of a brain-controlled wheelchair that can navigate inside a typical office or hospital environment. This article describes the BCW, our control strategy, and the system's performance in a typical building environment. This brain-controlled wheelchair prototype uses a P300 EEG signal and a motion guidance strategy to navigate in a building safely and efficiently without complex sensors or sensor processing", "title": "Controlling a Wheelchair Indoors Using Thought"}, "0f4107df9063c1a22cc39c464e8e2fc1d384de6d": {"paper_id": "0f4107df9063c1a22cc39c464e8e2fc1d384de6d", "abstract": "JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org.. Society for Industrial and Applied Mathematics is collaborating with JSTOR to digitize, preserve and extend access to SIAM Review.", "title": "Elements of Information Theory"}, "3971078915f40fe2187a495dc1fc9383268fa09c": {"paper_id": "3971078915f40fe2187a495dc1fc9383268fa09c", "abstract": "Your use of the JSTOR archive indicates your acceptance of JSTOR's Terms and Conditions of Use, available at http://www.jstor.org/page/info/about/policies/terms.jsp. JSTOR's Terms and Conditions of Use provides, in part, that unless you have obtained prior permission, you may not download an entire issue of a journal or multiple copies of articles, and you may use content in the JSTOR archive only for your personal, non-commercial use.", "title": "Regularized Discriminant Analysis"}, "e107f4dd2d579c087afc082d4bbfdf3e23e9dfed": {"paper_id": "e107f4dd2d579c087afc082d4bbfdf3e23e9dfed", "abstract": "Event-related desynchronization (ERD) 2.0 sec before and 1.0 sec after movement in the frequency bands of 8-10, 10-12, 12-20 and 20-30 Hz and movement-related cortical potentials (MRCPs) to self-paced movements were studied from subdural recordings over the central region in 3 patients, and from scalp-recorded EEGs in 20 normal volunteers. In direct cortical recordings, the peak ERD response and peak MRCP amplitude to self-paced finger movements were maximal over recording sites in the contralateral hand motor representations. The topography and time of onset of the ERD response to finger and foot movements suggest that the ERD responses in the 8-10 Hz and 10-12 Hz bands are more somatotopically restricted than the responses in the higher frequency bands. The power recovery and subsequent overshoot in the different frequency bands occurred in an orderly fashion with the faster frequencies recovering earlier. The ERD responses on the scalp-recorded EEGs were of lower magnitude and more widely distributed than those occurring on the subdural recordings. Across the population, there was no relation between the magnitude of the ERD response in any of the frequency bands studied and the peak amplitude of the negative slope (pNS') and the frontal peak of the motor potential (fpMP) of the MRCPs. MRCPs and ERD responses originate in similar cortical regions and share some common timing features, but the magnitude and spatial distribution of the two responses appear to be independent of each other, which suggests that the physiological mechanisms governing these two events are different and may represent different aspects of motor cortex activation. Differences in the timing and topographical features of the ERD responses in the various frequency bands also suggest a distinct functional significance for the various spectral components of the electrical activity in the motor cortex.", "title": "Event-related desynchronization and movement-related cortical potentials on the ECoG and EEG."}, "c263dfdce0a38362717fa4139880103f67dcc6d1": {"paper_id": "c263dfdce0a38362717fa4139880103f67dcc6d1", "abstract": "Over the past decade, many laboratories have begun to explore brain-computer interface (BCI) technology as a radically new communication option for those with neuromuscular impairments that prevent them from using conventional augmentative communication methods. BCI's provide these users with communication channels that do not depend on peripheral nerves and muscles. This article summarizes the first international meeting devoted to BCI research and development. Current BCI's use electroencephalographic (EEG) activity recorded at the scalp or single-unit activity recorded from within cortex to control cursor movement, select letters or icons, or operate a neuroprosthesis. The central element in each BCI is a translation algorithm that converts electrophysiological input from the user into output that controls external devices. BCI operation depends on effective interaction between two adaptive controllers, the user who encodes his or her commands in the electrophysiological input provided to the BCI, and the BCI which recognizes the commands contained in the input and expresses them in device control. Current BCI's have maximum information transfer rates of 5-25 b/min. Achievement of greater speed and accuracy depends on improvements in signal processing, translation algorithms, and user training. These improvements depend on increased interdisciplinary cooperation between neuroscientists, engineers, computer programmers, psychologists, and rehabilitation specialists, and on adoption and widespread application of objective methods for evaluating alternative methods. The practical use of BCI technology depends on the development of appropriate applications, identification of appropriate user groups, and careful attention to the needs and desires of individual users. BCI research and development will also benefit from greater emphasis on peer-reviewed publications, and from adoption of standard venues for presentations and discussion.", "title": "Brain-computer interface technology: a review of the first international meeting."}, "0307ef4f7b9b401ffa079b528e6235509307ef5f": {"paper_id": "0307ef4f7b9b401ffa079b528e6235509307ef5f", "abstract": "Driven by the progress in the field of single-trial analysis of EEG, there is a growing interest in brain computer interfaces (BCIs), i.e., systems that enable human subjects to control a computer only by means of their brain signals. In a pseudo-online simulation our BCI detects upcoming finger movements in a natural keyboard typing condition and predicts their laterality. This can be done on average 100\u2013230ms before the respective key is actually pressed, i.e., long before the onset of EMG. Our approach is appealing for its short response time and high classification accuracy (>96%) in a binary decision where no human training is involved. We compare discriminative classifiers like Support Vector Machines (SVMs) and different variants of Fisher Discriminant that possess favorable regularization properties for dealing with high noise cases (inter-trial variablity).", "title": "Classifying Single Trial EEG: Towards Brain Computer Interfacing"}, "2eb2ca05a79d1d81033237aad416ad4a1ce90a70": {"paper_id": "2eb2ca05a79d1d81033237aad416ad4a1ce90a70", "abstract": "This paper provides an introduction to support vector machines, kernel Fisher discriminant analysis, and kernel principal component analysis, as examples for successful kernel-based learning methods. We first give a short background about Vapnik-Chervonenkis theory and kernel feature spaces and then proceed to kernel based learning in supervised and unsupervised scenarios including practical and algorithmic considerations. We illustrate the usefulness of kernel algorithms by discussing applications such as optical character recognition and DNA analysis.", "title": "An introduction to kernel-based learning algorithms"}, "66764732b5f1e002ad3dc2352433a75f362a4afd": {"paper_id": "66764732b5f1e002ad3dc2352433a75f362a4afd", "abstract": "Recent studies have provided long-sought evidence that behavioural learning involves specific synapse gain and elimination processes, which lead to memory traces that influence behaviour. The connectivity rearrangements are preceded by enhanced synapse turnover, which can be modulated through changes in inhibitory connectivity. Behaviourally related synapse rearrangement events tend to co-occur spatially within short stretches of dendrites, and involve signalling pathways partially overlapping with those controlling the functional plasticity of synapses. The new findings suggest that a mechanistic understanding of learning and memory processes will require monitoring ensembles of synapses in situ and the development of synaptic network models that combine changes in synaptic function and connectivity.", "title": "Structural plasticity upon learning: regulation and functions"}, "27794bc96c696004ae64be093f259a0698298f91": {"paper_id": "27794bc96c696004ae64be093f259a0698298f91", "abstract": "As the advancement of technology, online shopping channel develops rapidly in recent years. According to the report of Taiwan Network Information Center, there are almost eighty percents of internet population shopping in online channel. Synthesizing insights from the previous research, this study develops the conceptual model to integrate Theory of Perceived Risk (TPR) and Technology Acceptance Model (TAM) to apply in online shopping. Using data collected from 637 respondents from online survey website, we use structural equation modeling to test measurement and structural models. The results suggest the need for consideration of perceived risk as an antecedent in the Technology Acceptance Model. The limitations and implications are discussed. Keywords\u2014perceived risk, perceived usefulness, perceived ease of use, behavioral intention, actual purchase behavior", "title": "Applying Theory of Perceived Risk and Technology Acceptance Model in the Online Shopping Channel A"}, "4ec1e0fa79f8377c2b16d9c1e00a6a3d16021126": {"paper_id": "4ec1e0fa79f8377c2b16d9c1e00a6a3d16021126", "abstract": "An e-vendor\u2019s website inseparably embodies an interaction with the vendor and an interaction with the IT website interface. Accordingly, research has shown two sets of unrelated usage antecedents by customers: 1) customer trust in the e-vendor and 2) customer assessments of the IT itself, specifically the perceived usefulness and perceived ease-of-use of the website as depicted in the technology acceptance model (TAM). Research suggests, however, that the degree and impact of trust, perceived usefulness, and perceived ease of use change with experience. Using existing, validated scales, this study describes a free-simulation experiment that compares the degree and relative importance of customer trust in an e-vendor vis-\u00e0-vis TAM constructs of the website, between potential (i.e., new) customers and repeat (i.e., experienced) ones. The study found that repeat customers trusted the e-vendor more, perceived the website to be more useful and easier to use, and were more inclined to purchase from it. The data also show that while repeat customers\u2019 purchase intentions were influenced by both their trust in the e-vendor and their perception that the website was useful, potential customers were not influenced by perceived usefulness, but only by their trust in the e-vendor. Implications of this apparent trust-barrier and guidelines for practice are discussed.", "title": "Inexperience and experience with online stores: the importance of TAM and trust"}, "ba0644aa7569f33194090ade9f8f91fa51968b18": {"paper_id": "ba0644aa7569f33194090ade9f8f91fa51968b18", "abstract": "Computer systems cannot improve organizational performance if they aren't used. Unfortunately, resistance to end-user systems by managers and professionals is a widespread problem. To better predict, explain, and increase user acceptance, we need to better understand why people accept or reject computers. This research addresses the ability to predict peoples' computer acceptance from a measure oftheir intentions, and the ability to explain their intentions in terms oftheir attitudes, subjective norms, perceived usefulness, perceived ease of use, and related variables. In a longitudinal study of 107 users, intentions to use a specific system, measured after a onehour introduction to the system, were correlated 0.35 with system use 14 weeks later. The intentionusage correlation was 0.63 at the end of this time period. Perceived usefulness strongly influenced peoples' intentions, explaining more than half of the variance in intentions at the end of 14 weeks. Perceived ease of use had a small but significant effect on intentions as well, although this effect subsided over time. Attitudes only partially mediated the effects of these beliefs on intentions. Subjective norms had no effect on intentions. These results suggest the possibility of simple but powerful models ofthe determinants of user acceptance, with practical value for evaluating systems and guiding managerial interventions aimed at reducing the problem of underutilized computer technology. (INFORMATION TECHNOLOGY; USER ACCEPTANCE; INTENTION MODELS)", "title": "USER ACCEPTANCE OF COMPUTER TECHNOLOGY : A COMPARISON OF TWO THEORETICAL MODELS *"}, "65cf764e5ecfc9662d4fe3f90b34acb1b91f4c5c": {"paper_id": "65cf764e5ecfc9662d4fe3f90b34acb1b91f4c5c", "abstract": null, "title": "Enticing online consumers: an extended technology acceptance perspective"}, "c0a965c8531f55be4ad5654bed4dd5e7df519be9": {"paper_id": "c0a965c8531f55be4ad5654bed4dd5e7df519be9", "abstract": "This paper critically reviews measures of user information satisfaction and selects one for replication and extension. A survey of production managers is used to provide additional support for the instrument, eliminate scales that are psychometrically unsound, and develop a standard short form for use when only an overall assessment of information satisfaction is required and survey time is limited.", "title": "The Measurement of User Information Satisfaction"}, "38166fcb7d6383265971868eed974fd0ee4b2992": {"paper_id": "38166fcb7d6383265971868eed974fd0ee4b2992", "abstract": "This study proposed and tested a model of consumer online buying behavior. The model posits that consumer online buying behavior is affected by demographics, channel knowledge, perceived channel utilities, and shopping orientations. Data were collected by a research company using an online survey of 999 U.S. Internet users, and were cross-validated with other similar national surveys before being used to test the model. Findings of the study indicated that education, convenience orientation, P\u00e1gina 1 de 20 Psychographics of the Consumers in Electronic Commerce 11/10/01 http://www.ascusc.org/jcmc/vol5/issue2/hairong.html experience orientation, channel knowledge, perceived distribution utility, and perceived accessibility are robust predictors of online buying status (frequent online buyer, occasional online buyer, or non-online buyer) of Internet users. Implications of the findings and directions for future research were discussed.", "title": "The Impact of Perceived Channel Utilities, Shopping Orientations, and Demographics on the Consumer's Online Buying Behavior"}, "1f2b0942d24762c0c6eb1e7d37baabc6118b8ed0": {"paper_id": "1f2b0942d24762c0c6eb1e7d37baabc6118b8ed0", "abstract": "Evidence exists that service quality delivery through Web sites is an essential strategy to success, possibly more important than low price and Web presence. To deliver superior service quality, managers of companies with Web presences must first understand how customers perceive and evaluate online customer service. Information on this topic is beginning to emerge from both academic and practitioner sources, but this information has not yet been examined as a whole. The goals of this article are to review and synthesize the literature about service quality delivery through Web sites, describe what is known about the topic, and develop an agenda for needed research.", "title": "Service quality delivery through Web sites : A critical review of extant know"}, "843927d55bf91abc21961a1f9435e31ec3e92326": {"paper_id": "843927d55bf91abc21961a1f9435e31ec3e92326", "abstract": "Trust is a key enabler of cooperative human actions. Three main deficiencies about our current knowledge of trust are addressed by this paper. First, due to widely divergent conceptual definitions of trust, the literature on trust is in a state of construct confusion. Second, too little is understood about how trust forms and on what trust is based. Third, little has been discussed about the role of emotion in trust formation. To address the first deficiency, this paper develops a typology of trust. The rest of the paper addresses the second and third deficiencies by proposing a model of how trust is initially formed, including the role of emotion. Dispositional, interpersonal, and impersonal (system) trust are integrated in the model. The paper also clarifies the cognitive and emotional bases on which interpersonal trust is formed in early relationships. The implications of the model are drawn for future research.", "title": "TRUST FORMATION IN NEW ORGANIZATIONAL RELATIONSHIPS"}, "222717d12ef311906161096bc5e5e325f0bd5fe5": {"paper_id": "222717d12ef311906161096bc5e5e325f0bd5fe5", "abstract": "The present research develops and tests a theoretical extension of the Technology Acceptance Model (TAM) that explains perceived usefulness and usage intentions in terms of social influence and cognitive instrumental processes. The extended model, referred to as TAM2, was tested using longitudinal data collected regarding four different systems at four organizations (N 156), two involving voluntary usage and two involving mandatory usage. Model constructs were measured at three points in time at each organization: preimplementation, one month postimplementation, and three months postimplementation. The extended model was strongly supported for all four organizations at all three points of measurement, accounting for 40%\u201360% of the variance in usefulness perceptions and 34%\u201352% of the variance in usage intentions. Both social influence processes (subjective norm, voluntariness, and image) and cognitive instrumental processes (job relevance, output quality, result demonstrability, and perceived ease of use) significantly influenced user acceptance. These findings advance theory and contribute to the foundation for future research aimed at improving our understanding of user adoption behavior. (Adoption of Information Technology; Technology Acceptance Model; Social Influence; Perceived Usefulness)", "title": "A Theoretical Extension of the Technology Acceptance Model : Four Longitudinal Field Studies"}, "257e30fba2d8e159f73d362ee4ba3c38216ff715": {"paper_id": "257e30fba2d8e159f73d362ee4ba3c38216ff715", "abstract": "Ease of use and usefulness are believed to be fundamental in determining the acceptance and use of various, corporate ITs. These beliefs, however, may not explain the user's behavior toward newly emerging ITs, such as the World-Wide-Web (WWW). In this study, we introduce playfulness as a new factor that re \u0304ects the user's intrinsic belief in WWW acceptance. Using it as an intrinsic motivation factor, we extend and empirically validate the Technology Acceptance Model (TAM) for the WWW context. # 2001 Elsevier Science B.V. All rights reserved.", "title": "Extending the TAM for a World-Wide-Web context"}, "a3c3c084d4c30cf40e134314a5dcaf66b4019171": {"paper_id": "a3c3c084d4c30cf40e134314a5dcaf66b4019171", "abstract": null, "title": "Predicting User Intentions: Comparing the Technology Acceptance Model with the Theory of Planned Behavior"}, "0b990a9c6000b80dc00b69b68f6091844b898215": {"paper_id": "0b990a9c6000b80dc00b69b68f6091844b898215", "abstract": "This paper addresses the role of marketing in hypermedia computer-mediated environments (CMEs). Our approach considers hypermedia CMEs to be large-scale (i.e. national or global) networked environments, of which the World Wide Web on the Internet is the first and current global implementation. We introduce marketers to this revolutionary new medium, and propose two structural models of consumer behavior in a CME. Then we examine the set of consequent testable research propositions and marketing implications that flow from the models. Marketing in Hypermedia Computer-Mediated Environments: Conceptual Foundations 1) Introduction Firms communicate with their customers through various media. Traditionally, these media follow a passive one-to-many communication model whereby a firm reaches many current and potential customers, segmented or not, through marketing efforts that allow only limited forms of feedback on the part of the customer. For several years now, a revolution has been developing that is dramatically altering this traditional view of advertising and communication media. This revolution is the Internet, the massive global network of interconnected packet-switched computer networks, and as a new marketing medium, has the potential to radically change the way firms do business with their customers. The Internet operationalizes a model of distributed computing that facilitates interactive multimedia many-to-many communication. As such, the Internet supports discussion groups (e.g. USENET news and moderated and unmoderated mailing lists), multi-player games and communications systems (e.g. MUDs, irc, chat, MUSEs), file transfer, electronic mail, and global information access and retrieval systems (e.g. archie, gopher, and the World Wide Web). The business implications of this model \"[where] the engine of democratization sitting on so many desktops is already out of control, is already creating new players in a new game\" (Carroll 1994), will be played out in as yet unknown ways for years to come. This paper is concerned with the marketing implications of commercializing hypermedia computer-mediated environments (CMEs), of which the World Wide Web (Berners-Lee et. al. 1992, 1993) on the Internet is the first and current networked global implementation. While we provide a formal definition subsequently, at this point we informally define a hypermedia CME as a distributed computer network used to access and provide hypermedia content (i.e., multimedia content connected across the network with hypertext links). Though other CMEs are relevant to marketers, including private bulletin board systems (Bunch 1994); public conferencing systems such as the WELL (Figallo 1993; Rheingold 1992, 1993) and ECHO; and commercial online services such as America On-Line, Prodigy, and CompuServe, we restrict our current focus to marketing activities in hypermedia CMEs accessible via the \"Web\" on the Internet. The Internet is an important focus for marketers because consumers and firms are conducting business on the Internet in proportions that dwarf the commercial provider base of the other CMEs combined. There are over 21,700 commercial Internet addressess (Verity and Hof 1994), and an increasing percentage of these commercial addresses are providing Web services. As of December 28, 1994, 1465 firms were listed in Open Market\u2019s (1994) directory of \"Commercial Services on the Net,\" and there were 6370 entries in the \"Business/Corporations\" directory of the Yahoo Guide to WWW (Filo and Yang 1994). The central thesis driving this research is that hypermedia CMEs, such as but not limited to the World Wide Web on the Internet, require the development and application of new marketing concepts and models. This is because hypermedia CMEs possess unique characteristics, including machine-interactivity, telepresence, hypermedia, and network navigation, which distinguish them from traditional media and some interactive multimedia, on which conventional concepts and models are based. Hoffman & Novak (1995), \"Marketing in Hypermedia CMEs: Conceptual Foundations\" page 1", "title": "Marketing in hypermedia computer-mediated environment: Conceptual foundations"}, "21b4f1e91ea6056be14761ce46be587d5ae2c7ad": {"paper_id": "21b4f1e91ea6056be14761ce46be587d5ae2c7ad", "abstract": "\u017d . The technology acceptance model TAM proposes that ease of use and usefulness predict applications usage. The current research investigated TAM for work-related tasks with the World Wide Web as the application. One hundred and sixty-three subjects responded to an e-mail survey about a Web site they access often in their jobs. The results support TAM. They also \u017d . \u017d . demonstrate that 1 ease of understanding and ease of finding predict ease of use, and that 2 information quality predicts usefulness for revisited sites. In effect, the investigation applies TAM to help Web researchers, developers, and managers understand antecedents to users\u2019 decisions to revisit sites relevant to their jobs. q 2000 Elsevier Science B.V. All rights reserved.", "title": "The technology acceptance model and the World Wide Web"}, "d06fef6e6e7862c22a393f1801f9fdea392ad2c1": {"paper_id": "d06fef6e6e7862c22a393f1801f9fdea392ad2c1", "abstract": "Motivations to engage in retail shopping include both utilitarian and hedonic dimensions. Business to consumer e-commerce conducted via the mechanism of web-shopping provides an expanded opportunity for companies to create a cognitively and esthetically rich shopping environment in ways not readily imitable in the nonelectronic shopping world. In this article an attitudinal model is developed and empirically tested integrating constructs from technology acceptance research and constructs derived from models of web behavior. Results of two studies from two distinct categories of the interactive shopping environment support the differential importance of immersive, hedonic aspects of the new media as well as the more traditional utilitarian motivations. In addition, navigation, convenience, and the substitutability of the electronic environment to personally examining products were found to be important predictors of online shopping attitudes. Results are discussed in terms of insights for the creation of the online shopping webmosphere through more effective design of interactive retail shopping environments. \u00a9 2001 by New York University. All rights reserved.", "title": "Hedonic and utilitarian motivations for online retail shopping behavior"}, "578261ab399f7602c54f20b917d4046560f0d425": {"paper_id": "578261ab399f7602c54f20b917d4046560f0d425", "abstract": "The technology acceptance model (Davis 1989) is one of the most widely used models of IT adoption. According to TAM, IT adoption is influenced by two perceptions: usefulness and ease-of-use. Research has shown that perceived usefulness (PU) affects intended adoption of IT, but has mostly failed to do so regarding perceived ease of use (PEOU). The basic proposition of this study is that this varying importance of PEOU may be related to the nature of the task. PEOU relates to assessments of the intrinsic characteristics of IT, such as the ease of use, ease of learning, flexibility, and clarity of its interface. PU, on the other hand, is a response to user assessment of its extrinsic, i.e., task-oriented, outcomes: how IT", "title": "The Relative Importance of Perceived Ease of Use in IS Adoption: A Study of E-Commerce Adoption"}, "dda72e7b2446b592900d9c974749410d7605bf72": {"paper_id": "dda72e7b2446b592900d9c974749410d7605bf72", "abstract": "A key issue facing information systems researchers and practitioners has been the difficulty in creating favorable user reactions to new technologies. Insufficient or ineffective training has been identified as one of the key factors underlying this disappointing reality. Among the various enhancements to training being examined in research, the role of intrinsic motivation as a lever to create favorable user perceptions has not been sufficiently exploited. In this research, two studies were conducted to compare a traditional training method with a training method that included a component aimed at enhancing intrinsic motivation. The results strongly favored the use of an intrinsic motivator during training. Key implications for theory and practice are discussed. 1Allen Lee was the accepting senior editor for this paper. Sometimes when I am at my computer, I say to my wife, \"1'11 be done in just a minute\" and the next thing I know she's standing over me saying, \"It's been an hour!\" (Collins 1989, p. 11). Investment in emerging information technology applications can lead to productivity gains only if they are accepted and used. Several theoretical perspectives have emphasized the importance of user perceptions of ease of use as a key factor affecting acceptance of information technology. Favorable ease of use perceptions are necessary for initial acceptance (Davis et al. 1989), which of course is essential for adoption and continued use. During the early stages of learning and use, ease of use perceptions are significantly affected by training (e.g., Venkatesh and Davis 1996). Investments in training by organizations have been very high and have continued to grow rapidly. Kelly (1982) reported a figure of $100B, which doubled in about a decade (McKenna 1990). In spite of such large investments in training , only 10% of training leads to a change in behavior On trainees' jobs (Georgenson 1982). Therefore, it is important to understand the most effective training methods (e.g., Facteau et al. 1995) and to improve existing training methods in order to foster favorable perceptions among users about the ease of use of a technology, which in turn should lead to acceptance and usage. Prior research in psychology (e.g., Deci 1975) suggests that intrinsic motivation during training leads to beneficial outcomes. However, traditional training methods in information systems research have tended to emphasize imparting knowledge to potential users (e.g., Nelson and Cheney 1987) while not paying Sufficient attention to intrinsic motivation during training. The two field \u2026", "title": "Creation of Favorable User Perceptions: Exploring the Role of Intrinsic Motivation"}, "dc6bb165980d99bd7788be0d1089810362d38c19": {"paper_id": "dc6bb165980d99bd7788be0d1089810362d38c19", "abstract": "Sirkka L. Jarvenpaa a, Noam Tractinsky b and Michael Vitale c a Department of Management Science and Information Systems, CBA 5.202, B6500, University of Texas at Austin, Austin, TX 78712-1175, USA E-mail: sjarvenpaa@mail.utexas.edu b Industrial Engineering and Management, Ben-Gurion University, Beer Sheva 84105, Israel E-mail: noamt@bgumail.bgu.ac.il c Melbourne Business School, The University of Melbourne, Carlton 3053, Victoria, Australia E-mail: m.vitale@mbs.unimelb.edu.au", "title": "Consumer trust in an Internet store"}, "fc657010a6185e3f4fc3ba682286227138132cfb": {"paper_id": "fc657010a6185e3f4fc3ba682286227138132cfb", "abstract": "This paper discusses requirements and strategies for the next generation of IMA modules in line with the goals of the European Research project SCARLETT. Even though the SCARLETT project starts from concepts and technology which is already available in modern aircrafts, the project's vision is to push the state of the art far beyond what is available today, with substantial consequences for the aerospace industry. SYSGO is one of the key members of the SCARLETT project and is responsible for the evaluation and specification of the platform software requirements, including multi-core support. With the operating system PikeOS, a field-proven implementation of the concept of SSV (Safe and Secure Virtualization) technology, SYSGO provides the foundation for the realization of various components within these projects, ranging from Core Processing Modules, Remote Data Concentrators and Remote Electronics as well as Simulation Environments. This paper provides a short overview of the project's framework, the key drivers, the goals and the anticipated innovations. It presents the new architecture for electronic device integration proposed by the SCARLETT project together with the resulting requirements for the platform software. In addition to SCARLETT project overview, this paper discusses specific software aspects resulting from the project and how they can be addressed with a safe and secure virtualization technology of a modern operating system.", "title": "Preparing the next generation of IMA: A new technology for the scarlett program"}, "5bcd50fb1bac64a4d0b351ef2c72c9cc25f432f3": {"paper_id": "5bcd50fb1bac64a4d0b351ef2c72c9cc25f432f3", "abstract": "The usage of Internet is getting widespread, and the service of online video is getting more and more popular. The revenue of the web service providers comes mostly from the advertisements. This study investigates the attitudes toward the advertisements while watching online videos in YouTube. We followed the research of users' attitudes toward advertisements (Brackett & Carr, 2001) and combined it with the theory of reasoned action and the flow theory in the psychology. This study investigates the factor affecting attitudes toward advertisements and the influence to behaviors. Our findings show that the model explained most of the variance of attitudes toward advertisements in sites providing services of online videos indicating that the model is confirmed in the situation of online video advertising. The conclusion and managerial implications have further discussions.", "title": "Consumer attitudes toward online video advertising: An empirical study on YouTube as platform"}, "1d145b63fd065c562ed2fecb3f34643fc9653b60": {"paper_id": "1d145b63fd065c562ed2fecb3f34643fc9653b60", "abstract": "The rapid growth of investment in information technology (IT) by organizations worldwide has made user acceptance an increasingly critical technology implementation a d management issue. While such acceptance has received fairly extensive attention from previous research, additional efforts are needed to examine or validate existing research results, particularly those involving different technologies, user populations, and/or organizational contexts. In response, this paper reports a research work that examined the applicability of the Technology Acceptance Model (\u03a4\u0391\u039c) in explaining physicians' decisions to accept telemedicine technology in the health-care context. The technology, the user group, and the organizational context are all new to IT acceptance/adoption research. The study also addressed a pragmatic technology management need resulting from millions of dollars invested by healthcare organizations in developing and implementing telemedicine programs in recent years. The model's overall fit, explanatory power, and the individual causal links that it postulates were evaluated by examining the acceptance of telemedicine technology among physicians practicing at public tertiary hospitals in Hong Kong. Our results suggested that \u03a4\u0391\u039c was able to provide a reasonable depiction of physicians' intention to use telemedicine technology. Perceived usefulness was found to be a significant determinant ofattitude and intention but perceived ease of use was not. The relatively low R-square of the model suggests both the limitations of the parsimonious model and the need for incorporating additional factors or integrating with other IT acceptance models in order to improve its specificity and explanatory utility in a health-care context. Based on the study findings, implications for user technology acceptance research and telemedicine management are discussed.", "title": "Examining the Technology Acceptance Model Using Physician Acceptance of Telemedicine Technology"}, "3ef1a603cc99ce85f814992e72f53b01bff5683c": {"paper_id": "3ef1a603cc99ce85f814992e72f53b01bff5683c", "abstract": "Ground reaction force (GRF) measurement is important in the analysis of human body movements. The main drawback of the existing measurement systems is the restriction to a laboratory environment. This study proposes an ambulatory system for assessing the dynamics of ankle and foot, which integrates the measurement of the GRF with the measurement of human body movement. The GRF and the center of pressure (CoP) are measured using two 6D force/moment sensors mounted beneath the shoe. The movement of the foot and the lower leg is measured using three miniature inertial sensors, two rigidly attached to the shoe and one to the lower leg. The proposed system is validated using a force plate and an optical position measurement system as a reference. The results show good correspondence between both measurement systems, except for the ankle power. The root mean square (rms) difference of the magnitude of the GRF over 10 evaluated trials was 0.012 \u00b1 0.001 N/N (mean \u00b1 standard deviation), being 1.1 \u00b1 0.1 % of the maximal GRF magnitude. It should be noted that the forces, moments, and powers are normalized with respect to body weight. The CoP estimation using both methods shows good correspondence, as indicated by the rms difference of 5.1\u00b1 0.7 mm, corresponding to 1.7 \u00b1 0.3 % of the length of the shoe. The rms difference between the magnitudes of the heel position estimates was calculated as 18 \u00b1 6 mm, being 1.4 \u00b1 0.5 % of the maximal magnitude. The ankle moment rms difference was 0.004 \u00b1 0.001 Nm/N, being 2.3 \u00b1 0.5 % of the maximal magnitude. Finally, the rms difference of the estimated power at the ankle was 0.02 \u00b1 0.005 W/N, being 14 \u00b1 5 % of the maximal power. This power difference is caused by an inaccurate estimation of the angular velocities using the optical reference measurement system, which is due to considering the foot as a single segment. The ambulatory system considers separate heel and forefoot segments, thus allowing an additional foot moment and power to be estimated. Based on the results of this research, it is concluded that the combination of the instrumented shoe and inertial sensing is a promising tool for the assessment of the dynamics of foot and ankle in an ambulatory setting.", "title": "Ambulatory assessment of human body kinematics and kinetics"}, "0408046175e478299888def19ab29dff80c3d45a": {"paper_id": "0408046175e478299888def19ab29dff80c3d45a", "abstract": "Purpose \u2013 This paper\u2019s aim is to examine the influence of perceived cost of sharing knowledge and affective trust in colleagues on the relationship between affective commitment and knowledge sharing. Design/methodology/approach \u2013 The methodology used was a survey of 496 employees from 15 organizations across ten industries. Findings \u2013 Affective trust in colleagues moderates the relationship between affective commitment and knowledge sharing and the relationship between cost of knowledge sharing and knowledge sharing. Research limitations/implications \u2013 Future researchers should operationalize the perceived cost of knowledge sharing construct to include other potential group barriers; for instance, politics and organizational barriers, management commitment and lack of trust. Practical implications \u2013 The findings of this study suggest that employees who value social relationships and social resources tend to view knowledge as a collectively owned commodity. As such, their knowledge sharing behavior reflects the model of reciprocal social exchanges. Social implications \u2013 The results of this study indicate that an organizational culture that encourages affect-based trust between colleagues will facilitate knowledge sharing. Originality/value \u2013 The paper bridges the gap between the literature on knowledge sharing, perceived cost of knowledge sharing, affective organizational commitment and trust in a single model.", "title": "Knowledge Sharing: Influences of Trust, Commitment and Cost"}, "6a3192bc7321a242f1d37359234b86cde1ea57f2": {"paper_id": "6a3192bc7321a242f1d37359234b86cde1ea57f2", "abstract": "The two goals of surgery for lower rectal cancer surgery are to obtain clear \"curative\" margins and to limit post-surgical functional disorders. The question of whether or not to preserve the anal sphincter lies at the center of the therapeutic choice. Histologically, tumor-free distal and circumferential margins of>1mm allow a favorable oncologic outcome. Whether such margins can be obtained depends of TNM staging, tumor location, response to chemoradiotherapy and type of surgical procedure. The technique of intersphincteric resection relies on these narrow margins to spare the sphincter. This procedure provides satisfactory oncologic outcome with a rate of circumferential margin involvement ranging from 5% to 11%, while good continence is maintained in half of the patients. The extralevator abdominoperineal resection provides good oncologic results, however this procedure requires a permanent colostomy. A permanent colostomy alters several domains of quality of life when located at the classical abdominal site but not when brought out at the perineal site as a perineal colostomy.", "title": "Surgical strategy for low rectal cancers."}, "60b895083412d0c44dd070d038f80f93f41fb25e": {"paper_id": "60b895083412d0c44dd070d038f80f93f41fb25e", "abstract": "Development and optimization of the aperture-coupled stacked microstrip patch antenna is presented, which has dual polarization with great enhancement of bandwidth. Stacked antenna with aperture feeding mechanism results in great isolation between two feeds. Varying several physical parameters of aperture and matching feed line lengths had very large effect on the performance of the antenna. The microstrip antenna has 42% bandwidth from 7.3 to 11.2 GHz with below 25 dB of isolation between two feeds. The gain was average of 7.5 dBi that highest gain was 10 dBi at feed1 for 9 GHz.", "title": "A dual-polarization aperture coupled stacked microstrip patch antenna for wideband application"}, "f3b52d86821455fcfa8170781c9e44d938ec8bc8": {"paper_id": "f3b52d86821455fcfa8170781c9e44d938ec8bc8", "abstract": "This paper describes a system for semi-automatic tr nscription of prosody based on a stylization of the fundamenta l frequency data (contour) for vocalic (or syllabic) nuclei. Th e stylization is a simulation of tonal perception of human listen ers. The system requires a time-aligned phonetic annotation. The transcription has been applied to several speech co rpora.", "title": "The Prosogram: Semi-Automatic Transcription of Prosody Based on a Tonal Perception Model"}, "4046d6b47f21cb9510186da544c7fb48168f4068": {"paper_id": "4046d6b47f21cb9510186da544c7fb48168f4068", "abstract": "Currently, many operators worldwide are deploying Long Term Evolution (LTE) to provide much faster access with lower latency and higher efficiency than its predecessors 3G and 3.5G. Meanwhile, the service rollout of LTE-Advanced, which is an evolution of LTE and a \u201ctrue 4G\u201d mobile broadband, is being underway to further enhance LTE performance. However, the anticipated challenges of the next decade (2020s) are so tremendous and diverse that there is a vastly increased need for a new generation mobile communications system with even further enhanced capabilities and new functionalities, namely a fifth generation (5G) system. Envisioning the development of a 5G system by 2020, at DOCOMO we started studies on future radio access as early as 2010, just after the launch of LTE service. The aim at that time was to anticipate the future user needs and the requirements of 10 years later (2020s) in order to identify the right concept and radio access technologies for the next generation system. The identified 5G concept consists of an efficient integration of existing spectrum bands for current cellular mobile and future new spectrum bands including higher frequency bands, e.g., millimeter wave, with a set of spectrum specific and spectrum agnostic technologies. Since a few years ago, we have been conducting several proof-of-concept activities and investigations on our 5G concept and its key technologies, including the development of a 5G real-time simulator, experimental trials of a wide range of frequency bands and technologies and channel measurements for higher frequency bands. In this paper, we introduce an overview of our views on the requirements, concept and promising technologies for 5G radio access, in addition to our ongoing activities for paving the way toward the realization of 5G by 2020. key words: next generation mobile communications system, 5G, 4G, LTE, LTE-advanced", "title": "5G Radio Access: Requirements, Concept and Experimental Trials"}, "8437b9eb07166ccf3b1c8c32e0783dae768a0b51": {"paper_id": "8437b9eb07166ccf3b1c8c32e0783dae768a0b51", "abstract": "As a promising downlink multiple access scheme for further LTE enhancement and future radio access (FRA), this paper investigates the system-level performance of non-orthogonal multiple access (NOMA) with a successive interference canceller (SIC) on the receiver side. The goal is to clarify the potential gains of NOMA over orthogonal multiple access (OMA) such as OFDMA, taking into account key link adaptation functionalities of the LTE radio interface such as adaptive modulation and coding (AMC), hybrid automatic repeat request (HARQ), time/frequency-domain scheduling, and outer loop link adaptation (OLLA), in addition to NOMA specific functionalities such as dynamic multi-user power allocation. Based on computer simulations, we show under multiple configurations that the system-level performance achieved by NOMA is superior to that for OMA.", "title": "System-level performance evaluation of downlink non-orthogonal multiple access (NOMA)"}, "9482abb8261440ce4d4c235e79eadc94561da2f4": {"paper_id": "9482abb8261440ce4d4c235e79eadc94561da2f4", "abstract": "Rate Control for Communication Networks: Shadow Prices, Proportional Fairness and Stability Author(s): F. P. Kelly, A. K. Maulloo and D. K. H. Tan Source:  The Journal of the Operational Research Society, Vol. 49, No. 3 (Mar., 1998), pp. 237-252 Published by: on behalf of the Palgrave Macmillan Journals Operational Research Society Stable URL: http://www.jstor.org/stable/3010473 Accessed: 07-10-2015 21:10 UTC", "title": "Rate control for communication networks: shadow prices, proportional fairness and stability"}, "378dd81d0716e6ccdd804b6398b162ecc19eb2c9": {"paper_id": "378dd81d0716e6ccdd804b6398b162ecc19eb2c9", "abstract": "As a promising downlink multiple access scheme for future radio access (FRA), this paper discusses the concept and practical considerations of non-orthogonal multiple access (NOMA) with a successive interference canceller (SIC) at the receiver side. The goal is to clarify the benefits of NOMA over orthogonal multiple access (OMA) such as OFDMA adopted by Long-Term Evolution (LTE). Practical considerations of NOMA, such as multi-user power allocation, signalling overhead, SIC error propagation, performance in high mobility scenarios, and combination with multiple input multiple output (MIMO) are discussed. Using computer simulations, we provide system-level performance of NOMA taking into account practical aspects of the cellular system and some of the key parameters and functionalities of the LTE radio interface such as adaptive modulation and coding (AMC) and frequency-domain scheduling. We show under multiple configurations that the system-level performance achieved by NOMA is higher by more than 30% compared to OMA.", "title": "Concept and practical considerations of non-orthogonal multiple access (NOMA) for future radio access"}, "2a82a192eee0ef4842359477b75e7e3ab502aa0f": {"paper_id": "2a82a192eee0ef4842359477b75e7e3ab502aa0f", "abstract": "OBJECTIVE\nSmaller hippocampal volume has been reported only in some but not all studies of unipolar major depressive disorder. Severe stress early in life has also been associated with smaller hippocampal volume and with persistent changes in the hypothalamic-pituitary-adrenal axis. However, prior hippocampal morphometric studies in depressed patients have neither reported nor controlled for a history of early childhood trauma. In this study, the volumes of the hippocampus and of control brain regions were measured in depressed women with and without childhood abuse and in healthy nonabused comparison subjects.\n\n\nMETHOD\nStudy participants were 32 women with current unipolar major depressive disorder-21 with a history of prepubertal physical and/or sexual abuse and 11 without a history of prepubertal abuse-and 14 healthy nonabused female volunteers. The volumes of the whole hippocampus, temporal lobe, and whole brain were measured on coronal MRI scans by a single rater who was blind to the subjects' diagnoses.\n\n\nRESULTS\nThe depressed subjects with childhood abuse had an 18% smaller mean left hippocampal volume than the nonabused depressed subjects and a 15% smaller mean left hippocampal volume than the healthy subjects. Right hippocampal volume was similar across the three groups. The right and left hippocampal volumes in the depressed women without abuse were similar to those in the healthy subjects.\n\n\nCONCLUSIONS\nA smaller hippocampal volume in adult women with major depressive disorder was observed exclusively in those who had a history of severe and prolonged physical and/or sexual abuse in childhood. An unreported history of childhood abuse in depressed subjects could in part explain the inconsistencies in hippocampal volume findings in prior studies in major depressive disorder.", "title": "Childhood trauma associated with smaller hippocampal volume in women with major depression."}, "7086ec31796bc1308e4620145a183eb0cce895b4": {"paper_id": "7086ec31796bc1308e4620145a183eb0cce895b4", "abstract": "Widespread application of IGBTs in the past decade has resulted in dramatic improvement in performance of power electronic converters, with simultaneous reduction in costs. In order to further reduce the cost of power electronics systems and improve their reliability it is imperative that techniques for standardized and integrated architectures be adopted, allowing them to be mass-customized for a larger class of applications. One of the key factors that allow broad application of a power switching devices rated at a lower power level across large power levels is ease of parallel operation. This paper is devoted to presenting the results from investigation of parallel operation of IGBT devices. Experimental and computer simulation results are presented.", "title": "Investigation of parallel operation of IGBTs"}, "53444de423d3abdd7cbc9ad3bfe39c68589f20b1": {"paper_id": "53444de423d3abdd7cbc9ad3bfe39c68589f20b1", "abstract": "Adversarial samples are perturbed inputs crafted to mislead the machine learning systems. A training mechanism, called adversarial training, which presents adversarial samples along with clean samples has been introduced to learn robust models. In order to scale adversarial training for large datasets, these perturbations can only be crafted using fast and simple methods (e.g., gradient ascent). However, it is shown that adversarial training converges to a degenerate minimum, where the model appears to be robust by generating weaker adversaries. As a result, the models are vulnerable to simple black-box attacks. In this paper we, (i) demonstrate the shortcomings of existing evaluation policy, (ii) introduce novel variants of white-box and black-box attacks, dubbed \u201cgray-box adversarial attacks\u201d based on which we propose novel evaluation method to assess the robustness of the learned models, and (iii) propose a novel variant of adversarial training, named \u201cGraybox Adversarial Training\u201d that uses intermediate versions of the models to seed the adversaries. Experimental evaluation demonstrates that the models trained using our method exhibit better robustness compared to both undefended and adversarially trained models.", "title": "Gray-Box Adversarial Training"}, "39a651ace163e7741bc98e266201afe83ad63219": {"paper_id": "39a651ace163e7741bc98e266201afe83ad63219", "abstract": "Machine learning systems offer unparalled flexibility in dealing with evolving input in a variety of applications, such as intrusion detection systems and spam e-mail filtering. However, machine learning algorithms themselves can be a target of attack by a malicious adversary. This paper provides a framework for answering the question, \"Can machine learning be secure?\" Novel contributions of this paper include a taxonomy of different types of attacks on machine learning techniques and systems, a variety of defenses against those attacks, a discussion of ideas that are important to security for machine learning, an analytical model giving a lower bound on attacker's work function, and a list of open problems.", "title": "Can machine learning be secure?"}, "38418928d6d842fe6edadc809f384278d793d610": {"paper_id": "38418928d6d842fe6edadc809f384278d793d610", "abstract": "Deep learning algorithms have been shown to perform extremely well on many classical machine learning problems. However, recent studies have shown that deep learning, like other machine learning techniques, is vulnerable to adversarial samples: inputs crafted to force a deep neural network (DNN) to provide adversary-selected outputs. Such attacks can seriously undermine the security of the system supported by the DNN, sometimes with devastating consequences. For example, autonomous vehicles can be crashed, illicit or illegal content can bypass content filters, or biometric authentication systems can be manipulated to allow improper access. In this work, we introduce a defensive mechanism called defensive distillation to reduce the effectiveness of adversarial samples on DNNs. We analytically investigate the generalizability and robustness properties granted by the use of defensive distillation when training DNNs. We also empirically study the effectiveness of our defense mechanisms on two DNNs placed in adversarial settings. The study shows that defensive distillation can reduce effectiveness of sample creation from 95% to less than 0.5% on a studied DNN. Such dramatic gains can be explained by the fact that distillation leads gradients used in adversarial sample creation to be reduced by a factor of 1030. We also find that distillation increases the average minimum number of features that need to be modified to create adversarial samples by about 800% on one of the DNNs we tested.", "title": "Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks"}, "dc52d1ede1b90bf9d296bc5b34c9310b7eaa99a2": {"paper_id": "dc52d1ede1b90bf9d296bc5b34c9310b7eaa99a2", "abstract": "please note that your browser may uncompress these files without telling you. If the files you downloaded have a larger size than the above, they have been uncompressed by your browser. Simply rename them to remove the .gz extension. Some people have asked me \"my application can't open your image files\". These files are not in any standard image format. You have to write your own (very simple) program to read them. The file format is described at the bottom of this page.", "title": "The mnist database of handwritten digits"}, "b5ec486044c6218dd41b17d8bba502b32a12b91a": {"paper_id": "b5ec486044c6218dd41b17d8bba502b32a12b91a", "abstract": null, "title": "1 Adversarial Perturbations of Deep Neural Networks"}, "20f28af7a5f14c994b5c62315f215d95939de18a": {"paper_id": "20f28af7a5f14c994b5c62315f215d95939de18a", "abstract": "Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake. Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model. Up to now, all previous work have assumed a threat model in which the adversary can feed data directly into the machine learning classifier. This is not always the case for systems operating in the physical world, for example those which are using signals from cameras and other sensors as an input. This paper shows that even in such physical world scenarios, machine learning systems are vulnerable to adversarial examples. We demonstrate this by feeding adversarial images obtained from cell-phone camera to an ImageNet Inception classifier and measuring the classification accuracy of the system. We find that a large fraction of adversarial examples are classified incorrectly even when perceived through the camera.", "title": "Adversarial examples in the physical world"}, "5e4fa9397c18062b970910f8ee168d3297cf098f": {"paper_id": "5e4fa9397c18062b970910f8ee168d3297cf098f", "abstract": "Machine learning is widely used to develop classifiers for security tasks. However, the robustness of these methods against motivated adversaries is uncertain. In this work, we propose a generic method to evaluate the robustness of classifiers under attack. The key idea is to stochastically manipulate a malicious sample to find a variant that preserves the malicious behavior but is classified as benign by the classifier. We present a general approach to search for evasive variants and report on results from experiments using our techniques against two PDF malware classifiers, PDFrate and Hidost. Our method is able to automatically find evasive variants for both classifiers for all of the 500 malicious seeds in our study. Our results suggest a general method for evaluating classifiers used in security applications, and raise serious doubts about the effectiveness of classifiers based on superficial features in the presence of adversaries.", "title": "Automatically Evading Classifiers: A Case Study on PDF Malware Classifiers"}, "2786feab5c644bf0bde98fb6f9d1dbd0b58ca80c": {"paper_id": "2786feab5c644bf0bde98fb6f9d1dbd0b58ca80c", "abstract": "Recently, deep networks were proved to be more effective than shallow architectures to face complex real\u2013world applications. However, theoretical results supporting this claim are still few and incomplete. In this paper, we propose a new topological measure to study how the depth of feedforward networks impacts on their ability of implementing high complexity functions. Upper and lower bounds on network complexity are established, based on the number of hidden units and on their activation functions, showing that deep architectures are able, with the same number of resources, to address more difficult classification problems.", "title": "On the complexity of shallow and deep neural network classifiers"}, "5115f3ff1ac45486a50ad3834a40490f9ca4bcea": {"paper_id": "5115f3ff1ac45486a50ad3834a40490f9ca4bcea", "abstract": "We study the complexity of functions computable by deep feedforward neural networks with piecewise linear activations in terms of the symmetries and the number of linear regions that they have. Deep networks are able to sequentially map portions of each layer\u2019s input-space to the same output. In this way, deep models compute functions that react equally to complicated patterns of different inputs. The compositional structure of these functions enables them to re-use pieces of computation exponentially often in terms of the network\u2019s depth. This paper investigates the complexity of such compositional maps and contributes new theoretical results regarding the advantage of depth for neural networks with piecewise linear activation functions. In particular, our analysis is not specific to a single family of models, and as an example, we employ it for rectifier and maxout networks. We improve complexity bounds from pre-existing work and investigate the behavior of units in higher layers.", "title": "On the Number of Linear Regions of Deep Neural Networks"}, "cd85a549add0c7c7def36aca29837efd24b24080": {"paper_id": "cd85a549add0c7c7def36aca29837efd24b24080", "abstract": "While depth tends to improve network performances, it also m akes gradient-based training more difficult since deeper networks tend to be more non-linear. The recently proposed knowledge distillation approach is aimed a t obtaining small and fast-to-execute models, and it has shown that a student netw ork could imitate the soft output of a larger teacher network or ensemble of networ ks. In this paper, we extend this idea to allow the training of a student that is d eeper and thinner than the teacher, using not only the outputs but also the inte rmediate representations learned by the teacher as hints to improve the traini ng process and final performance of the student. Because the student intermedia te hidden layer will generally be smaller than the teacher\u2019s intermediate hidde n layer, additional parameters are introduced to map the student hidden layer to th e prediction of the teacher hidden layer. This allows one to train deeper studen s that can generalize better or run faster, a trade-off that is controlled by the ch osen student capacity. For example, on CIFAR-10, a deep student network with almost 10.4 times less parameters outperforms a larger, state-of-the-art teache r network.", "title": "FitNets: Hints for Thin Deep Nets"}, "8ecc044d920df247fbd455b752fd7cc0f7363ad7": {"paper_id": "8ecc044d920df247fbd455b752fd7cc0f7363ad7", "abstract": "Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned. Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods su ce for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.", "title": "On the importance of initialization and momentum in deep learning"}, "600a5d60cb96eda2a9849413e747547d70dfb00a": {"paper_id": "600a5d60cb96eda2a9849413e747547d70dfb00a", "abstract": "Inspired by biophysical principles underlying nonlinear dendritic computation in neural circuits, we develop a scheme to train deep neural networks to make them robust to adversarial attacks. Our scheme generates highly nonlinear, saturated neural networks that achieve state of the art performance on gradient based adversarial examples on MNIST, despite never being exposed to adversarially chosen examples during training. Moreover, these networks exhibit unprecedented robustness to targeted, iterative schemes for generating adversarial examples, including second-order methods. We further identify principles governing how these networks achieve their robustness, drawing on methods from information geometry. We find these networks progressively create highly flat and compressed internal representations that are sensitive to very few input dimensions, while still solving the task. Moreover, they employ highly kurtotic weight distributions, also found in the brain, and we demonstrate how such kurtosis can protect even linear classifiers from adversarial attack.", "title": "Biologically inspired protection of deep networks from adversarial attacks"}, "6bb2326c8981a07498555df64416d764f03a30c0": {"paper_id": "6bb2326c8981a07498555df64416d764f03a30c0", "abstract": "Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08% top-5 error on the test set of the ImageNet classification (CLS) challenge.", "title": "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning"}, "68e4e5b7c1b862ba5d0cfe046f2e84c6a8c3c394": {"paper_id": "68e4e5b7c1b862ba5d0cfe046f2e84c6a8c3c394", "abstract": "An intriguing property of deep neural networks is the existence of adversarial examples, which can transfer among different architectures. These transferable adversarial examples may severely hinder deep neural network-based applications. Previous works mostly study the transferability using small scale datasets. In this work, we are the first to conduct an extensive study of the transferability over large models and a large scale dataset, and we are also the first to study the transferability of targeted adversarial examples with their target labels. We study both non-targeted and targeted adversarial examples, and show that while transferable non-targeted adversarial examples are easy to find, targeted adversarial examples generated using existing approaches almost never transfer with their target labels. Therefore, we propose novel ensemble-based approaches to generating transferable adversarial examples. Using such approaches, we observe a large proportion of targeted adversarial examples that are able to transfer with their target labels for the first time. We also present some geometric studies to help understanding the transferable adversarial examples. Finally, we show that the adversarial examples generated using ensemble-based approaches can successfully attack Clarifai.com, which is a black-box image classification system.", "title": "Delving into Transferable Adversarial Examples and Black-box Attacks"}, "b90dd2f366988d9bb76399d4137c1768fe460c8f": {"paper_id": "b90dd2f366988d9bb76399d4137c1768fe460c8f", "abstract": "Owed to their versatile functionality and widespread adoption, PDF documents have become a popular avenue for user exploitation ranging from large-scale phishing attacks to targeted attacks. In this paper, we present a framework for robust detection of malicious documents through machine learning. Our approach is based on features extracted from document metadata and structure. Using real-world datasets, we demonstrate the the adequacy of these document properties for malware detection and the durability of these features across new malware variants. Our analysis shows that the Random Forests classification method, an ensemble classifier that randomly selects features for each individual classification tree, yields the best detection rates, even on previously unseen malware.\n Indeed, using multiple datasets containing an aggregate of over 5,000 unique malicious documents and over 100,000 benign ones, our classification rates remain well above 99% while maintaining low false positives of 0.2% or less for different classification parameters and experimental scenarios. Moreover, the classifier has the ability to detect documents crafted for targeted attacks and separate them from broadly distributed malicious PDF documents. Remarkably, we also discovered that by artificially reducing the influence of the top features in the classifier, we can still achieve a high rate of detection in an adversarial setting where the attacker is aware of both the top features utilized in the classifier and our normality model. Thus, the classifier is resilient against mimicry attacks even with knowledge of the document features, classification method, and training set.", "title": "Malicious PDF detection using metadata and structural features"}, "96296e8a5db32308ae55654b384501ba51369f35": {"paper_id": "96296e8a5db32308ae55654b384501ba51369f35", "abstract": "Malicious PDF files have been used to harm computer security during the past two-three years, and modern antivirus are proving to be not completely effective against this kind of threat. In this paper an innovative technique, which combines a feature extractor module strongly related to the structure of PDF files and an effective classifier, is presented. This system has proven to be more effective than other stateof-the-art research tools for malicious PDF detection, as well as than most of antivirus in commerce. Moreover, its flexibility allows adopting it either as a stand-alone tool or as plug-in to improve the performance of an already installed antivirus.", "title": "A Pattern Recognition System for Malicious PDF Files Detection"}, "5656fa5aa6e1beeb98703fc53ec112ad227c49ca": {"paper_id": "5656fa5aa6e1beeb98703fc53ec112ad227c49ca", "abstract": "We introduce the multi-prediction deep Boltzmann machine (MP-DBM). The MPDBM can be seen as a single probabilistic model trained to maximize a variational approximation to the generalized pseudolikelihood, or as a family of recurrent nets that share parameters and approximately solve different inference problems. Prior methods of training DBMs either do not perform well on classification tasks or require an initial learning pass that trains the DBM greedily, one layer at a time. The MP-DBM does not require greedy layerwise pretraining, and outperforms the standard DBM at classification, classification with missing inputs, and mean field prediction tasks.1", "title": "Multi-Prediction Deep Boltzmann Machines"}, "1780f4fc05d87356e923a75a8ab3ff4ce79b9fb0": {"paper_id": "1780f4fc05d87356e923a75a8ab3ff4ce79b9fb0", "abstract": "Machine learning\u2019s ability to rapidly evolve to changing and complex situations has helped it become a fundamental tool for computer security. That adaptability is also a vulnerability: attackers can exploit machine learning systems. We present a taxonomy identifying and analyzing attacks against machine learning systems. We show how these classes influence the costs for the attacker and defender, and we give a formal structure defining their interaction. We use our framework to survey and analyze the literature of attacks against machine learning systems. We also illustrate our taxonomy by showing how it can guide attacks against SpamBayes, a popular statistical spam filter. Finally, we discuss how our taxonomy suggests new lines of defenses.", "title": "The security of machine learning"}, "7cdf1c29cb63423c9638dd4f5620956b3fe80d11": {"paper_id": "7cdf1c29cb63423c9638dd4f5620956b3fe80d11", "abstract": "We investigate a family of poisoning attacks against Support Vector Machines (SVM). Such attacks inject specially crafted training data that increases the SVM\u2019s test error. Central to the motivation for these attacks is the fact that most learning algorithms assume that their training data comes from a natural or well-behaved distribution. However, this assumption does not generally hold in security-sensitive settings. As we demonstrate, an intelligent adversary can, to some extent, predict the change of the SVM\u2019s decision function due to malicious input and use this ability to construct malicious data. The proposed attack uses a gradient ascent strategy in which the gradient is computed based on properties of the SVM\u2019s optimal solution. This method can be kernelized and enables the attack to be constructed in the input space even for non-linear kernels. We experimentally demonstrate that our gradient ascent procedure reliably identifies good local maxima of the non-convex validation error surface, which significantly increases the classifier\u2019s test error.", "title": "Poisoning Attacks against Support Vector Machines"}, "38502c84f76aaebc436317fb1ec086c66b158d40": {"paper_id": "38502c84f76aaebc436317fb1ec086c66b158d40", "abstract": "Deep neural networks (DNNs) have recently been achieving state-of-the-art performance on a variety of pattern-recognition tasks, most notably visual classification problems. Given that DNNs are now able to classify objects in images with near-human-level performance, questions naturally arise as to what differences remain between computer and human vision. A recent study [30] revealed that changing an image (e.g. of a lion) in a way imperceptible to humans can cause a DNN to label the image as something else entirely (e.g. mislabeling a lion a library). Here we show a related result: it is easy to produce images that are completely unrecognizable to humans, but that state-of-the-art DNNs believe to be recognizable objects with 99.99% confidence (e.g. labeling with certainty that white noise static is a lion). Specifically, we take convolutional neural networks trained to perform well on either the ImageNet or MNIST datasets and then find images with evolutionary algorithms or gradient ascent that DNNs label with high confidence as belonging to each dataset class. It is possible to produce images totally unrecognizable to human eyes that DNNs believe with near certainty are familiar objects, which we call \u201cfooling images\u201d (more generally, fooling examples). Our results shed light on interesting differences between human vision and current DNNs, and raise questions about the generality of DNN computer vision.", "title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images"}, "d43ef06299691af9f1e3acfca863cee8881c2e8a": {"paper_id": "d43ef06299691af9f1e3acfca863cee8881c2e8a", "abstract": "Battista Biggio battista.biggio@diee.unica.it Dept. of Electrical and Electronic Engineering University of Cagliari Piazza d\u2019Armi, 09123, Cagliari, Italy and Blaine Nelson blaine.nelson@wsii.uni-tuebingen.de Dept. of Mathematics and Natural Sciences Eberhard-Karls-Universit\u00e4t T\u00fcbingen Sand 1, 72076, T\u00fcbingen, Germany and Pavel Laskov pavel.laskov@uni-tuebingen.de Dept. of Mathematics and Natural Sciences Eberhard-Karls-Universit\u00e4t T\u00fcbingen Sand 1, 72076, T\u00fcbingen, Germany", "title": "Support Vector Machines Under Adversarial Label Noise"}, "c85d46a94768bdcf7ffcb844b47c5b8e8e8234a3": {"paper_id": "c85d46a94768bdcf7ffcb844b47c5b8e8e8234a3", "abstract": "Long Short-Term Memory (LSTM) is a specific recurrent neural network (RNN) architecture that was designed to model temporal sequences and their long-range dependencies more accurately than conventional RNNs. In this paper, we explore LSTM RNN architectures for large scale acoustic modeling in speech recognition. We recently showed that LSTM RNNs are more effective than DNNs and conventional RNNs for acoustic modeling, considering moderately-sized models trained on a single machine. Here, we introduce the first distributed training of LSTM RNNs using asynchronous stochastic gradient descent optimization on a large cluster of machines. We show that a two-layer deep LSTM RNN where each LSTM layer has a linear recurrent projection layer can exceed state-of-the-art speech recognition performance. This architecture makes more effective use of model parameters than the others considered, converges quickly, and outperforms a deep feed forward neural network having an order of magnitude more parameters.", "title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling"}, "a665ca4946191a7b8b5cfb38b07c1a1da2bf99c4": {"paper_id": "a665ca4946191a7b8b5cfb38b07c1a1da2bf99c4", "abstract": "Pattern classification systems are commonly used in adversarial applications, like biometric authentication, network intrusion detection, and spam filtering, in which data can be purposely manipulated by humans to undermine their operation. As this adversarial scenario is not taken into account by classical design methods, pattern classification systems may exhibit vulnerabilities,whose exploitation may severely affect their performance, and consequently limit their practical utility. Extending pattern classification theory and design methods to adversarial settings is thus a novel and very relevant research direction, which has not yet been pursued in a systematic way. In this paper, we address one of the main open issues: evaluating at design phase the security of pattern classifiers, namely, the performance degradation under potential attacks they may incur during operation. We propose a framework for empirical evaluation of classifier security that formalizes and generalizes the main ideas proposed in the literature, and give examples of its use in three real applications. Reported results show that security evaluation can provide a more complete understanding of the classifier\u2019s behavior in adversarial environments, and lead to better design choices micans infotech +91 90036 28940 +91 94435 11725 MICANS INFOTECH, NO: 8 , 100 FEET ROAD,PONDICHERRY. WWW.MICANSINFOTECH.COM ; MICANSINFOTECH@GMAIL.COM +91 90036 28940; +91 94435 11725 IEEE Projects 100% WORKING CODE + DOCUMENTATION+ EXPLAINATION \u2013 BEST PRICE LOW PRICE GUARANTEED", "title": "Security Evaluation of Pattern Classifiers under Attack"}, "b2ecb64b13f1eb2ecb24a1217fea477474de4c30": {"paper_id": "b2ecb64b13f1eb2ecb24a1217fea477474de4c30", "abstract": "Drying and dewatering plays a major role in food manufacturing or food processing activities worldwide. Often one of the last operations in the food processing, it controls to a large extent the quality of the final product. Drying is applied to a wide variety of food products, from cereals to finished goods, from raw materials to byproducts. The processes used are numerous, according to the type and quantity of product to dry, the amount of water to eliminate, the final desired quality or functionality of the dried product (Tab. 1.1). Drying and dewatering impact the mechanical, sensory and nutritional properties of food products, and can be used to create new functionalities (Bonazzi and Bimbenet, 2003, 2008). Drying is one of the main techniques for preserving agricultural and food products; it takes place in the processing of many products, as the main operation or as a consequence of other processing steps. Heat and mass transfer phenomena which are typical of drying also appear during other processes, as in cooking, baking, roasting, smoking, refrigeration, freezing, during storage, and during pneumatic transportation. The main objective of drying is to decrease the water activity (aw) of various perishable materials to values <0.5, in order to enable their storage at ambient temperature.The importanceofaw incontrolling theshelf-lifeof foodsbysuppressing the growth of micro-organisms, by reducing the rates of chemical reactions, and by inhibiting enzymatic deterioration is well established. The respective relationships have been summarized by Labuza et al. (1970) in a diagram similar to that of Fig. 1.1. Water activity is more important to the stability of a food than the total amount of water present, and it makes it possible to develop generalized rules or limits for the stability of foods. For most foods, the critical point below which no micro-organism can grow is in the 0.6\u20130.7 water activity range. A food product is most stable at its monolayer moisture content, which varies with the chemical composition and structure. Water activity is also useful for predicting the final moisture content at equilibrium versus drying conditions (temperature and relative humidity), and for", "title": "1 Quality Changes in Food Materials as Influenced by Drying Processes"}, "a7fb667a605640404ea903e33694a1d4f3cd876a": {"paper_id": "a7fb667a605640404ea903e33694a1d4f3cd876a", "abstract": "An additional mechanical mechanism for a passive parallelogram-based exoskeleton arm-support is presented. It consists of several levers and joints and an attached extension coil spring. The additional mechanism has two favourable features. On the one hand it exhibits an almost iso-elastic behaviour whereby the lifting force of the mechanism is constant for a wide working range. Secondly, the value of the supporting force can be varied by a simple linear movement of a supporting joint. Furthermore a standard tension spring can be used to gain the desired behavior. The additional mechanism is a 4-link mechanism affixed to one end of the spring within the parallelogram arm-support. It has several geometrical parameters which influence the overall behaviour. A standard optimisation routine with constraints on the parameters is used to find an optimal set of geometrical parameters. Based on the optimized geometrical parameters a prototype was constructed and tested. It is a lightweight wearable system, with a weight of 1.9 kg. Detailed experiments reveal a difference between measured and calculated forces. These variations can be explained by a 60 % higher pre load force of the tension spring and a geometrical offset in the construction.", "title": "Design of a passive , iso-elastic upper limb exoskeleton for gravity compensation"}, "978695060439845d37cbaba68e5db0bd6deb93c7": {"paper_id": "978695060439845d37cbaba68e5db0bd6deb93c7", "abstract": "The implementation of a monitoring and control system for the induction motor based on programmable logic controller (PLC) technology is described. Also, the implementation of the hardware and software for speed control and protection with the results obtained from tests on induction motor performance is provided. The PLC correlates the operational parameters to the speed requested by the user and monitors the system during normal operation and under trip conditions. Tests of the induction motor system driven by inverter and controlled by PLC prove a higher accuracy in speed regulation as compared to a conventional V/f control system. The efficiency of PLC control is increased at high speeds up to 95% of the synchronous speed. Thus, PLC proves themselves as a very versatile and effective tool in industrial control of electric drives.", "title": "Design and implementation of PLC-based monitoring control system for induction motor"}, "382903a745ae23518c1e98899e00bafd2c4ac076": {"paper_id": "382903a745ae23518c1e98899e00bafd2c4ac076", "abstract": "We describe a pre-existing rule-based homograph disambiguation system used for text-to-speech synthesis at Google, and compare it to a novel system which performs disambiguation using classifiers trained on a small amount of labeled data. An evaluation of these systems, using a new, freely available English data set, finds that hybrid systems (making use of both rules and machine learning) are significantly more accurate than either hand-written rules or machine learning alone. The evaluation also finds minimal performance degradation when the hybrid system is configured to run on limited-resource mobile devices rather than on production servers. The two best systems described here are used for homograph disambiguation on all US English text-to-speech traffic at Google.", "title": "Improving homograph disambiguation with supervised machine learning"}, "5211c32fb5849a14855a91a7ab16cfb83483cc1d": {"paper_id": "5211c32fb5849a14855a91a7ab16cfb83483cc1d", "abstract": "This paper presents an unsupervised learn ing algorithm for sense disambiguation that when trained on unannotated English text rivals the performance of supervised techniques that require time consuming hand annotations The algorithm is based on two powerful constraints that words tend to have one sense per discourse and one sense per collocation exploited in an iterative bootstrapping procedure Tested accuracy exceeds Introduction This paper presents an unsupervised algorithm that can accurately disambiguate word senses in a large completely untagged corpus The algorithm avoids the need for costly hand tagged training data by ex ploiting two powerful properties of human language One sense per collocation Nearby words provide strong and consistent clues to the sense of a target word conditional on relative dis tance order and syntactic relationship One sense per discourse The sense of a tar get word is highly consistent within any given document Moreover language is highly redundant so that the sense of a word is e ectively overdetermined by and above The algorithm uses these prop erties to incrementally identify collocations for tar get senses of a word given a few seed collocations Note that the problem here is sense disambiguation assigning each instance of a word to established sense de nitions such as in a dictionary This di ers from sense induction using distributional similarity to parti tion word instances into clusters that may have no rela tion to standard sense partitions Here I use the traditional dictionary de nition of collocation appearing in the same location a juxta position of words No idiomatic or non compositional interpretation is implied for each sense This procedure is robust and self correcting and exhibits many strengths of super vised approaches including sensitivity to word order information lost in earlier unsupervised algorithms One Sense Per Discourse The observation that words strongly tend to exhibit only one sense in a given discourse or document was stated and quanti ed in Gale Church and Yarowsky Yet to date the full power of this property has not been exploited for sense disambiguation The work reported here is the rst to take advan tage of this regularity in conjunction with separate models of local context for each word Importantly I do not use one sense per discourse as a hard con straint it a ects the classi cation probabilistically and can be overridden when local evidence is strong In this current work the one sense per discourse hypothesis was tested on a set of examples hand tagged over a period of years the same data studied in the disambiguation experiments For these words the table below measures the claim s accuracy when the word occurs more than once in a discourse how often it takes on the majority sense for the discourse and applicability how often the word does occur more than once in a discourse The one sense per discourse hypothesis Word Senses Accuracy Applicblty plant living factory tank vehicle contnr poach steal boil palm tree hand axes grid tools sake bene t drink bass sh music space volume outer motion legal physical crane bird machine Average Clearly the claim holds with very high reliability for these words and may be con dently exploited as another source of evidence in sense tagging One Sense Per Collocation The strong tendency for words to exhibit only one sense in a given collocation was observed and quan ti ed in Yarowsky This e ect varies de pending on the type of collocation It is strongest for immediately adjacent collocations and weakens with distance It is much stronger for words in a predicate argument relationship than for arbitrary associations at equivalent distance It is very much stronger for collocations with content words than those with function words In general the high reli ability of this behavior in excess of for adjacent content words for example makes it an extremely useful property for sense disambiguation A supervised algorithm based on this property is given in Yarowsky Using a decision list control structure based on Rivest this al gorithm integrates a wide diversity of potential ev idence sources lemmas in ected forms parts of speech and arbitrary word classes in a wide di versity of positional relationships including local and distant collocations trigram sequences and predicate argument association The training pro cedure computes the word sense probability distri butions for all such collocations and orders them by the log likelihood ratio Log Pr SenseAjCollocationi Pr SenseBjCollocationi with optional steps for interpolation and pruning New data are classi ed by using the single most predictive piece of disambiguating evidence that ap pears in the target context By not combining prob abilities this decision list approach avoids the prob lematic complexmodeling of statistical dependencies It is interesting to speculate on the reasons for this phenomenon Most of the tendency is statistical two distinct arbitrary terms of moderate corpus frequency are quite unlikely to co occur in the same discourse whether they are homographs or not This is particu larly true for content words which exhibit a bursty distribution However it appears that human writers also have some active tendency to avoid mixing senses within a discourse In a small study homograph pairs were observed to co occur roughly times less often than arbitrary word pairs of comparable frequency Regard less of origin this phenomenon is strong enough to be of signi cant practical use as an additional probabilistic disambiguation constraint This latter e ect is actually a continuous function conditional on the burstiness of the word the tendency of a word to deviate from a constant Poisson distribution in a corpus As most ratios involve a for some observed value smoothing is crucial The process employed here is sen sitive to variables including the type of collocation ad jacent bigrams or wider context collocational distance type of word content word vs function word and the expected amount of noise in the training data Details are provided in Yarowsky to appear encountered in other frameworks The algorithm is especially well suited for utilizing a large set of highly non independent evidence such as found here In general the decision list algorithm is well suited for the task of sense disambiguation and will be used as a component of the unsupervised algorithm below Unsupervised Learning Algorithm Words not only tend to occur in collocations that reliably indicate their sense they tend to occur in multiple such collocations This provides a mecha nism for bootstrapping a sense tagger If one begins with a small set of seed examples representative of two senses of a word one can incrementally aug ment these seed examples with additional examples of each sense using a combination of the one sense per collocation and one sense per discourse tenden cies Although several algorithms can accomplish sim ilar ends the following approach has the advan tages of simplicity and the ability to build on an existing supervised classi cation algorithm without modi cation As shown empirically it also exhibits considerable e ectiveness The algorithm will be illustrated by the disam biguation of instances of the polysemous word plant in a previously untagged corpus STEP In a large corpus identify all examples of the given polysemous word storing their contexts as lines in an initially untagged training set For example Sense Training Examples Keyword in Context company said the plant is still operating Although thousands of plant and animal species zonal distribution of plant life to strain microscopic plant life from the vinyl chloride monomer plant which is and Golgi apparatus of plant and animal cells computer disk drive plant located in divide life into plant and animal kingdom close up studies of plant life and natural Nissan car and truck plant in Japan is keep a manufacturing plant pro table without molecules found in plant and animal tissue union responses to plant closures animal rather than plant tissues can be many dangers to plant and animal life company manufacturing plant is in Orlando growth of aquatic plant life in water automated manufacturing plant in Fremont Animal and plant life are delicately discovered at a St Louis plant manufacturing computer manufacturing plant and adjacent the proliferation of plant and animal life Including variants of the EM algorithm Baum Dempster et al especially as applied in Gale Church and Yarowsky Indeed any supervised classi cation algorithm that returns probabilities with its classi cations may poten tially be used here These include Bayesian classi ers Mosteller and Wallace and some implementa tions of neural nets but not Brill rules Brill STEP For each possible sense of the word identify a rel atively small number of training examples represen tative of that sense This could be accomplished by hand tagging a subset of the training sentences However I avoid this laborious procedure by iden tifying a small number of seed collocations repre sentative of each sense and then tagging all train ing examples containing the seed collocates with the seed s sense label The remainder of the examples typically constitute an untagged residual Several strategies for identifying seeds that require minimal or no human participation are discussed in Section In the example below the words life and manufac turing are used as seed collocations for the two major senses of plant labeled A and B respectively This partitions the training set into examples of living plants examples of manufacturing plants and residual examples Sense Training Examples Keyword in Context A used to strain microscopic plant life from the A zonal distribution of plant life A close up studies of plant life and natural A too rapid growth of aquatic plant life in water A the proliferation of plant and animal life A establishment phase of the plant virus life cycle A that divide l", "title": "Unsupervised Word Sense Disambiguation Rivaling Supervised Methods"}, "918b9bd1a40247de43256758364a65c10f4856aa": {"paper_id": "918b9bd1a40247de43256758364a65c10f4856aa", "abstract": "In this paper we present a clean, yet effective, model for word sense disambiguation. Our approach leverage a bidirectional long short-term memory network which is shared between all words. This enables the model to share statistical strength and to scale well with vocabulary size. The model is trained end-to-end, directly from the raw text to sense labels, and makes effective use of word order. We evaluate our approach on two standard datasets, using identical hyperparameter settings, which are in turn tuned on a third set of held out data. We employ no external resources (e.g. knowledge graphs, part-of-speech tagging, etc), language specific features, or hand crafted rules, but still achieve statistically equivalent results to the best state-of-the-art systems, that employ no such limitations.", "title": "Word Sense Disambiguation using a Bidirectional LSTM"}, "02ff3c7fbe943910d30f046ed0076a06021ecd50": {"paper_id": "02ff3c7fbe943910d30f046ed0076a06021ecd50", "abstract": "One of the weaknesses of current supervised word sense disambiguation (WSD) systems is that they only treat a word as a discrete entity. However, a continuous-space representation of words (word embeddings) can provide valuable information and thus improve generalization accuracy. Since word embeddings are typically obtained from unlabeled data using unsupervised methods, this method can be seen as a semi-supervised word sense disambiguation approach. This paper investigates two ways of incorporating word embeddings in a word sense disambiguation setting and evaluates these two methods on some SensEval/SemEval lexical sample and all-words tasks and also a domain-specific lexical sample task. The obtained results show that such representations consistently improve the accuracy of the selected supervised WSD system. Moreover, our experiments on a domainspecific dataset show that our supervised baseline system beats the best knowledge-based systems by a large margin.", "title": "Semi-Supervised Word Sense Disambiguation Using Word Embeddings in General and Specific Domains"}, "cd175f80821300931e81ed716b1e351b1a3aa11e": {"paper_id": "cd175f80821300931e81ed716b1e351b1a3aa11e", "abstract": "Most word representation methods assume that each word owns a single semantic vector. This is usually problematic because lexical ambiguity is ubiquitous, which is also the problem to be resolved by word sense disambiguation. In this paper, we present a unified model for joint word sense representation and disambiguation, which will assign distinct representations for each word sense.1 The basic idea is that both word sense representation (WSR) and word sense disambiguation (WSD) will benefit from each other: (1) highquality WSR will capture rich information about words and senses, which should be helpful for WSD, and (2) high-quality WSD will provide reliable disambiguated corpora for learning better sense representations. Experimental results show that, our model improves the performance of contextual word similarity compared to existing WSR methods, outperforms stateof-the-art supervised methods on domainspecific WSD, and achieves competitive performance on coarse-grained all-words WSD.", "title": "A Unified Model for Word Sense Representation and Disambiguation"}, "7f90ef42f22d4f9b86d33b0ad7f16261273c8612": {"paper_id": "7f90ef42f22d4f9b86d33b0ad7f16261273c8612", "abstract": "a r t i c l e i n f o a b s t r a c t We present an automatic approach to the construction of BabelNet, a very large, wide-coverage multilingual semantic network. Key to our approach is the integration of lexicographic and encyclopedic knowledge from WordNet and Wikipedia. In addition, Machine Translation is applied to enrich the resource with lexical information for all languages. We first conduct in vitro experiments on new and existing gold-standard datasets to show the high quality and coverage of BabelNet. We then show that our lexical resource can be used successfully to perform both monolingual and cross-lingual Word Sense Disambiguation: thanks to its wide lexical coverage and novel semantic relations, we are able to achieve state-of the-art results on three different SemEval evaluation tasks.", "title": "BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network"}, "9c2a953fb4d7e1da64eb21c58d52061b3d5dfc8d": {"paper_id": "9c2a953fb4d7e1da64eb21c58d52061b3d5dfc8d", "abstract": "We present an estimate of an upper bound of 1.75 bits for the entropy of characters in printed English, obtained by constructing a word trigram model and then computing the cross-entropy between this model and a balanced sample of English text. We suggest the well-known and widely available Brown Corpus of printed English as a standard against which to measure progress in language modeling and offer our bound as the first of what we hope will be a series of steadily decreasing bounds.", "title": "An Estimate of an Upper Bound for the Entropy of English"}, "7d5a6708f9e4fb730653f01d630dca41a5da40a4": {"paper_id": "7d5a6708f9e4fb730653f01d630dca41a5da40a4", "abstract": "Two new wideband in-phase and out-of-phase balanced power dividing/combining networks are proposed in this paper. Based on matrix transformation, the differential-mode and common-mode equivalent circuits of the two wideband in-phase and out-of-phase networks can be easily deduced. A patterned ground-plane technique is used to realize the strong coupling of the shorted coupled lines for the differential mode. Two planar wideband in-phase and out-of-phase balanced networks with bandwidths of 55.3% and 64.4% for the differential mode with wideband common-mode suppression are designed and fabricated. The theoretical and measured results agree well with each other and show good in-band performances.", "title": "Wideband In-Phase and Out-of-Phase Balanced Power Dividing and Combining Networks"}, "864342bee40e75cc3719652a18f971ea97424648": {"paper_id": "864342bee40e75cc3719652a18f971ea97424648", "abstract": "Social network sites (SNS) have attracted considerable attention among teens and young adults who tend to connect and share common interest. Despite this popularity, the issue of students\u2019 adoption of social network sites is still being unexplored fully in Malaysia. Driven by this factor, this study was designed to analyze the impact of social network sites on students\u2019 academic performance in Malaysia. Using a conceptual approach, the study gathered that more students prefer the use of Facebook and Twitter in academic related discussions in complementingconventional classroom teaching and learning process. Thus, it is imperative that lecturers and academic institutions should implement the use of these applications in promoting academic excellence. As for profit oriented organizations such as bookshops, computer and smartphoneone vendors, they can promote their products through these applications and engage students to make purchases via them having understood that many students prefer and use Facebook, Twitter and Google+. The discussion from this study however does not represent the general sampling of Malaysian university students.", "title": "The Influence of Social Network Sites ( SNS ) upon Academic Performance of Malaysian Students"}, "2484c717f39bf94e46503744201f4b57a5f1be4b": {"paper_id": "2484c717f39bf94e46503744201f4b57a5f1be4b", "abstract": "This study investigates the development of trust in a Web-based vendor during two stages of a consumer\u2019s Web experience: exploration and commitment. Through an experimental design, the study tests the effects of third party endorsements, reputation, and individual differences on trust in the vendor during these two stages.", "title": "Trust in e-commerce vendors: a two-stage model"}, "e078cba1fc980ba10fd0735ca93e19585c5dc99f": {"paper_id": "e078cba1fc980ba10fd0735ca93e19585c5dc99f", "abstract": "This paper discusses the trust related issues and arguments (evidence) Internet stores need to provide in order to increase consumer trust. Based on a model of trust from academic literature, in addition to a model of the customer service life cycle, the paper develops a framework that identifies key trust-related issues and organizes them into four categories: personal information, product quality and price, customer service, and store presence. It is further validated by comparing the issues it raises to issues identified in a review of academic studies, and to issues of concern identified in two consumer surveys. The framework is also applied to ten well-known web sites to demonstrate its applicability. The proposed framework will benefit both practitioners and researchers by identifying important issues regarding trust, which need to be accounted for in Internet stores. For practitioners, it provides a guide to the issues Internet stores need to address in their use of arguments. For researchers, it can be used as a foundation for future empirical studies investigating the effects of trust-related arguments on consumers\u2019 trust in Internet stores.", "title": "Trust-Related Arguments in Internet Stores: A Framework for Evaluation"}, "c6790d86b62c5f2849144700271490554d94a72d": {"paper_id": "c6790d86b62c5f2849144700271490554d94a72d", "abstract": "Participants in social network sites create self-descriptive profiles that include their links to other members, creating a visible network of connections \u2014 the ostensible purpose of these sites is to use this network to make friends, dates, and business connections. In this paper we explore the social implications of the public display of one\u2019s social network. Why do people display their social connections in everyday life, and why do they do so in these networking sites? What do people learn about another\u2019s identity through the signal of network display? How does this display facilitate connections, and how does it change the costs and benefits of making and brokering such connections compared to traditional means? The paper includes several design recommendations for future networking sites.", "title": "Public Displays of Connection"}, "5e30227914559ce088a750885761adbb7d2edbbf": {"paper_id": "5e30227914559ce088a750885761adbb7d2edbbf", "abstract": "Teenagers will freely give up personal information to join social networks on the Internet. Afterwards, they are surprised when their parents read their journals. Communities are outraged by the personal information posted by young people online and colleges keep track of student activities on and off campus. The posting of personal information by teens and students has consequences. This article will discuss the uproar over privacy issues in social networks by describing a privacy paradox; private versus public space; and, social networking privacy issues. It will finally discuss proposed privacy solutions and steps that can be taken to help resolve the privacy paradox.", "title": "A privacy paradox: Social networking in the United States"}, "1a9bb0a637cbaca8489afd69d6840975a9834a05": {"paper_id": "1a9bb0a637cbaca8489afd69d6840975a9834a05", "abstract": "We investigate projection methods, for evaluating a linear approximation of the value function of a policy in a Markov Decision Process context. We consider two popular approaches, the one-step Temporal Difference fix-point computation (TD(0)) and the Bellman Residual (BR) minimization. We describe examples, where each method outperforms the other. We highlight a simple relation between the objective function they minimize, and show that while BR enjoys a performance guarantee, TD(0) does not in general. We then propose a unified view in terms of oblique projections of the Bellman equation, which substantially simplifies and extends the characterization of Schoknecht (2002) and the recent analysis of Yu & Bertsekas (2008). Eventually, we describe some simulations that suggest that if the TD(0) solution is usually slightly better than the BR solution, its inherent numerical instability makes it very bad in some cases, and thus worse on average.", "title": "Should one compute the Temporal Difference fix point or minimize the Bellman Residual? The unified oblique projection view"}, "020f6457af71c6926281b06f5c5bb8222fa45ce6": {"paper_id": "020f6457af71c6926281b06f5c5bb8222fa45ce6", "abstract": "We introduce the first algorithm for off-policy temporal-difference learning that is stable with linear function approximation. Off-policy learning is of interest because it forms the basis for popular reinforcement learning methods such as Q-learning, which has been known to diverge with linear function approximation, and because it is critical to the practical utility of multi-scale, multi-goal, learning frameworks such as options, HAMs, and MAXQ. Our new algorithm combines TD(\u03bb) over state\u2013action pairs with importance sampling ideas from our previous work. We prove that, given training under any -soft policy, the algorithm converges w.p.1 to a close approximation (as in Tsitsiklis and Van Roy, 1997; Tadic, 2001) to the action-value function for an arbitrary target policy. Variations of the algorithm designed to reduce variance introduce additional bias but are also guaranteed convergent. We also illustrate our method empirically on a small policy evaluation problem. Our current results are limited to episodic tasks with episodes of bounded length. 1Although Q-learning remains the most popular of all reinforcement learning algorithms, it has been known since about 1996 that it is unsound with linear function approximation (see Gordon, 1995; Bertsekas and Tsitsiklis, 1996). The most telling counterexample, due to Baird (1995) is a seven-state Markov decision process with linearly independent feature vectors, for which an exact solution exists, yet This is a re-typeset version of an article published in the Proceedings of the 18th International Conference on Machine Learning (2001). It differs from the original in line and page breaks, is crisper for electronic viewing, and has this funny footnote, but otherwise it is identical to the published article. for which the approximate values found by Q-learning diverge to infinity. This problem prompted the development of residual gradient methods (Baird, 1995), which are stable but much slower than Q-learning, and fitted value iteration (Gordon, 1995, 1999), which is also stable but limited to restricted, weaker-than-linear function approximators. Of course, Q-learning has been used with linear function approximation since its invention (Watkins, 1989), often with good results, but the soundness of this approach is no longer an open question. There exist non-pathological Markov decision processes for which it diverges; it is absolutely unsound in this sense. A sensible response is to turn to some of the other reinforcement learning methods, such as Sarsa, that are also efficient and for which soundness remains a possibility. An important distinction here is between methods that must follow the policy they are learning about, called on-policy methods, and those that can learn from behavior generated by a different policy, called off-policy methods. Q-learning is an off-policy method in that it learns the optimal policy even when actions are selected according to a more exploratory or even random policy. Q-learning requires only that all actions be tried in all states, whereas on-policy methods like Sarsa require that they be selected with specific probabilities. Although the off-policy capability of Q-learning is appealing, it is also the source of at least part of its instability problems. For example, in one version of Baird\u2019s counterexample, the TD(\u03bb) algorithm, which underlies both Qlearning and Sarsa, is applied with linear function approximation to learn the action-value function Q for a given policy \u03c0. Operating in an on-policy mode, updating state\u2013 action pairs according to the same distribution they would be experienced under \u03c0, this method is stable and convergent near the best possible solution (Tsitsiklis and Van Roy, 1997; Tadic, 2001). However, if state-action pairs are updated according to a different distribution, say that generated by following the greedy policy, then the estimated values again diverge to infinity. This and related counterexamples suggest that at least some of the reason for the instability of Q-learning is that it is an off-policy method; they also make it clear that this part of the problem can be studied in a purely policy-evaluation context. Despite these problems, there remains substantial reason for interest in off-policy learning methods. Several researchers have argued for an ambitious extension of reinforcement learning ideas into modular, multi-scale, and hierarchical architectures (Sutton, Precup & Singh, 1999; Parr, 1998; Parr & Russell, 1998; Dietterich, 2000). These architectures rely on off-policy learning to learn about multiple subgoals and multiple ways of behaving from the singular stream of experience. For these approaches to be feasible, some efficient way of combining off-policy learning and function approximation must be found. Because the problems with current off-policy methods become apparent in a policy evaluation setting, it is there that we focus in this paper. In previous work we considered multi-step off-policy policy evaluation in the tabular case. In this paper we introduce the first off-policy policy evaluation method consistent with linear function approximation. Our mathematical development focuses on the episodic case, and in fact on a single episode. Given a starting state and action, we show that the expected offpolicy update under our algorithm is the same as the expected on-policy update under conventional TD(\u03bb). This, together with some variance conditions, allows us to prove convergence and bounds on the error in the asymptotic approximation identical to those obtained by Tsitsiklis and Van Roy (1997; Bertsekas and Tsitsiklis, 1996). 1. Notation and Main Result We consider the standard episodic reinforcement learning framework (see, e.g., Sutton & Barto, 1998) in which a learning agent interacts with a Markov decision process (MDP). Our notation focuses on a single episode of T time steps, s0, a0, r1, s1, a1, r2, . . . , rT , sT , with states st \u2208 S, actions at \u2208 A, and rewards rt \u2208 <. We take the initial state and action, s0 and a0, to be given arbitrarily. Given a state and action, st and at, the next reward, rt+1, is a random variable with mean rt st and the next state, st+1, is chosen with probabilities pt stst+1 . The final state is a special terminal state that may not occur on any preceding time step. Given a state, st, 0 < t < T , the action at is selected according to probability \u03c0(st, at) or b(st, at) depending on whether policy \u03c0 or policy b is in force. We always use \u03c0 to denote the target policy, the policy that we are learning about. In the on-policy case, \u03c0 is also used to generate the actions of the episode. In the off-policy case, the actions are instead generated by b, which we call the behavior policy. In either case, we seek an approximation to the action-value function Q : S \u00d7A 7\u2192 < for the target policy \u03c0: Q(s, a) = E\u03c0 { rt+1 + \u00b7 \u00b7 \u00b7+ \u03b3rT | st = s, at = a } , where 0 \u2264 \u03b3 \u2264 1 is a discount-rate parameter. We consider approximations that are linear in a set of feature vectors {\u03c6sa}, s \u2208 S, a \u2208 A: Q(s, a) \u2248 \u03b8 \u03c6sa = n \u2211", "title": "Off-Policy Temporal Difference Learning with Function Approximation"}, "1b8e45c10238f261a468171374f0c515be790650": {"paper_id": "1b8e45c10238f261a468171374f0c515be790650", "abstract": "A number of reinforcement learning algorithms have been developed that are guaranteed to converge to the optimal solution when used with lookup tables. It is shown, however, that these algorithms can easily become unstable when implemented directly with a general function-approximation system, such as a sigmoidal multilayer perceptron, a radial-basisfunction system, a memory-based learning system, or even a linear function-approximation system. A new class of algorithms, residual gradient algorithms, is proposed, which perform gradient descent on the mean squared Bellman residual, guaranteeing convergence. It is shown, however, that they may learn very slowly in some cases. A larger class of algorithms, residual algorithms, is proposed that has the guaranteed convergence of the residual gradient algorithms, yet can retain the fast learning speed of direct algorithms. In fact, both direct and residual gradient algorithms are shown to be special cases of residual algorithms, and it is shown that residual algorithms can combine the advantages of each approach. The direct, residual gradient, and residual forms of value iteration, Qlearning, and advantage learning are all presented. Theoretical analysis is given explaining the properties these algorithms have, and simulation results are given that demonstrate these properties.", "title": "Residual Algorithms: Reinforcement Learning with Function Approximation"}, "28be6c2ed074a7fa63818a1730b04219d8a01c02": {"paper_id": "28be6c2ed074a7fa63818a1730b04219d8a01c02", "abstract": "The method of temporal differences (TD) is one way of making consistent predictions about the future. This paper uses some analysis of Watkins (1989) to extend a convergence theorem due to Sutton (1988) from the case which only uses information from adjacent time steps to that involving information from arbitrary ones. It also considers how this version of TD behaves in the face of linearly dependent representations for states\u2014demonstrating that it still converges, but to a different answer from the least mean squares algorithm. Finally it adapts Watkins' theorem that Q-learning, his closely related prediction and action learning method, converges with probability one, to demonstrate this strong form of convergence for a slightly modified version of TD.", "title": "The convergence of TD(\u03bb) for general \u03bb"}, "0d31a7a58c61ebe6f80d64a7b2e3e59a59d33a82": {"paper_id": "0d31a7a58c61ebe6f80d64a7b2e3e59a59d33a82", "abstract": "We introduce the first temporal-difference learning algorithm that is stable with linear function approximation and off-policy training, for any finite Markov decision process, behavior policy, and target policy, and whose complexity scales linearly in the number of parameters. We consider an i.i.d. policy-evaluation setting in which the data need not come from on-policy experience. The gradient temporal-difference (GTD) algorithm estimates the expected update vector of the TD(0) algorithm and performs stochastic gradient descent on its L2 norm. We prove that this algorithm is stable and convergent under the usual stochastic approximation conditions to the same least-squares solution as found by the LSTD, but without LSTD\u2019s quadratic computational complexity. GTD is online and incremental, and does not involve multiplying by products of likelihood ratios as in importance-sampling methods. 1 Off-policy learning methods Off-policy methods have an important role to play in the larger ambitions of modern reinforcement learning. In general, updates to a statistic of a dynamical process are said to be \u201coff-policy\u201d if their distribution does not match the dynamics of the process, particularly if the mismatch is due to the way actions are chosen. The prototypical example in reinforcement learning is the learning of the value function for one policy, the target policy, using data obtained while following another policy, the behavior policy. For example, the popular Q-learning algorithm (Watkins 1989) is an offpolicy temporal-difference algorithm in which the target policy is greedy with respect to estimated action values, and the behavior policy is something more exploratory, such as a corresponding greedy policy. Off-policy methods are also critical to reinforcement-learning-based efforts to model human-level world knowledge and state representations as predictions of option outcomes (e.g., Sutton, Precup & Singh 1999; Sutton, Rafols & Koop 2006). Unfortunately, off-policy methods such as Q-learning are not sound when used with approximations that are linear in the learned parameters\u2014the most popular form of function approximation in reinforcement learning. Counterexamples have been known for many years (e.g., Baird 1995) in which Q-learning\u2019s parameters diverge to infinity for any positive step size. This is a severe problem in so far as function approximation is widely viewed as necessary for large-scale applications of reinforcement learning. The need is so great that practitioners have often simply ignored the problem and continued to use Q-learning with linear function approximation anyway. Although no instances of absolute divergence in applications have been reported in the literature, the potential for instability is disturbing and probably belies real but less obvious problems.", "title": "A Convergent O ( n ) Algorithm for Off-policy Temporal-difference Learning with Linear Function Approximation"}, "c1b0d029c0ada83f7694971a24597b4530cf1df7": {"paper_id": "c1b0d029c0ada83f7694971a24597b4530cf1df7", "abstract": "We explore an application to the game of Go of a reinforcement learning approach based on a linear evaluation function and large numbers of binary features. This strategy has proved effective in game playing programs and other reinforcement learning applications. We apply this strategy to Go by creating over a million features based on templates for small fragments of the board, and then use temporal difference learning and self-play. This method identifies hundreds of low level shapes with recognisable significance to expert Go players, and provides quantitive estimates of their values. We analyse the relative contributions to performance of templates of different types and sizes. Our results show that small, translation-invariant templates are surprisingly effective. We assess the performance of our program by playing against the Average Liberty Player and a variety of computer opponents on the 9\u00d79Computer Go Server. Our linear evaluation function appears to outperform all other static evaluation functions that do not incorporate substantial domain knowledge.", "title": "Reinforcement Learning of Local Shape in the Game of Go"}, "621c03cd67b0b7bac665f3c7887481b4b42f269c": {"paper_id": "621c03cd67b0b7bac665f3c7887481b4b42f269c", "abstract": "We provide some general results on the convergence of a class of stochastic approximation algorithms and their parallel and asynchronous variants. We then use these results to study the Q-learning algorithm, a reinforcement learning method for solving Markov decision problems, and establish its convergence under conditions more general than previously available.", "title": "Asynchronous Stochastic Approximation and Q-Learning"}, "0667b9b284fc06cdf7ee2775ccf8a88f8989c4a2": {"paper_id": "0667b9b284fc06cdf7ee2775ccf8a88f8989c4a2", "abstract": "To appear in G Tesauro D S Touretzky and T K Leen eds Advances in Neural Information Processing Systems MIT Press Cambridge MA A straightforward approach to the curse of dimensionality in re inforcement learning and dynamic programming is to replace the lookup table with a generalizing function approximator such as a neu ral net Although this has been successful in the domain of backgam mon there is no guarantee of convergence In this paper we show that the combination of dynamic programming and function approx imation is not robust and in even very benign cases may produce an entirely wrong policy We then introduce Grow Support a new algorithm which is safe from divergence yet can still reap the bene ts of successful generalization", "title": "Generalization in Reinforcement Learning: Safely Approximating the Value Function"}, "f28cb37e0f1a225f0d4f27f43ef4e05eee8b321c": {"paper_id": "f28cb37e0f1a225f0d4f27f43ef4e05eee8b321c", "abstract": "In Singapore and Malaysia, people often speak a mixture of Mandarin and English within a single sentence. We call such sentences intra-sentential code-switch sentences. In this paper, we report on the development of a Mandarin-English codeswitching spontaneous speech corpus: SEAME. The corpus is developed as part of a multilingual speech recognition project and will be used to examine how Mandarin-English codeswitch speech occurs in the spoken language in South-East Asia. Additionally, it can provide insights into the development of large vocabulary continuous speech recognition (LVCSR) for code-switching speech. The corpus collected consists of intra-sentential code-switching utterances that are recorded under both interview and conversational settings. This paper describes the corpus design and the analysis of collected corpus.", "title": "SEAME: a Mandarin-English code-switching speech corpus in south-east asia"}, "9ddda6862a8eefbb04682b449eefd6f37a45949e": {"paper_id": "9ddda6862a8eefbb04682b449eefd6f37a45949e", "abstract": "In this paper, we explore the role and scope of technology in value co-creation, service innovation and service systems\u2014value co-creation configurations of people technology and value propositions (Maglio and Spohrer in J Acad Mark Sci 36:18\u201320, 2008). We draw on a structurational model of technology (Orlikowsky in Organ Sci 3(3):398\u2013427, 1992) to provide a framework for considering the role of technology in service systems and how it influences and is influenced by human actions (i.e., practices) and institutions. We broaden the scope of technology in this model, beyond a material artifact, or outcome of human actions, by applying an S-D logic, service ecosystems (Vargo and Lusch in J Market 68(1):1\u201317, 2004, Ind Mark Manag 40(2):181\u2013187, 2011a) approach, which focuses on the processes by which value is co-created and new ways of creating value (i.e., innovation) emerge. In this view, technology can be conceptualized as an operant resource\u2014one that is capable of acting on other resources to create value\u2014and, thus, becomes a critical resource for value co-creation, service innovation and systems (re)formation. We argue that the consideration of technology as an operant resource in service (eco)systems provides a more encompassing view for systematically studying the way in which technologies are integrated as resources, value is collaboratively created, and service is innovated.", "title": "Technology as an operant resource in service (eco)systems"}, "7b97e7e267862103cedbe7a56f8bce0e53a01f07": {"paper_id": "7b97e7e267862103cedbe7a56f8bce0e53a01f07", "abstract": "The erosion of service quality throughout the economy is a frequent concern in the popular press. The American Customer Satisfaction Index for services fell in 2000 to 69.4%, down 5 percentage points from 1994. We hypothesize that the characteristics of services\u2014 inseparability, intangibility, and labor intensity\u2014interact with management practices to bias service providers toward reducing the level of service they deliver, often locking entire industries into a vicious cycle of eroding service standards. To explore this proposition we develop a formal model that integrates the structural elements of service delivery. We use econometric estimation, interviews, observations, and archival data to calibrate the model for a consumer-lending service center in a major bank in the United Kingdom. We find that temporary imbalances between service capacity and demand interact with decision rules for effort allocation, capacity management, overtime, and quality aspirations to yield permanent erosion of the service standards and loss of revenue. We explore policies to improve performance and implications for organizational design in the service sector. (Organizational Learning; Service Management Performance; Service Operations; Service Quality; Simulation; System Dynamics)", "title": "Cutting Corners and Working Overtime: Quality Erosion in the Service Industry"}, "02beb87623eaa4d0476a814788fe2ab3847f9dbd": {"paper_id": "02beb87623eaa4d0476a814788fe2ab3847f9dbd", "abstract": "Since the introductory article for what has become known as the \u201cservice-dominant (S-D) logic of marketing,\u201d \u201cEvolving to a New Dominant Logic for Marketing,\u201d was published in the Journal of Marketing (Vargo, S. L., & Lusch, R. F. (2004a)), there has been considerable discussion and elaboration of its specifics. This article highlights and clarifies the salient issues associated with S-D logic and updates the original foundational premises (FPs) and adds an FP. Directions for future work are also discussed.", "title": "Service-dominant logic : continuing the evolution"}, "bd1c85bf52295adad0b1a59d79d7429091beeb22": {"paper_id": "bd1c85bf52295adad0b1a59d79d7429091beeb22", "abstract": "Marketing inherited a model of exchange from economics, which had a dominant logic based on the exchange of \u201cgoods,\u201d which usually are manufactured output. The dominant logic focused on tangible resources, embedded value, and transactions. Over the past several decades, new perspectives have emerged that have a revised logic focused on intangible resources, the cocreation of value, and relationships. The authors believe that the new perspectives are converging to form a new dominant logic for marketing, one in which service provision rather than goods is fundamental to economic exchange. The authors explore this evolving logic and the corresponding shift in perspective for marketing scholars, marketing practitioners, and marketing educators.", "title": "Evolving to a New Dominant Logic for Marketing"}, "7a19282f51b78f7c18fd1e94cf0e9fa5b1599b73": {"paper_id": "7a19282f51b78f7c18fd1e94cf0e9fa5b1599b73", "abstract": "The services sector has grown over the last 50 years to dominate economic activity in most advanced industrial economies, yet scientific understanding of modern services is rudimentary. Here, we argue for a services science discipline to integrate across academic silos and advance service innovation more rapidly.", "title": "A research manifesto for services science"}, "46fb7520f6ce62ceed1fdf85154f719607209db1": {"paper_id": "46fb7520f6ce62ceed1fdf85154f719607209db1", "abstract": "There are two logics or mindsets from which to consider and motivate a transition from goods to service(s). The first, \u201cgoods-dominant (G-D) logic\u201d, views services in terms of a type of (e.g., intangible) good and implies that goods production and distribution practices should be modified to deal with the differences between tangible goods and services. The second logic, \u201cservice-dominant (S-D) logic\u201d, considers service \u2013 a process of using ones resources for the benefit of and in conjunction with another party \u2013 as the fundamental purpose of economic exchange and implies the need for a revised, service-driven framework for all of marketing. This transition to a service-centered logic is consistent with and partially derived from a similar transition found in the business-marketing literature \u2014 for example, its shift to understanding exchange in terms value rather than products and networks rather than dyads. It also parallels transitions in other sub-disciplines, such as service marketing. These parallels and the implications for marketing theory and practice of a full transition to a service-logic are explored. \u00a9 2008 Elsevier Inc. All rights reserved. Over the last several decades, leading-edge firms, as well as many business scholars and consultants, have advocated the need for refocusing substantial firm activity or transforming the entire firm orientation from producing output, primarily manufactured goods, to a concern with service(s) (see, e.g., Davies, Brady, & Hobday, 2007; Gebauer& Fleisch, 2007). These initiatives can be found in both business-to-business (e.g., IBM, GE) and businessto-consumer enterprises (e.g. Lowe's, Kodak, Apple) and in some cases entire industries (e.g., software-as-a-service). The common justification is that these initiatives are analogous with the shift from a manufacturing to a service economy in developed countries, if not globally. That is, it is based on the idea that virtually all economies are producing and exchanging more services than they are goods; thus, services require increased attention. This perception suggests that firms need to redirect the production and marketing strategy that they have adopted for manufactured goods by adjusting them for the distinguishing characteristics of services. \u204e Corresponding author. Tel.: +1 808 956 8167; fax: +1 808 956 9886. E-mail addresses: svargo@hawaii.edu (S.L. Vargo), rlusch@eller.arizona.edu (R.F. Lusch). 1 Tel.: +1 520 621 7480. 0019-8501/$ see front matter \u00a9 2008 Elsevier Inc. All rights reserved. doi:10.1016/j.indmarman.2007.07.004 Please cite this article as: Vargo, S. L., & Lusch, R. F., From goods to service(s): (2008), doi:10.1016/j.indmarman.2007.07.004 This logic of the need for a shift in the activities of the enterprise and/or industry to match the analogous shift in the economy is so intuitively compelling that it is an apparent truism. It is a logic that follows naturally from marketing's traditional foundational thought. But is it the only logic; is it the correct logic? Does it move business-to-business (B2B) firms and/or academic marketing thought in a desirable and enhanced direction? While we agree that a shift to a service focus is desirable, if not essential to a firm's well being and the advancement of academic thought, we question the conventional, underlying rationale and the associated, implied approach. The purpose of this commentary is to explore this traditional logical foundation with its roots in the manufacturing and provision of tangible output and to propose an alternative logic, one grounded in a revised understanding of the meaning of service as a process and its central role in economic exchange. It is a logic that represents the convergence and extension of divergent marketing thought by sub-disciplines and other research initiatives. We argue that this more service-centric logic not only amplifies the necessity for the development of a service focus but it also provides a stronger foundation for theory development and, consequently, application. It is a logic that provides a framework for elevating knowledge discovery in business marketing (as well as other \u201csub-disciplines\u201d) beyond the Divergences and convergences of logics, Industrial Marketing Management 2 S.L. Vargo, R.F. Lusch / Industrial Marketing Management xx (2008) xxx\u2013xxx ARTICLE IN PRESS identification and explanation of B2B marketing differences from other forms of marketing to a level capable of informing not only the business-marketing firm but \u201cmainstream\u201d marketing in general. Thus, we argue a service-centered focus is enriching and unifying. 1. Alternative logics Broadly speaking, there are two perspectives for the consideration of service(s). One views goods (tangible output embedded with value) as the primary focus of economic exchange and \u201cservices\u201d (usually plural) as either (1) a restricted type of (intangible) good (i.e., as units of output) or (2) an add-on that enhances the value of a good. We (Vargo & Lusch, 2004a; Lusch & Vargo, 2006a) call this logic goods-dominant (G-D) logic. Others have referred to it as the \u201cneoclassical economics research tradition\u201d (e.g., Hunt, 2000), \u201cmanufacturing logic\u201d (e.g., Normann, 2001), \u201cold enterprise logic\u201d (Zuboff & Maxmin, 2002), and \u201cmarketing management\u201d (Webster, 1992). Regardless of the label, G-D logic points toward using principles developed to manage goods production to manage services \u201cproduction\u201d and \u201cdelivery\u201d. The second logic considers \u201cservice\u201d (singular) \u2013 a process of doing something for another party \u2013 in its own right, without reference to goods and identifies service as the primary focus of exchange activity. We (Vargo & Lusch, 2004a, 2006) call this logic service-dominant (S-D) logic. In S-D logic, goods continue to play an important, service-delivery role, at least in a subset of economic exchange. In contrast to implying the modification of goods-based models of exchange to fit a transition to service, S-D logic provides a service-based foundation centered on service-driven principles. We show that this transition is highly consistent with many contemporary business-marketing models. 2. Goods-dominant logic As the label implies, G-D logic is centered on the good \u2013 or more recently, the \u201cproduct\u201d, to include both tangible (goods) and intangible (services) units of output \u2013 as archetypical units of exchange. The essence of G-D logic is that economic exchange is fundamentally concerned with units of output (products) that are embedded with value during the manufacturing (or farming, or extraction) process. For efficiency, this production ideally takes place in isolation from the customer and results in standardized, inventoriable goods. The roots of G-D logic are found in the work of Smith (1776) and took firmer, paradigmatic grasp in the context of the Industrial Revolution during the quest for a science of economics, at a time when \u201cscience\u201d meant Newtonian mechanics, a paradigm for which the idea of goods embedded with value was particularly amenable. Management and mainstream academic marketing, as well as society in general, inherited this logic from economics (see Vargo, Lusch, & Morgan, 2006; Vargo & Morgan, 2005). However, since formal marketing thought emerged over 100 years ago, G-D logic and its associated concept of embedded value (or utility) have caused problems for marketers. For exPlease cite this article as: Vargo, S. L., & Lusch, R. F., From goods to service(s): (2008), doi:10.1016/j.indmarman.2007.07.004 ample, in the mid 20th Century, it caused Alderson (1957, p. 69) to declare: \u201cWhat is needed is not an interpretation of the utility created bymarketing, but a marketing interpretation of the whole process of creating utility\u201d. But the G-D-logic-based economic theory, with its co-supportive concepts of embedded value (production) and value destruction (consumption) was itself deeply embedded in marketing thought. It was not long after this period, we believe for related reasons, that academic marketing started becoming fragmented, with various marketing concerns taking on an increasingly separate, or sub-disciplinarian, identity. 3. Subdividing and breaking free from G-D logic Arguably, the establishment of many of the sub-disciplines of marketing, such as business-to-business marketing, services marketing, and international marketing, is a response to the limitations and lack of robustness of G-D logic as a foundation for understanding value creation and exchange. That is, while GD logic might have been reasonably adequate as a foundation when marketing was primarily concerned with the distribution of commodities, the foundation was severely restricted as marketing expanded its scope to the more general issues of value creation and exchange. 3.1. Business-to-business marketing Initial sub-disciplinary approaches have typically involved trying to fit the models of mainstream marketing to the particular phenomena of concern. For example, as marketers (both academic and applied) began to address issues of industrial marketing and found that manymainstreammarketingmodels did not seem to apply, the natural course of action was not to question the paradigmatic foundation but rather first to identify how B2B marketing was different from mainstream, consumer marketing and then to identify the ways that business marketers needed to adjust. Thus, early attempts led to the identification of prototypical characteristics of business marketing \u2014 derived demand, fluctuating demand, professional buyers, etc. (see Fern & Brown, 1984). But we suggest that the creation of business-tobusiness marketing as a sub-discipline was more because of the inability of the G-D-logic-grounded mainstream marketing to provide a suitable foundation for understanding inter-enterprise exchange phenomena than it was because of any real and essential difference compared to enterprise-to-individual exchange. Support for this contention can be found in the fact that busin", "title": "From goods to service ( s ) : Divergences and convergences of logics"}, "4f33a439da2d2e5e9f7454bc7c8a6bdb234d0861": {"paper_id": "4f33a439da2d2e5e9f7454bc7c8a6bdb234d0861", "abstract": "DEAR EDITOR, Classically, systemic antifungal treatment is usually mandatory in ringworm affecting cutaneous appendages: the hair or nails. In contrast, in the hairless skin only topical treatment can be used, except in cases of extensive, multiple or recurrent lesions, or in immunocompromised patients. Recently, we described in this journal a new criterion to start systemic antifungal therapy in tinea of vellus hair skin: the observation of parasitized vellus hairs on direct examination. To date, 20 isolated cases of ringworm of the vellus hair have been described, and we reported the largest series of tinea of the vellus hair. We highlighted the potential importance of this finding, which is one cause of resistance to topical treatment in tinea of vellus hair skin. We assessed whether dermoscopy could be useful to identify parasitism of vellus hair, in the same way that it has proven useful in the diagnosis of tinea capitis. We present a prospective observational study of six patients with tinea of vellus hair skin who attended our mycology unit at the Regional University Hospital of Malaga, Spain, during the period 2011\u20132014. Cases were analysed from a clinical, dermoscopic, mycological and therapeutic standpoint. All patients underwent the following: history taking, clinical examination, digital photography of any lesion using a Canon PowerShot G11 camera (Canon, Lake Success, NY, U.S.A.), microscopic examination of skin scraping, and use of KOH 20%. Fungal culture was performed on Sabouraud dextrose agar with and without actidione, incubated at 27 \u00b0C and examined frequently for 3 weeks. Dermoscopic examination (DermLite Pro II HR; 3Gen Inc., San Juan Capistrano, CA, U.S.A.) using a transparent film dressing was performed, after disinfection of the lens with an alcohol swab to avoid transmission of infection. The patients (three male and three female) were aged 4\u201340 years (mean 20 8). Six patients had single ringworm lesions at the time of diagnosis (Table 1). Most (67%) had received topical antifungal agents before seeing the dermatologist, and two cases (33%) had received topical corticosteroids before. Fifty per cent reported previous contact with a pet. Two patients had lesions located on the arm (Figs 1a, 2a), one on the axillae, one on the chest, one on the leg and one on the cheek (Figs 1a, 2a). Clinically, follicular micropustules were seen in four lesions (67%). Under dermoscopy (Figs 1b, 2b) we observed scaly plaques (100%), translucent hairs (83%), follicular pustules (67%), broken hairs (50%), corkscrew hair (17%), black dots (17%), dystrophic hair (17%) and Morse code hair (17%). Empty follicles were observed in only one lesion. In all cases, during the skin scraping, short thin hairs fell easily onto the slide (Fig. 1c). In all patients all the few vellus hairs identified on direct examination with KOH were affected (Fig. 2c). When antifungal topical cream had been used previously there was no evidence of parasitism of the scales, with the condition of the infected vellus hair being the only finding under direct microscopy; this occurred in five lesions (83%). Geophilic species were identified in one lesion (17%), with the remaining cases all being zoophilic (50%) or anthropophilic species (33%). All cases healed properly after 6 weeks of oral antifungal treatment, with no recurrences. We used griseofulvin when Microsporum canis or Microsporum gypseum were isolated, and terbinafine in the remaining cases.", "title": "Using dermoscopy to detect tinea of vellus hair."}, "13125ac8226fe3fc297d4880012fd3531f1305bd": {"paper_id": "13125ac8226fe3fc297d4880012fd3531f1305bd", "abstract": "The Aurora system [1] is an experimental data stream management system with a fully functional prototype. It includes both a graphical development environment, and a runtime system. We propose to demonstrate the Aurora system with its development environment and runtime system, with several example monitoring applications developed in consultation with defense, financial, and natural science communities. We will also demonstrate the effect of various system alternatives on various workloads. For example, we will show how different scheduling algorithms affect tuple latency and internal queue lengths. We will use some of our visualization tools to accomplish this. Data Stream Management Aurora is a data stream management system for monitoring applications. Streams are continuous data feeds from such sources as sensors, satellites and stock feeds. Monitoring applications track the data from numerous streams, filtering them for signs of abnormal activity and processing them for purposes of aggregation, reduction and correlation. The management requirements for monitoring applications differ profoundly from those satisfied by a traditional DBMS: o A traditional DBMS assumes a passive model where most data processing results from humans issuing transactions and queries. Data stream management requires a more active approach, monitoring data feeds from unpredictable external sources (e.g., sensors) and alerting humans when abnormal activity is detected. o A traditional DBMS manages data that is currently in its tables. Data stream management often requires processing data that is bounded by some finite window of values, and not over an unbounded past. o A traditional DBMS provides exact answers to exact queries, and is blind to real-time deadlines. Data stream management often must respond to real-time deadlines (e.g., military applications monitoring positions of enemy platforms) and therefore must often provide reasonable approximations to queries. o A traditional query processor optimizes all queries in the same way (typically focusing on response time). A stream data manager benefits from application specific optimization criteria (QoS). o A traditional DBMS assumes pull-based queries to be the norm. Push-based data processing is the norm for a data stream management system. A Brief Summary of Aurora Aurora has been designed to deal with very large numbers of data streams. Users build queries out of a small set of operators (a.k.a. boxes). The current implementation provides a user interface for tapping into pre-existing inputs and network flows and for wiring boxes together to produces answers at the outputs. While it is certainly possible to accept input as declarative queries, we feel that for a very large number of such queries, the process of common sub-expression elimination is too difficult. An example of an Aurora network is given in Screen Shot 1. A simple stream is a potentially infinite sequence of tuples that all have the same stream ID. An arc carries multiple simple streams. This is important so that simple streams can be added and deleted from the system without having to modify the basic network. A query, then, is a sub-network that ends at a single output and includes an arbitrary number of inputs. Boxes can connect to multiple downstream boxes. All such path splits carry identical tuples. Multiple streams can be merged since some box types accept more than one input (e.g., Join, Union). We do not allow any cycles in an operator network. Each output is supplied with a Quality of Service (QoS) specification. Currently, QoS is captured by three functions (1) a latency graph, (2) a value-based graph, and (3) a loss-tolerance graph. The latency graph indicates how utility drops as an answer is delayed. The value-based graph shows which values of the output space are most important. The loss-tolerance graph is a simple way to describe how averse the application is to approximate answers. Tuples arrive at the input and are queued for processing. A scheduler selects a box with waiting tuples and executes that box on one or more of the input tuples. The output tuples of a box are queued at the input of the next box in sequence. In this way, tuples make their way from the inputs to the outputs. If the system is overloaded, QoS is adversely affected. In this case, we invoke a load shedder to strategically eliminate Aurora supports persistent storage in two different ways. First, when box queues consume more storage than available RAM, the system will spill tuples that are less likely to be needed soon to secondary storage. Second, ad hoc queries can be connected to (and disconnected from) any arc for which a connection point has been defined. A connection point stores a historical portion of a stream that has flowed on the arc. For example, one could define a connection point as the last hour\u2019s worth of data that has been seen on a given arc. Any ad hoc query that connects to a connection point has access to the full stored history as well as any additional data that flows past while the query is connected.", "title": "Aurora: A Data Stream Management System"}, "02bb762c3bd1b3d1ad788340d8e9cdc3d85f33e1": {"paper_id": "02bb762c3bd1b3d1ad788340d8e9cdc3d85f33e1", "abstract": "We describe a family of caching protocols for distrib-uted networks that can be used to decrease or eliminate the occurrence of hot spots in the network. Our protocols are particularly designed for use with very large networks such as the Internet, where delays caused by hot spots can be severe, and where it is not feasible for every server to have complete information about the current state of the entire network. The protocols are easy to implement using existing network protocols such as TCP/fF\u2019,and require very little overhead. The protocols work with local control, make efficient use of existing resources, and scale gracefully as the network grows. Our caching protocols are based on a special kind of hashing that we call consistent hashing. Roughly speaking, a consistent hash function is one which changes nr.inimaflyas the range of the function changes. Through the development of good consistent hash functions, we are able to develop caching protocols which do not require users to have a current or even consistent view of the network. We believe that consistent hash functions may eventually prove to be useful in other applications such as distributed name servers and/or quorum systems.", "title": "Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web"}, "72417ba72a69b9d2e84fb4228a6398c79a16e11a": {"paper_id": "72417ba72a69b9d2e84fb4228a6398c79a16e11a", "abstract": "This paper examines what motivates employees in the retail industry, and examines their level of job satisfaction, using Herzberg's hygiene factors and motivators. In this study, convenience sampling was used to select sales personnel from women's clothing stores in Bandar Sunway shopping mall in the state of Selangor. The results show that hygiene factors were the dominant motivators of sales personnel job satisfaction. Working conditions were the most significant in motivating sales personnel. Recognition was second, followed by company policy and salary. There is a need to delve more deeply into why salespeople place such a high importance on money. Further analysis was performed to assess how much the love of money mediates the relationship between salary and job satisfaction. Based on the general test for mediation, the love of money could explain the relationship between salary and job satisfaction. The main implication of this study is that sales personnel who value money highly are satisfied with their salary and job when they receive a raise.", "title": "HERZBERG ' S MOTIVATION-HYGIENE THEORY AND JOB SATISFACTION IN THE MALAYSIAN RETAIL SECTOR : THE MEDIATING EFFECT OF LOVE OF MONEY"}, "f533d701baf8e7072cd5fd825b5d70180aa8c967": {"paper_id": "f533d701baf8e7072cd5fd825b5d70180aa8c967", "abstract": "Factor analyses of 75 facet scales from 2 major Big Five inventories, in the Eugene-Springfield community sample (N=481), produced a 2-factor solution for the 15 facets in each domain. These findings indicate the existence of 2 distinct (but correlated) aspects within each of the Big Five, representing an intermediate level of personality structure between facets and domains. The authors characterized these factors in detail at the item level by correlating factor scores with the International Personality Item Pool (L. R. Goldberg, 1999). These correlations allowed the construction of a 100-item measure of the 10 factors (the Big Five Aspect Scales [BFAS]), which was validated in a 2nd sample (N=480). Finally, the authors examined the correlations of the 10 factors with scores derived from 10 genetic factors that a previous study identified underlying the shared variance among the Revised NEO Personality Inventory facets (K. L. Jang et al., 2002). The correspondence was strong enough to suggest that the 10 aspects of the Big Five may have distinct biological substrates.", "title": "Between facets and domains: 10 aspects of the Big Five."}, "207c650e37f7ebc299b283eadbf01d4bf367f74b": {"paper_id": "207c650e37f7ebc299b283eadbf01d4bf367f74b", "abstract": "Heterogeneous computer architectures, where CPUs co-exist with accelerators such as vector coprocessors, GPUs and FPGAs, are rapidly evolving to be powerful platforms for tomorrow's exa-scale computing. The Intel\u00ae Many Integrated Core (MIC) architecture is Intel's first step towards heterogeneous computing. This paper investigates the performance of the MIC platform in the context of medical imaging and signal processing. Specifically, we analyze the achieved performance of two popular algorithms: Complex Finite Impulse Response (FIR) filtering which is used in ultrasound signal processing and Simultaneous Algebraic Reconstruction Technique (SART) which is used in 3D Computed tomography (CT) volume reconstruction. These algorithms are evaluated on Intel\u00ae Xeon Phi\u2122 using Intel's heterogeneous offload model. Our analysis indicates that execution times of both of these algorithms are dominated by the memory access times and hence effective cache utilization as well as vectorization play a significant role in determining the achieved performance. Overall, we perceive that Intel\u00ae MIC is an easy-to-program accelerator of the future that shows good potential in terms of performance.", "title": "Performance evaluation of medical imaging algorithms on Intel\u00ae MIC platform"}, "45590b1b07fc3a96d3b74fad1f8e91aea6fdb298": {"paper_id": "45590b1b07fc3a96d3b74fad1f8e91aea6fdb298", "abstract": "Michael Weiss Compass, Inc. 550 Edgewater Dr. Wakefield, MA A compiler for vector processors must strip mine statements to fit the vector register length. The same compiler technology can be applied to SIMD machines, removing the need for a virtual processor mechanism and offering significant advantages over that approach. The different memory organizations of SIMD and vector machines give rise to strip mining differences. SIMD interprocessor communication must be stripped. Array allocation gives SIMD strip mining a global aspect. SIMD loads and stores maybe uniform or ragged. These all contribute to the complexity of the SIMD strip mining problem, and provide opportunities for a compiler to make use of its wider view of the code.", "title": "Strip mining on SIMD architectures"}, "56a3fc8fc92cd363b008110ec15a24a3394e1d5e": {"paper_id": "56a3fc8fc92cd363b008110ec15a24a3394e1d5e", "abstract": "Data-parallel languages like Fortran 90 express parallelism in the form of operations on data aggregates such as arrays. Misalignment of the operands of an array operation can reduce program performance on a distributed-memory parallel machine by requiring nonlocal data accesses. Determining array alignments that reduce communication is therefore a key issue in compiling such languages.\nWe present a framework for the automatic determination of array alignments in data-parallel languages such as Fortran 90. Our language model handles array sectioning, reductions, spreads, transpositions, and masked operations. We decompose alignment functions into three constituents: axis, stride, and offset. For each of these subproblems, we show how to solve the alignment problem for a basic block of code, possibly containing common subexpressions. Alignments are generated for all array objects in the code, both named program variables and intermediate results. The alignments obtained by our algorithms are more general than those provided by the \u201cowner-computes\u201d rule. Finally, we present some ideas for dealing with control flow, replication, and dynamic alignments that depend on loop induction variables.", "title": "Automatic Array Alignment in Data-Parallel Programs"}, "8bc8d89aeaaed3b8815f8f9273e71d5070278a26": {"paper_id": "8bc8d89aeaaed3b8815f8f9273e71d5070278a26", "abstract": null, "title": "Iterative methods for the three-dimensional reconstruction of an object from projections."}, "0c5d343a758e6e5a4e94f1c0bc8d95c6bda3e530": {"paper_id": "0c5d343a758e6e5a4e94f1c0bc8d95c6bda3e530", "abstract": "OBJECTIVES\nTo compare the effectiveness of a mixture of acacia fiber, psyllium fiber, and fructose (AFPFF) with polyethylene glycol 3350 combined with electrolytes (PEG+E) in the treatment of children with chronic functional constipation (CFC); and to evaluate the safety and effectiveness of AFPFF in the treatment of children with CFC.\n\n\nSTUDY DESIGN\nThis was a randomized, open label, prospective, controlled, parallel-group study involving 100 children (M/F: 38/62; mean age \u00b1 SD: 6.5 \u00b1 2.7 years) who were diagnosed with CFC according to the Rome III Criteria. Children were randomly divided into 2 groups: 50 children received AFPFF (16.8 g daily) and 50 children received PEG+E (0.5 g/kg daily) for 8 weeks. Primary outcome measures were frequency of bowel movements, stool consistency, fecal incontinence, and improvement of other associated gastrointestinal symptoms. Safety was assessed with evaluation of clinical adverse effects and growth measurements.\n\n\nRESULTS\nCompliance rates were 72% for AFPFF and 96% for PEG+E. A significant improvement of constipation was seen in both groups. After 8 weeks, 77.8% of children treated with AFPFF and 83% of children treated with PEG+E had improved (P = .788). Neither PEG+E nor AFPFF caused any clinically significant side effects during the entire course of the study period.\n\n\nCONCLUSIONS\nIn this randomized study, we did not find any significant difference between the efficacy of AFPFF and PEG+E in the treatment of children with CFC. Both medications were proved to be safe for CFC treatment, but PEG+E was better accepted by children.", "title": "A randomized, prospective, comparison study of a mixture of acacia fiber, psyllium fiber, and fructose vs polyethylene glycol 3350 with electrolytes for the treatment of chronic functional constipation in childhood."}, "308a57d3339e2297cb3709360781f7cff8459ee3": {"paper_id": "308a57d3339e2297cb3709360781f7cff8459ee3", "abstract": "This tutorial introduces fingerprint recognition systems and their main components: sensing, feature extraction and matching. The basic technologies are surveyed and some state-of-the-art algorithms are discussed. Due to the extent of this topic it is not possible to provide here all the details and to cover a number of interesting issues such as classification, indexing and multimodal systems. Interested readers can find in [21] a complete and comprehensive guide to fingerprint recognition.", "title": "A Tutorial on Fingerprint Recognition"}, "67301c286439a7c24368300ea13e9785bd666aed": {"paper_id": "67301c286439a7c24368300ea13e9785bd666aed", "abstract": "A method is described which quantifies the speed and direction of several moving objects in a sequence of digital images. A relationship between the time variation of intensity, the spatial gradient, and velocity has been developed which allows the determination of motion using clustering techniques. This paper describes these relationships, the clustering technique, and provides examples of the technique on real images containing several moving objects.", "title": "Velocity determination in scenes containing several moving objects"}, "581e66c12e5cd92c1c006828caf54340ce0508df": {"paper_id": "581e66c12e5cd92c1c006828caf54340ce0508df", "abstract": "This work complements our previous efforts in generating realistic fingerprint images for test purposes. The main variability which characterizes the acquisition of a fingerprint through an on-line sensor is modeled and a sequence of steps is defined to derive a series of impressions from the same master-fingerprint. This allows large fingerprint databases to be randomly generated according to some given parameters. The experimental results validate our technique and prove that it can be very useful for performance evaluation, learning and testing in fingerprint-based systems.", "title": "Synthetic Fingerprint-Database Generation"}, "1f8c88e2f5d33d73d2857fb1727b1845ad3a32be": {"paper_id": "1f8c88e2f5d33d73d2857fb1727b1845ad3a32be", "abstract": "Music perception is highly intertwined with both emotions and context. Not surprisingly, many of the users\u2019 information seeking actions aim at retrieving music songs based on these perceptual dimensions \u2013 moods and themes, expressing how people feel about music or which situations they associate it with. In order to successfully support music retrieval along these dimensions, powerful methods are needed. Still, most existing approaches aiming at inferring some of the songs\u2019 latent characteristics focus on identifying musical genres. In this paper we aim at bridging this gap between users\u2019 information needs and indexed music features by developing algorithms for classifying music songs by moods and themes. We extend existing approaches by also considering the songs\u2019 thematic dimensions and by using social data from the Last.fm music portal, as support for the classification tasks. Our methods exploit both audio features and collaborative user annotations, fusing them to improve overall performance. Evaluation performed against the AllMusic.com ground truth shows that both kinds of information are complementary and should be merged for enhanced classification accuracy.", "title": "Music Mood and Theme Classification - a Hybrid Approach"}, "7e7a9899a839eda239ebb001bc6b45002fc1a2b7": {"paper_id": "7e7a9899a839eda239ebb001bc6b45002fc1a2b7", "abstract": "This paper reviews the state-of-the-art in automatic genre classification of music collections through three main paradigms: expert systems, unsupervised classification, and supervised classification. The paper discusses the importance of music genres with their definitions and hierarchies. It also presents techniques to extract meaningful information from audio data to characterize musical excerpts. The paper also presents the results of new emerging research fields and techniques that investigate the proximity of music genres", "title": "Automatic genre classification of music content: a survey"}, "f1161cd952f7c52266d0761ab4973c1c134b4962": {"paper_id": "f1161cd952f7c52266d0761ab4973c1c134b4962", "abstract": "Algorithms which construct classifiers from sample data -such as neural networks, radial basis functions, and decision trees -have attracted growing attention for their wide applicability. Researchers in the fields of Statistics, Artificial Intelligence, Machine Learning, Data Mining, and Pattern Recognition are continually introducing (or rediscovering) induction methods, and often publishing implementing code. It is natural for practitioners and potential users to wonder, \"Which classification technique is best?\", or more realistically, \"What subset of methods tend to work well for a given type of dataset?\". This book provides perhaps the best current answer to that question.", "title": "Machine Learning, Neural and Statistical Classification"}, "2a7442d3e699515bf6c29a876cc714ae77faf6bd": {"paper_id": "2a7442d3e699515bf6c29a876cc714ae77faf6bd", "abstract": "We discuss a strategy for polychotomous classification that involves estimating class probabilities for each pair of classes, and then coupling the estimates together. The coupling model is similar to the Bradley-Terry method for paired comparisons. We study the nature of the class probability estimates that arise, and examine the performance of the procedure in simulated datasets. The classifiers used include linear discriminants and nearest neighbors: application to support vector machines is also briefly described.", "title": "Classification by Pairwise Coupling"}, "0cf464e413e1e051b535d4d42de4ecd49853053e": {"paper_id": "0cf464e413e1e051b535d4d42de4ecd49853053e", "abstract": "We present a computer audition system that can both annotate novel audio tracks with semantically meaningful words and retrieve relevant tracks from a database of unlabeled audio content given a text-based query. We consider the related tasks of content-based audio annotation and retrieval as one supervised multiclass, multilabel problem in which we model the joint probability of acoustic features and words. We collect a data set of 1700 human-generated annotations that describe 500 Western popular music tracks. For each word in a vocabulary, we use this data to train a Gaussian mixture model (GMM) over an audio feature space. We estimate the parameters of the model using the weighted mixture hierarchies expectation maximization algorithm. This algorithm is more scalable to large data sets and produces better density estimates than standard parameter estimation techniques. The quality of the music annotations produced by our system is comparable with the performance of humans on the same task. Our ldquoquery-by-textrdquo system can retrieve appropriate songs for a large number of musically relevant words. We also show that our audition system is general by learning a model that can annotate and retrieve sound effects.", "title": "Semantic Annotation and Retrieval of Music and Sound Effects"}, "95dcdc9f2de1f30a4f0c79c378f98f11aa618f40": {"paper_id": "95dcdc9f2de1f30a4f0c79c378f98f11aa618f40", "abstract": "Content-based music genre classification is a fundamental component of music information retrieval systems and has been gaining importance and enjoying a growing amount of attention with the emergence of digital music on the Internet. Currently little work has been done on automatic music genre classification, and in addition, the reported classification accuracies are relatively low. This paper proposes a new feature extraction method for music genre classification, DWCHs. DWCHs stands for Daubechies Wavelet Coefficient Histograms. DWCHs capture the local and global information of music signals simultaneously by computing histograms on their Daubechies wavelet coefficients. Effectiveness of this new feature and of previously studied features are compared using various machine learning classification algorithms, including Support Vector Machines and Linear Discriminant Analysis. It is demonstrated that the use of DWCHs significantly improves the accuracy of music genre classification.", "title": "A comparative study on content-based music genre classification"}, "300ce829f75184b656540857acaf6c9ed3c68bea": {"paper_id": "300ce829f75184b656540857acaf6c9ed3c68bea", "abstract": "Social tags are user-generated keywords associated with some resource on the Web. In the case of music, social tags have become an important component of \u201cWeb2.0\u201d recommender systems, allowing users to generate playlists based on use-dependent terms such as chill or jogging that have been applied to particular songs. In this paper, we propose a method for predicting these social tags directly from MP3 files. Using a set of boosted classifiers, we map audio features onto social tags collected from the Web. The resulting automatic tags (or autotags) furnish information about music that is otherwise untagged or poorly tagged, allowing for insertion of previously unheard music into a social recommender. This avoids the \u201dcold-start problem\u201d common in such systems. Autotags can also be used to smooth the tag space from which similarities and recommendations are made by providing a set of comparable baseline tags for all tracks in a recommender system.", "title": "Automatic Generation of Social Tags for Music Recommendation"}, "196523c04f9e845cc6dde43f50e40d38909ffafd": {"paper_id": "196523c04f9e845cc6dde43f50e40d38909ffafd", "abstract": "Automated collaborative filtering (ACF) systems predict a person's affinity for items or information by connecting that person's recorded interests with the recorded interests of a community of people and sharing ratings between like-minded persons. However, current recommender systems are black boxes, providing no transparency into the working of the recommendation. Explanations provide that transparency, exposing the reasoning and data behind a recommendation. In this paper, we address explanation interfaces for ACF systems - how they should be implemented and why they should be implemented. To explore how, we present a model for explanations based on the user's conceptual model of the recommendation process. We then present experimental results demonstrating what components of an explanation are the most compelling. To address why, we present experimental evidence that shows that providing explanations can improve the acceptance of ACF systems. We also describe some initial explorations into measuring how explanations can improve the filtering performance of users.", "title": "Explaining collaborative filtering recommendations"}, "1e87631bef617e3696e8b5bfc90ae98cf0f67bdf": {"paper_id": "1e87631bef617e3696e8b5bfc90ae98cf0f67bdf", "abstract": "Deep image translation methods have recently shown excellent results, outputting high-quality images covering multiple modes of the data distribution. There has also been increased interest in disentangling the internal representations learned by deep methods to further improve their performance and achieve a finer control. In this paper, we bridge these two objectives and introduce the concept of crossdomain disentanglement. We aim to separate the internal representation into three parts. The shared part contains information for both domains. The exclusive parts, on the other hand, contain only factors of variation that are particular to each domain. We achieve this through bidirectional image translation based on Generative Adversarial Networks and cross-domain autoencoders, a novel network component. Our model offers multiple advantages. We can output diverse samples covering multiple modes of the distributions of both domains, perform domainspecific image transfer and interpolation, and cross-domain retrieval without the need of labeled data, only paired images. We compare our model to the state-ofthe-art in multi-modal image translation and achieve better results for translation on challenging datasets as well as for cross-domain retrieval on realistic datasets.", "title": "Image-to-image translation for cross-domain disentanglement"}, "d59101076dd96b6b39f58c3d3e3747e5cf5f98ab": {"paper_id": "d59101076dd96b6b39f58c3d3e3747e5cf5f98ab", "abstract": "Amazon Mechanical Turk (AMT) is an online crowdsourcing service where anonymous online workers complete web-based tasks for small sums of money. The service has attracted attention from experimental psychologists interested in gathering human subject data more efficiently. However, relative to traditional laboratory studies, many aspects of the testing environment are not under the experimenter's control. In this paper, we attempt to empirically evaluate the fidelity of the AMT system for use in cognitive behavioral experiments. These types of experiment differ from simple surveys in that they require multiple trials, sustained attention from participants, comprehension of complex instructions, and millisecond accuracy for response recording and stimulus presentation. We replicate a diverse body of tasks from experimental psychology including the Stroop, Switching, Flanker, Simon, Posner Cuing, attentional blink, subliminal priming, and category learning tasks using participants recruited using AMT. While most of replications were qualitatively successful and validated the approach of collecting data anonymously online using a web-browser, others revealed disparity between laboratory results and online results. A number of important lessons were encountered in the process of conducting these replications that should be of value to other researchers.", "title": "Evaluating Amazon's Mechanical Turk as a Tool for Experimental Behavioral Research"}, "a29f2bd2305e11d8fe139444e733e9b50ea210d6": {"paper_id": "a29f2bd2305e11d8fe139444e733e9b50ea210d6", "abstract": "This paper introduces a novel large dataset for example-based single image super-resolution and studies the state-of-the-art as emerged from the NTIRE 2017 challenge. The challenge is the first challenge of its kind, with 6 competitions, hundreds of participants and tens of proposed solutions. Our newly collected DIVerse 2K resolution image dataset (DIV2K) was employed by the challenge. In our study we compare the solutions from the challenge to a set of representative methods from the literature and evaluate them using diverse measures on our proposed DIV2K dataset. Moreover, we conduct a number of experiments and draw conclusions on several topics of interest. We conclude that the NTIRE 2017 challenge pushes the state-of-the-art in single-image super-resolution, reaching the best results to date on the popular Set5, Set14, B100, Urban100 datasets and on our newly proposed DIV2K.", "title": "NTIRE 2017 Challenge on Single Image Super-Resolution: Dataset and Study"}, "6c11626ae08706e6185fceff0a6d05e4bfd6bd06": {"paper_id": "6c11626ae08706e6185fceff0a6d05e4bfd6bd06", "abstract": "This is a review of unsupervised learning applied to videos with the aim of learning visual representations. We look at different realizations of the notion of temporal coherence across various models. We try to understand the challenges being faced, the strengths and weaknesses of different approaches and identify directions for future work. Unsupervised Learning of Visual Representations using Videos Nitish Srivastava Department of Computer Science, University of Toronto", "title": "Unsupervised Learning of Visual Representations using Videos"}, "39c3ba33f6ec8f9de36f0a8461c5f78e4f911d98": {"paper_id": "39c3ba33f6ec8f9de36f0a8461c5f78e4f911d98", "abstract": "We present a highly accurate single-image superresolution (SR) method. Our method uses a very deep convolutional network inspired by VGG-net used for ImageNet classification [19]. We find increasing our network depth shows a significant improvement in accuracy. Our final model uses 20 weight layers. By cascading small filters many times in a deep network structure, contextual information over large image regions is exploited in an efficient way. With very deep networks, however, convergence speed becomes a critical issue during training. We propose a simple yet effective training procedure. We learn residuals only and use extremely high learning rates (104 times higher than SRCNN [6]) enabled by adjustable gradient clipping. Our proposed method performs better than existing methods in accuracy and visual improvements in our results are easily noticeable.", "title": "Accurate Image Super-Resolution Using Very Deep Convolutional Networks"}, "5943a8e565009d04203704dc2fb08b3d338faa12": {"paper_id": "5943a8e565009d04203704dc2fb08b3d338faa12", "abstract": "Objects make distinctive sounds when they are hit or scratched. These sounds reveal aspects of an object's material properties, as well as the actions that produced them. In this paper, we propose the task of predicting what sound an object makes when struck as a way of studying physical interactions within a visual scene. We present an algorithm that synthesizes sound from silent videos of people hitting and scratching objects with a drumstick. This algorithm uses a recurrent neural network to predict sound features from videos and then produces a waveform from these features with an example-based synthesis procedure. We show that the sounds predicted by our model are realistic enough to fool participants in a \"real or fake\" psychophysical experiment, and that they convey significant information about material properties and physical interactions.", "title": "Visually Indicated Sounds"}, "6d07e309d74c91e5f95783c84555d9bdaacbf304": {"paper_id": "6d07e309d74c91e5f95783c84555d9bdaacbf304", "abstract": "VGGNets have turned out to be effective for object recognition in still images. However, it is unable to yield good performance by directly adapting the VGGNet models trained on the ImageNet dataset for scene recognition. This report describes our implementation of training the VGGNets on the large-scale Places205 dataset. Specifically, we train three VGGNet models, namely VGGNet-11, VGGNet-13, and VGGNet-16, by using a Multi-GPU extension of Caffe toolbox with high computational efficiency. We verify the performance of trained Places205-VGGNet models on three datasets: MIT67, SUN397, and Places205. Our trained models achieve the state-of-the-art performance o n these datasets and are made public available 1.", "title": "Places205-VGGNet Models for Scene Recognition"}, "1ede6ed261ef497046446f7a63d0fc08deba16fc": {"paper_id": "1ede6ed261ef497046446f7a63d0fc08deba16fc", "abstract": null, "title": "Colorization using optimization"}, "1d827e24143e5fdfe709d33b7b13a9a24d402efd": {"paper_id": "1d827e24143e5fdfe709d33b7b13a9a24d402efd", "abstract": "[1] L. Fei-Fei, R. Fergus, and P. Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. Computer Vision and Image Understanding, 2007. [2] G. Griffin, A. Holub, and P. Perona. Caltech-256 object category dataset. 2007. [3] S. Lazebnik, C. Schmid, and J. Ponce. Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories. In Proc. CVPR, 2006. [4] L.-J. Li and L. Fei-Fei. What, where and who? classifying events by scene and object recognition. In Proc. ICCV, 2007. [5] G. Patterson and J. Hays. Sun attribute database: Discovering, annotating, and recognizing scene attributes. In Proc. CVPR, 2012.", "title": "Learning Deep Features for Scene Recognition using Places Database"}, "0f84a81f431b18a78bd97f59ed4b9d8eda390970": {"paper_id": "0f84a81f431b18a78bd97f59ed4b9d8eda390970", "abstract": "Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding \u2013 and building on other recent work for finding simple network structures \u2013 we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the \u201cdeconvolution approach\u201d for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.", "title": "Striving for Simplicity: The All Convolutional Net"}, "9201bf6f8222c2335913002e13fbac640fc0f4ec": {"paper_id": "9201bf6f8222c2335913002e13fbac640fc0f4ec", "abstract": null, "title": "Fully convolutional networks for semantic segmentation"}, "03dbc94b54c85cc34815df629fb508c6729e6eab": {"paper_id": "03dbc94b54c85cc34815df629fb508c6729e6eab", "abstract": "While there has been remarkable progress in the performance of visual recognition algorithms, the state-of-the-art models tend to be exceptionally data-hungry. Large labeled training datasets, expensive and tedious to produce, are required to optimize millions of parameters in deep network models. Lagging behind the growth in model capacity, the available datasets are quickly becoming outdated in terms of size and density. To circumvent this bottleneck, we propose to amplify human effort through a partially automated labeling scheme, leveraging deep learning with humans in the loop. Starting from a large set of candidate images for each category, we iteratively sample a subset, ask people to label them, classify the others with a trained model, split the set into positives, negatives, and unlabeled based on the classification confidence, and then iterate with the unlabeled set. To assess the effectiveness of this cascading procedure and enable further progress in visual recognition research, we construct a new image dataset, LSUN. It contains around one million labeled images for each of 10 scene categories and 20 object categories. We experiment with training popular convolutional networks and find that they achieve substantial performance gains when trained on this dataset.", "title": "LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop"}, "0e4a97a0ccbf699272e3d6dc25b6fe16eb35382d": {"paper_id": "0e4a97a0ccbf699272e3d6dc25b6fe16eb35382d", "abstract": "Modern applications and progress in deep learning research have created renewed interest for generative models of text and of images. However, even today it is unclear what objective functions one should use to train and evaluate these models. In this paper we present two contributions. Firstly, we present a critique of scheduled sampling, a state-of-the-art training method that contributed to the winning entry to the MSCOCO image captioning benchmark in 2015. Here we show that despite this impressive empirical performance, the objective function underlying scheduled sampling is improper and leads to an inconsistent learning algorithm. Secondly, we revisit the problems that scheduled sampling was meant to address, and present an alternative interpretation. We argue that maximum likelihood is an inappropriate training objective when the end-goal is to generate natural-looking samples. We go on to derive an ideal objective function to use in this situation instead. We introduce a generalisation of adversarial training, and show how such method can interpolate between maximum likelihood training and our ideal training objective. To our knowledge this is the first theoretical analysis that explains why adversarial training tends to produce samples with higher perceived quality.", "title": "How (not) to Train your Generative Model: Scheduled Sampling, Likelihood, Adversary?"}, "9a700c7a7e7468e436f00c34551fbe3e0f70e42f": {"paper_id": "9a700c7a7e7468e436f00c34551fbe3e0f70e42f", "abstract": "The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of generative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first section introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a practical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.", "title": "Towards Principled Methods for Training Generative Adversarial Networks"}, "f9c431f58565f874f76a024add2aa80717ec5cf5": {"paper_id": "f9c431f58565f874f76a024add2aa80717ec5cf5", "abstract": "We propose a semi-supervised approach to solve the task of emotion recognition in 2D face images using recent ideas in deep learning for handling the factors of variation present in data. An emotion classification algorithm should be both robust to (1) remaining variations due to the pose of the face in the image after centering and alignment, (2) the identity or morphology of the face. In order to achieve this invariance, we propose to learn a hierarchy of features in which we gradually filter the factors of variation arising from both (1) and (2). We address (1) by using a multi-scale contractive convolutional network (CCNET) in order to obtain invariance to translations of the facial traits in the image. Using the feature representation produced by the CCNET, we train a Contractive Discriminative Analysis (CDA) feature extractor, a novel variant of the Contractive Auto-Encoder (CAE), designed to learn a representation separating out the emotion-related factors from the others (which mostly capture the subject identity, and what is left of pose after the CCNET). This system beats the state-of-the-art on a recently proposed dataset for facial expression recognition, the Toronto Face Database, moving the state-of-art accuracy from 82.4% to 85.0%, while the CCNET and CDA improve accuracy of a standard CAE by 8%.", "title": "Disentangling Factors of Variation for Facial Expression Recognition"}, "4030a62e75313110dc4a4c78483f4459dc4526bc": {"paper_id": "4030a62e75313110dc4a4c78483f4459dc4526bc", "abstract": "We describe the neural-network training framework used in the Kaldi speech recognition toolkit, which is geared towards training DNNs with large amounts of training data using multiple GPU-equipped or multicore machines. In order to be as hardwareagnostic as possible, we needed a way to use multiple machines without generating excessive network traffic. Our method is to average the neural network parameters periodically (typically every minute or two), and redistribute the averaged parameters to the machines for further training. Each machine sees different data. By itself, this method does not work very well. However, we have another method, an approximate and efficient implementation of Natural Gradient for Stochastic Gradient Descent (NG-SGD), which seems to allow our periodic-averaging method to work well, as well as substantially improving the convergence of SGD on a single machine.", "title": "Parallel training of Deep Neural Networks with Natural Gradient and Parameter Averaging"}, "a538b05ebb01a40323997629e171c91aa28b8e2f": {"paper_id": "a538b05ebb01a40323997629e171c91aa28b8e2f", "abstract": "Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these \u201cStepped Sigmoid Units\u201d are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.", "title": "Rectified Linear Units Improve Restricted Boltzmann Machines"}, "085ceda1c65caf11762b3452f87660703f914782": {"paper_id": "085ceda1c65caf11762b3452f87660703f914782", "abstract": "Large-pose face alignment is a very challenging problem in computer vision, which is used as a prerequisite for many important vision tasks, e.g, face recognition and 3D face reconstruction. Recently, there have been a few attempts to solve this problem, but still more research is needed to achieve highly accurate results. In this paper, we propose a face alignment method for large-pose face images, by combining the powerful cascaded CNN regressor method and 3DMM. We formulate the face alignment as a 3DMM fitting problem, where the camera projection matrix and 3D shape parameters are estimated by a cascade of CNN-based regressors. The dense 3D shape allows us to design pose-invariant appearance features for effective CNN learning. Extensive experiments are conducted on the challenging databases (AFLW and AFW), with comparison to the state of the art.", "title": "Large-Pose Face Alignment via CNN-Based Dense 3D Model Fitting"}, "468bde5efbad7b1bfc7b0c85ac8dfd50c1c21e04": {"paper_id": "468bde5efbad7b1bfc7b0c85ac8dfd50c1c21e04", "abstract": "The GANs are generative models whose random samples realistically reflect natural images. It also can generate samples with specific attributes by concatenating a condition vector into the input, yet research on this field is not well studied. We propose novel methods of conditioning generative adversarial networks (GANs) that achieve state-of-the-art results on MNIST and CIFAR-10. We mainly introduce two models: an information retrieving model that extracts conditional information from the samples, and a spatial bilinear pooling model that forms bilinear features derived from the spatial cross product of an image and a condition vector. These methods significantly enhance log-likelihood of test data under the conditional distributions compared to the methods of concatenation.", "title": "Ways of Conditioning Generative Adversarial Networks"}, "788a7b59ea72e23ef4f86dc9abb4450efefeca41": {"paper_id": "788a7b59ea72e23ef4f86dc9abb4450efefeca41", "abstract": "Recently, it has been shown that excellent results can be achieved in both facial landmark localization and pose-invariant face recognition. These breakthroughs are attributed to the efforts of the community to manually annotate facial images in many different poses and to collect 3D facial data. In this paper, we propose a novel method for joint frontal view reconstruction and landmark localization using a small set of frontal images only. By observing that the frontal facial image is the one having the minimum rank of all different poses, an appropriate model which is able to jointly recover the frontalized version of the face as well as the facial landmarks is devised. To this end, a suitable optimization problem, involving the minimization of the nuclear norm and the matrix l1 norm is solved. The proposed method is assessed in frontal face reconstruction, face landmark localization, pose-invariant face recognition, and face verification in unconstrained conditions. The relevant experiments have been conducted on 8 databases. The experimental results demonstrate the effectiveness of the proposed method in comparison to the state-of-the-art methods for the target problems.", "title": "Robust Statistical Face Frontalization"}, "543f21d81bbea89f901dfcc01f4e332a9af6682d": {"paper_id": "543f21d81bbea89f901dfcc01f4e332a9af6682d", "abstract": "In this paper we present a method for learning a discriminative classifier from unlabeled or partially labeled data. Our approach is based on an objective function that trades-off mutual information between observed examples and their predicted categorical class distribution, against robustness of the classifier to an adversarial generative model. The resulting algorithm can either be interpreted as a natural generalization of the generative adversarial networks (GAN) framework or as an extension of the regularized information maximization (RIM) framework to robust classification against an optimal adversary. We empirically evaluate our method \u2013 which we dub categorical generative adversarial networks (or CatGAN) \u2013 on synthetic data as well as on challenging image classification tasks, demonstrating the robustness of the learned classifiers. We further qualitatively assess the fidelity of samples generated by the adversarial generator that is learned alongside the discriminative classifier, and identify links between the CatGAN objective and discriminative clustering algorithms (such as RIM).", "title": "Unsupervised and Semi-supervised Learning with Categorical Generative Adversarial Networks"}, "100105d6c97b23059f7aa70589ead2f61969fbc3": {"paper_id": "100105d6c97b23059f7aa70589ead2f61969fbc3", "abstract": "We have collected a new face data set that will facilitate research in the problem of frontal to profile face verification `in the wild'. The aim of this data set is to isolate the factor of pose variation in terms of extreme poses like profile, where many features are occluded, along with other `in the wild' variations. We call this data set the Celebrities in Frontal-Profile (CFP) data set. We find that human performance on Frontal-Profile verification in this data set is only slightly worse (94.57% accuracy) than that on Frontal-Frontal verification (96.24% accuracy). However we evaluated many state-of-the-art algorithms, including Fisher Vector, Sub-SML and a Deep learning algorithm. We observe that all of them degrade more than 10% from Frontal-Frontal to Frontal-Profile verification. The Deep learning implementation, which performs comparable to humans on Frontal-Frontal, performs significantly worse (84.91% accuracy) on Frontal-Profile. This suggests that there is a gap between human performance and automatic face recognition methods for large pose variation in unconstrained images.", "title": "Frontal to profile face verification in the wild"}, "64d5772f44efe32eb24c9968a3085bc0786bfca7": {"paper_id": "64d5772f44efe32eb24c9968a3085bc0786bfca7", "abstract": "Fully automatic Face Recognition Across Pose (FRAP) is one of the most desirable techniques, however, also one of the most challenging tasks in face recognition field. Matching a pair of face images in different poses can be converted into matching their pixels corresponding to the same semantic facial point. Following this idea, given two images G and P in different poses, we propose a novel method, named Morphable Displacement Field (MDF), to match G with P \u2019s virtual view under G\u2019s pose. By formulating MDF as a convex combination of a number of template displacement fields generated from a 3D face database, our model satisfies both global conformity and local consistency. We further present an approximate but effective solution of the proposed MDF model, named implicit Morphable Displacement Field (iMDF), which synthesizes virtual view implicitly via an MDF by minimizing matching residual. This formulation not only avoids intractable optimization of the high-dimensional displacement field but also facilitates a constrained quadratic optimization. The proposed method can work well even when only 2 facial landmarks are labeled, which makes it especially suitable for fully automatic FRAP system. Extensive evaluations on FERET, PIE and Multi-PIE databases show considerable improvement over state-ofthe-art FRAP algorithms in both semi-automatic and fully automatic evaluation protocols.", "title": "Morphable Displacement Field Based Image Matching for Face Recognition across Pose"}, "5d2a01e3a445a92ecdce5f20656fd87e65982708": {"paper_id": "5d2a01e3a445a92ecdce5f20656fd87e65982708", "abstract": "We propose an unsupervised method for learning multi-stage hierarchies of sparse convolutional features. While sparse coding has become an in creasingly popular method for learning visual features, it is most often traine d at the patch level. Applying the resulting filters convolutionally results in h ig ly redundant codes because overlapping patches are encoded in isolation. By tr aining convolutionally over large image windows, our method reduces the redudancy b etween feature vectors at neighboring locations and improves the efficienc y of the overall representation. In addition to a linear decoder that reconstruct s the image from sparse features, our method trains an efficient feed-forward encod er that predicts quasisparse features from the input. While patch-based training r arely produces anything but oriented edge detectors, we show that convolution al training produces highly diverse filters, including center-surround filters, corner detectors, cross detectors, and oriented grating detectors. We show that using these filters in multistage convolutional network architecture improves perfor mance on a number of visual recognition and detection tasks.", "title": "Learning Convolutional Feature Hierarchies for Visual Recognition"}, "0d735e7552af0d1dcd856a8740401916e54b7eee": {"paper_id": "0d735e7552af0d1dcd856a8740401916e54b7eee", "abstract": "There are two competing theories of facial expression recognition. Some researchers have suggested that it is an example of categorical perception. In this view, expression categories are considered to be discrete entities with sharp boundaries, and discrimination of nearby pairs of expressive faces is enhanced near those boundaries. Other researchers, however, suggest that facial expression perception is more graded and that facial expressions are best thought of as points in a continuous, low-dimensional space, where, for instance, surprise expressions lie between happiness and fear expressions due to their perceptual similarity. In this article, we show that a simple yet biologically plausible neural network model, trained to classify facial expressions into six basic emotions, predicts data used to support both of these theories. Without any parameter tuning, the model matches a variety of psychological data on categorization, similarity, reaction times, discrimination, and recognition difficulty, both qualitatively and quantitatively. We thus explain many of the seemingly complex psychological phenomena related to facial expression perception as natural consequences of the tasks' implementations in the brain.", "title": "EMPATH: A Neural Network that Categorizes Facial Expressions"}, "f3946643efd1d1827d4fcf0a19ac91a80014e5a2": {"paper_id": "f3946643efd1d1827d4fcf0a19ac91a80014e5a2", "abstract": "Partial least squares (PLS) was not originally designed as a tool for statistical discrimination. In spite of this, applied scientists routinely use PLS for classification and there is substantial empirical evidence to suggest that it performs well in that role. The interesting question is: why can a procedure that is principally designed for overdetermined regression problems locate and emphasize group structure? Using PLS in this manner has heurestic support owing to the relationship between PLS and canonical correlation analysis (CCA) and the relationship, in turn, between CCA and linear discriminant analysis (LDA). This paper replaces the heuristics with a formal statistical explanation. As a consequence, it will become clear that PLS is to be preferred over PCA when discrimination is the goal and dimension reduction is needed.", "title": "Partial Least Squares for Discrimination"}, "42f6f5454dda99d8989f9814989efd50fe807ee8": {"paper_id": "42f6f5454dda99d8989f9814989efd50fe807ee8", "abstract": "We apply an extension of generative adversarial networks (GANs) [8] to a conditional setting. In the GAN framework, a \u201cgenerator\u201d network is tasked with fooling a \u201cdiscriminator\u201d network into believing that its own samples are real data. We add the capability for each network to condition on some arbitrary external data which describes the image being generated or discriminated. By varying the conditional information provided to this extended GAN, we can use the resulting generative model to generate faces with specific attributes from nothing but random noise. We evaluate the likelihood of real-world faces under the generative model, and examine how to deterministically control face attributes by modifying the conditional information provided to the model.", "title": "Conditional generative adversarial nets for convolutional face generation"}, "5fb50e750f700f920f06b3982bd16ea920d11f68": {"paper_id": "5fb50e750f700f920f06b3982bd16ea920d11f68", "abstract": "Based on life-long observations of physical, chemical, and biologic phenomena in the natural world, humans can often easily picture in their minds what an object will look like in the future. But, what about computers? In this paper, we learn computational models of object transformations from time-lapse videos. In particular, we explore the use of generative models to create depictions of objects at future times. These models explore several different prediction tasks: generating a future state given a single depiction of an object, generating a future state given two depictions of an object at different times, and generating future states recursively in a recurrent framework. We provide both qualitative and quantitative evaluations of the generated results, and also conduct a human evaluation to compare variations of our models.", "title": "Learning Temporal Transformations From Time-Lapse Videos"}, "5c4d1c6ea78d0fe13e9fab182649131bb0cfe062": {"paper_id": "5c4d1c6ea78d0fe13e9fab182649131bb0cfe062", "abstract": "Interactive Image Generation User edits Generated images User edits Generated images User edits Generated images [1] Zhu et al. Learning a Discriminative Model for the Perception of Realism in Composite Images. ICCV 2015. [2] Goodfellow et al. Generative Adversarial Nets. NIPS 2014 [3] Radford et al. Unsupervised representation learning with deep convolutional generative adversarial networks. ICLR 2016 Reference : Natural images 0, I , Unif 1, 1", "title": "Generative Visual Manipulation on the Natural Image Manifold"}, "325d145af5f38943e469da6369ab26883a3fd69e": {"paper_id": "325d145af5f38943e469da6369ab26883a3fd69e", "abstract": "Given a grayscale photograph as input, this paper attacks the problem of hallucinating a plausible color version of the photograph. This problem is clearly underconstrained, so previous approaches have either relied on significant user interaction or resulted in desaturated colorizations. We propose a fully automatic approach that produces vibrant and realistic colorizations. We embrace the underlying uncertainty of the problem by posing it as a classification task and use class-rebalancing at training time to increase the diversity of colors in the result. The system is implemented as a feed-forward pass in a CNN at test time and is trained on over a million color images. We evaluate our algorithm using a \u201ccolorization Turing test,\u201d asking human participants to choose between a generated and ground truth color image. Our method successfully fools humans on 32% of the trials, significantly higher than previous methods. Moreover, we show that colorization can be a powerful pretext task for self-supervised feature learning, acting as a cross-channel encoder. This approach results in state-of-the-art performance on several feature learning benchmarks.", "title": "Colorful Image Colorization"}, "57bbbfea63019a57ef658a27622c357978400a50": {"paper_id": "57bbbfea63019a57ef658a27622c357978400a50", "abstract": null, "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation"}, "c6450f1a7c12a568348812f3c1152dd6b39394df": {"paper_id": "c6450f1a7c12a568348812f3c1152dd6b39394df", "abstract": "A small and single-layer crossed dipole antenna with two metal strips loaded for phase delay has been proposed to obtain CP radiation. The proposed antenna uses two unequal length dipoles connected in parallel to a coaxial probe feed and requires no matching network. The required power and phase relationships are obtained by proper choice of the two dipole lengths. Additionally, the antenna operates at its half wavelength orthogonal modes for the UHF band, and the two resonant modes for circular polarization radiation are generated by the phase delayed metal strip loaded. The measured impedance bandwidth for 10 dB return loss is 5.3%, ranging from 907 to 957 MHz, and the 3-dB axial-ratio bandwidth is about 12 MHz or 1.3% around the center frequency at 925 MHz.", "title": "Circularly polarized crossed dipole antennas for handheld RFID reader"}, "efe9f16b3072340981411a3fab9dd8886981f89c": {"paper_id": "efe9f16b3072340981411a3fab9dd8886981f89c", "abstract": "3-D printing is used to fabricate mold for micro-structuring the polydimethylsiloxane (PDMS) elastomeric dielectric layer in capacitive flexible pressure sensors. It is shown that, despite of the limited resolution of the used commercial 3-D printer for producing the mold, the fabricated sensor with the micro-structured PDMS layers can achieve sensitivity higher than previous work using micro-fabricated silicon wafer molds. The devices also present very low detection limit, fast response/recovery speed, excellent durability and good tolerance to variations of ambient temperature and humidity, which enables to reliably monitor weak human physiological signals in real time. As an application example, the flexible pressure sensor is integrated in a wearable system to monitor wrist pulse.", "title": "High Sensitivity Flexible Capacitive Pressure Sensor Using Polydimethylsiloxane Elastomer Dielectric Layer Micro-Structured by 3-D Printed Mold"}, "a7e194c7f18f2c4f83993207f737733ff31f03b1": {"paper_id": "a7e194c7f18f2c4f83993207f737733ff31f03b1", "abstract": "Most approaches to statistical stylometry have concentrated on lexical features, such as relative word frequencies or type-token ratios. Syntactic features have been largely ignored. This work attempts to fill that void by introducing a technique for authorship attribution based on dependency grammar. Syntactic features are extracted from texts using a common dependency parser, and those features are used to train a classifier to identify texts by author. While the method described does not outperform existing methods on most tasks, it does demonstrate that purely syntactic features carry information which could be useful for stylometric analysis. Index words: stylometry, authorship attribution, dependency grammar, machine learning Syntactic Stylometry: Using Sentence Structure for Authorship Attribution", "title": "Syntactic Stylometry: Using Sentence Structure for Authorship Attribution"}, "ababc1999b5f31409c78c39d1842219821e37a6d": {"paper_id": "ababc1999b5f31409c78c39d1842219821e37a6d", "abstract": "This paper illustrates a sentiment analysis approach to extract sentiments associated with polarities of positive or negative for specific subjects from a document, instead of classifying the whole document into positive or negative.The essential issues in sentiment analysis are to identify how sentiments are expressed in texts and whether the expressions indicate positive (favorable) or negative (unfavorable) opinions toward the subject. In order to improve the accuracy of the sentiment analysis, it is important to properly identify the semantic relationships between the sentiment expressions and the subject. By applying semantic analysis with a syntactic parser and sentiment lexicon, our prototype system achieved high precision (75-95%, depending on the data) in finding sentiments within Web pages and news articles.", "title": "Sentiment analysis: capturing favorability using natural language processing"}, "033b62167e7358c429738092109311af696e9137": {"paper_id": "033b62167e7358c429738092109311af696e9137", "abstract": "This paper presents a simple unsupervised learning algorithm for classifying reviews as recommended (thumbs up) or not recommended (thumbs down). The classification of a review is predicted by the average semantic orientation of the phrases in the review that contain adjectives or adverbs. A phrase has a positive semantic orientation when it has good associations (e.g., \u201csubtle nuances\u201d) and a negative semantic orientation when it has bad associations (e.g., \u201cvery cavalier\u201d). In this paper, the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word \u201cexcellent\u201d minus the mutual information between the given phrase and the word \u201cpoor\u201d. A review is classified as recommended if the average semantic orientation of its phrases is positive. The algorithm achieves an average accuracy of 74% when evaluated on 410 reviews from Epinions, sampled from four different domains (reviews of automobiles, banks, movies, and travel destinations). The accuracy ranges from 84% for automobile reviews to 66% for movie reviews.", "title": "Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews"}, "02880c9ac973046bf8d2fc802fb7ee4fc60c193b": {"paper_id": "02880c9ac973046bf8d2fc802fb7ee4fc60c193b", "abstract": "Opinion question answering is a challenging task for natural language processing. In this paper, we discuss a necessary component for an opinion question answering system: separating opinions from fact, at both the document and sentence level. We present a Bayesian classifier for discriminating between documents with a preponderance of opinions such as editorials from regular news stories, and describe three unsupervised, statistical techniques for the significantly harder task of detecting opinions at the sentence level. We also present a first model for classifying opinion sentences as positive or negative in terms of the main perspective being expressed in the opinion. Results from a large collection of news stories and a human evaluation of 400 sentences are reported, indicating that we achieve very high performance in document classification (upwards of 97% precision and recall), and respectable performance in detecting opinions and classifying them at the sentence level as positive, negative, or neutral (up to 91% accuracy).", "title": "Towards Answering Opinion Questions: Separating Facts from Opinions and Identifying the Polarity of Opinion Sentences"}, "0c97e8fcd80d9a3779826f2930724c9d789faa05": {"paper_id": "0c97e8fcd80d9a3779826f2930724c9d789faa05", "abstract": "In this paper, we describe an automated learning approach to text categorization based on perception learning and a new feature selection metric, called correlation coefficient. Our approach has been teated on the standard Reuters text categorization collection. Empirical results indicate that our approach outperforms the best published results on this % uters collection. In particular, our new feature selection method yields comiderable improvement. We also investigate the usability of our automated hxu-n~ approach by actually developing a system that categorizes texts into a treeof categories. We compare tbe accuracy of our learning approach to a rrddmsed, expert system ap preach that uses a text categorization shell built by Cams gie Group. Although our automated learning approach still gives a lower accuracy, by appropriately inmrporating a set of manually chosen worda to use as f~ures, the combined, semi-automated approach yields accuracy close to the * baaed approach.", "title": "Feature Selection, Perceptron Learning, and a Usability Case Study for Text Categorization"}, "e9fd1a7ae0322d417ab2d32017e373dd50efc063": {"paper_id": "e9fd1a7ae0322d417ab2d32017e373dd50efc063", "abstract": "This paper examines the use of inductive learning to categorize natural language documents into predeened content categories. Categorization of text is of increasing importance in information retrieval and natural language processing systems. Previous research on automated text categorization has mixed machine learning and knowledge engineering methods, making it diicult to draw conclusions about the performance of particular methods. In this paper we present empirical results on the performance of a Bayesian classiier and a decision tree learning algorithm on two text categorization data sets. We nd that both algorithms achieve reasonable performance and allow controlled tradeoos between false positives and false negatives. The stepwise feature selection in the decision tree algorithm is particularly eeective in dealing with the large feature sets common in text categorization. However, even this algorithm is aided by an initial preeltering of features, connrming the results found by Almuallim and Dietterich on artiicial data sets. We also demonstrate the impact of the time-varying nature of category deenitions.", "title": "A comparison of two learning algorithms for text categorization"}, "836cd47fa670c345b47e217419bd7e7bf8d0aab5": {"paper_id": "836cd47fa670c345b47e217419bd7e7bf8d0aab5", "abstract": "[Context] It is an enigma that agile projects can succeed \u2018without requirements\u2019 when weak requirements engineering is a known cause for project failures. While agile development projects often manage well without extensive requirements test cases are commonly viewed as requirements and detailed requirements are documented as test cases. [Objective] We have investigated this agile practice of using test cases as requirements to understand how test cases can support the main requirements activities, and how this practice varies. [Method] We performed an iterative case study at three companies and collected data through 14 interviews and 2 focus groups. [Results] The use of test cases as requirements poses both benefits and challenges when eliciting, validating, verifying, and managing requirements, and when used as a documented agreement. We have identified five variants of the test-cases-as-requirements practice, namely de facto, behaviour-driven, story-test driven, stand-alone strict and stand-alone manual for which the application of the practice varies concerning the time frame of requirements documentation, the requirements format, the extent to which the test cases are a machine executable specification and the use of tools which provide specific support for the practice of using test cases as requirements. [Conclusions] The findings provide empirical insight into how agile development projects manage and communicate requirements. The identified variants of the practice of using test cases as requirements can be used to perform in-depth investigations into agile requirements engineering. Practitioners can use the provided recommendations as a guide in designing and improving their agile requirements practices based on project characteristics such as number of stakeholders and rate of change.", "title": "A multi-case study of agile requirements engineering and the use of test cases as requirements"}, "058f712a7dd173dd0eb6ece7388bd9cdd6f77d67": {"paper_id": "058f712a7dd173dd0eb6ece7388bd9cdd6f77d67", "abstract": "Although many view iterative and incremental development as a modern practice, its application dates as far back as the mid-1950s. Prominent software-engineering thought leaders from each succeeding decade supported IID practices, and many large projects used them successfully. These practices may have differed in their details, but all had a common theme-to avoid a single-pass sequential, document-driven, gated-step approach.", "title": "Iterative and incremental developments. a brief history"}, "469d5e3085227f7a67b6aaa23660ebd371a1ad52": {"paper_id": "469d5e3085227f7a67b6aaa23660ebd371a1ad52", "abstract": "In a software development group of IBM Retail Store Solutions, we built a non-trivial software system based on a stable standard specification using a disciplined, rigorous unit testing and build approach based on the test- driven development (TDD) practice. Using this practice, we reduced our defect rate by about 50 percent compared to a similar system that was built using an ad-hoc unit testing approach. The project completed on time with minimal development productivity impact. Additionally, the suite of automated unit test cases created via TDD is a reusable and extendable asset that will continue to improve quality over the lifetime of the software system. The test suite will be the basis for quality checks and will serve as a quality contract between all members of the team.", "title": "Assessing Test-Driven Development at IBM"}, "51b64144ea8026984333b038aec9679d4a41c1a8": {"paper_id": "51b64144ea8026984333b038aec9679d4a41c1a8", "abstract": "The purpose of this paper is to build the foundation for software architecture. We first develop an intuition for software architecture by appealing to several well-established architectural disciplines. On the basis of this intuition, we present a model of software architecture that consists of three components: elements, form, and rationale. Elements are either processing, data, or connecting elements. Form is defined in terms of the properties of, and the relationships among, the elements --- that is, the constraints on the elements. The rationale provides the underlying basis for the architecture in terms of the system constraints, which most often derive from the system requirements. We discuss the components of the model in the context of both architectures and architectural styles and present an extended example to illustrate some important architecture and style considerations. We conclude by presenting some of the benefits of our approach to software architecture, summarizing our contributions, and relating our approach to other current work.", "title": "Foundations for the study of software architecture"}, "ad37eaa119819f19a9864d86ec27c9d98ce817f5": {"paper_id": "ad37eaa119819f19a9864d86ec27c9d98ce817f5", "abstract": "Your use of the JSTOR archive indicates your acceptance of JSTOR's Terms and Conditions of Use, available at http://www.jstor.org/page/info/about/policies/terms.jsp. JSTOR's Terms and Conditions of Use provides, in part, that unless you have obtained prior permission, you may not download an entire issue of a journal or multiple copies of articles, and you may use content in the JSTOR archive only for your personal, non-commercial use.", "title": "Building Theories from Case Study Research"}, "c7687f7a8683a76fa6c96ace79e5ce1c40ff868f": {"paper_id": "c7687f7a8683a76fa6c96ace79e5ce1c40ff868f", "abstract": "Using grounded theory as an example, this paper examines three methodological questions that are generally applicable to all qualitative methods. How should the usual scientific canons be reinterpreted for qualitative research? How should researchers report the procedures and canons used in their research? What evaluative criteria should be used in judging the research products? We propose that the criteria should be adapted to fit the procedures of the method. We demonstrate how this can be done for grounded theory and suggest criteria for evaluating studies following this approach. We argue that other qualitative researchers might be similarly specific about their procedures and evaluative criteria.", "title": "Grounded Theory Research : Procedures , Canons , and Evaluative Criteria"}, "69c703189432e28e3f075ceec17abeb8090e5dd0": {"paper_id": "69c703189432e28e3f075ceec17abeb8090e5dd0", "abstract": "Let X and Y be any two strings of finite length. The problem of transforming X to Y using the edit operations of substitution, deletion, and insertion has been extensively studied in the literature. The problem can be solved in quadratic time if the edit operations are extended to include the operation of transposition of adjacent characters, and is NP-complete if the characters can be edited repeatedly. In this paper we consider the problem of transforming X to Y when the set of edit operations is extended to include the squashing and expansion operations. Whereas in the squashing operation two (or more) contiguous characters of X can be transformed into a single character of Y, in the expansion operation a single character in X may be expanded into two or more contiguous characters of Y. These operations are typically found in the recognition of cursive script. A quadratic time solution to the problem has been presented. This solution is optimal for the infinite-alphabet case. The strategy to compute the sequence of edit operations is also presented.", "title": "String Alignment with Substitution, Insertion, Deletion, Squashing and Expansion Operations"}, "455e1168304e0eb2909093d5ab9b5ec85cda5028": {"paper_id": "455e1168304e0eb2909093d5ab9b5ec85cda5028", "abstract": "The string-to-string correction problem is to determine the distance between two strings as measured by the minimum cost sequence of \u201cedit operations\u201d needed to change the one string into the other. The edit operations investigated allow changing one symbol of a string into another single symbol, deleting one symbol from a string, or inserting a single symbol into a string. An algorithm is presented which solves this problem in time proportional to the product of the lengths of the two strings. Possible applications are to the problems of automatic spelling correction and determining the longest subsequence of characters common to two strings.", "title": "The String-to-String Correction Problem"}, "acc37c78d1840109e056261640c1a6434421841e": {"paper_id": "acc37c78d1840109e056261640c1a6434421841e", "abstract": "We present a data structure, based upon a stratified binary tree, which enables us to manipulate on-line a priority queue whose priorities are selected from the interval 1...n, with an average and worst case processing time of O(log log n) per instruction. The structure is used to obtain a mergeable heap whose time requirements are about as good.", "title": "Preserving order in a forest in less than logarithmic time"}, "d669dbcf0c564348b37776d637913d90a98a6ef3": {"paper_id": "d669dbcf0c564348b37776d637913d90a98a6ef3", "abstract": "HEALTH SERVICES RESEARCH IMPACTS DEBATE In a highly controversial decision in 1965, the American Nurses Association called for the baccalaureate degree in nursing (BSN) to be required for licensure of professional nurses by 1985. However, hospital diploma education\u2014the pathway to nursing for most students in 1965\u2014gave way not to bachelor\u2019s education but largely to associate degree education, fueled by public subsidies to community colleges. Meanwhile, many other countries, including Canada, Australia, New Zealand, Norway, Spain, Philippines, and many in South America, standardized entry into professional nursing at the baccalaureate level. In the United States, where physicians have the most years of education of any country in the world, nursing education pathways remain varied and confusing to prospective nurses, employers, and consumers alike, and two thirds of new RNs still enter practice with less than a baccalaureate degree. However, a decade of health services research showing better patient outcomes associated with better educated nurses has had a remarkable impact on employer preferences for nurses with baccalaureate qualifications, the return of 100,000 RNs to school to obtain BSNs, and may well be the catalyst for achieving in the future a largely bachelor\u2019s educated nurse workforce in the United States after decades of debate. The breakthrough in health services research came with the reconceptualization of nurses\u2019 education as a modifiable property of a health care organization, much like the Institute of Medicine\u2019s redefinition of patient safety as a property of an organization. In both cases, new definitions served as catalysts for action on old problems by putting the onus on health care organizations to respond. The initial paper defining BSN education as a modifiable property of hospitals was a study of outcomes following general surgery in 168 Pennsylvania hospitals in 1999. Each 10% increase in the proportion of BSN staff nurses was associated with 5% lower odds on death and failure to rescue after taking into account how sick the patients were and other characteristics of hospitals that had been shown to be associated with mortality rates, including physician qualifications. A front page story about these findings in the Newark Star Ledger published the names of major New Jersey hospitals linked to the proportion of their bedside nurses who were bachelor\u2019s qualified, an illustration of the action potential for changing practice by engaging employers for the first time in a meaningful way. The American Organization of Nurse Executives, a subsidiary of the American Hospital Association, issued a historic statement not long after the publication and extensive media coverage of the paper supporting the BSN as the desired credential for hospital nurses. From a methodological perspective, nurses\u2019 education as an organizational property, operationalized as the percentage of bedside care nurses with BSNs or higher, was a reasonably straightforward dashboard measure that could be monitored by individual organizations and included in large-scale studies of hospital performance. Other studies followed, replicating and expanding the evidence of an association between BSN qualifications and better patient outcomes. Replications in other countries", "title": "Baccalaureate nurses and hospital outcomes: more evidence."}, "7ab6a049f86af1dfce04116aa7cc66d8e9c248a8": {"paper_id": "7ab6a049f86af1dfce04116aa7cc66d8e9c248a8", "abstract": "We present a technique that, given a sequence of musical note onset times, performs simultaneous iden-tiication of the notated rhythm and the variable tempo associated with the times. Our formulation is probabilistic: We develop a stochastic model for the interconnected evolution of a rhythm process, a tempo process, and an observable process. This model allows the globally optimal identiication of the most likely rhythm and tempo sequence, given the observed onset times. We demonstrate applications to a sequence of times derived from a sampled audio le and to MIDI data.", "title": "Automated Rhythm Transcription"}, "55669977308a84cf0e5fc16822e9c0dcd5e908c5": {"paper_id": "55669977308a84cf0e5fc16822e9c0dcd5e908c5", "abstract": "Prepositional phrases (PPs) express crucial information that knowledge base construction methods need to extract. However, PPs are a major source of syntactic ambiguity and still pose problems in parsing. We present a method for resolving ambiguities arising from PPs, making extensive use of semantic knowledge from various resources. As training data, we use both labeled and unlabeled data, utilizing an expectation maximization algorithm for parameter estimation. Experiments show that our method yields improvements over existing methods including a state of the art dependency parser.", "title": "A Knowledge-Intensive Model for Prepositional Phrase Attachment"}, "094fc15bc058b0d62a661a1460885a9490bdb1bd": {"paper_id": "094fc15bc058b0d62a661a1460885a9490bdb1bd", "abstract": "The Rocchio relevance feedback algorithm is one of the most popular and widely applied learning methods from information retrieval. Here, a probabilistic analysis of this algorithm is presented in a text categorization framework. The analysis gives theoretical insight into the heuristics used in the Rocchio algorithm, particularly the word weighting scheme and the similarity metric. It also suggests improvements which lead to a probabilistic variant of the Rocchio classi er. The Rocchio classi er, its probabilistic variant, and a naive Bayes classi er are compared on six text categorization tasks. The results show that the probabilistic algorithms are preferable to the heuristic Rocchio classi er not only because they are more well-founded, but also because they achieve better performance.", "title": "A Probabilistic Analysis of the Rocchio Algorithm with TFIDF for Text Categorization"}, "c608ec27a937361122d178b38b6b7387440b58eb": {"paper_id": "c608ec27a937361122d178b38b6b7387440b58eb", "abstract": "We address statistical classifier design given a mixed training set consisting of a small labelled feature set and a (generally larger) set of unlabelled features. This situation arises, e.g., for medical images, where although training features may be plentiful, expensive expertise is required to extract their class labels. We propose a classifier structure and learning algorithm that make effective use of unlabelled data to improve performance. The learning is based on maximization of the total data likelihood, i.e. over both the labelled and unlabelled data subsets. Two distinct EM learning algorithms are proposed, differing in the EM formalism applied for unlabelled data. The classifier, based on a joint probability model for features and labels, is a \"mixture of experts\" structure that is equivalent to the radial basis function (RBF) classifier, but unlike RBFs, is amenable to likelihood-based training. The scope of application for the new method is greatly extended by the observation that test data, or any new data to classify, is in fact additional, unlabelled data thus, a combined learning/classification operation much akin to what is done in image segmentation can be invoked whenever there is new data to classify. Experiments with data sets from the UC Irvine database demonstrate that the new learning algorithms and structure achieve substantial performance gains over alternative approaches.", "title": "A Mixture of Experts Classifier with Learning Based on Both Labelled and Unlabelled Data"}, "441d270d8685aaae4ba2711595e3bc2611702564": {"paper_id": "441d270d8685aaae4ba2711595e3bc2611702564", "abstract": "Real-world learning tasks may involve high-dimensional data sets with arbitrary patterns of missing data. In this paper we present a framework based on maximum likelihood density estimation for learning from such data set.s. VVe use mixture models for the density estimates and make two distinct appeals to the ExpectationMaximization (EM) principle (Dempster et al., 1977) in deriving a learning algorithm-EM is used both for the estimation of mixture components and for coping wit.h missing dat.a. The resulting algorithm is applicable t.o a wide range of supervised as well as unsupervised learning problems. Result.s from a classification benchmark-t.he iris data set-are presented.", "title": "Supervised learning from incomplete data via an EM approach"}, "0013fae7390cbd34aade7959b4476512d8ab9aa3": {"paper_id": "0013fae7390cbd34aade7959b4476512d8ab9aa3", "abstract": "Current captioning approaches can describe images using black-box architectures whose behavior is hardly controllable and explainable from the exterior. As an image can be described in infinite ways depending on the goal and the context at hand, a higher degree of controllability is needed to apply captioning algorithms in complex scenarios. In this paper, we introduce a novel framework for image captioning which can generate diverse descriptions by allowing both grounding and controllability. Given a control signal in the form of a sequence or set of image regions, we generate the corresponding caption through a recurrent architecture which predicts textual chunks explicitly grounded on regions, following the constraints of the given control. Experiments are conducted on Flickr30k Entities and on COCO Entities, an extended version of COCO in which we add grounding annotations collected in a semi-automatic manner. Results demonstrate that our method achieves state of the art performances on controllable image captioning, in terms of caption quality and diversity. Code and annotations are publicly available at: https://github.com/ aimagelab/show-control-and-tell.", "title": "Show, Control and Tell: A Framework for Generating Controllable and Grounded Captions"}, "9dadf5bb0a2182b1509c5ea60d434bb35d4701c1": {"paper_id": "9dadf5bb0a2182b1509c5ea60d434bb35d4701c1", "abstract": "We describe Hafez, a program that generates any number of distinct poems on a usersupplied topic. Poems obey rhythmic and rhyme constraints. We describe the poetrygeneration algorithm, give experimental data concerning its parameters, and show its generality with respect to language and poetic form.", "title": "Generating Topical Poetry"}, "696ca58d93f6404fea0fc75c62d1d7b378f47628": {"paper_id": "696ca58d93f6404fea0fc75c62d1d7b378f47628", "abstract": "In this paper we describe the Microsoft COCO Caption dataset and evaluation server. When completed, the dataset will contain over one and a half million captions describing over 330,000 images. For the training and validation images, five independent human generated captions will be provided. To ensure consistency in evaluation of automatic caption generation algorithms, an evaluation server is used. The evaluation server receives candidate captions and scores them using several popular metrics, including BLEU, METEOR, ROUGE and CIDEr. Instructions for using the evaluation server are provided.", "title": "Microsoft COCO Captions: Data Collection and Evaluation Server"}, "3bfa75238e15e869b902ceb62b31ffddbe8ccb0d": {"paper_id": "3bfa75238e15e869b902ceb62b31ffddbe8ccb0d", "abstract": "The Visual Dependency Representation (VDR) is an explicit model of the spatial relationships between objects in an image. In this paper we present an approach to training a VDR Parsing Model without the extensive human supervision used in previous work. Our approach is to find the objects mentioned in a given description using a state-of-the-art object detector, and to use successful detections to produce training data. The description of an unseen image is produced by first predicting its VDR over automatically detected objects, and then generating the text with a template-based generation model using the predicted VDR. The performance of our approach is comparable to a state-ofthe-art multimodal deep neural network in images depicting actions.", "title": "Describing Images using Inferred Visual Dependency Representations"}, "18f1143c64e6557c933b206fb8b2a7bd1f389afd": {"paper_id": "18f1143c64e6557c933b206fb8b2a7bd1f389afd", "abstract": "We present an image caption system that addresses new challenges of automatically describing images in the wild. The challenges include generating high quality caption with respect to human judgments, out-of-domain data handling, and low latency required in many applications. Built on top of a state-of-the-art framework, we developed a deep vision model that detects a broad range of visual concepts, an entity recognition model that identifies celebrities and landmarks, and a confidence model for the caption output. Experimental results show that our caption engine outperforms previous state-of-the-art systems significantly on both in-domain dataset (i.e. MS COCO) and out-of-domain datasets. We also make the system publicly accessible as a part of the Microsoft Cognitive Services.", "title": "Rich Image Captioning in the Wild"}, "04483e2c56695b19f6912b061769eb8c175a5a7a": {"paper_id": "04483e2c56695b19f6912b061769eb8c175a5a7a", "abstract": "We propose to use the visual denotations of linguistic expressions (i.e. the set of images they describe) to define novel denotational similarity metrics, which we show to be at least as beneficial as distributional similarities for two tasks that require semantic inference. To compute these denotational similarities, we construct a denotation graph, i.e. a subsumption hierarchy over constituents and their denotations, based on a large corpus of 30K images and 150K descriptive captions.", "title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions"}, "4d3fb523b7892e0b7a7990366e02fae46d63dcb4": {"paper_id": "4d3fb523b7892e0b7a7990366e02fae46d63dcb4", "abstract": "Automatic image annotation is a difficult and highly relevant machine learning task. Recent advances have significantly improved the state-of-the-art in retrieval accuracy with algorithms based on nearest neighbor classification in carefully learned metric spaces. But this comes at a price of increased computational complexity during training and testing. We propose FastTag, a novel algorithm that achieves comparable results with two simple linear mappings that are co-regularized in a joint convex loss function. The loss function can be efficiently optimized in closed form updates, which allows us to incorporate a large number of image descriptors cheaply. On several standard real-world benchmark data sets, we demonstrate that FastTag matches the current state-of-the-art in tagging quality, yet reduces the training and testing times by several orders of magnitude and has lower asymptotic complexity.", "title": "Fast Image Tagging"}, "26adb749fc5d80502a6d889966e50b31391560d3": {"paper_id": "26adb749fc5d80502a6d889966e50b31391560d3", "abstract": "Parameter set learned using all WMT12 data (Callison-Burch et al., 2012): \u2022 100,000 binary rankings covering 8 language directions. \u2022Restrict scoring for all languages to exact and paraphrase matching. Parameters encode human preferences that generalize across languages: \u2022Prefer recall over precision. \u2022Prefer word choice over word order. \u2022Prefer correct translations of content words over function words. \u2022Prefer exact matches over paraphrase matches, while still giving significant credit to paraphrases. Visualization", "title": "Meteor Universal: Language Specific Translation Evaluation for Any Target Language"}, "4b3c5b49fa099a77c98bad4b5299c6b4eb0a8f2e": {"paper_id": "4b3c5b49fa099a77c98bad4b5299c6b4eb0a8f2e", "abstract": "In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network (CNN) to have remarkable localization ability despite being trained on imagelevel labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that exposes the implicit attention of CNNs on an image. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1% top-5 error for object localization on ILSVRC 2014 without training on any bounding box annotation. We demonstrate in a variety of experiments that our network is able to localize the discriminative image regions despite just being trained for solving classification task1.", "title": "Learning Deep Features for Discriminative Localization"}, "79dc84a3bf76f1cb983902e2591d913cee5bdb0e": {"paper_id": "79dc84a3bf76f1cb983902e2591d913cee5bdb0e", "abstract": "Previous work on Recursive Neural Networks (RNNs) shows that these models can produce compositional feature vectors for accurately representing and classifying sentences or images. However, the sentence vectors of previous models cannot accurately represent visually grounded meaning. We introduce the DT-RNN model which uses dependency trees to embed sentences into a vector space in order to retrieve images that are described by those sentences. Unlike previous RNN-based models which use constituency trees, DT-RNNs naturally focus on the action and agents in a sentence. They are better able to abstract from the details of word order and syntactic expression. DT-RNNs outperform other recursive and recurrent neural networks, kernelized CCA and a bag-of-words baseline on the tasks of finding an image that fits a sentence description and vice versa. They also give more similar representations to sentences that describe the same image.", "title": "Grounded Compositional Semantics for Finding and Describing Images with Sentences"}, "b2624c3cb508bf053e620a090332abce904099a1": {"paper_id": "b2624c3cb508bf053e620a090332abce904099a1", "abstract": "Neural network architectures with memory and attention mechanisms exhibit certain reasoning capabilities required for question answering. One such architecture, the dynamic memory network (DMN), obtained high accuracy on a variety of language tasks. However, it was not shown whether the architecture achieves strong results for question answering when supporting facts are not marked during training or whether it could be applied to other modalities such as images. Based on an analysis of the DMN, we propose several improvements to its memory and input modules. Together with these changes we introduce a novel input module for images in order to be able to answer visual questions. Our new DMN+ model improves the state of the art on both the Visual Question Answering dataset and the bAbI-10k text question-answering dataset without supporting fact supervision.", "title": "Dynamic Memory Networks for Visual and Textual Question Answering"}, "5e0f8c355a37a5a89351c02f174e7a5ddcb98683": {"paper_id": "5e0f8c355a37a5a89351c02f174e7a5ddcb98683", "abstract": "We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.", "title": "Microsoft COCO: Common Objects in Context"}, "255e97d82f528b613dbe8883727abfd14f3f9f39": {"paper_id": "255e97d82f528b613dbe8883727abfd14f3f9f39", "abstract": "ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It includes measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans. The measures count the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans. This paper introduces four different ROUGE measures: ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S included in the ROUGE summarization evaluation package and their evaluations. Three of them have been used in the Document Understanding Conference (DUC) 2004, a large-scale summarization evaluation sponsored by NIST.", "title": "ROUGE: A Package For Automatic Evaluation Of Summaries"}, "11da2d589485685f792a8ac79d4c2e589e5f77bd": {"paper_id": "11da2d589485685f792a8ac79d4c2e589e5f77bd", "abstract": "Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.", "title": "Show and tell: A neural image caption generator"}, "90fbeb4c871d3916c2b428645a1e1482f05826e1": {"paper_id": "90fbeb4c871d3916c2b428645a1e1482f05826e1", "abstract": "We propose a novel module, the reviewer module, to improve the encoder-decoder learning framework. The reviewer module is generic, and can be plugged into an existing encoder-decoder model. The reviewer module performs a number of review steps with attention mechanism on the encoder hidden states, and outputs a fact vector after each review step; the fact vectors are used as the input of the attention mechanism in the decoder. We show that the conventional encoderdecoders are a special case of our framework. Empirically, we show that our framework can improve over state-of-the-art encoder-decoder systems on the tasks of image captioning and source code captioning.", "title": "Encode, Review, and Decode: Reviewer Module for Caption Generation"}, "a0e03c5b647438299c79c71458e6b1776082a37b": {"paper_id": "a0e03c5b647438299c79c71458e6b1776082a37b", "abstract": "We propose \u201cAreas of Attention\u201d, a novel attentionbased model for automatic image captioning. Our approach models the dependencies between image regions, caption words, and the state of an RNN language model, using three pairwise interactions. In contrast to previous attentionbased approaches that associate image regions only to the RNN state, our method allows a direct association between caption words and image regions. During training these associations are inferred from image-level captions, akin to weakly-supervised object detector training. These associations help to improve captioning by localizing the corresponding regions during testing. We also propose and compare different ways of generating attention areas: CNN activation grids, object proposals, and spatial transformers nets applied in a convolutional fashion. Spatial transformers give the best results. They allow for image specific attention areas, and can be trained jointly with the rest of the network. Our attention mechanism and spatial transformer attention areas together yield state-of-the-art results on the MSCOCO dataset.", "title": "Areas of Attention for Image Captioning"}, "424561d8585ff8ebce7d5d07de8dbf7aae5e7270": {"paper_id": "424561d8585ff8ebce7d5d07de8dbf7aae5e7270", "abstract": "State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet <xref ref-type=\"bibr\" rid=\"ref1\">[1]</xref> and Fast R-CNN <xref ref-type=\"bibr\" rid=\"ref2\">[2]</xref> have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a <italic>Region Proposal Network</italic> (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features\u2014using the recently popular terminology of neural networks with \u2019attention\u2019 mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model <xref ref-type=\"bibr\" rid=\"ref3\">[3]</xref> , our detection system has a frame rate of 5 fps (<italic>including all steps</italic>) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.", "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"}, "21a1654b856cf0c64e60e58258669b374cb05539": {"paper_id": "21a1654b856cf0c64e60e58258669b374cb05539", "abstract": "We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.", "title": "You Only Look Once: Unified, Real-Time Object Detection"}, "4991785cb0e6ee3d0b7823b59e144fb80ca3a83e": {"paper_id": "4991785cb0e6ee3d0b7823b59e144fb80ca3a83e", "abstract": null, "title": "VQA: Visual Question Answering"}, "3d275a4e4f44d452f21e0e0ff6145a5e18e6cf87": {"paper_id": "3d275a4e4f44d452f21e0e0ff6145a5e18e6cf87", "abstract": "Automatically describing an image with a sentence is a long-standing challenge in computer vision and natural language processing. Due to recent progress in object detection, attribute classification, action recognition, etc., there is renewed interest in this area. However, evaluating the quality of descriptions has proven to be challenging. We propose a novel paradigm for evaluating image descriptions that uses human consensus. This paradigm consists of three main parts: a new triplet-based method of collecting human annotations to measure consensus, a new automated metric that captures consensus, and two new datasets: PASCAL-50S and ABSTRACT-50S that contain 50 sentences describing each image. Our simple metric captures human judgment of consensus better than existing metrics across sentences generated by various sources. We also evaluate five state-of-the-art image description approaches using this new protocol and provide a benchmark for future comparisons. A version of CIDEr named CIDEr-D is available as a part of MS COCO evaluation server to enable systematic evaluation and benchmarking.", "title": "CIDEr: Consensus-based image description evaluation"}, "20f90d4c4974692092c3ba78c4957422abea8ac2": {"paper_id": "20f90d4c4974692092c3ba78c4957422abea8ac2", "abstract": "A new hypothesis about the role of focused attention is proposed. The feature-integration theory of attention suggests that attention must be directed serially to each stimulus in a display whenever conjunctions of more than one separable feature are needed to characterize or distinguish the possible objects presented. A number of predictions were tested in a variety of paradigms including visual search, texture segregation, identification and localization, and using both separable dimensions (shape and color) and local elements or parts of figures (lines, curves, etc. in letters) as the features to be integrated into complex wholes. The results were in general consistent with the hypothesis. They offer a new set of criteria for distinguishing separable from integral features and a new rationale for predicting which tasks will show attention limits and which will not.", "title": "A feature-integration theory of attention"}, "ba753286b9e2f32c5d5a7df08571262e257d2e53": {"paper_id": "ba753286b9e2f32c5d5a7df08571262e257d2e53", "abstract": "Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.", "title": "Conditional Generative Adversarial Nets"}, "5287d8fef49b80b8d500583c07e935c7f9798933": {"paper_id": "5287d8fef49b80b8d500583c07e935c7f9798933", "abstract": "Automatic synthesis of realistic images from text would be interesting and useful, but current AI systems are still far from this goal. However, in recent years generic and powerful recurrent neural network architectures have been developed to learn discriminative text feature representations. Meanwhile, deep convolutional generative adversarial networks (GANs) have begun to generate highly compelling images of specific categories, such as faces, album covers, and room interiors. In this work, we develop a novel deep architecture and GAN formulation to effectively bridge these advances in text and image modeling, translating visual concepts from characters to pixels. We demonstrate the capability of our model to generate plausible images of birds and flowers from detailed text descriptions.", "title": "Generative Adversarial Text to Image Synthesis"}, "072fd0b8d471f183da0ca9880379b3bb29031b6a": {"paper_id": "072fd0b8d471f183da0ca9880379b3bb29031b6a", "abstract": "We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Moreover, since the release of the pix2pix software associated with this paper, hundreds of twitter users have posted their own artistic experiments using our system. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without handengineering our loss functions either.", "title": "Image-to-Image Translation with Conditional Adversarial Networks"}, "dd9a9175fa952a3888da3aafba13e777588dc574": {"paper_id": "dd9a9175fa952a3888da3aafba13e777588dc574", "abstract": "Recently it has been shown that policy-gradient methods for reinforcement learning can be utilized to train deep end-to-end systems directly on non-differentiable metrics for the task at hand. In this paper we consider the problem of optimizing image captioning systems using reinforcement learning, and show that by carefully optimizing our systems using the test metrics of the MSCOCO task, significant gains in performance can be realized. Our systems are built using a new optimization approach that we call self-critical sequence training (SCST). SCST is a form of the popular REINFORCE algorithm that, rather than estimating a baseline to normalize the rewards and reduce variance, utilizes the output of its own test-time inference algorithm to normalize the rewards it experiences. Using this approach, estimating the reward signal (as actor-critic methods must do) and estimating normalization (as REINFORCE algorithms typically do) is avoided, while at the same time harmonizing the model with respect to its test-time inference procedure. Empirically we find that directly optimizing the CIDEr metric with SCST and greedy decoding at test-time is highly effective. Our results on the MSCOCO evaluation sever establish a new state-of-the-art on the task, improving the best result in terms of CIDEr from 104.9 to 114.7.", "title": "Self-Critical Sequence Training for Image Captioning"}, "968ef7f6ce0fdc4f7fef0bd51b06bbb139d5381f": {"paper_id": "968ef7f6ce0fdc4f7fef0bd51b06bbb139d5381f", "abstract": "Humans can prepare concise descriptions of pictures, focusing on what they find important. We demonstrate that automatic methods can do so too. We describe a system that can compute a score linking an image to a sentence. This score can be used to attach a descriptive sentence to a given image, or to obtain images that illustrate a given sentence. The score is obtained by comparing an estimate of meaning obtained from the image to one obtained from the sentence. Each estimate of meaning comes from a discriminative procedure that is learned using data. We evaluate on a novel dataset consisting of human-annotated images. While our underlying estimate of meaning is impoverished, it is sufficient to produce very good quantitative results, evaluated with a novel score that can account for synecdoche.", "title": "Every Picture Tells a Story: Generating Sentences from Images"}, "0e0900b88c33b671be5dd2ded9885b6526d6b429": {"paper_id": "0e0900b88c33b671be5dd2ded9885b6526d6b429", "abstract": "This paper presents a novel approach for automatically generating image descriptions: visual detectors, language models, and multimodal similarity models learnt directly from a dataset of image captions. We use multiple instance learning to train visual detectors for words that commonly occur in captions, including many different parts of speech such as nouns, verbs, and adjectives. The word detector outputs serve as conditional inputs to a maximum-entropy language model. The language model learns from a set of over 400,000 image descriptions to capture the statistics of word usage. We capture global semantics by re-ranking caption candidates using sentence-level features and a deep multimodal similarity model. Our system is state-of-the-art on the official Microsoft COCO benchmark, producing a BLEU-4 score of 29.1%. When human judges compare the system captions to ones written by other people on our held-out test set, the system captions have equal or better quality 34% of the time.", "title": "From captions to visual concepts and back"}, "2e36ea91a3c8fbff92be2989325531b4002e2afc": {"paper_id": "2e36ea91a3c8fbff92be2989325531b4002e2afc", "abstract": "Inspired by recent advances in multimodal learning and machine translation, we introduce an encoder-decoder pipeline that learns (a): a multimodal joint embedding space with images and text and (b): a novel language model for decoding distributed representations from our space. Our pipeline effectively unifies joint image-text embedding models with multimodal neural language models. We introduce the structure-content neural language model that disentangles the structure of a sentence to its content, conditioned on representations produced by the encoder. The encoder allows one to rank images and sentences while the decoder can generate novel descriptions from scratch. Using LSTM to encode sentences, we match the state-of-the-art performance on Flickr8K and Flickr30K without using object detections. We also set new best results when using the 19-layer Oxford convolutional network. Furthermore we show that with linear encoders, the learned embedding space captures multimodal regularities in terms of vector space arithmetic e.g. *image of a blue car* \"blue\" + \"red\" is near images of red cars. Sample captions generated for 800 images are made available for comparison.", "title": "Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models"}, "355de7460120ddc1150d9ce3756f9848983f7ff4": {"paper_id": "355de7460120ddc1150d9ce3756f9848983f7ff4", "abstract": "This paper introduces a novel generation system that composes humanlike descriptions of images from computer vision detections. By leveraging syntactically informed word co-occurrence statistics, the generator filters and constrains the noisy detections output from a vision system to generate syntactic trees that detail what the computer vision system sees. Results show that the generation system outperforms state-of-the-art systems, automatically generating some of the most natural image descriptions to date.", "title": "Midge: Generating Image Descriptions From Computer Vision Detections"}, "f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97": {"paper_id": "f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97", "abstract": "We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, and machine translation.", "title": "Recurrent Neural Network Regularization"}, "b9ed0bb4aae5d89c48ce2ce372f2891c228fe0f8": {"paper_id": "b9ed0bb4aae5d89c48ce2ce372f2891c228fe0f8", "abstract": "This paper studies the problem of associating images with descriptive sentences by embedding them in a common latent space. We are interested in learning such embeddings from hundreds of thousands or millions of examples. Unfortunately, it is prohibitively expensive to fully annotate this many training images with ground-truth sentences. Instead, we ask whether we can learn better image-sentence embeddings by augmenting small fully annotated training sets with millions of images that have weak and noisy annotations (titles, tags, or descriptions). After investigating several state-of-the-art scalable embedding methods, we introduce a new algorithm called Stacked Auxiliary Embedding that can successfully transfer knowledge from millions of weakly annotated images to improve the accuracy of retrieval-based image description.", "title": "Improving Image-Sentence Embeddings Using Large Weakly Annotated Photo Collections"}, "19d3b02185ad36fb0b792f2a15a027c58ac91e8e": {"paper_id": "19d3b02185ad36fb0b792f2a15a027c58ac91e8e", "abstract": "We develop and demonstrate automatic image description methods using a large captioned photo collection. One contribution is our technique for the automatic collection of this new dataset \u2013 performing a huge number of Flickr queries and then filtering the noisy results down to 1 million images with associated visually relevant captions. Such a collection allows us to approach the extremely challenging problem of description generation using relatively simple non-parametric methods and produces surprisingly effective results. We also develop methods incorporating many state of the art, but fairly noisy, estimates of image content to produce even more pleasing results. Finally we introduce a new objective performance measure for image captioning.", "title": "Im2Text: Describing Images Using 1 Million Captioned Photographs"}, "154898f34460e95aef932bec5615bbd995824cad": {"paper_id": "154898f34460e95aef932bec5615bbd995824cad", "abstract": "Stereo matching is one of the most active research areas in computer vision. While a large number of algorithms for stereo correspondence have been developed, relatively little work has been done on characterizing their performance. In this paper, we present a taxonomy of dense, two-frame stereo methods. Our taxonomy is designed to assess the different components and design decisions made in individual stereo algorithms. Using this taxonomy, we compare existing stereo methods and present experiments evaluating the performance of many different variants. In order to establish a common software platform and a collection of data sets for easy evaluation, we have designed a stand-alone, flexible C++ implementation that enables the evaluation of individual components and that can easily be extended to include new algorithms. We have also produced several new multi-frame stereo data sets with ground truth and are making both the code and data sets available on the Web. Finally, we include a comparative evaluation of a large set of today's best-performing stereo algorithms.", "title": "A Taxonomy and Evaluation of Dense Two-Frame Stereo Correspondence Algorithms"}, "6e6f47c4b2109e7824cd475336c3676faf9b113e": {"paper_id": "6e6f47c4b2109e7824cd475336c3676faf9b113e", "abstract": "We posit that visually descriptive language offers computer vision researchers both information about the world, and information about how people describe the world. The potential benefit from this source is made more significant due to the enormous amount of language data easily available today. We present a system to automatically generate natural language descriptions from images that exploits both statistics gleaned from parsing large quantities of text data and recognition algorithms from computer vision. The system is very effective at producing relevant sentences for images. It also generates descriptions that are notably more true to the specific image content than previous work.", "title": "Baby Talk : Understanding and Generating Image Descriptions"}, "0c739b915d633cc3c162e4ef1e57b796c2dc2217": {"paper_id": "0c739b915d633cc3c162e4ef1e57b796c2dc2217", "abstract": "Broad-coverage repositories of semantic relations between verbs could benefit many NLP tasks. We present a semi-automatic method for extracting fine-grained semantic relations between verbs. We detect similarity, strength, antonymy, enablement, and temporal happens-before relations between pairs of strongly associated verbs using lexicosyntactic patterns over the Web. On a set of 29,165 strongly associated verb pairs, our extraction algorithm yielded 65.5% accuracy. Analysis of error types shows that on the relation strength we achieved 75% accuracy. We provide the resource, called VERBOCEAN, for download at http://semantics.isi.edu/ocean/.", "title": "VerbOcean: Mining the Web for Fine-Grained Semantic Verb Relations"}, "00bbba51721dee6e0b1cd2a5b614ab46f33abab6": {"paper_id": "00bbba51721dee6e0b1cd2a5b614ab46f33abab6", "abstract": "1. Please explain how this manuscript advances this field of research and/or contributes something new to the literature. They tackle the problem of semantic similarity based on the information content the concepts share. They combine the taxonomic structure with the empirical problem estimates which provides better way of adapting knowledge to multiple context concepts. They also try to find a solution to the multiple inheritance problems.", "title": "Using Information Content to Evaluate Semantic Similarity in a Taxonomy"}, "528fa9bb03644ba752fb9491be49b9dd1bce1d52": {"paper_id": "528fa9bb03644ba752fb9491be49b9dd1bce1d52", "abstract": "Semantic Textual Similarity (STS) measures the degree of semantic equivalence between two texts. This paper presents the results of the STS pilot task in Semeval. The training data contained 2000 sentence pairs from previously existing paraphrase datasets and machine translation evaluation resources. The test data also comprised 2000 sentences pairs for those datasets, plus two surprise datasets with 400 pairs from a different machine translation evaluation corpus and 750 pairs from a lexical resource mapping exercise. The similarity of pairs of sentences was rated on a 0-5 scale (low to high similarity) by human judges using Amazon Mechanical Turk, with high Pearson correlation scores, around 90%. 35 teams participated in the task, submitting 88 runs. The best results scored a Pearson correlation>80%, well above a simple lexical baseline that only scored a 31% correlation. This pilot task opens an exciting way ahead, although there are still open issues, specially the evaluation metric.", "title": "SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity"}, "9f263d4288ef4dd9b773363f436eb6d5d1e64f4d": {"paper_id": "9f263d4288ef4dd9b773363f436eb6d5d1e64f4d", "abstract": "This article concerns the real-world importance of leadership for the success or failure of organizations and social institutions. The authors propose conceptualizing leadership and evaluating leaders in terms of the performance of the team or organization for which they are responsible. The authors next offer a taxonomy of the dependent variables used as criteria in leadership studies. A review of research using this taxonomy suggests that the vast empirical literature on leadership may tell us more about the success of individual managerial careers than the success of these people in leading groups, teams, and organizations. The authors then summarize the evidence showing that leaders do indeed affect the performance of organizations--for better or for worse--and conclude by describing the mechanisms through which they do so.", "title": "Leadership and the fate of organizations."}, "2dd3dbd30428ff63c75c12dcd941da367ff492ca": {"paper_id": "2dd3dbd30428ff63c75c12dcd941da367ff492ca", "abstract": "Distributional semantic models learn vector representations of words through the contexts they occur in. Although the choice of context (which often takes the form of a sliding window) has a direct influence on the resulting embeddings, the exact role of this model component is still not fully understood. This paper presents a systematic analysis of context windows based on a set of four distinct hyperparameters. We train continuous SkipGram models on two English-language corpora for various combinations of these hyper-parameters, and evaluate them on both lexical similarity and analogy tasks. Notable experimental results are the positive impact of cross-sentential contexts and the surprisingly good performance of right-context windows.", "title": "Redefining Context Windows for Word Embedding Models: An Experimental Study"}, "7f0eb2cb332a8ce5fafaa7c280b5c5ab9c7ca95a": {"paper_id": "7f0eb2cb332a8ce5fafaa7c280b5c5ab9c7ca95a", "abstract": "To facilitate future research in unsupervised induction of syntactic structure and to standardize best-practices, we propose a tagset that consists of twelve universal part-of-speech categories. In addition to the tagset, we develop a mapping from 25 different treebank tagsets to this universal set. As a result, when combined with the original treebank data, this universal tagset and mapping produce a dataset consisting of common parts-of-speech for 22 different languages. We highlight the use of this resource via three experiments, that (1) compare tagging accuracies across languages, (2) present an unsupervised grammar induction approach that does not use gold standard part-of-speech tags, and (3) use the universal tags to transfer dependency parsers between languages, achieving state-of-the-art results.", "title": "A Universal Part-of-Speech Tagset"}, "7c5bac84a0ef1701047bd70ae33b3e89008260db": {"paper_id": "7c5bac84a0ef1701047bd70ae33b3e89008260db", "abstract": "We present freely available open-source toolkit for training recurrent neural network based language models. I t can be easily used to improve existing speech recognition and ma chine translation systems. Also, it can be used as a baseline for fu ture research of advanced language modeling techniques. In the p a er, we discuss optimal parameter selection and different modes of functionality. The toolkit, example scripts and basic setups are freely available at http://rnnlm.sourceforge.net/. I. I NTRODUCTION, MOTIVATION AND GOALS Statistical language modeling attracts a lot of attention, as models of natural languages are important part of many practical systems today. Moreover, it can be estimated that with further research progress, language models will becom e closer to human understanding [1] [2], and completely new applications will become practically realizable. Immedia tely, any significant progress in language modeling can be utilize d in the esisting speech recognition and statistical machine translation systems. However, the whole research field struggled for decades to overcome very simple, but also effective models based on ngram frequencies [3] [4]. Many techniques were developed to beat n-grams, but the improvements came at the cost of computational complexity. Moreover, the improvements wer e often reported on very basic systems, and after application to state-of-the-art setups and comparison to n-gram models trained on large amounts of data, improvements provided by many techniques vanished. This has lead to scepticism among speech recognition researchers. In our previous work, we have compared many major advanced language modeling techniques, and found that neur al network based language models (NNLM) perform the best on several standard setups [5]. Models of this type were introduced by Bengio in [6], about ten years ago. Their main weaknesses were huge computational complexity, and nontrivial implementation. Successful training of neural net works require well chosen hyper-parameters, such as learning rat e and size of hidden layer. To help overcome these basic obstacles, we have decided to release our toolkit for training recurrent neural network b ased language models (RNNLM). We have shown that the recurrent architecture outperforms the feedforward one on several se tup in [7]. Moreover, the implemenation is simple and easy to understand. The most importantly, recurrent neural networ ks are very interesting from the research point of view, as they allow effective processing of sequences and patterns with arbitraty length these models can learn to store informati on in the hidden layer. Recurrent neural networks can have memory , and are thus important step forward to overcome the most painful and often criticized drawback of n-gram models dependence on previous two or three words only. In this paper we present an open source and freely available toolkit for training statistical language models base d or recurrent neural networks. It includes techniques for redu cing computational complexity (classes in the output layer and direct connections between input and output layer). Our too lkit has been designed to provide comparable results to the popul ar toolkit for training n-gram models, SRILM [8]. The main goals for the RNNLM toolkit are these: \u2022 promotion of research of advanced language modeling techniques \u2022 easy usage \u2022 simple portable code without any dependencies \u2022 computational efficiency In the paper, we describe how to easily make RNNLM part of almost any speech recognition or machine translation syste m that produces lattices. II. RECURRENTNEURAL NETWORK The recurrent neural network architecture used in the toolk it is shown at Figure 1 (usually called Elman network, or simple RNN). The input layer uses the 1-of-N representation of the previous wordw(t) concatenated with previous state of the hidden layers(t \u2212 1). The neurons in the hidden layer s(t) use sigmoid activation function. The output layer (t) has the same dimensionality as w(t), and after the network is trained, it represents probability distribution of the next word giv en the previous word and state of the hidden layer in the previous time step [9]. The class layer c(t) can be optionally used to reduce computational complexity of the model, at a small cost of accuracy [7]. Training is performed by the standard stochastic gradient descent algorithm, and the matrix W that", "title": "RNNLM - Recurrent Neural Network Language Modeling Toolkit"}, "783480acff435bfbc15ffcdb4f15eccddaa0c810": {"paper_id": "783480acff435bfbc15ffcdb4f15eccddaa0c810", "abstract": "We address the problem of predicting a word from previous words in a sample of text. In particular, we discuss n-gram models based on classes of words. We also discuss several statistical algorithms for assigning words to classes based on the frequency of their co-occurrence with other words. We find that we are able to extract classes that have the flavor of either syntactically based groupings or semantically based groupings, depending on the nature of the underlying statistics.", "title": "Class-Based n-gram Models of Natural Language"}, "b92513dac9d5b6a4683bcc625b94dd1ced98734e": {"paper_id": "b92513dac9d5b6a4683bcc625b94dd1ced98734e", "abstract": "We present two simple modifications to the models in the popular Word2Vec tool, in order to generate embeddings more suited to tasks involving syntax. The main issue with the original models is the fact that they are insensitive to word order. While order independence is useful for inducing semantic representations, this leads to suboptimal results when they are used to solve syntax-based problems. We show improvements in part-ofspeech tagging and dependency parsing using our proposed models.", "title": "Two/Too Simple Adaptations of Word2Vec for Syntax Problems"}, "0c77a0ef7ce3e46cd14376bf6cb56bd3b2f13fb5": {"paper_id": "0c77a0ef7ce3e46cd14376bf6cb56bd3b2f13fb5", "abstract": "We introduce a framework for unsupervised learning of structured predictors with overlapping, global features. Each input\u2019s latent representation is predicted conditional on the observed data using a feature-rich conditional random field (CRF). Then a reconstruction of the input is (re)generated, conditional on the latent structure, using a generative model which factorizes similarly to the CRF. The autoencoder formulation enables efficient exact inference without resorting to unrealistic independence assumptions or restricting the kinds of features that can be used. We illustrate connections to traditional autoencoders, posterior regularization, and multi-view learning. We then show competitive results with instantiations of the framework for two canonical tasks in natural language processing: part-of-speech induction and bitext word alignment, and show that training the proposed model can be substantially more efficient than a comparable feature-rich baseline.", "title": "Conditional Random Field Autoencoders for Unsupervised Structured Prediction"}, "4561c13c2907f15398cdd34a272eb099be0c0587": {"paper_id": "4561c13c2907f15398cdd34a272eb099be0c0587", "abstract": "The idea that at least some aspects of word meaning can be induced from patterns of word co-occurrence is becoming increasingly popular. However, there is less agreement about the precise computations involved, and the appropriate tests to distinguish between the various possibilities. It is important that the effect of the relevant design choices and parameter values are understood if psychological models using these methods are to be reliably evaluated and compared. In this article, we present a systematic exploration of the principal computational possibilities for formulating and validating representations of word meanings from word co-occurrence statistics. We find that, once we have identified the best procedures, a very simple approach is surprisingly successful and robust over a range of psychologically relevant evaluation measures.", "title": "Extracting semantic representations from word co-occurrence statistics: a computational study."}, "3bff03b7b0b0c4e8f6384dbb2a95e4338d156524": {"paper_id": "3bff03b7b0b0c4e8f6384dbb2a95e4338d156524", "abstract": "We introduce a novel machine learning framework based on recursive autoencoders for sentence-level prediction of sentiment label distributions. Our method learns vector space representations for multi-word phrases. In sentiment prediction tasks these representations outperform other state-of-the-art approaches on commonly used datasets, such as movie reviews, without using any pre-defined sentiment lexica or polarity shifting rules. We also evaluate the model\u2019s ability to predict sentiment distributions on a new dataset based on confessions from the experience project. The dataset consists of personal user stories annotated with multiple labels which, when aggregated, form a multinomial distribution that captures emotional reactions. Our algorithm can more accurately predict distributions over such labels compared to several competitive baselines.", "title": "Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions"}, "10dd5320f568a41ac72cdb4a148cf6809eadd0dd": {"paper_id": "10dd5320f568a41ac72cdb4a148cf6809eadd0dd", "abstract": "This paper presents and compares WordNetbased and distributional similarity approaches. The strengths and weaknesses of each approach regarding similarity and relatedness tasks are discussed, and a combination is presented. Each of our methods independently provide the best results in their class on the RG and WordSim353 datasets, and a supervised combination of them yields the best published results on all datasets. Finally, we pioneer cross-lingual similarity, showing that our methods are easily adapted for a cross-lingual task with minor losses.", "title": "A Study on Similarity and Relatedness Using Distributional and WordNet-based Approaches"}, "3a0e788268fafb23ab20da0e98bb578b06830f7d": {"paper_id": "3a0e788268fafb23ab20da0e98bb578b06830f7d", "abstract": "Computers understand very little of the meaning of human language. This profoundly limits our ability to give instructions to computers, the ability of computers to explain their actions to us, and the ability of computers to analyse and process text. Vector space models (VSMs) of semantics are beginning to address these limits. This paper surveys the use of VSMs for semantic processing of text. We organize the literature on VSMs according to the structure of the matrix in a VSM. There are currently three broad classes of VSMs, based on term\u2013document, word\u2013context, and pair\u2013pattern matrices, yielding three classes of applications. We survey a broad range of applications in these three categories and we take a detailed look at a specific open source project in each category. Our goal in this survey is to show the breadth of applications of VSMs for semantics, to provide a new perspective on VSMs for those who are already familiar with the area, and to provide pointers into the literature for those who are less familiar with the field.", "title": "From Frequency to Meaning: Vector Space Models of Semantics"}, "247feaf26d9605f9adf0ee1364790f006adff445": {"paper_id": "247feaf26d9605f9adf0ee1364790f006adff445", "abstract": "OpenSubtitles.org provides a large collection of user contributed subtitles in various languages for movies and TV programs. Subtitle translations are valuable resources for cross-lingual studies and machine translation research. A less explored feature of the collection is the inclusion of alternative translations, which can be very useful for training paraphrase systems or collecting multi-reference test suites for machine translation. However, differences in translation may also be due to misspellings, incomplete or corrupt data files, or wrongly aligned subtitles. This paper reports our efforts in recognising and classifying alternative subtitle translations with language independent techniques. We use time-based alignment with lexical re-synchronisation techniques and BLEU score filters and sort alternative translations into categories using edit distance metrics and heuristic rules. Our approach produces large numbers of sentence-aligned translation alternatives for over 50 languages provided via the OPUS corpus collection.", "title": "Finding Alternative Translations in a Large Corpus of Movie Subtitle"}, "11aedb8f95a007363017dae311fc525f67bd7876": {"paper_id": "11aedb8f95a007363017dae311fc525f67bd7876", "abstract": "Often, the training procedure for statistical machine translation models is based on maximum likelihood or related criteria. A general problem of this approach is that there is only a loose relation to the final translation quality on unseen text. In this paper, we analyze various training criteria which directly optimize translation quality. These training criteria make use of recently proposed automatic evaluation metrics. We describe a new algorithm for efficient training an unsmoothed error count. We show that significantly better results can often be obtained if the final evaluation criterion is taken directly into account as part of the training procedure.", "title": "Minimum Error Rate Training in Statistical Machine Translation"}, "0275420087c2f015794287a7d3204f40bf8dbacc": {"paper_id": "0275420087c2f015794287a7d3204f40bf8dbacc", "abstract": "\u25cfObjective: Create a Japanese morphological analyzer (word segmentation + POS tagging) that is robust and adaptable to new domains \u25cfApproach: Use pointwise prediction, which estimates all tags independently of other tags \u25cfPointwise prediction: \u25cfRobust: does not rely on dictionaries as much as previous methods \u25cfAdaptable: it can be learned from single annotated words, not full sentences \u25cfWorks with active learning: Single words to annotate can be chosen effectively \u25cfEvaluation on Japanese morphological analysis shows improvement over traditional methods 1", "title": "Pointwise Prediction for Robust, Adaptable Japanese Morphological Analysis"}, "61777bb512c5c1327778d9d7698e1994bdf5ca9a": {"paper_id": "61777bb512c5c1327778d9d7698e1994bdf5ca9a", "abstract": "Recent work in learning bilingual representations tend to tailor towards achieving good performance on bilingual tasks, most often the crosslingual document classification (CLDC) evaluation, but to the detriment of preserving clustering structures of word representations monolingually. In this work, we propose a joint model to learn word representations from scratch that utilizes both the context coocurrence information through the monolingual component and the meaning equivalent signals from the bilingual constraint. Specifically, we extend the recently popular skipgram model to learn high quality bilingual representations efficiently. Our learned embeddings achieve a new state-of-the-art accuracy of 80.3 for the German to English CLDC task and a highly competitive performance of 90.7 for the other classification direction. At the same time, our models outperform best embeddings from past bilingual representation work by a large margin in the monolingual word similarity evaluation.1", "title": "Bilingual Word Representations with Monolingual Quality in Mind"}, "1e83c2798e87b9d84a8281cc3e9750c948d62cb3": {"paper_id": "1e83c2798e87b9d84a8281cc3e9750c948d62cb3", "abstract": "We present a simple log-linear reparameterization of IBM Model 2 that overcomes problems arising from Model 1\u2019s strong assumptions and Model 2\u2019s overparameterization. Efficient inference, likelihood evaluation, and parameter estimation algorithms are provided. Training the model is consistently ten times faster than Model 4. On three large-scale translation tasks, systems built using our alignment model outperform IBM Model 4. An open-source implementation of the alignment model described in this paper is available from http://github.com/clab/fast align .", "title": "A Simple, Fast, and Effective Reparameterization of IBM Model 2"}, "e50a316f97c9a405aa000d883a633bd5707f1a34": {"paper_id": "e50a316f97c9a405aa000d883a633bd5707f1a34", "abstract": "The experimental evidence accumulated over the past 20 years indicates that text indexing systems based on the assignment of appropriately weighted single terms produce retrieval results that are superior to those obtainable with other more elaborate text representations. These results depend crucially on the choice of effective termweighting systems. This article summarizes the insights gained in automatic term weighting, and provides baseline single-term-indexing models with which other more elaborate content analysis procedures can be compared. 1. AUTOMATIC TEXT ANALYSIS In the late 195Os, Luhn [l] first suggested that automatic text retrieval systems could be designed based on a comparison of content identifiers attached both to the stored texts and to the users\u2019 information queries. Typically, certain words extracted from the texts of documents and queries would be used for content identification; alternatively, the content representations could be chosen manually by trained indexers familiar with the subject areas under consideration and with the contents of the document collections. In either case, the documents would be represented by term vectors of the form D= (ti,tj,...ytp) (1) where each tk identifies a content term assigned to some sample document D. Analogously, the information requests, or queries, would be represented either in vector form, or in the form of Boolean statements. Thus, a typical query Q might be formulated as Q = (qa,qbr.. . ,4r) (2)", "title": "Term-Weighting Approaches in Automatic Text Retrieval"}, "5feac39a50e5bf240a64f42dbc881a16e8f8e659": {"paper_id": "5feac39a50e5bf240a64f42dbc881a16e8f8e659", "abstract": "We introduce a simple wrapper method that uses off-the-shelf word embedding algorithms to learn task-specific bilingual word embeddings. We use a small dictionary of easily-obtainable task-specific word equivalence classes to produce mixed context-target pairs that we use to train off-the-shelf embedding models. Our model has the advantage that it (a) is independent of the choice of embedding algorithm, (b) does not require parallel data, and (c) can be adapted to specific tasks by re-defining the equivalence classes. We show how our method outperforms off-the-shelf bilingual embeddings on the task of unsupervised cross-language partof-speech (POS) tagging, as well as on the task of semi-supervised cross-language super sense (SuS) tagging.", "title": "Simple task-specific bilingual word embeddings"}, "4b75d707eb3ffe4607c8cdd5436c8d7f8573fed9": {"paper_id": "4b75d707eb3ffe4607c8cdd5436c8d7f8573fed9", "abstract": "We present a novel technique for learning semantic representations, which extends the distributional hypothesis to multilingual data and joint-space embeddings. Our models leverage parallel data and learn to strongly align the embeddings of semantically equivalent sentences, while maintaining sufficient distance between those of dissimilar sentences. The models do not rely on word alignments or any syntactic information and are successfully applied to a number of diverse languages. We extend our approach to learn semantic representations at the document level, too. We evaluate these models on two cross-lingual document classification tasks, outperforming the prior state of the art. Through qualitative analysis and the study of pivoting effects we demonstrate that our representations are semantically plausible and can capture semantic relationships across languages without parallel data.", "title": "Multilingual Models for Compositional Distributed Semantics"}, "32a437ecdc20cc76c01e37c08920317ea18fde50": {"paper_id": "32a437ecdc20cc76c01e37c08920317ea18fde50", "abstract": "This paper describes a system for extracting typed dependency parses of English sentences from phrase structure parses. In order to capture inherent relations occurring in corpus texts that can be critical in real-world applications, many NP relations are included in the set of grammatical relations used. We provide a comparison of our system with Minipar and the Link parser. The typed dependency extraction facility described here is integrated in the Stanford Parser, available for download.", "title": "Generating Typed Dependency Parses from Phrase Structure Parses"}, "06d0a9697a0f0242dbdeeff08ec5266b74bfe457": {"paper_id": "06d0a9697a0f0242dbdeeff08ec5266b74bfe457", "abstract": "We presenta novel generati ve model for natural languagetree structuresin whichsemantic(lexical dependenc y) andsyntacticstructuresare scoredwith separatemodels.Thisfactorizationprovidesconceptual simplicity, straightforwardopportunitiesfor separatelyimproving the componentmodels,anda level of performancealreadycloseto thatof similar, non-factoredmodels.Most importantly, unlikeothermodernparsing models,thefactoredmodeladmitsanextremelyeffectiveA parsingalgorithm,which makesefficient,exactinferencefeasible.", "title": "Fast Exact Inference with a Factored Model for Natural Language Parsing"}, "4f410ab5c8b12b34b38421241366ee456bbebab9": {"paper_id": "4f410ab5c8b12b34b38421241366ee456bbebab9", "abstract": "Most current statistical natural language processing models use only local features so as to permit dynamic programming in inference, but this makes them unable to fully account for the long distance structure that is prevalent in language use. We show how to solve this dilemma with Gibbs sampling, a simple Monte Carlo method used to perform approximate inference in factored probabilistic models. By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs, CMMs, and CRFs, it is possible to incorporate non-local structure while preserving tractable inference. We use this technique to augment an existing CRF-based information extraction system with long-distance dependency models, enforcing label consistency and extraction template consistency constraints. This technique results in an error reduction of up to 9% over state-of-the-art systems on two established information extraction tasks.", "title": "Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling"}, "b2da18b28fde224af80f8865f6c82f3d4ed0e755": {"paper_id": "b2da18b28fde224af80f8865f6c82f3d4ed0e755", "abstract": "We propose a new deterministic approach to coreference resolution that combines the global information and precise features of modern machine-learning models with the transparency and modularity of deterministic, rule-based systems. Our sieve architecture applies a battery of deterministic coreference models one at a time from highest to lowest precision, where each model builds on the previous model's cluster output. The two stages of our sieve-based architecture, a mention detection stage that heavily favors recall, followed by coreference sieves that are precision-oriented, offer a powerful way to achieve both high precision and high recall. Further, our approach makes use of global information through an entity-centric model that encourages the sharing of features across all mentions that point to the same real-world entity. Despite its simplicity, our approach gives state-of-the-art performance on several corpora and genres, and has also been incorporated into hybrid state-of-the-art coreference systems for Chinese and Arabic. Our system thus offers a new paradigm for combining knowledge in rule-based systems that has implications throughout computational linguistics.", "title": "Deterministic Coreference Resolution Based on Entity-Centric, Precision-Ranked Rules"}, "3493c33b36f005d230469c456256cb1fe144376f": {"paper_id": "3493c33b36f005d230469c456256cb1fe144376f", "abstract": "Natural Language Processing continues to grow in popularity in a range of research and commercial applications, yet managing the wide array of potential NLP components remains a difficult problem. This paper describes CURATOR, an NLP management framework designed to address some common problems and inefficiencies associated with building NLP process pipelines; and EDISON, an NLP data structure library in Java that provides streamlined interactions with CURATOR and offers a range of useful supporting functionality.", "title": "An NLP Curator (or: How I Learned to Stop Worrying and Love NLP Pipelines)"}, "7a65f23d990231d461418067c808b09d84c19b2c": {"paper_id": "7a65f23d990231d461418067c808b09d84c19b2c", "abstract": null, "title": "Natural Language Processing with Python"}, "21dbd7c2b7f87ed057608be5108586025159c801": {"paper_id": "21dbd7c2b7f87ed057608be5108586025159c801", "abstract": "Research into corpus-based semantics has focused on the development of ad hoc models that treat single tasks, or sets of closely related tasks, as unrelated challenges to be tackled by extracting different kinds of distributional information from the corpus. As an alternative to this \u201cone task, one model\u201d approach, the Distributional Memory framework extracts distributional information once and for all from the corpus, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor. Different matrices are then generated from the tensor, and their rows and columns constitute natural spaces to deal with different semantic problems. In this way, the same distributional information can be shared across tasks such as modeling word similarity judgments, discovering synonyms, concept categorization, predicting selectional preferences of verbs, solving analogy problems, classifying relations between word pairs, harvesting qualia structures with patterns or example pairs, predicting the typical properties of concepts, and classifying verbs into alternation classes. Extensive empirical testing in all these domains shows that a Distributional Memory implementation performs competitively against task-specific algorithms recently reported in the literature for the same tasks, and against our implementations of several state-of-the-art methods. The Distributional Memory approach is thus shown to be tenable despite the constraints imposed by its multi-purpose nature.", "title": "Distributional Memory: A General Framework for Corpus-Based Semantics"}, "142f38642629b9d268999ad876af482177d36697": {"paper_id": "142f38642629b9d268999ad876af482177d36697", "abstract": "Unsupervised word representations are very useful in NLP tasks both as inputs to learning algorithms and as extra word features in NLP systems. However, most of these models are built with only local context and one representation per word. This is problematic because words are often polysemous and global context can also provide useful information for learning word meanings. We present a new neural network architecture which 1) learns word embeddings that better capture the semantics of words by incorporating both local and global document context, and 2) accounts for homonymy and polysemy by learning multiple embeddings per word. We introduce a new dataset with human judgments on pairs of words in sentential context, and evaluate our model on it, showing that our model outperforms competitive baselines and other neural language models. 1", "title": "Improving Word Representations via Global Context and Multiple Word Prototypes"}, "0f3ab6835042ea45d2aab8e4a70151c11ca9a1d6": {"paper_id": "0f3ab6835042ea45d2aab8e4a70151c11ca9a1d6", "abstract": "Processing language requires the retrieval of concepts from memory in response to an ongoing stream of information. This retrieval is facilitated if one can infer the gist of a sentence, conversation, or document and use that gist to predict related concepts and disambiguate words. This article analyzes the abstract computational problem underlying the extraction and use of gist, formulating this problem as a rational statistical inference. This leads to a novel approach to semantic representation in which word meanings are represented in terms of a set of probabilistic topics. The topic model performs well in predicting word association and the effects of semantic association and ambiguity on a variety of language-processing and memory tasks. It also provides a foundation for developing more richly structured statistical models of language, as the generative process assumed in the topic model can easily be extended to incorporate other kinds of semantic and syntactic structure.", "title": "Topics in semantic representation."}, "0157dcd6122c20b5afc359a799b2043453471f7f": {"paper_id": "0157dcd6122c20b5afc359a799b2043453471f7f", "abstract": "Dictionaries and phrase tables are the basis of modern statistical machine translation systems. This paper develops a method that can automate the process of generating and extending dictionaries and phrase tables. Our method can translate missing word and phrase entries by learning language structures based on large monolingual data and mapping between languages from small bilingual data. It uses distributed representation of words and learns a linear mapping between vector spaces of languages. Despite its simplicity, our method is surprisingly effective: we can achieve almost 90% precision@5 for translation of words between English and Spanish. This method makes little assumption about the languages, so it can be used to extend and refine dictionaries and translation tables for any language pairs.", "title": "Exploiting Similarities among Languages for Machine Translation"}, "5002a020c0aa0d7838bb51c1ab24b23d2385e9d9": {"paper_id": "5002a020c0aa0d7838bb51c1ab24b23d2385e9d9", "abstract": "Multi-modal models that learn semantic representations from both linguistic and perceptual input outperform language-only models on a range of evaluations, and better reflect human concept acquisition. Most perceptual input to such models corresponds to concrete noun concepts and the superiority of the multi-modal approach has only been established when evaluating on such concepts. We therefore investigate which concepts can be effectively learned by multi-modal models. We show that concreteness determines both which linguistic features are most informative and the impact of perceptual input in such models. We then introduce ridge regression as a means of propagating perceptual information from concrete nouns to more abstract concepts that is more robust than previous approaches. Finally, we present weighted gram matrix combination, a means of combining representations from distinct modalities that outperforms alternatives when both modalities are sufficiently rich.", "title": "Multi-Modal Models for Concrete and Abstract Concept Meaning"}, "251b6cff9656d80f0829b511aefa45523285e252": {"paper_id": "251b6cff9656d80f0829b511aefa45523285e252", "abstract": "Search on the web is a delay process and it can be hard task especially for beginners when they attempt to use a keyword query language. Beginner (inexpert) searchers commonly attempt to find information with ambiguous queries. These ambiguous queries make the search engine returns irrelevant results. This work aims to get more relevant pages to query through query reformulation and expanding search space. The proposed system has three basic parts WordNet, Google search engine and Genetic Algorithm. Every part has a special task. The system uses WordNet to remove ambiguity from queries by displaying the meaning of every keyword in user query and selecting the proper meaning for keywords. The system obtains synonym for every keyword from WordNet and generates query list. Genetic algorithm is used to create generation for every query in query list. Every query in system is navigated using Google search engine to obtain results from group of documents on the Web. The system has been tested on number of ambiguous queries and it has obtained more relevant URL to user query especially when the query has one keyword. The results are promising and therefore open further research directions.", "title": "Query reformulation using WordNet and genetic algorithm"}, "d67cd96b0a30c5d2b2703491e02ec0f7d1954bb8": {"paper_id": "d67cd96b0a30c5d2b2703491e02ec0f7d1954bb8", "abstract": "Owing to important applications such as mining web page traversal sequences, many algorithms have been introduced in the area of sequential pattern mining over the last decade, most of which have also been modified to support concise representations like closed, maximal, incremental or hierarchical sequences. This article presents a taxonomy of sequential pattern-mining techniques in the literature with web usage mining as an application. This article investigates these algorithms by introducing a taxonomy for classifying sequential pattern-mining algorithms based on important key features supported by the techniques. This classification aims at enhancing understanding of sequential pattern-mining problems, current status of provided solutions, and direction of research in this area. This article also attempts to provide a comparative performance analysis of many of the key techniques and discusses theoretical aspects of the categories in the taxonomy.", "title": "A taxonomy of sequential pattern mining algorithms"}, "8a86ed981a439bf9b65ddccff02358f0ed23b776": {"paper_id": "8a86ed981a439bf9b65ddccff02358f0ed23b776", "abstract": "Introductory psychology students (120 females and 120 males) rated attractiveness and fecundity of one of six computer-altered female \u008e gures representing three body-weight categories (underweight, normal weight and overweight) and two levels of waist-to-hip ratio (WHR), one in the ideal range (0.72) and one in the non-ideal range (0.86). Both females and males judged underweight \u008e gures to be more attractive than normal or overweight \u008e gures, regardless of WHR. The female \u008e gure with the high WHR (0.86) was judged to be more attractive than the \u008e gure with the low WHR (0.72) across all body-weight conditions. Analyses of fecundity ratings revealed an interaction between weight and WHR such that the models did not differ in the normal weight category, but did differ in the underweight (model with WHR of 0.72 was less fecund) and overweight (model with WHR of 0.86 was more fecund) categories. These \u008endings lend stronger support to sociocultural rather than evolutionary hypotheses.", "title": "Predicting female physical attractiveness Waist-to-hip ratio versus thinness"}, "7cc382e31f9b263892c7f66023ec441dd1df321d": {"paper_id": "7cc382e31f9b263892c7f66023ec441dd1df321d", "abstract": "Neural Machine Translation (NMT) has recently attracted a l ot of attention due to the very high performance achieved by deep neural network s in other domains. An inherent weakness in existing NMT systems is their inabil ity to correctly translate rare words: end-to-end NMTs tend to have relatively sma ll vocabularies with a single \u201cunknown-word\u201d symbol representing every possibl e out-of-vocabulary (OOV) word. In this paper, we propose and implement a simple t echnique to address this problem. We train an NMT system on data that is augm ented by the output of a word alignment algorithm, allowing the NMT syste m to output, for each OOV word in the target sentence, its corresponding word in the source sentence. This information is later utilized in a post-process ing step that translates every OOV word using a dictionary. Our experiments on the WMT \u201914 English to French translation task show that this simple method prov ides a substantial improvement over an equivalent NMT system that does not use thi technique. The performance of our system achieves a BLEU score of 36.9, whic h improves the previous best end-to-end NMT by 2.1 points. Our model matche s t performance of the state-of-the-art system while using three times less data.", "title": "Addressing the Rare Word Problem in Neural Machine Translation"}, "f14049b1f44b782134e248102c1f03539436fa4c": {"paper_id": "f14049b1f44b782134e248102c1f03539436fa4c", "abstract": "During the last years there has been growing interest in using neural networks for language modeling. In contrast to the well known back-offn-gram language models, the neural network approach attempts to overcome the data sparseness problem by performing the estimation in a continuous space. This type of language model was mostly used for tasks for which only a very limited amount of in-domain training data is available. In this paper we present new algorithms to train a neural network language model on very large text corpora. This makes possible the use of the approach in domains where several hundreds of millions words of texts are available. The neural network language model is evaluated in a state-ofthe-art real-time continuous speech recognizer for French Broadcast News. Word error reductions of 0.5% absolute are reported using only a very limited amount of additional processing time.", "title": "Training Neural Network Language Models on Very Large Corpora"}, "55b81991fbb025038d98e8c71acf7dc2b78ee5e9": {"paper_id": "55b81991fbb025038d98e8c71acf7dc2b78ee5e9", "abstract": "Learning algorithms related to artificial neural networks and in particular for Deep Learning may seem to involve many bells and whistles, called hyperparameters. This chapter is meant as a practical guide with recommendations for some of the most commonly used hyper-parameters, in particular in the context of learning algorithms based on backpropagated gradient and gradient-based optimization. It also discusses how to deal with the fact that more interesting results can be obtained when allowing one to adjust many hyper-parameters. Overall, it describes elements of the practice used to successfully and efficiently train and debug large-scale and often deep multi-layer neural networks. It closes with open questions about the training difficulties observed with deeper architectures.", "title": "Practical recommendations for gradient-based training of deep architectures"}, "64da1980714cfc130632c5b92b9d98c2f6763de6": {"paper_id": "64da1980714cfc130632c5b92b9d98c2f6763de6", "abstract": "Deep neural networks have recently become the gold standard for acoustic modeling in speech recognition systems. The key computational unit of a deep network is a linear projection followed by a point-wise non-linearity, which is typically a logistic function. In this work, we show that we can improve generalization and make training of deep networks faster and simpler by substituting the logistic units with rectified linear units. These units are linear when their input is positive and zero otherwise. In a supervised setting, we can successfully train very deep nets from random initialization on a large vocabulary speech recognition task achieving lower word error rates than using a logistic network with the same topology. Similarly in an unsupervised setting, we show how we can learn sparse features that can be useful for discriminative tasks. All our experiments are executed in a distributed environment using several hundred machines and several hundred hours of speech data.", "title": "On rectified linear units for speech processing"}, "17452b482179fd45aa958b1b8b440fa4c117be6d": {"paper_id": "17452b482179fd45aa958b1b8b440fa4c117be6d", "abstract": "The prefrontal cortex has long been thought to subserve both working memory and \"executive\" function, but the mechanistic basis of their integrated function has remained poorly understood, often amounting to a homunculus. This paper reviews the progress in our laboratory and others pursuing a long-term research agenda to deconstruct this homunculus by elucidating the precise computational and neural mechanisms underlying these phenomena. We outline six key functional demands underlying working memory, and then describe the current state of our computational model of the prefrontal cortex and associated systems in the basal ganglia (BG). The model, called PBWM (prefrontal cortex, basal ganglia working memory model), relies on actively maintained representations in the prefrontal cortex, which are dynamically updated/gated by the basal ganglia. It is capable of developing human-like performance largely on its own by taking advantage of powerful reinforcement learning mechanisms, based on the midbrain dopaminergic system and its activation via the basal ganglia and amygdala. These learning mechanisms enable the model to learn to control both itself and other brain areas in a strategic, task-appropriate manner. The model can learn challenging working memory tasks, and has been corroborated by several important empirical studies.", "title": "Banishing the homunculus: Making working memory work"}, "56cbfcbfffd8c54bd8477d10b6e0e17e097b97c7": {"paper_id": "56cbfcbfffd8c54bd8477d10b6e0e17e097b97c7", "abstract": "This paper explores the difference between Connectionist proposals for cognitive a r c h i t e c t u r e a n d t h e s o r t s o f m o d e l s t hat have traditionally been assum e d i n c o g n i t i v e s c i e n c e . W e c l a i m t h a t t h e m a j o r d i s t i n c t i o n i s t h a t , w h i l e b o t h Connectionist and Classical architectures postulate representational mental states, the latter but not the former are committed to a symbol-level of representation, or to a \u2018language of thought\u2019: i.e., to representational states that have combinatorial syntactic and semantic structure. Several arguments for combinatorial structure in mental representations are then reviewed. These include arguments based on the \u2018systematicity\u2019 of mental representation: i.e., on the fact that cognitive capacities always exhibit certain symmetries, so that the ability to entertain a given thought implies the ability to entertain thoughts with semantically related contents. We claim that such arguments make a powerful case that mind/brain architecture is not Connectionist at the cognitive level. We then consider the possibility that Connectionism may provide an account of the neural (or \u2018abstract neurological\u2019) structures in which Classical cognitive architecture is implemented. We survey a n u m b e r o f t h e s t a n d a r d a r g u m e n t s t h a t h a v e b e e n o f f e r e d i n f a v o r o f Connectionism, and conclude that they are coherent only on this interpretation. Connectionist or PDP models are catching on. There are conferences and new books nearly every day, and the popular science press hails this new wave of theorizing as a breakthrough in understanding the mind (a typical example is the article in the May issue of Science 86, called \u201cHow we think: A new theory\u201d). There are also, inevitably, descriptions of the emergence of", "title": "Connectionism and cognitive architecture: A critical analysis"}, "6a1fc0fe123c329a18aa99fa33cab8325d591e6c": {"paper_id": "6a1fc0fe123c329a18aa99fa33cab8325d591e6c", "abstract": "In this response to Pinker and Jackendoff's critique, we extend our previous framework for discussion of language evolution, clarifying certain distinctions and elaborating on a number of points. In the first half of the paper, we reiterate that profitable research into the biology and evolution of language requires fractionation of \"language\" into component mechanisms and interfaces, a non-trivial endeavor whose results are unlikely to map onto traditional disciplinary boundaries. Our terminological distinction between FLN and FLB is intended to help clarify misunderstandings and aid interdisciplinary rapprochement. By blurring this distinction, Pinker and Jackendoff mischaracterize our hypothesis 3 which concerns only FLN, not \"language\" as a whole. Many of their arguments and examples are thus irrelevant to this hypothesis. Their critique of the minimalist program is for the most part equally irrelevant, because very few of the arguments in our original paper were tied to this program; in an online appendix we detail the deep inaccuracies in their characterization of this program. Concerning evolution, we believe that Pinker and Jackendoff's emphasis on the past adaptive history of the language faculty is misplaced. Such questions are unlikely to be resolved empirically due to a lack of relevant data, and invite speculation rather than research. Preoccupation with the issue has retarded progress in the field by diverting research away from empirical questions, many of which can be addressed with comparative data. Moreover, offering an adaptive hypothesis as an alternative to our hypothesis concerning mechanisms is a logical error, as questions of function are independent of those concerning mechanism. The second half of our paper consists of a detailed response to the specific data discussed by Pinker and Jackendoff. Although many of their examples are irrelevant to our original paper and arguments, we find several areas of substantive disagreement that could be resolved by future empirical research. We conclude that progress in understanding the evolution of language will require much more empirical research, grounded in modern comparative biology, more interdisciplinary collaboration, and much less of the adaptive storytelling and phylogenetic speculation that has traditionally characterized the field.", "title": "The evolution of the language faculty: Clarifications and implications"}, "b57927b713a6f9b73c7941f99144165396483478": {"paper_id": "b57927b713a6f9b73c7941f99144165396483478", "abstract": "Asymptotic behavior of a recurrent neural network changes qualitatively at certain points in the parameter space, which are known as \\bifurcation points\". At bifurcation points, the output of a network can change discontinuously with the change of parameters and therefore convergence of gradient descent algorithms is not guaranteed. Furthermore, learning equations used for error gradient estimation can be unstable. However, some kinds of bifurcations are inevitable in training a recurrent network as an automaton or an oscillator. Some of the factors underlying successful training of recurrent networks are investigated, such as choice of initial connections, choice of input patterns, teacher forcing, and truncated learning equations.", "title": "Bifurcations of Recurrent Neural Networks in Gradient Descent Learning"}, "ca7c44df33389ad1dfd81b3fac1aa9176bd4b386": {"paper_id": "ca7c44df33389ad1dfd81b3fac1aa9176bd4b386", "abstract": "-Backpropagation is often viewed as a method for adapting artificial neural networks to classify patterns. Based on parts of the book by Rumelhart and colleagues, many authors equate backpropagation with the generalized delta rule applied to fully-connected feedforward networks. This paper will summarize a more general Jbrmulation of backpropagation, developed in 1974, which does more justice to the roots of the method in numerical analysis and statistics, and also does more justice to creative approaches expressed by neural modelers in the past year or two. It will discuss applications of backpropagation to forecasting over time (where errors have been halved by using methods other than least squares), to optimization, to sensitivity analysis, and to brain research. This paper will go on to derive a generalization of backpropagation to recurrent systems (which input their own output), such as hybrids of perceptron-style networks and Grossberg/HopfieM networks. Unlike the proposal of Rumelhart, Hinton, and Williams, this generalization does not require the storage of intermediate iterations to deal with continuous recurrence. This generalization was applied in 1981 to a model of natural gas markets, where it located sources of forecast uncertainty related to the use o f least squares to estimate the model parameters in the first place. Keywords--Backpropagation, Recurrent, Continuous time, Reinforcement learning, Energy models, Prediction, Modelling, Cerebral cortex. 1. I N T R O D U C T I O N Backpropagation, as formulated by Rumelhart, Hinton, and Williams (1986) with acknowledgement of the prior work by David Parker (1985), may well be the most widely-used method to adapt artificial neural networks, for use in pattern classification. Nevertheless, the limitations of that formulation have been severely criticized by neuropsychologists and by classical computer scientists. The neuropsychologists have argued that simple feedforward networks cannot do justice to the structure and power of the brain. Neuropsychologists and computer scientists have argued that complex, interesting problems tend to require iterative procedures ( or networks) for their solution. Many other criticisms have been raised, which merit serious attention. Section 2 of this paper will review a different formulation of backpropagation, developed in the period The views expressed in this paper are those of the author, and do not necessarily reflect those of any component of the Federal governmeat. Requests for reprints should be sent to Paul J. Werbos, Neuroengineering Program, Room 1134, NSF, 1800 G Street N.W., Washington, DC 20550. 339 between 1968 and 1974, which can overcome many of these difficulties. This formulation deals with the general case of nonlinear systems of equations. It lacks the concrete, specialized appeal of Rumelhart 's discussion, but it can apply to neural networks, econometric models, and other systems as special cases. Applications to prediction, optimization and sensitivity analysis become possible; as an example, this paper will discuss an application to the sensitivity analysis of a natural gas market model developed by the Depar tment of Energy. Werbos (1987a) discussed at length a research strategy for brain research and factory automation based upon this formulation. Section 3 of this paper will show how derivatives may also be propagated through recurrent networks (such as those discussed by Grossberg, 1976 and Hopfield and Tank, 1986) without the expensive storage of information for each iteration (as required by the approach of Rumelhart et al., 1986). Our approach will require storage, however, to handle true external time lags; the significance of this will be discussed, along with ways to implement this storage and issues related to real-time adaptation. When external time lags are totally absent, our method is closely related to the 340 1~ .l. 14\"~,rbo; method of Almeida (1987), though slightly more general. Finally, Section 4 will display a practical application of the methods given m Section 3 an analysis of the properties of a natural gas market model, actually used by the Depar tment of Energy several years ago. The conclusions of this analysis were double-checked by explicit numerical perturbations of the model. This analysis provided an insight into the limitations of the model, which are related to certain limitations of multiple regression, the method used to estimate (adapt) the model in the first place. Multiple regression is closely related to the generalized delta rule for network adaptation; however, alternative estimation (adaptation) rules exist which have overcome these limitations m simulation studies and in several practical examples (Werbos, 1974. 1983a, 1988a; Werbos &Titus. t978). Those alternative rules are consistent with the general framework proposed here. 2. G E N E R A L F R A M E W O R K : BACKGROUND, T E R M I N O L O G Y , AND APPLICATIONS Rumelhart, Hinton, and Williams (RHW) Debates about backpropagation have been confused. in part, by different definitions of the word. The index to Rumelhar t et al. (1986) defines the word backpropagation by pointing to three pages of text which discuss the generalized delta rule. The generalized delta rule. in turn, is defined as a set of three steps to be applied to feedforward networks. ( R H W also discuss recurrent networks, but that extension will not be discussed until Section 3. in order to simplify things here.) R H W specify feedforward networks as:", "title": "Generalization of backpropagation with application to a recurrent gas market model"}, "1fd7fc06653723b05abe5f3d1de393ddcf6bdddb": {"paper_id": "1fd7fc06653723b05abe5f3d1de393ddcf6bdddb", "abstract": "We explore the performance of several types of language mode ls on the word-level and the character-level language modelin g tasks. This includes two recently proposed recurrent neural netwo rk architectures, a feedforward neural network model, a maximum ent ropy model and the usual smoothed n-gram models. We then propose a simple technique for learning sub-word level units from th e data, and show that it combines advantages of both character and wo rdlevel models. Finally, we show that neural network based lan gu ge models can be order of magnitude smaller than compressed n-g ram models, at the same level of performance when applied to a Bro dcast news RT04 speech recognition task. By using sub-word un its, the size can be reduced even more.", "title": "SUBWORD LANGUAGE MODELING WITH NEURAL NETWORKS"}, "6af0d89fc3183704485c2b360db7c2f914ced629": {"paper_id": "6af0d89fc3183704485c2b360db7c2f914ced629", "abstract": "Echo State Networks and Liquid State Machines introduced a new paradigm in artificial recurrent neural network (RNN) training, where an RNN (the reservoir) is generated randomly and only a readout is trained. The paradigm, becoming known as reservoir computing, greatly facilitated the practical application of RNNs and outperformed classical fully trained RNNs in many tasks. It has lately become a vivid research field with numerous extensions of the basic idea, including reservoir adaptation, thus broadening the initial paradigm to using different methods for training the reservoir and the readout. This review systematically surveys both: current ways of generating/adapting the reservoirs and training different types of readouts. It offers a natural conceptual classification of the techniques, which transcends boundaries of the current \u201cbrand-names\u201d of reservoir methods, and thus aims to help unifying the field and providing the reader with a detailed \u201cmap\u201d of", "title": "Reservoir computing approaches to recurrent neural network training"}, "0d6203718c15f137fda2f295c96269bc2b254644": {"paper_id": "0d6203718c15f137fda2f295c96269bc2b254644", "abstract": "In this work we resolve the long-outstanding problem of how to effectively train recurrent neural networks (RNNs) on complex and difficult sequence modeling problems which may contain long-term data dependencies. Utilizing recent advances in the Hessian-free optimization approach (Martens, 2010), together with a novel damping scheme, we successfully train RNNs on two sets of challenging problems. First, a collection of pathological synthetic datasets which are known to be impossible for standard optimization approaches (due to their extremely long-term dependencies), and second, on three natural and highly complex real-world sequence datasets where we find that our method significantly outperforms the previous state-of-theart method for training neural sequence models: the Long Short-term Memory approach of Hochreiter and Schmidhuber (1997). Additionally, we offer a new interpretation of the generalized Gauss-Newton matrix of sch (2002) which is used within the HF approach of Martens.", "title": "Learning Recurrent Neural Networks with Hessian-Free Optimization"}, "1680eaa31f30911e7460ff694b51794391bc7514": {"paper_id": "1680eaa31f30911e7460ff694b51794391bc7514", "abstract": "We present a generative model for the unsupervised learning of dependency structures. We also describe the multiplicative combination of this dependency model with a model of linear constituency. The product model outperforms both components on their respective evaluation metrics, giving the best published figures for unsupervised dependency parsing and unsupervised constituency parsing. We also demonstrate that the combined model works and is robust cross-linguistically, being able to exploit either attachment or distributional reg ularities that are salient in the data.", "title": "Corpus-Based Induction of Syntactic Structure: Models of Dependency and Constituency"}, "ad5974c04b316f4f379191e4dbea836fd766f47c": {"paper_id": "ad5974c04b316f4f379191e4dbea836fd766f47c", "abstract": "This paper reports on the benefits of largescale statistical language modeling in machine translation. A distributed infrastructure is proposed which we use to train on up to 2 trillion tokens, resulting in language models having up to 300 billion n-grams. It is capable of providing smoothed probabilities for fast, single-pass decoding. We introduce a new smoothing method, dubbed Stupid Backoff, that is inexpensive to train on large data sets and approaches the quality of Kneser-Ney Smoothing as the amount of training data increases.", "title": "Large Language Models in Machine Translation"}, "a2fc9f1e34b8ab8c350b294c95fbe6a52fd853de": {"paper_id": "a2fc9f1e34b8ab8c350b294c95fbe6a52fd853de", "abstract": "High data rate E-band (71 GHz- 76 GHz, 81 GHz - 86 GHz, 92 GHz - 95 GHz) communication systems will benefit from power amplifiers that are more than twice as powerful than commercially available GaAs pHEMT MMICs. We report development of three stage GaN MMIC power amplifiers for E-band radio applications that produce 500 mW of saturated output power in CW mode and have > 12 dB of associated power gain. The output power density from 300 mum output gate width GaN MMICs is seven times higher than the power density of commercially available GaAs pHEMT MMICs in this frequency range.", "title": "GaN MMIC PAs for E-Band (71 GHz - 95 GHz) Radio"}, "4b60e45b6803e2e155f25a2270a28be9f8bec130": {"paper_id": "4b60e45b6803e2e155f25a2270a28be9f8bec130", "abstract": "Over the last years, the robotics community has made substantial progress in detection and 3D pose estimation of known and unknown objects. However, the question of how to identify objects based on language descriptions has not been investigated in detail. While the computer vision community recently started to investigate the use of attributes for object recognition, these approaches do not consider the task settings typically observed in robotics, where a combination of appearance attributes and object names might be used in referral language to identify specific objects in a scene. In this paper, we introduce an approach for identifying objects based on natural language containing appearance and name attributes. To learn rich RGB-D features needed for attribute classification, we extend recently introduced sparse coding techniques so as to automatically learn attribute-dependent features. We introduce a large data set of attribute descriptions of objects in the RGB-D object dataset. Experiments on this data set demonstrate the strong performance of our approach to language based object identification. We also show that our attribute-dependent features provide significantly better generalization to previously unseen attribute values, thereby enabling more rapid learning of new attribute values.", "title": "Attribute based object identification"}, "6efebd4cb8cb39da348e6818b94c680d3cfe198c": {"paper_id": "6efebd4cb8cb39da348e6818b94c680d3cfe198c", "abstract": "We present a method for learning sparse representations shared across multiple tasks. This method is a generalization of the well-known single-task 1-norm regularization. It is based on a novel non-convex regularizer which controls the number of learned features common across the tasks. We prove that the method is equivalent to solving a convex optimization problem for which there is an iterative algorithm which converges to an optimal solution. The algorithm has a simple interpretation: it alternately performs a supervised and an unsupervised step, where in the former step it learns task-specific functions and in the latter step it learns common-across-tasks sparse representations for these functions. We also provide an extension of the algorithm which learns sparse nonlinear representations using kernels. We report experiments on simulated and real data sets which demonstrate that the proposed method can both improve the performance relative to learning each task independently and lead to a few learned features common across related tasks. Our algorithm can also be used, as a special case, to simply select\u2014not learn\u2014a few common variables across the tasks.", "title": "Convex multi-task feature learning"}, "0addfc35fc8f4419f9e1adeccd19c07f26d35cac": {"paper_id": "0addfc35fc8f4419f9e1adeccd19c07f26d35cac", "abstract": "This paper describes a discriminatively trained, multiscale, deformable part model for object detection. Our system achieves a two-fold improvement in average precision over the best performance in the 2006 PASCAL person detection challenge. It also outperforms the best results in the 2007 challenge in ten out of twenty categories. The system relies heavily on deformable parts. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL challenge. Our system also relies heavily on new methods for discriminative training. We combine a margin-sensitive approach for data mining hard negative examples with a formalism we call latent SVM. A latent SVM, like a hidden CRF, leads to a non-convex training problem. However, a latent SVM is semi-convex and the training problem becomes convex once latent information is specified for the positive examples. We believe that our training methods will eventually make possible the effective use of more latent information such as hierarchical (grammar) models and models involving latent three dimensional pose.", "title": "A discriminatively trained, multiscale, deformable part model"}, "6d43c41e19d994b802f5cff6fbe4e1feffd0d81f": {"paper_id": "6d43c41e19d994b802f5cff6fbe4e1feffd0d81f", "abstract": "We consider the problem of multiclass classification. Our main thesis is that a simple \u201cone-vs-all\u201d scheme is as accurate as any other approach, assuming that the underlying binary classifiers are well-tuned regularized classifiers such as support vector machines. This thesis is interesting in that it disagrees with a large body of recent published work on multiclass classification. We support our position by means of a critical review of the existing literature, a substantial collection of carefully controlled experimental work, and theoretical arguments.", "title": "In Defense of One-Vs-All Classification"}, "2aa330150205f30ce23a6fce88c1de6776744574": {"paper_id": "2aa330150205f30ce23a6fce88c1de6776744574", "abstract": "Current object recognition systems aim at recognizing numerous object classes under limited supervision conditions. This paper provides a benchmark for evaluating progress on this fundamental task. Several methods have recently proposed to utilize the commonalities between object classes in order to improve generalization accuracy. Such methods can be termed interclass transfer techniques. However, it is currently difficult to asses which of the proposed methods maximally utilizes the shared structure of related classes. In order to facilitate the development, as well as the assessment of methods for dealing with multiple related classes, a new dataset including images of several hundred mammal classes, is provided, together with preliminary results of its use. The images in this dataset are organized into five levels of variability, and their labels include information on the objects\u2019 identity, location and pose. From this dataset, a classification benchmark has been derived, requiring fine distinctions between 72 mammal classes. It is then demonstrated that a recognition method which is highly successful on the Caltech101, attains limited accuracy on the current benchmark (36.5%). Since this method does not utilize the shared structure between classes, the question remains as to whether interclass transfer methods can increase the accuracy to the level of human performance (90%). We suggest that a labeled benchmark of the type provided, containing a large number of related classes is crucial for the development and evaluation of classification methods which make efficient use of interclass transfer.", "title": "From Aardvark to Zorro: A Benchmark for Mammal Image Classification"}, "96bea45d9b962c8159883a0d07493a2ea42784e4": {"paper_id": "96bea45d9b962c8159883a0d07493a2ea42784e4", "abstract": "We present a method for learning image representations using a two-layer sparse coding scheme at the pixel level. The first layer encodes local patches of an image. After pooling within local regions, the first layer codes are then passed to the second layer, which jointly encodes signals from the region. Unlike traditional sparse coding methods that encode local patches independently, this approach accounts for high-order dependency among patterns in a local image neighborhood. We develop algorithms for data encoding and codebook learning, and show in experiments that the method leads to more invariant and discriminative image representations. The algorithm gives excellent results for hand-written digit recognition on MNIST and object recognition on the Caltech101 benchmark. This marks the first time that such accuracies have been achieved using automatically learned features from the pixel level, rather than using hand-designed descriptors.", "title": "Learning image representations from the pixel level via hierarchical sparse coding"}, "2071f3ee9ec4d17250b00626d55e47bf75ae2726": {"paper_id": "2071f3ee9ec4d17250b00626d55e47bf75ae2726", "abstract": "where fk is the current approximation, and Rk f the current residual (error). Using initial values of Ro f = f , fo = 0, and k = 1 , the M P algorithm is comprised of the following steps, In this paper we describe a recursive algorithm to compute representations of functions with respect to nonorthogonal and possibly overcomplete dictionaries (I) Compute the inner-products {(Rkf, Zn)},. of elementary building blocks e.g. affine (wavelet) frames. We propose a modification to the Matching Pursuit algorithm of Mallat and Zhang (1992) that maintains full backward orthogonality of the residual convergence. We refer to this modified algorithm as Orthogonal Matching Pursuit (OMP). It is shown that all additional computation required for the OMP algorithm may be performed recursively. (11) Find nktl such that (error) at every step and thereby leads to improved I ( R k f , 'nk+I) l 2 asYp I(Rkf, zj)I 1", "title": "Orthogonal matching pursuit: recursive function approximation with applications to wavelet decomposition"}, "0b7c1bcd0289058b5dfc0d3ff114972712bc7f1a": {"paper_id": "0b7c1bcd0289058b5dfc0d3ff114972712bc7f1a", "abstract": "The traditional SPM approach based on bag-of-features (BoF) requires nonlinear classifiers to achieve good image classification performance. This paper presents a simple but effective coding scheme called Locality-constrained Linear Coding (LLC) in place of the VQ coding in traditional SPM. LLC utilizes the locality constraints to project each descriptor into its local-coordinate system, and the projected coordinates are integrated by max pooling to generate the final representation. With linear classifier, the proposed approach performs remarkably better than the traditional nonlinear SPM, achieving state-of-the-art performance on several benchmarks. Compared with the sparse coding strategy [22], the objective function used by LLC has an analytical solution. In addition, the paper proposes a fast approximated LLC method by first performing a K-nearest-neighbor search and then solving a constrained least square fitting problem, bearing computational complexity of O(M + K2). Hence even with very large codebooks, our system can still process multiple frames per second. This efficiency significantly adds to the practical values of LLC for real applications.", "title": "Locality-constrained Linear Coding for image classification"}, "2221ef8e7fa3f6805cb493543e983c76d233cc69": {"paper_id": "2221ef8e7fa3f6805cb493543e983c76d233cc69", "abstract": "Many successful models for scene or object recognition transform low-level descriptors (such as Gabor filter responses, or SIFT descriptors) into richer representations of intermediate complexity. This process can often be broken down into two steps: (1) a coding step, which performs a pointwise transformation of the descriptors into a representation better adapted to the task, and (2) a pooling step, which summarizes the coded features over larger neighborhoods. Several combinations of coding and pooling schemes have been proposed in the literature. The goal of this paper is threefold. We seek to establish the relative importance of each step of mid-level feature extraction through a comprehensive cross evaluation of several types of coding modules (hard and soft vector quantization, sparse coding) and pooling schemes (by taking the average, or the maximum), which obtains state-of-the-art performance or better on several recognition benchmarks. We show how to improve the best performing coding scheme by learning a supervised discriminative dictionary for sparse coding. We provide theoretical and empirical insight into the remarkable performance of max pooling. By teasing apart components shared by modern mid-level feature extractors, our approach aims to facilitate the design of better recognition architectures.", "title": "Learning mid-level features for recognition"}, "12692fbe915e6bb1c80733519371bbb90ae07539": {"paper_id": "12692fbe915e6bb1c80733519371bbb90ae07539", "abstract": "Robust low-level image features have been proven to be effective representations for a variety of visual recognition tasks such as object recognition and scene classification; but pixels, or even local image patches, carry little semantic meanings. For high level visual tasks, such low-level image representations are potentially not enough. In this paper, we propose a high-level image representation, called the Object Bank, where an image is represented as a scale-invariant response map of a large number of pre-trained generic object detectors, blind to the testing dataset or visual task. Leveraging on the Object Bank representation, superior performances on high level visual recognition tasks can be achieved with simple off-the-shelf classifiers such as logistic regression and linear SVM. Sparsity algorithms make our representation more efficient and scalable for large scene datasets, and reveal semantically meaningful feature patterns.", "title": "Object Bank: A High-Level Image Representation for Scene Classification & Semantic Feature Sparsification"}, "659ccf39592fedb0ee0cd38cf26bff9200b8609f": {"paper_id": "659ccf39592fedb0ee0cd38cf26bff9200b8609f", "abstract": "With an increasing emphasis on security, automated personal identification based on biometrics has been receiving extensive attention over the past decade. Iris recognition, as an emerging biometric recognition approach, is becoming a very active topic in both research and practical applications. In general, a typical iris recognition system includes iris imaging, iris liveness detection, and recognition. This paper focuses on the last issue and describes a new scheme for iris recognition from an image sequence. We first assess the quality of each image in the input sequence and select a clear iris image from such a sequence for subsequent recognition. A bank of spatial filters, whose kernels are suitable for iris recognition, is then used to capture local characteristics of the iris so as to produce discriminating texture features. Experimental results show that the proposed method has an encouraging performance. In particular, a comparative study of existing methods for iris recognition is conducted on an iris image database including 2,255 sequences from 213 subjects. Conclusions based on such a comparison using a nonparametric statistical method (the bootstrap) provide useful information for further research.", "title": "Personal Identification Based on Iris Texture Analysis"}, "088eb2d102c6bb486f5270d0b2adff76961994cf": {"paper_id": "088eb2d102c6bb486f5270d0b2adff76961994cf", "abstract": "Finding human faces automatically in an image is a dif cult yet important rst step to a fully automatic face recognition system This paper presents an example based learning approach for locating unoccluded frontal views of human faces in complex scenes The technique represents the space of human faces by means of a few view based face and non face pattern prototypes At each image location a value distance measure is com puted between the local image pattern and each prototype A trained classi er determines based on the set of dis tance measurements whether a human face exists at the current image location We show empirically that our distance metric is critical for the success of our system", "title": "Example-Based Learning for View-Based Human Face Detection"}, "0160ec003ae238a98676b6412b49d4b760f63544": {"paper_id": "0160ec003ae238a98676b6412b49d4b760f63544", "abstract": "We present an unsupervised technique for visual learning, which is based on density estimation in high-dimensional spaces using an eigenspace decomposition. Two types of density estimates are derived for modeling the training data: a multivariate Gaussian (for unimodal distributions) and a Mixture-of-Gaussians model (for multimodal distributions). These probability densities are then used to formulate a maximum-likelihood estimation framework for visual search and target detection for automatic object recognition and coding. Our learning technique is applied to the probabilistic visual modeling, detection, recognition, and coding of human faces and nonrigid objects, such as hands.", "title": "Probabilistic Visual Learning for Object Representation"}, "f7625798c6e08daa9a603551c71bab68f9abe5dd": {"paper_id": "f7625798c6e08daa9a603551c71bab68f9abe5dd", "abstract": "A b m t-I n this survey we review the impge processing literature on the various approaches and models investigators have uaed for texture.", "title": "Statistical and Structural Approaches to Texture"}, "6513888c5ef473bdbb3167c7b52f0985be071f7a": {"paper_id": "6513888c5ef473bdbb3167c7b52f0985be071f7a", "abstract": "A three-layered neural network is described for transforming two-dimensional discrete signals into generalized nonorthogonal 2-D \u201cGabor\u201d representations for image analysis, segmentation, and compression. These transforms are conjoint spat iahpectral representations [lo], [15], which provide a complete image description in terms of locally windowed 2-D spectral coordinates embedded within global 2-D spatial coordinates. Because intrinsic redundancies within images a re extracted, the resulting image codes can be very compact. However, these conjoint transforms are inherently difficult to compute because t e elementary expansion functions a re not orthogonal. One o r t h o g o n k i n g approach developed for 1-D signals by Bastiaans [SI, based on biorthonormal expansions, is restricted by constraints on the conjoint sampling rates and invariance of the windowing function, as well as by the fact that the auxiliary orthogonalizing functions are nonlocal infinite series. In the present \u201cneural network\u201d approach, based upon interlaminar interactions involving two layers with fixed weights and one layer with adjustable weights, the network finds coefficients for complete conjoint 2-D Gabor transforms without these restrictive conditions. For arbitrary noncomplete transforms, in which the coefficients might be interpreted simply as signifying the presence of certain features in the image, the network finds optimal coefficients in the sense of minimal mean-squared-error in representing the image. I n one algebraically complete scheme permitting exact reconstruction, the network finds expansion coefficients that reduce entropy from 7.57 in the pixel representation to 2.55 in the complete 2-D Gabor transform. In \u201cwavelet\u201d expansions based on a biologically inspired log-polar ensemble of dilations, rotations, and translations of a single underlying 2-D Gabor wavelet template, image compression is illustrated with ratios up to 20: 1. Also demonstrated is image segmentation based on the clustering of coefficients in the complete 2-D Gabor transform. This coefficient-finding network for implementing useful nonorthogonal image transforms may also have neuroscientific relevance, because the network layers with fixed weights use empirical 2-D receptive field profiles obtained from orientation-selective neurons in cat visual cortex as the weighting functions, and the resulting transform mimics the biological visual strategy of embedding angular and spectral analysis within global spatial coordinates.", "title": "Complete discrete 2-D Gabor transforms by neural networks for image analysis and compression"}, "050b2048c2e102219cf41c1a7189365857d014f1": {"paper_id": "050b2048c2e102219cf41c1a7189365857d014f1", "abstract": "This paper presents the transformation of generalized Chebyshev lowpass filter prototype to highpass filter using Suspended Stripline Structure (SSS) technology. The study involves circuit analysis to determine generalized Chebyshev responses with a transmission zero at finite frequency. The transformation of the highpass filter from the lowpass filter prototype provides a cutoff frequency of 3.1 GHz with a return loss better than -20 dB. The design is simulated on a Roger Duroid RO4350 with a dielectric constant, \u03b5r of 3.48 and a thickness of 0.168 mm. The simulation performance results show promising results that could be further examined during the experimental works. This class of generalized Chebyshev highpass filter with finite transmission zero would be useful in any RF/ microwave communication systems particularly in wideband applications where the reduction of overall physical volume and weight as well as cost very important, while maintaining its excellent performance.", "title": "Transformation of generalized chebyshev lowpass filter prototype to Suspended Stripline Structure highpass filter for wideband communication systems"}, "335ba7e12a0f8602769e788212b6dc7b5b0ce5db": {"paper_id": "335ba7e12a0f8602769e788212b6dc7b5b0ce5db", "abstract": "Motivated by a real-life problem of sharing social network data that contain sensitive personal information, we propose a novel approach to release and analyze synthetic graphs in order to protect privacy of individual relationships captured by the social network while maintaining the validity of statistical results. A case study using a version of the Enron e-mail corpus dataset demonstrates the application and usefulness of the proposed techniques in solving the challenging problem of maintaining privacy and supporting open access to network data to ensure reproducibility of existing studies and discovering new scientific insights that can be obtained by analyzing such data. We use a simple yet effective randomized response mechanism to generate synthetic networks under -edge differential privacy, and then use likelihood based inference for missing data and Markov chain Monte Carlo techniques to fit exponential-family random graph models to the generated synthetic networks.", "title": "Sharing Social Network Data: Differentially Private Estimation of Exponential-Family Random Graph Models"}, "19a09658e6c05b44136baf54571a884eb1a7b52e": {"paper_id": "19a09658e6c05b44136baf54571a884eb1a7b52e", "abstract": "A fruitful direction for future data mining research will be the development of techniques that incorporate privacy concerns. Specifically, we address the following question. Since the primary task in data mining is the development of models about aggregated data, can we develop accurate models without access to precise information in individual data records? We consider the concrete case of building a decision-tree classifier from training data in which the values of individual records have been perturbed. The resulting data records look very different from the original records and the distribution of data values is also very different from the original distribution. While it is not possible to accurately estimate original values in individual data records, we propose a novel reconstruction procedure to accurately estimate the distribution of original data values. By using these reconstructed distributions, we are able to build classifiers whose accuracy is comparable to the accuracy of classifiers built with the original data.", "title": "Privacy-Preserving Data Mining"}, "12a9466f10f0c64d4957e3cb4296a4cc2c740761": {"paper_id": "12a9466f10f0c64d4957e3cb4296a4cc2c740761", "abstract": "The increasing ability to track and collect large amounts of data with the use of current hardware technology has lead to an interest in the development of data mining algorithms which preserve user privacy. A recently proposed technique addresses the issue of privacy preservation by perturbing the data and reconstructing distributions at an aggregate level in order to perform the mining. This method is able to retain privacy while accessing the information implicit in the original attributes. The distribution reconstruction process naturally leads to some loss of information which is acceptable in many practical situations. This paper discusses an Expectation Maximization (EM) algorithm for distribution reconstruction which is more effective than the currently available method in terms of the level of information loss. Specifically, we prove that the EM algorithm converges to the maximum likelihood estimate of the original distribution based on the perturbed data. We show that when a large amount of data is available, the EM algorithm provides robust estimates of the original distribution. We propose metrics for quantification and measurement of privacy-preserving data mining algorithms. Thus, this paper provides the foundations for measurement of the effectiveness of privacy preserving data mining algorithms. Our privacy metrics illustrate some interesting results on the relative effectiveness of different perturbing distributions.", "title": "On the Design and Quantification of Privacy Preserving Data Mining Algorithms"}, "bae89bebe8348c15358b9abbd1c7e4b9fd9d4a94": {"paper_id": "bae89bebe8348c15358b9abbd1c7e4b9fd9d4a94", "abstract": "Le Mode\u00c1 le de Culture Fit explique la manie\u00c1 re dont l'environnement socioculturel influence la culture interne au travail et les pratiques de la direction des ressources humaines. Ce mode\u00c1 le a e\u00c2 te\u00c2 teste\u00c2 sur 2003 salarie\u00c2 s d'entreprises prive\u00c2 es dans 10 pays. Les participants ont rempli un questionnaire de 57 items, destine\u00c2 a\u00c1 mesurer les perceptions de la direction sur 4 dimensions socioculturelles, 6 dimensions de culture interne au travail, et les pratiques HRM (Management des Ressources Humaines) dans 3 zones territoiriales. Une analyse ponde\u00c2 re\u00c2 e par re\u00c2 gressions multiples, au niveau individuel, a montre\u00c2 que les directeurs qui caracte\u00c2 risaient leurs environnement socio-culturel de fac\u00cb on fataliste, supposaient aussi que les employe\u00c2 s n'e\u00c2 taient pas malle\u00c2 ables par nature. Ces directeurs ne pratiquaient pas l'enrichissement des postes et donnaient tout pouvoir au contro\u00c3 le et a\u00c1 la re\u00c2 mune\u00c2 ration en fonction des performances. Les directeurs qui appre\u00c2 ciaient une grande loyaute\u00c2 des APPLIED PSYCHOLOGY: AN INTERNATIONAL REVIEW, 2000, 49 (1), 192\u00b1221", "title": "Impact of Culture on Human Resource Management Practices : A 10-Country Comparison"}, "fde62b90b4aa1a735e7b67533cbb272a9df2f28f": {"paper_id": "fde62b90b4aa1a735e7b67533cbb272a9df2f28f", "abstract": "In today's era service development demand increasing day by day and client's ever changing requirements which made agile methodology to come in existence due to faster production, flexible, improved quality and moreover it can accommodate changes, faster feedbacks and transparent communication. But now Due to geographical distribution of team members and client, leading IT thinkers thought to strengthen the agile methodology by giving user's access an environment that has wide-spread access i.e CLOUD. This combination of agile and cloud provide a positive impact to IT companies. But for achieving Quality, we need to test all functionality so in our work we emphasis on testing assimilation using cloud services in agile environment. Cloud team members can perform testing easily using Test-as-a-Service without increasing the cost of the project and get the results faster. We can also use agile management tools to reduce the problem of communication and for all the updates in the product backlog as well as Sprint backlog list.", "title": "Emphasis on testing assimilation using cloud computing for improvised agile SCRUM framework"}, "ca010ffddf272414ad38a9718674e264d1f5d0c5": {"paper_id": "ca010ffddf272414ad38a9718674e264d1f5d0c5", "abstract": "Recurrent neural networks like long short-term memory (LSTM) are important architectures for sequential prediction tasks. LSTMs (and RNNs in general) model sequences along the forward time direction. Bidirectional LSTMs (Bi-LSTMs) on the other hand model sequences along both forward and backward directions and are generally known to perform better at such tasks because they capture a richer representation of the data. In the training of Bi-LSTMs, the forward and backward paths are learned independently. We propose a variant of the Bi-LSTM architecture, which we call Variational Bi-LSTM, that creates a channel between the two paths (during training, but which may be omitted during inference); thus optimizing the two paths jointly. We arrive at this joint objective for our model by minimizing a variational lower bound of the joint likelihood of the data sequence. Our model acts as a regularizer and encourages the two networks to inform each other in making their respective predictions using distinct information. We perform ablation studies to better understand the different components of our model and evaluate the method on various benchmarks, showing state-of-the-art performance.", "title": "Variational Bi-LSTMs"}, "149622ff56b7810ee8f79a135ced2f1d386443f9": {"paper_id": "149622ff56b7810ee8f79a135ced2f1d386443f9", "abstract": "We describe a new approach for modeling the distribution of high-dimensional vectors of discrete variables. This model is inspired by the restricted Boltzmann machine (RBM), which has been shown to be a powerful model of such distributions. However, an RBM typically does not provide a tractable distribution estimator, since evaluating the probability it assigns to some given observation requires the computation of the so-called partition function, which itself is intractable for RBMs of even moderate size. Our model circumvents this difficulty by decomposing the joint distribution of observations into tractable conditional distributions and modeling each conditional using a non-linear function similar to a conditional of an RBM. Our model can also be interpreted as an autoencoder wired such that its output can be used to assign valid probabilities to observations. We show that this new model outperforms other multivariate binary distribution estimators on several datasets and performs similarly to a large (but intractable) RBM.", "title": "The Neural Autoregressive Distribution Estimator"}, "0a2a4b2627121ac71b5953572f61cbbd290209c7": {"paper_id": "0a2a4b2627121ac71b5953572f61cbbd290209c7", "abstract": "We present weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch. This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited. Although our method is much simpler, it still provides much of the speed-up of full batch normalization. In addition, the computational overhead of our method is lower, permitting more optimization steps to be taken in the same amount of time. We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning.", "title": "Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks"}, "17f5c7411eeeeedf25b0db99a9130aa353aee4ba": {"paper_id": "17f5c7411eeeeedf25b0db99a9130aa353aee4ba", "abstract": "We investigate the task of building open domain, conversational dialogue systems based on large dialogue corpora using generative models. Generative models produce system responses that are autonomously generated word-by-word, opening up the possibility for realistic, flexible interactions. In support of this goal, we extend the recently proposed hierarchical recurrent encoder-decoder neural network to the dialogue domain, and demonstrate that this model is competitive with state-of-the-art neural language models and backoff n-gram models. We investigate the limitations of this and similar approaches, and show how its performance can be improved by bootstrapping the learning from a larger questionanswer pair corpus and from pretrained word embeddings.", "title": "Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models"}, "0808bb50993547a533ea5254e0454024d98c5e2f": {"paper_id": "0808bb50993547a533ea5254e0454024d98c5e2f", "abstract": "In this paper, we explore the inclusion of latent random variables into the dynamic hidden state of a recurrent neural network (RNN) by combining elements of the variational autoencoder. We argue that through the use of high-level latent random variables, our variational RNN (VRNN) is able to learn to model the kind of variability observed in highly-structured sequential data (such as speech). We empirically evaluate the proposed model against related sequential models on five sequential datasets, four of speech and one of handwriting. Our results show the importance of the role that latent random variables can play in the RNN dynamic hidden state.", "title": "A Recurrent Latent Variable Model for Sequential Data"}, "52eec5b914f72c4cd3f03eaedf1d38bb9a4df6de": {"paper_id": "52eec5b914f72c4cd3f03eaedf1d38bb9a4df6de", "abstract": "This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-ofthe-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.", "title": "WaveNet: A Generative Model for Raw Audio"}, "5c3785bc4dc07d7e77deef7e90973bdeeea760a5": {"paper_id": "5c3785bc4dc07d7e77deef7e90973bdeeea760a5", "abstract": "Mart\u0131\u0301n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Man\u00e9, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi\u00e9gas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng Google Research\u2217 Abstract", "title": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems"}, "29cb27e32d56f39b9fa5c5bf62225580530a014c": {"paper_id": "29cb27e32d56f39b9fa5c5bf62225580530a014c", "abstract": "We present Deep Voice, a production-quality text-to-speech system constructed entirely from deep neural networks. Deep Voice lays the groundwork for truly end-to-end neural speech synthesis. The system comprises five major building blocks: a segmentation model for locating phoneme boundaries, a grapheme-tophoneme conversion model, a phoneme duration prediction model, a fundamental frequency prediction model, and an audio synthesis model. For the segmentation model, we propose a novel way of performing phoneme boundary detection with deep neural networks using connectionist temporal classification (CTC) loss. For the audio synthesis model, we implement a variant of WaveNet that requires fewer parameters and trains faster than the original. By using a neural network for each component, our system is simpler and more flexible than traditional text-tospeech systems, where each component requires laborious feature engineering and extensive domain expertise. Finally, we show that inference with our system can be performed faster than real time and describe optimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x speedups over existing implementations.", "title": "Deep Voice: Real-time Neural Text-to-Speech"}, "eb5280aff90135c4c3a14f0bf6d6d298260a9887": {"paper_id": "eb5280aff90135c4c3a14f0bf6d6d298260a9887", "abstract": "This paper presents a challenge to the community: given a large corpus of written text aligned to its normalized spoken form, train an RNN to learn the correct normalization function. We present a data set of general text where the normalizations were generated using an existing text normalization component of a text-to-speech system. This data set will be released open-source in the near future. We also present our own experiments with this data set with a variety of different RNN architectures. While some of the architectures do in fact produce very good results when measured in terms of overall accuracy, the errors that are produced are problematic, since they would convey completely the wrong message if such a system were deployed in a speech application. On the other hand, we show that a simple FST-based filter can mitigate those errors, and achieve a level of accuracy not achievable by the RNN alone. Though our conclusions are largely negative on this point, we are actually not arguing that the text normalization problem is intractable using an pure RNN approach, merely that it is not going to be something that can be solved merely by having huge amounts of annotated text data and feeding that to a general RNN model. Andwhenwe open-source our data, we will be providing a novel data set for sequenceto-sequence modeling in the hopes that the the community can find better solutions.", "title": "RNN Approaches to Text Normalization: A Challenge"}, "7c4824cc17bf735a7f80d128cc7ac6c7a8ab8aec": {"paper_id": "7c4824cc17bf735a7f80d128cc7ac6c7a8ab8aec", "abstract": "Grapheme-to-phoneme (G2P) models are key components in speech recognition and text-to-speech systems as they describe how words are pronounced. We propose a G2P model based on a Long Short-Term Memory (LSTM) recurrent neural network (RNN). In contrast to traditional joint-sequence based G2P approaches, LSTMs have the flexibility of taking into consideration the full context of graphemes and transform the problem from a series of grapheme-to-phoneme conversions to a word-to-pronunciation conversion. Training joint-sequence based G2P require explicit grapheme-to-phoneme alignments which are not straightforward since graphemes and phonemes don't correspond one-to-one. The LSTM based approach forgoes the need for such explicit alignments. We experiment with unidirectional LSTM (ULSTM) with different kinds of output delays and deep bidirectional LSTM (DBLSTM) with a connectionist temporal classification (CTC) layer. The DBLSTM-CTC model achieves a word error rate (WER) of 25.8% on the public CMU dataset for US English. Combining the DBLSTM-CTC model with a joint n-gram model results in a WER of 21.3%, which is a 9% relative improvement compared to the previous best WER of 23.4% from a hybrid system.", "title": "Grapheme-to-phoneme conversion using Long Short-Term Memory recurrent neural networks"}, "5a61035df1c7ae84a2db92c41b63b84f812ba3fd": {"paper_id": "5a61035df1c7ae84a2db92c41b63b84f812ba3fd", "abstract": "MOS (mean opinion score) subjective quality studies are used to evaluate many signal processing methods. Since laboratory quality studies are time consuming and expensive, researchers often run small studies with less statistical significance or use objective measures which only approximate human perception. We propose a cost-effective and convenient measure called crowdMOS, obtained by having internet users participate in a MOS-like listening study. Workers listen and rate sentences at their leisure, using their own hardware, in an environment of their choice. Since these individuals cannot be supervised, we propose methods for detecting and discarding inaccurate scores. To automate crowdMOS testing, we offer a set of freely distributable, open-source tools for Amazon Mechanical Turk, a platform designed to facilitate crowdsourcing. These tools implement the MOS testing methodology described in this paper, providing researchers with a user-friendly means of performing subjective quality evaluations without the overhead associated with laboratory studies. Finally, we demonstrate the use of crowdMOS using data from the Blizzard text-to-speech competition, showing that it delivers accurate and repeatable results.", "title": "CROWDMOS: An approach for crowdsourcing mean opinion score studies"}, "736aebd8036bc3a1afc787afe256d9159c155c78": {"paper_id": "736aebd8036bc3a1afc787afe256d9159c155c78", "abstract": "We introduce Opcodes, a Python package which presents x86 and x86-64 instruction sets as a set of high-level objects. Opcodes provides information about instruction names, implicit and explicit operands, and instruction encoding. We use the Opcodes package to auto-generate instruction classes for PeachPy, an x86-64 assembler embedded in Python, and enable new functionality.\n The new PeachPy functionality lets low-level optimization experts write high-performance assembly kernels in Python, load them as callable Python functions, test the kernels using numpy and generate object files for Windows, Linux, and Mac OS X entirely within Python. Additionally, the new PeachPy can generate and run assembly code inside Chromium-based browsers by leveraging Native Client technology. Beyond that, PeachPy gained ability to target Google Go toolchain, by generating either source listing for Go assembler, or object files that can be linked with Go toolchain.\n With backends for Windows, Linux, Mac OS X, Native Client, and Go, PeachPy is the most portable way to write high-performance kernels for x86-64 architecture.", "title": "PeachPy meets Opcodes: direct machine code generation from Python"}, "1f75ebc70603046c690699499b069e207035ef1a": {"paper_id": "1f75ebc70603046c690699499b069e207035ef1a", "abstract": "Conventional approaches to statistical parametric speech synthesis typically use decision tree-clustered context-dependent hidden Markov models (HMMs) to represent probability densities of speech parameters given texts. Speech parameters are generated from the probability densities to maximize their output probabilities, then a speech waveform is reconstructed from the generated parameters. This approach is reasonably effective but has a couple of limitations, e.g. decision trees are inefficient to model complex context dependencies. This paper examines an alternative scheme that is based on a deep neural network (DNN). The relationship between input texts and their acoustic realizations is modeled by a DNN. The use of the DNN can address some limitations of the conventional approach. Experimental results show that the DNN-based systems outperformed the HMM-based systems with similar numbers of parameters.", "title": "Statistical parametric speech synthesis using deep neural networks"}, "9a0fff9611832cd78a82a32f47b8ca917fbd4077": {"paper_id": "9a0fff9611832cd78a82a32f47b8ca917fbd4077", "abstract": null, "title": "Text-To-Speech Synthesis"}, "146f6f6ed688c905fb6e346ad02332efd5464616": {"paper_id": "146f6f6ed688c905fb6e346ad02332efd5464616", "abstract": "Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-theart performance on three benchmark datasets: Flickr9k, Flickr30k and MS COCO.", "title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention"}, "2ceaa8d6ee74105a6b5561661db299c885f9135b": {"paper_id": "2ceaa8d6ee74105a6b5561661db299c885f9135b", "abstract": "We introduce a general strategy for improving neural sequence generation by incorporating knowledge about the future. Our decoder combines a standard sequence decoder with a \u2018soothsayer\u2019 prediction function Q that estimates the outcome in the future of generating a word in the present. Our model draws on the same intuitions as reinforcement learning, but is both simpler and higher performing, avoiding known problems with the use of reinforcement learning in tasks with enormous search spaces like sequence generation. We demonstrate our model by incorporating Q functions that incrementally predict what the future BLEU or ROUGE score of the completed sequence will be, its future length, and the backwards probability of the source given the future target sequence. Experimental results show that future prediction yields improved performance in abstractive summarization and conversational response generation and the state-of-the-art in machine translation, while also enabling the decoder to generate outputs that have specific properties.", "title": "Learning to Decode for Future Success"}, "0c7d7b4c546e38a4097a97bf1d16a60012916758": {"paper_id": "0c7d7b4c546e38a4097a97bf1d16a60012916758", "abstract": "We describe the design of Kaldi, a free, open-source toolkit for speech recognition research. Kaldi provides a speech recognition system based on finite-state transducers (using the freely available OpenFst), together with detailed documentation and scripts for building complete recognition systems. Kaldi is written is C++, and the core library supports modeling of arbitrary phonetic-context sizes, acoustic modeling with subspace Gaussian mixture models (SGMM) as well as standard Gaussian mixture models, together with all commonly used linear and affine transforms. Kaldi is released under the Apache License v2.0, which is highly nonrestrictive, making it suitable for a wide community of users.", "title": "The Kaldi Speech Recognition Toolkit"}, "6b570069f14c7588e066f7138e1f21af59d62e61": {"paper_id": "6b570069f14c7588e066f7138e1f21af59d62e61", "abstract": "Theano is a Python library that allows to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. Since its introduction, it has been one of the most used CPU and GPU mathematical compilers - especially in the machine learning community - and has shown steady performance improvements. Theano is being actively and continuously developed since 2008, multiple frameworks have been built on top of it and it has been used to produce many state-of-the-art machine learning models. The present article is structured as follows. Section I provides an overview of the Theano software and its community. Section II presents the principal features of Theano and how to use them, and compares them with other similar projects. Section III focuses on recently-introduced functionalities and improvements. Section IV compares the performance of Theano against Torch7 and TensorFlow on several machine learning models. Section V discusses current limitations of Theano and potential ways of improving it.", "title": "Theano: A Python framework for fast computation of mathematical expressions"}, "26c8d040bef85ad6dde55a8f71af936fb38356ad": {"paper_id": "26c8d040bef85ad6dde55a8f71af936fb38356ad", "abstract": "Recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks including machine translation, handwriting synthesis [1, 2] and image caption generation [3]. We extend the attention-mechanism with features needed for speech recognition. We show that while an adaptation of the model used for machine translation in [2] reaches a competitive 18.7% phoneme error rate (PER) on the TIMIT phoneme recognition task, it can only be applied to utterances which are roughly as long as the ones it was trained on. We offer a qualitative explanation of this failure and propose a novel and generic method of adding location-awareness to the attention mechanism to alleviate this issue. The new method yields a model that is robust to long inputs and achieves 18% PER in single utterances and 20% in 10-times longer (repeated) utterances. Finally, we propose a change to the attention mechanism that prevents it from concentrating too much on single frames, which further reduces PER to 17.6% level.", "title": "Attention-Based Models for Speech Recognition"}, "59b81ff81da55efc724c84ddc9d3ffd8e57a8d0e": {"paper_id": "59b81ff81da55efc724c84ddc9d3ffd8e57a8d0e", "abstract": "We introduce two Python frameworks to train neural networks on large datasets: Blocks and Fuel. Blocks is based on Theano, a linear algebra compiler with CUDA-support (Bastien et al., 2012; Bergstra et al., 2010). It facilitates the training of complex neural network models by providing parametrized Theano operations, attaching metadata to Theano\u2019s symbolic computational graph, and providing an extensive set of utilities to assist training the networks, e.g. training algorithms, logging, monitoring, visualization, and serialization. Fuel provides a standard format for machine learning datasets. It allows the user to easily iterate over large datasets, performing many types of pre-processing on the fly.", "title": "Blocks and Fuel: Frameworks for deep learning"}, "9665247ea3421929f9b6ad721f139f11edb1dbb8": {"paper_id": "9665247ea3421929f9b6ad721f139f11edb1dbb8", "abstract": "Recurrent neural network is a powerful model that learns temporal patterns in sequential data. For a long time, it was believed that recurrent networks are difficult to train using simple optimizers, such as stochastic gradient descent, due to the so-called vanishing gradient problem. In this paper, we show that learning longer term patterns in real data, such as in natural language, is perfectly possible using gradient descent. This is achieved by using a slight structural modification of the simple recurrent neural network architecture. We encourage some of the hidden units to change their state slowly by making part of the recurrent weight matrix close to identity, thus forming a kind of longer term memory. We evaluate our model on language modeling tasks on benchmark datasets, where we obtain similar performance to the much more complex Long Short Term Memory (LSTM) networks (Hochreiter & Schmidhuber, 1997).", "title": "Learning Longer Memory in Recurrent Neural Networks"}, "033eb044ef6a865a53878397633876827b7a8f20": {"paper_id": "033eb044ef6a865a53878397633876827b7a8f20", "abstract": "We describe a simple neural language model that relies only on character-level inputs. Predictions are still made at the word-level. Our model employs a convolutional neural network (CNN) and a highway network over characters, whose output is given to a long shortterm memory (LSTM) recurrent neural network language model (RNN-LM). On the English Penn Treebank the model is on par with the existing state-ofthe-art despite having 60% fewer parameters. On languages with rich morphology (Czech, German, French, Spanish, Russian), the model consistently outperforms a Kneser-Ney baseline and word-level/morpheme-level LSTM baselines, again with far fewer parameters. Our results suggest that on many languages, character inputs are sufficient for language modeling.", "title": "Character-Aware Neural Language Models"}, "a38168015a783fecc5830260a7eb5b9e3e945ee2": {"paper_id": "a38168015a783fecc5830260a7eb5b9e3e945ee2", "abstract": "Very deep convolutional networks with hundreds of layers have led to significant reductions in error on competitive benchmarks. Although the unmatched expressiveness of the many layers can be highly desirable at test time, training very deep networks comes with its own set of challenges. The gradients can vanish, the forward flow often diminishes, and the training time can be painfully slow. To address these problems, we propose stochastic depth, a training procedure that enables the seemingly contradictory setup to train short networks and use deep networks at test time. We start with very deep networks but during training, for each mini-batch, randomly drop a subset of layers and bypass them with the identity function. This simple approach complements the recent success of residual networks. It reduces training time substantially and improves the test error significantly on almost all data sets that we used for evaluation. With stochastic depth we can increase the depth of residual networks even beyond 1200 layers and still yield meaningful improvements in test error (4.91% on CIFAR-10).", "title": "Deep Networks with Stochastic Depth"}, "533ee188324b833e059cb59b654e6160776d5812": {"paper_id": "533ee188324b833e059cb59b654e6160776d5812", "abstract": "In this paper, we explore different ways to extend a recurrent neural network (RNN) to a deep RNN. We start by arguing that the concept of depth in an RNN is not as clear as it is in feedforward neural networks. By carefully analyzing and understanding the architecture of an RNN, however, we find three points of an RNN which may be made deeper; (1) input-to-hidden function, (2) hidden-tohidden transition and (3) hidden-to-output function. Based on this observation, we propose two novel architectures of a deep RNN which are orthogonal to an earlier attempt of stacking multiple recurrent layers to build a deep RNN (Schmidhuber, 1992; El Hihi and Bengio, 1996). We provide an alternative interpretation of these deep RNNs using a novel framework based on neural operators. The proposed deep RNNs are empirically evaluated on the tasks of polyphonic music prediction and language modeling. The experimental result supports our claim that the proposed deep RNNs benefit from the depth and outperform the conventional, shallow RNNs.", "title": "How to Construct Deep Recurrent Neural Networks"}, "5de063c31725b578dcbaa0d2cee514f7e13874aa": {"paper_id": "5de063c31725b578dcbaa0d2cee514f7e13874aa", "abstract": "We describe Swapout, a new stochastic training method, that outperforms ResNets of identical network structure yielding impressive results on CIFAR-10 and CIFAR100. Swapout samples from a rich set of architectures including dropout [20], stochastic depth [7] and residual architectures [5, 6] as special cases. When viewed as a regularization method swapout not only inhibits co-adaptation of units in a layer, similar to dropout, but also across network layers. We conjecture that swapout achieves strong regularization by implicitly tying the parameters across layers. When viewed as an ensemble training method, it samples a much richer set of architectures than existing methods such as dropout or stochastic depth. We propose a parameterization that reveals connections to exiting architectures and suggests a much richer set of architectures to be explored. We show that our formulation suggests an efficient training method and validate our conclusions on CIFAR-10 and CIFAR-100 matching state of the art accuracy. Remarkably, our 32 layer wider model performs similar to a 1001 layer ResNet model.", "title": "Swapout: Learning an ensemble of deep architectures"}, "02227c94dd41fe0b439e050d377b0beb5d427cda": {"paper_id": "02227c94dd41fe0b439e050d377b0beb5d427cda", "abstract": "Detecting and reading text from natural images is a hard computer vision task that is central to a variety of emerging applications. Related problems like document character recognition have been widely studied by computer vision and machine learning researchers and are virtually solved for practical applications like reading handwritten digits. Reliably recognizing characters in more complex scenes like photographs, however, is far more difficult: the best existing methods lag well behind human performance on the same tasks. In this paper we attack the problem of recognizing digits in a real application using unsupervised feature learning methods: reading house numbers from street level photos. To this end, we introduce a new benchmark dataset for research use containing over 600,000 labeled digits cropped from Street View images. We then demonstrate the difficulty of recognizing these digits when the problem is approached with hand-designed features. Finally, we employ variants of two recently proposed unsupervised feature learning methods and find that they are convincingly superior on our benchmarks.", "title": "Reading Digits in Natural Images with Unsupervised Feature Learning"}, "060be747e12b7f2b00fca7b5d5cdcba113daff76": {"paper_id": "060be747e12b7f2b00fca7b5d5cdcba113daff76", "abstract": "The Neural Autoregressive Distribution Estimator (NADE) and its real-valued version RNADE are competitive density models of multidimensional data across a variety of domains. These models use a fixed, arbitrary ordering of the data dimensions. One can easily condition on variables at the beginning of the ordering, and marginalize out variables at the end of the ordering, however other inference tasks require approximate inference. In this work we introduce an efficient procedure to simultaneously train a NADE model for each possible ordering of the variables, by sharing parameters across all these models. We can thus use the most convenient model for each inference task at hand, and ensembles of such models with different orderings are immediately available. Moreover, unlike the original NADE, our training procedure scales to deep models. Empirically, ensembles of Deep NADE models obtain state of the art density estimation performance.", "title": "A Deep and Tractable Density Estimator"}, "6ef259c2f6d50373abfec14fcb8fa924f7b7af0b": {"paper_id": "6ef259c2f6d50373abfec14fcb8fa924f7b7af0b", "abstract": "This paper presents experiments extending the work of Ba et al. (2014) on recurrent neural models for attention into less constrained visual environments, beginning with fine-grained categorization on the Stanford Dogs data set. In this work we use an RNN of the same structure but substitute a more powerful visual network and perform large-scale pre-training of the visual network outside of the attention RNN. Most work in attention models to date focuses on tasks with toy or more constrained visual environments. We present competitive results for fine-grained categorization. More importantly, we show that our model learns to direct high resolution attention to the most discriminative regions without any spatial supervision such as bounding boxes. Given a small input window, it is hence able to discriminate fine-grained dog breeds with cheap glances at faces and fur patterns, while avoiding expensive and distracting processing of entire images. In addition to allowing high resolution processing with a fixed budget and naturally handling static or sequential inputs, this approach has the major advantage of being trained end-to-end, unlike most current approaches which are heavily engineered.", "title": "Attention for Fine-Grained Categorization"}, "c060b61123ecbbe57c2db28f96d70544cb45250c": {"paper_id": "c060b61123ecbbe57c2db28f96d70544cb45250c", "abstract": "Training of the neural autoregressive density estimator (NADE) can be viewed as doing one step of probabilistic inference on missing values in data. We propose a new model that extends this inference scheme to multiple steps, arguing that it is easier to learn to improve a reconstruction in k steps rather than to learn to reconstruct in a single inference step. The proposed model is an unsupervised building block for deep learning that combines the desirable properties of NADE and multi-prediction training: (1) Its test likelihood can be computed analytically, (2) it is easy to generate independent samples from it, and (3) it uses an inference engine that is a superset of variational inference for Boltzmann machines. The proposed NADE-k is competitive with the state-of-the-art in density estimation on the two datasets tested.", "title": "Iterative Neural Autoregressive Distribution Estimator (NADE-k)"}, "2ae38a5aaf2a315f8726b885443ab2db5aa2897b": {"paper_id": "2ae38a5aaf2a315f8726b885443ab2db5aa2897b", "abstract": "There has been a lot of recent interest in designing neural network models to estimate a distribution from a set of examples. We introduce a simple modification for autoencoder neural networks that yields powerful generative models. Our method masks the autoencoder\u2019s parameters to respect autoregressive constraints: each input is reconstructed only from previous inputs in a given ordering. Constrained this way, the autoencoder outputs can be interpreted as a set of conditional probabilities, and their product, the full joint probability. We can also train a single network that can decompose the joint probability in multiple different orderings. Our simple framework can be applied to multiple architectures, including deep ones. Vectorized implementations, such as on GPUs, are simple and fast. Experiments demonstrate that this approach is competitive with stateof-the-art tractable distribution estimators. At test time, the method is significantly faster and scales better than other autoregressive estimators.", "title": "MADE: Masked Autoencoder for Distribution Estimation"}, "0523e14247d74c4505cd5e32e1f0495f291ec432": {"paper_id": "0523e14247d74c4505cd5e32e1f0495f291ec432", "abstract": "Generative models can be seen as the swiss army knives of machine learning, as many problems can be written probabilistically in terms of the distribution of the data, including prediction, reconstruction, imputation and simulation. One of the most promising directions for unsupervised learning may lie in Deep Learning methods, given their success in supervised learning. However, one of the current problems with deep unsupervised learning methods, is that they often are harder to scale. As a result there are some easier, more scalable shallow methods, such as the Gaussian Mixture Model and the Student-t Mixture Model, that remain surprisingly competitive. In this paper we propose a new scalable deep generative model for images, called the Deep Gaussian Mixture Model, that is a straightforward but powerful generalization of GMMs to multiple layers. The parametrization of a Deep GMM allows it to efficiently capture products of variations in natural images. We propose a new EM-based algorithm that scales well to large datasets, and we show that both the Expectation and the Maximization steps can easily be distributed over multiple machines. In our density estimation experiments we show that deeper GMM architectures generalize better than more shallow ones, with results in the same ballpark as the state of the art.", "title": "Factoring Variations in Natural Images with Deep Gaussian Mixture Models"}, "138933458b607ab24027d1ea3e8af3fed3774de8": {"paper_id": "138933458b607ab24027d1ea3e8af3fed3774de8", "abstract": "Convolutional Neural Networks (CNNs) can be shifted across 2D images or 3D videos to segment them. They have a fixed input size and typically perceive only small local contexts of the pixels to be classified as foreground or background. In contrast, Multi-Dimensional Recurrent NNs (MD-RNNs) can perceive the entire spatio-temporal context of each pixel in a few sweeps through all pixels, especially when the RNN is a Long Short-Term Memory (LSTM). Despite these theoretical advantages, however, unlike CNNs, previous MD-LSTM variants were hard to parallelise on GPUs. Here we re-arrange the traditional cuboid order of computations in MD-LSTM in pyramidal fashion. The resulting PyraMiD-LSTM is easy to parallelise, especially for 3D data such as stacks of brain slice images. PyraMiD-LSTM achieved best known pixel-wise brain image segmentation results on MRBrainS13 (and competitive results on EM-ISBI12).", "title": "Parallel Multi-Dimensional LSTM, With Application to Fast Biomedical Volumetric Image Segmentation"}, "94a62f470aeea69af436e2dd0b54cd50eaaa4b23": {"paper_id": "94a62f470aeea69af436e2dd0b54cd50eaaa4b23", "abstract": "As one of the most successful approaches to building recommender systems, collaborative filtering (CF) uses the known preferences of a group of users to make recommendations or predictions of the unknown preferences for other users. In this paper, we first introduce CF tasks and their main challenges, such as data sparsity, scalability, synonymy, gray sheep, shilling attacks, privacy protection, etc., and their possible solutions. We then present three main categories of CF techniques: memory-based, modelbased, and hybrid CF algorithms (that combine CF with other recommendation techniques), with examples for representative algorithms of each category, and analysis of their predictive performance and their ability to address the challenges. From basic techniques to the state-of-the-art, we attempt to present a comprehensive survey for CF techniques, which can be served as a roadmap for research and practice in this area.", "title": "A Survey of Collaborative Filtering Techniques"}, "23bbea130398331084021cc895a132064219b4b1": {"paper_id": "23bbea130398331084021cc895a132064219b4b1", "abstract": "In this paper, we consider collaborative filtering as a ranking problem. We present a method which uses Maximum Margin Matrix Factorization and optimizes ranking instead of rating. We employ structured output prediction to optimize directly for ranking scores. Experimental results show that our method gives very good ranking scores and scales well on collaborative filtering tasks.", "title": "COFI RANK - Maximum Margin Matrix Factorization for Collaborative Ranking"}, "8bbcded0c093f62bd2a54cad58bcc8114aed5aa6": {"paper_id": "8bbcded0c093f62bd2a54cad58bcc8114aed5aa6", "abstract": "Mining detailed opinions buried in the vast amount of review text data is an important, yet quite challenging task with widespread applications in multiple domains. Latent Aspect Rating Analysis (LARA) refers to the task of inferring both opinion ratings on topical aspects (e.g., location, service of a hotel) and the relative weights reviewers have placed on each aspect based on review content and the associated overall ratings. A major limitation of previous work on LARA is the assumption of pre-specified aspects by keywords. However, the aspect information is not always available, and it may be difficult to pre-define appropriate aspects without a good knowledge about what aspects are actually commented on in the reviews.\n In this paper, we propose a unified generative model for LARA, which does not need pre-specified aspect keywords and simultaneously mines 1) latent topical aspects, 2) ratings on each identified aspect, and 3) weights placed on different aspects by a reviewer. Experiment results on two different review data sets demonstrate that the proposed model can effectively perform the Latent Aspect Rating Analysis task without the supervision of aspect keywords. Because of its generality, the proposed model can be applied to explore all kinds of opinionated text data containing overall sentiment judgments and support a wide range of interesting application tasks, such as aspect-based opinion summarization, personalized entity ranking and recommendation, and reviewer behavior analysis.", "title": "Latent aspect rating analysis without aspect keyword supervision"}, "e25221b4c472c4337383341f6b2c9375e86709af": {"paper_id": "e25221b4c472c4337383341f6b2c9375e86709af", "abstract": null, "title": "Matrix Factorization Techniques for Recommender Systems"}, "8c4184b7c7d8c7f6d2801d75832b10693d690f20": {"paper_id": "8c4184b7c7d8c7f6d2801d75832b10693d690f20", "abstract": "Data sparsity, scalability and prediction quality have been recognized as the three most crucial challenges that every collaborative filtering algorithm or recommender system confronts. Many existing approaches to recommender systems can neither handle very large datasets nor easily deal with users who have made very few ratings or even none at all. Moreover, traditional recommender systems assume that all the users are independent and identically distributed; this assumption ignores the social interactions or connections among users. In view of the exponential growth of information generated by online social networks, social network analysis is becoming important for many Web applications. Following the intuition that a person's social network will affect personal behaviors on the Web, this paper proposes a factor analysis approach based on probabilistic matrix factorization to solve the data sparsity and poor prediction accuracy problems by employing both users' social network information and rating records. The complexity analysis indicates that our approach can be applied to very large datasets since it scales linearly with the number of observations, while the experimental results shows that our method performs much better than the state-of-the-art approaches, especially in the circumstance that users have made few or no ratings.", "title": "SoRec: social recommendation using probabilistic matrix factorization"}, "0ce331085b6b7b95a6e9b5c8fa7dea564d2161fe": {"paper_id": "0ce331085b6b7b95a6e9b5c8fa7dea564d2161fe", "abstract": "Wireless networking is widespread in public places such as cafes. Unsuspecting users may become victims of attacks based on \"evil twin\" access points. These rogue access points are operated by criminals in an attempt to launch man-in-the-middle attacks. We present a simple protection mechanism against binding to an evil twin. The mechanism leverages short authentication string protocols for the exchange of cryptographic keys. The short string verification is performed by encoding the short strings as a sequence of colors, rendered sequentially by the user's device and by the designated access point of the cafe. The access point must have a light capable of showing two colors and must be mounted prominently in a position where users can have confidence in its authenticity. We conducted a usability study with patrons in several cafes and participants found our mechanism very usable.", "title": "Simple and effective defense against evil twin access points"}, "4c4b482bde1f5cc2935b7a1dfcdd3dc124e4279c": {"paper_id": "4c4b482bde1f5cc2935b7a1dfcdd3dc124e4279c", "abstract": "Six and a half years after adoption. 6- to 12-year-old children reared in Romanian orphanages for more than 8 months in their first years of life (RO. n = 18) had higher cortisol levels over the daytime hours than did early adopted (EA, < or = 4 months of age, n = 15) and Canadian born (CB, n = 27) children. The effect was marked, with 22% of the RO children exhibiting cortisol levels averaged over the day that exceeded the mean plus 2 SD of the EA and CB levels. Furthermore, the longer beyond 8 months that the RO children remained institutionalized the higher their cortisol levels. Cortisol levels for EA children did not differ in any respect from those of CB comparison children. This latter finding reduces but does not eliminate concerns that the results could be due to prenatal effects or birth family characteristics associated with orphanage placement. Neither age at cortisol sampling nor low IQ measured earlier appeared to explain the findings. Because the conditions in Romanian orphanages at the time these children were adopted were characterized by multiple risk factors, including gross privation of basic needs and exposure to infectious agents, the factor(s) that produced the increase in cortisol production cannot be determined. Nor could we determine whether these results reflected effects on the limbic-hypothalamic-pituitary-adrenal axis directly or were mediated by differences in parent-child interactions or family stress occasion by behavioral problems associated with prolonged orphanage care in this sample.", "title": "Salivary cortisol levels in children adopted from romanian orphanages."}}
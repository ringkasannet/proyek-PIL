{"771636b26260fac6d215df5e76c9ce72c346ba88": {"paper_id": "771636b26260fac6d215df5e76c9ce72c346ba88", "abstract": null, "title": "Binary codes capable of correcting deletions, insertions and reversals"}, "4cdd5447031c4f934b171622f8694d76541cdb0d": {"paper_id": "4cdd5447031c4f934b171622f8694d76541cdb0d", "abstract": "From the ancient times, abortion appeared as a method of controlling the fertility. But starting with XIX century, modern states used the abortion as a mechanism of demographic policy, the state intending to adjust the fertility of population by this. It is worth underlining the ethical and moral aspects of abortion, which determined strong debates on the political scene and further at the level of civil society and mass media. So, in the last decades there have been questioned not only ethic and moral implications of abortion, but also practical outsets. A responsible demographic policy should take into account the whole set of socio-economic and cultural factors that condition a society.", "title": "THE 1966 LAW CONCERNING PROHIBITION OF ABORTION IN ROMANIA AND ITS CONSEQUENCES. THE FATE OF ONE GENERATION"}, "663444f7bb70eb20c1f9c6c084db4d7a1dff21c4": {"paper_id": "663444f7bb70eb20c1f9c6c084db4d7a1dff21c4", "abstract": "We define the task of teaser generation and provide an evaluation benchmark and baseline systems for it. A teaser is a short reading suggestion for an article that is illustrative and includes curiosity-arousing elements to entice potential readers to read the news item. Teasers are one of the main vehicles for transmitting news to social media users. We compile a novel dataset of teasers by systematically accumulating tweets and selecting ones that conform to the teaser definition. We compare a number of neural abstractive architectures on the task of teaser generation and the overall best performing system is See et al. (2017)\u2019s seq2seq with pointer network.", "title": "News Article Teaser Tweets and How to Generate Them"}, "480a3f5ff15485601d0346c17adc6334de430d63": {"paper_id": "480a3f5ff15485601d0346c17adc6334de430d63", "abstract": "In this paper we generalise the sentence compression task. Rather than simply shorten a sentence by deleting words or constituents, as in previous work, we rewrite it using additional operations such as substitution, reordering, and insertion. We present a new corpus that is suited to our task and a discriminative tree-totree transduction model that can naturally account for structural and lexical mismatches. The model incorporates a novel grammar extraction method, uses a language model for coherent output, and can be easily tuned to a wide range of compression specific loss functions.", "title": "Sentence Compression Beyond Word Deletion"}, "0104063400e6d69294edc95fb14c7e8fac347f6a": {"paper_id": "0104063400e6d69294edc95fb14c7e8fac347f6a", "abstract": "In typical classification tasks, we seek a function which assigns a label to a single object. Kernel-based approaches, such as support vector machines (SVMs), which maximize the margin of confidence of the classifier, are the method of choice for many such tasks. Their popularity stems both from the ability to use high-dimensional feature spaces, and from their strong theoretical guarantees. However, many real-world tasks involve sequential, spatial, or structured data, where multiple labels must be assigned. Existing kernel-based methods ignore structure in the problem, assigning labels independently to each object, losing much useful information. Conversely, probabilistic graphical models, such as Markov networks, can represent correlations between labels, by exploiting problem structure, but cannot handle high-dimensional feature spaces, and lack strong theoretical generalization guarantees. In this paper, we present a new framework that combines the advantages of both approaches: Maximum margin Markov (M ) networks incorporate both kernels, which efficiently deal with highdimensional features, and the ability to capture correlations in structured data. We present an efficient algorithm for learning M networks based on a compact quadratic program formulation. We provide a new theoretical bound for generalization in structured domains. Experiments on the task of handwritten character recognition, demonstrate very significant gains over previous approaches.", "title": "Max-Margin Markov Networks"}, "a2c2999b134ba376c5ba3b610900a8d07722ccb3": {"paper_id": "a2c2999b134ba376c5ba3b610900a8d07722ccb3", "abstract": null, "title": "Bleu: a Method for Automatic Evaluation of Machine Translation"}, "1c23e1ad1a538416e8123f128a87c928b09be868": {"paper_id": "1c23e1ad1a538416e8123f128a87c928b09be868", "abstract": "We present an unsupervised approach to symmetric word alignment in which two simple asymmetric models are trained jointly to maximize a combination of data likelihood and agreement between the models. Compared to the standard practice of intersecting predictions of independently-trained models, joint training provides a 32% reduction in AER. Moreover, a simple and efficient pair of HMM aligners provides a 29% reduction in AER over symmetrized IBM model 4 predictions. Disciplines Computer Sciences Comments Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics (HLT-NAACL '06). Association for Computational Linguistics, Stroudsburg, PA, USA, 104-111. DOI=10.3115/1220835.1220849 http://dx.doi.org/10.3115/1220835.1220849 \u00a9 ACM, 2006. This is the author's version of the work. It is posted here by permission of ACM for your personal use. Not for redistribution. The definitive version was published in Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, {(2006)} http://doi.acm.org/10.3115/1220835.1220849\" Email permissions@acm.org This conference paper is available at ScholarlyCommons: http://repository.upenn.edu/cis_papers/533 Alignment by Agreement Percy Liang UC Berkeley Berkeley, CA 94720 pliang@cs.berkeley.edu Ben Taskar UC Berkeley Berkeley, CA 94720 taskar@cs.berkeley.edu Dan Klein UC Berkeley Berkeley, CA 94720 klein@cs.berkeley.edu", "title": "Alignment by Agreement"}, "5f9623a2959117dc05f2af3b961fd035a3f22d41": {"paper_id": "5f9623a2959117dc05f2af3b961fd035a3f22d41", "abstract": "We present a novel sentence reduction system for automatically removing extraneous phrases from sentences that are extracted from a document for summarization purpose. The system uses multiple sources of knowledge to decide which phrases in an extracted sentence can be removed, including syntactic knowledge, context information, and statistics computed from a corpus which consists of examples written by human professionals. Reduction can significantly improve the conciseness of automatic summaries.", "title": "Sentence Reduction for Automatic Text Summarization"}, "175c1bb60ee46dac56d942ef8c7339977b4ebb0e": {"paper_id": "175c1bb60ee46dac56d942ef8c7339977b4ebb0e", "abstract": "This paper presents a Support Vector Method for optimizing multivariate nonlinear performance measures like the F1-score. Taking a multivariate prediction approach, we give an algorithm with which such multivariate SVMs can be trained in polynomial time for large classes of potentially non-linear performance measures, in particular ROCArea and all measures that can be computed from the contingency table. The conventional classification SVM arises as a special case of our method.", "title": "A support vector method for multivariate performance measures"}, "930498cecfed4b1fcfaf37c7a1543660f424c4c7": {"paper_id": "930498cecfed4b1fcfaf37c7a1543660f424c4c7", "abstract": "Existing timeline generation systems for complex events consider only information from traditional media, ignoring the rich social context provided by user-generated content that reveals representative public interests or insightful opinions. We instead aim to generate socially-informed timelines that contain both news article summaries and selected user comments. We present an optimization framework designed to balance topical cohesion between the article and comment summaries along with their informativeness and coverage of the event. Automatic evaluations on real-world datasets that cover four complex events show that our system produces more informative timelines than state-of-theart systems. In human evaluation, the associated comment summaries are furthermore rated more insightful than editor\u2019s picks and comments ranked highly by users.", "title": "Socially-Informed Timeline Generation for Complex Events"}, "3bc9f8eb5ba303816fd5f642f2e7408f0752d3c4": {"paper_id": "3bc9f8eb5ba303816fd5f642f2e7408f0752d3c4", "abstract": null, "title": "WORDNET: A Lexical Database For English"}, "e85a71c8cae795a1b2052a697d5e8182cc8c0655": {"paper_id": "e85a71c8cae795a1b2052a697d5e8182cc8c0655", "abstract": "We describe the design and use of the Stanford CoreNLP toolkit, an extensible pipeline that provides core natural language analysis. This toolkit is quite widely used, both in the research NLP community and also among commercial and government users of open source NLP technology. We suggest that this follows from a simple, approachable design, straightforward interfaces, the inclusion of robust and good quality analysis components, and not requiring use of a large amount of associated baggage.", "title": "The Stanford CoreNLP Natural Language Processing Toolkit"}, "0796f6cd7f0403a854d67d525e9b32af3b277331": {"paper_id": "0796f6cd7f0403a854d67d525e9b32af3b277331", "abstract": "Open Information Extraction (IE) is the task of extracting assertions from massive corpora without requiring a pre-specified vocabulary. This paper shows that the output of state-ofthe-art Open IE systems is rife with uninformative and incoherent extractions. To overcome these problems, we introduce two simple syntactic and lexical constraints on binary relations expressed by verbs. We implemented the constraints in the REVERB Open IE system, which more than doubles the area under the precision-recall curve relative to previous extractors such as TEXTRUNNER and WOE. More than 30% of REVERB\u2019s extractions are at precision 0.8 or higher\u2014 compared to virtually none for earlier systems. The paper concludes with a detailed analysis of REVERB\u2019s errors, suggesting directions for future work.1", "title": "Identifying Relations for Open Information Extraction"}, "0a10d64beb0931efdc24a28edaa91d539194b2e2": {"paper_id": "0a10d64beb0931efdc24a28edaa91d539194b2e2", "abstract": "We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.", "title": "Efficient Estimation of Word Representations in Vector Space"}, "280f9cc6ee7679d02a7b8b58d08173628057f3ea": {"paper_id": "280f9cc6ee7679d02a7b8b58d08173628057f3ea", "abstract": "Classic news summarization plays an important role with the exponential document growth on the Web. Many approaches are proposed to generate summaries but seldom simultaneously consider evolutionary characteristics of news plus to traditional summary elements. Therefore, we present a novel framework for the web mining problem named Evolutionary Timeline Summarization (ETS). Given the massive collection of time-stamped web documents related to a general news query, ETS aims to return the evolution trajectory along the timeline, consisting of individual but correlated summaries of each date, emphasizing relevance, coverage, coherence and cross-date diversity. ETS greatly facilitates fast news browsing and knowledge comprehension and hence is a necessity. We formally formulate the task as an optimization problem via iterative substitution from a set of sentences to a subset of sentences that satisfies the above requirements, balancing coherence/diversity measurement and local/global summary quality. The optimized substitution is iteratively conducted by incorporating several constraints until convergence. We develop experimental systems to evaluate on 6 instinctively different datasets which amount to 10251 documents. Performance comparisons between different system-generated timelines and manually created ones by human editors demonstrate the effectiveness of our proposed framework in terms of ROUGE metrics.", "title": "Evolutionary timeline summarization: a balanced optimization framework via iterative substitution"}, "3d378bdbf3564e341bec66c1974899b01e82507f": {"paper_id": "3d378bdbf3564e341bec66c1974899b01e82507f", "abstract": "Tweets are the most up-to-date and inclusive stream of in- formation and commentary on current events, but they are also fragmented and noisy, motivating the need for systems that can extract, aggregate and categorize important events. Previous work on extracting structured representations of events has focused largely on newswire text; Twitter's unique characteristics present new challenges and opportunities for open-domain event extraction. This paper describes TwiCal-- the first open-domain event-extraction and categorization system for Twitter. We demonstrate that accurately extracting an open-domain calendar of significant events from Twitter is indeed feasible. In addition, we present a novel approach for discovering important event categories and classifying extracted events based on latent variable models. By leveraging large volumes of unlabeled data, our approach achieves a 14% increase in maximum F1 over a supervised baseline. A continuously updating demonstration of our system can be viewed at http://statuscalendar.com; Our NLP tools are available at http://github.com/aritter/ twitter_nlp.", "title": "Open domain event extraction from twitter"}, "6c0cfbb0e02b8d5ea3f0d6f94eb25c7b93ff3e85": {"paper_id": "6c0cfbb0e02b8d5ea3f0d6f94eb25c7b93ff3e85", "abstract": "Timeline generation is an important research task which can help users to have a quick understanding of the overall evolution of any given topic. It thus attracts much attention from research communities in recent years. Nevertheless, existing work on timeline generation often ignores an important factor, the attention attracted to topics of interest (hereafter termed \"social attention\"). Without taking into consideration social attention, the generated timelines may not reflect users' collective interests. In this paper, we study how to incorporate social attention in the generation of timeline summaries. In particular, for a given topic, we capture social attention by learning users' collective interests in the form of word distributions from Twitter, which are subsequently incorporated into a unified framework for timeline summary generation. We construct four evaluation sets over six diverse topics. We demonstrate that our proposed approach is able to generate both informative and interesting timelines. Our work sheds light on the feasibility of incorporating social attention into traditional text mining tasks.", "title": "Timeline generation with social attention"}, "668db48c6a79826456341680ee1175dfc4cced71": {"paper_id": "668db48c6a79826456341680ee1175dfc4cced71", "abstract": "Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.", "title": "Get To The Point: Summarization with Pointer-Generator Networks"}, "1bc49abe5145055f1fa259bd4e700b1eb6b7f08d": {"paper_id": "1bc49abe5145055f1fa259bd4e700b1eb6b7f08d", "abstract": "We present SummaRuNNer, a Recurrent Neural Network (RNN) based sequence model for extractive summarization of documents and show that it achieves performance better than or comparable to state-of-the-art. Our model has the additional advantage of being very interpretable, since it allows visualization of its predictions broken up by abstract features such as information content, salience and novelty. Another novel contribution of our work is abstractive training of our extractive model that can train on human generated reference summaries alone, eliminating the need for sentence-level extractive labels.", "title": "SummaRuNNer: A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents"}, "9653d5c2c7844347343d073bbedd96e05d52f69b": {"paper_id": "9653d5c2c7844347343d073bbedd96e05d52f69b", "abstract": "We introduce a new neural architecture to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence. Such problems cannot be trivially addressed by existent approaches such as sequence-to-sequence [1] and Neural Turing Machines [2], because the number of target classes in each step of the output depends on the length of the input, which is variable. Problems such as sorting variable sized sequences, and various combinatorial optimization problems belong to this class. Our model solves the problem of variable size output dictionaries using a recently proposed mechanism of neural attention. It differs from the previous attention attempts in that, instead of using attention to blend hidden units of an encoder to a context vector at each decoder step, it uses attention as a pointer to select a member of the input sequence as the output. We call this architecture a Pointer Net (Ptr-Net). We show Ptr-Nets can be used to learn approximate solutions to three challenging geometric problems \u2013 finding planar convex hulls, computing Delaunay triangulations, and the planar Travelling Salesman Problem \u2013 using training examples alone. Ptr-Nets not only improve over sequence-to-sequence with input attention, but also allow us to generalize to variable size output dictionaries. We show that the learnt models generalize beyond the maximum lengths they were trained on. We hope our results on these tasks will encourage a broader exploration of neural learning for discrete problems.", "title": "Pointer Networks"}, "11c7dcdd20a2fa500162c3f1477d20bb18bfa15a": {"paper_id": "11c7dcdd20a2fa500162c3f1477d20bb18bfa15a", "abstract": "Attention mechanism has enhanced stateof-the-art Neural Machine Translation (NMT) by jointly learning to align and translate. It tends to ignore past alignment information, however, which often leads to over-translation and under-translation. To address this problem, we propose coverage-based NMT in this paper. We maintain a coverage vector to keep track of the attention history. The coverage vector is fed to the attention model to help adjust future attention, which lets NMT system to consider more about untranslated source words. Experiments show that the proposed approach significantly improves both translation quality and alignment quality over standard attention-based NMT.1", "title": "Modeling Coverage for Neural Machine Translation"}, "39dba6f22d72853561a4ed684be265e179a39e4f": {"paper_id": "39dba6f22d72853561a4ed684be265e179a39e4f", "abstract": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT\u201914 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM\u2019s BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM\u2019s performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.", "title": "Sequence to Sequence Learning with Neural Networks"}, "27e4b65121d3c88643d86dc91a9bdafdf223b988": {"paper_id": "27e4b65121d3c88643d86dc91a9bdafdf223b988", "abstract": "ive Text Summarization using Sequence-to-sequence RNNs and Beyond Ramesh Nallapati IBM Watson nallapati@us.ibm.com Bowen Zhou IBM Watson zhou@us.ibm.com Cicero dos Santos IBM Watson", "title": "Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond"}, "05aba481e8a221df5d8775a3bb749001e7f2525e": {"paper_id": "05aba481e8a221df5d8775a3bb749001e7f2525e", "abstract": "We present a new family of subgradient methods that dynamica lly incorporate knowledge of the geometry of the data observed in earlier iterations to perfo rm more informative gradient-based learning. Metaphorically, the adaptation allows us to find n eedles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems fro m recent advances in stochastic optimization and online learning which employ proximal funct ions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adap tively modifying the proximal function, which significantly simplifies setting a learning rate nd results in regret guarantees that are provably as good as the best proximal function that can be cho sen in hindsight. We give several efficient algorithms for empirical risk minimization probl ems with common and important regularization functions and domain constraints. We experimen tally study our theoretical analysis and show that adaptive subgradient methods outperform state-o f-the-art, yet non-adaptive, subgradient algorithms.", "title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization"}, "5e4b040741fb5e4944368885944cace5e31a17be": {"paper_id": "5e4b040741fb5e4944368885944cace5e31a17be", "abstract": "Social media such as Twitter have become an important method of communication, with potential opportunities for NLG to facilitate the generation of social media content. We focus on the generation of indicative tweets that contain a link to an external web page. While it is natural and tempting to view the linked web page as the source text from which the tweet is generated in an extractive summarization setting, it is unclear to what extent actual indicative tweets behave like extractive summaries. We collect a corpus of indicative tweets with their associated articles and investigate to what extent they can be derived from the articles using extractive methods. We also consider the impact of the formality and genre of the article. Our results demonstrate the limits of viewing indicative tweet generation as extractive summarization, and point to the need for the development of a methodology for tweet generation that is sensitive to genre-specific issues.", "title": "Indicative Tweet Generation: An Extractive Summarization Problem?"}, "92367dd4debbe058a2dc3f7ea840015f17821822": {"paper_id": "92367dd4debbe058a2dc3f7ea840015f17821822", "abstract": "In recent years, Twitter has become one of the most important microblogging services of the Web 2.0. Among the possible uses it allows, it can be employed for communicating and broadcasting information in real time. The goal of this research is to analyze the task of automatic tweet generation from a text summarization perspective in the context of the journalism genre. To achieve this, different state-of-the-art summarizers are selected and employed for producing multi-lingual tweets in two languages (English and Spanish). A wide experimental framework is proposed, comprising the creation of a new corpus, the generation of the automatic tweets, and their assessment through a quantitative and a qualitative evaluation, where informativeness, indicativeness and interest are key criteria that should be ensured in the proposed context. From the results obtained, it was observed that although the original tweets were considered as model tweets with respect to their informativeness, they were not among the most interesting ones from a human viewpoint. Therefore, relying only on these tweets may not be the ideal way to communicate news through Twitter, especially if a more personalized and catchy way of reporting news wants to be performed. In contrast, we showed that recent text summarization techniques may be more appropriate, reflecting a balance between indicativeness and interest, even if their content was different from the tweets delivered by the news providers.", "title": "Towards automatic tweet generation: A comparative study from the text summarization perspective in the journalism genre"}, "0dd5e618579061d70b93ef6dbb407663a8ab165c": {"paper_id": "0dd5e618579061d70b93ef6dbb407663a8ab165c", "abstract": "Due to the sheer volume of text generated by a micro log site like Twitter, it is often difficult to fully understand what is being said about various topics. In an attempt to understand micro logs better, this paper compares algorithms for extractive summarization of micro log posts. We present two algorithms that produce summaries by selecting several posts from a given set. We evaluate the generated summaries by comparing them to both manually produced summaries and summaries produced by several leading traditional summarization systems. In order to shed light on the special nature of Twitter posts, we include extensive analysis of our results, some of which are unexpected.", "title": "Comparing Twitter Summarization Algorithms for Multiple Post Summaries"}, "255e97d82f528b613dbe8883727abfd14f3f9f39": {"paper_id": "255e97d82f528b613dbe8883727abfd14f3f9f39", "abstract": "ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It includes measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans. The measures count the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans. This paper introduces four different ROUGE measures: ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S included in the ROUGE summarization evaluation package and their evaluations. Three of them have been used in the Document Understanding Conference (DUC) 2004, a large-scale summarization evaluation sponsored by NIST.", "title": "ROUGE: A Package For Automatic Evaluation Of Summaries"}, "2cb8497f9214735ffd1bd57db645794459b8ff41": {"paper_id": "2cb8497f9214735ffd1bd57db645794459b8ff41", "abstract": "Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.", "title": "Teaching Machines to Read and Comprehend"}, "7453a974d355883f342aaa6e29eb86a13edcedb9": {"paper_id": "7453a974d355883f342aaa6e29eb86a13edcedb9", "abstract": "In this paper we present a joint content selection and compression model for single-document summarization. The model operates over a phrase-based representation of the source document which we obtain by merging information from PCFG parse trees and dependency graphs. Using an integer linear programming formulation, the model learns to select and combine phrases subject to length, coverage and grammar constraints. We evaluate the approach on the task of generating \u201cstory highlights\u201d\u2014a small number of brief, self-contained sentences that allow readers to quickly gather information on news stories. Experimental results show that the model\u2019s output is comparable to human-written highlights in terms of both grammaticality and content.", "title": "Automatic Generation of Story Highlights"}, "2538e3eb24d26f31482c479d95d2e26c0e79b990": {"paper_id": "2538e3eb24d26f31482c479d95d2e26c0e79b990", "abstract": "We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.", "title": "Natural Language Processing (almost) from Scratch"}, "06bae254319f8d39e80c7254c841787b45baf820": {"paper_id": "06bae254319f8d39e80c7254c841787b45baf820", "abstract": "Recurrent neural networks are powerful sequence learners. They are able to incorporate context information in a flexible way, and are robust to localised distortions of the input data. These properties make them well suited to sequence labelling, where input sequences are transcribed with streams of labels. Long short-term memory is an especially promising recurrent architecture, able to bridge long time delays between relevant input and output events, and thereby access long range context. The aim of this thesis is to advance the state-of-the-art in supervised sequence labelling with recurrent networks in general, and long short-term memory in particular. Its two main contributions are (1) a new type of output layer that allows recurrent networks to be trained directly for sequence labelling tasks where the alignment between the inputs and the labels is unknown, and (2) an extension of long short-term memory to multidimensional data, such as images and video sequences. Experimental results are presented on speech recognition, online and offline handwriting recognition, keyword spotting, image segmentation and image classification, demonstrating the advantages of advanced recurrent networks over other sequential algorithms, such as hidden Markov Models.", "title": "Supervised sequence labelling with recurrent neural networks"}, "bb20f121c979b535bbeade5ac06676d627d4ad7d": {"paper_id": "bb20f121c979b535bbeade5ac06676d627d4ad7d", "abstract": "Interestingly, understanding natural language that you really wait for now is coming. It's significant to wait for the representative and beneficial books to read. Every book that is provided in better way and utterance will be expected by many peoples. Even you are a good reader or not, feeling to read this book will always appear when you find it. But, when you feel hard to find it as yours, what to do? Borrow to your friends and don't know when to give back it to her or him.", "title": "Understanding natural language"}, "0b3cfbf79d50dae4a16584533227bb728e3522aa": {"paper_id": "0b3cfbf79d50dae4a16584533227bb728e3522aa", "abstract": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.", "title": "Long Short-Term Memory"}, "26d4ab9b60b91bb610202b58fa1766951fedb9e9": {"paper_id": "26d4ab9b60b91bb610202b58fa1766951fedb9e9", "abstract": "This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural network architecture for image generation. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates images that cannot be distinguished from real data with the naked eye.", "title": "DRAW: A Recurrent Neural Network For Image Generation"}, "03ff3f8f4d5a700fbe8f3a3e63a39523c29bb60f": {"paper_id": "03ff3f8f4d5a700fbe8f3a3e63a39523c29bb60f", "abstract": "The ability to accurately represent sentences is central to language understanding. We describe a convolutional architecture dubbed the Dynamic Convolutional Neural Network (DCNN) that we adopt for the semantic modelling of sentences. The network uses Dynamic k-Max Pooling, a global pooling operation over linear sequences. The network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations. The network does not rely on a parse tree and is easily applicable to any language. We test the DCNN in four experiments: small scale binary and multi-class sentiment prediction, six-way question classification and Twitter sentiment prediction by distant supervision. The network achieves excellent performance in the first three tasks and a greater than 25% error reduction in the last task with respect to the strongest baseline.", "title": "A Convolutional Neural Network for Modelling Sentences"}, "00a64ca47f0c6e37d50190ea79a999407b0478f5": {"paper_id": "00a64ca47f0c6e37d50190ea79a999407b0478f5", "abstract": "We have created layers of annotation on the English Gigaword v.5 corpus to render it useful as a standardized corpus for knowledge extraction and distributional semantics. Most existing large-scale work is based on inconsistent corpora which often have needed to be re-annotated by research teams independently, each time introducing biases that manifest as results that are only comparable at a high level. We provide to the community a public reference set based on current state-of-the-art syntactic analysis and coreference resolution, along with an interface for programmatic access. Our goal is to enable broader involvement in large-scale knowledge-acquisition efforts by researchers that otherwise may not have had the ability to produce such a resource on their own.", "title": "Annotated Gigaword"}, "4546c7b4d2103523b104daddea6c7eaa1925cf12": {"paper_id": "4546c7b4d2103523b104daddea6c7eaa1925cf12", "abstract": "One of the potential advantages of data-driven approaches to natural language processing is that they can be ported to new languages, provided that the necessary linguistic data resources are available. In practice, this advantage can be hard to realize if models are overfitted to a particular language or linguistic annotation scheme. Thus, using two state-of-the-art statistical parsers developed for English to parse Italian, Corazza et al. [6] report an increase in error rate of 15\u201318%, and similar results have been reported for other languages [5, 1, 9, 14]. To overcome this kind of problem, systems for data-driven processing need to be designed for flexible reconfiguration, not only with respect to the selection of linguistic features but also with respect to algorithms and learning methods. In this paper, we present a data-driven approach to dependency parsing that has been applied to several languages, consistently giving a dependency accuracy of 80\u201390% without any language-specific enhancements and with fairly modest data resources. The methodology is based on three essential components:", "title": "MaltParser: A Language-Independent System for Data-Driven Dependency Parsing"}, "6c9bd4bd7e30470e069f8600dadb4fd6d2de6bc1": {"paper_id": "6c9bd4bd7e30470e069f8600dadb4fd6d2de6bc1", "abstract": "This paper describes a new language resource of events and semantic roles that characterize real-world situations. Narrative schemas contain sets of related events (edit and publish), a temporal ordering of the events (edit before publish), and the semantic roles of the participants (authors publish books). This type of world knowledge was central to early research in natural language understanding. Scripts were one of the main formalisms, representing common sequences of events that occur in the world. Unfortunately, most of this knowledge was hand-coded and time consuming to create. Current machine learning techniques, as well as a new approach to learning through coreference chains, has allowed us to automatically extract rich event structure from open domain text in the form of narrative schemas. The narrative schema resource described in this paper contains approximately 5000 unique events combined into schemas of varying sizes. We describe the resource, how it is learned, and a new evaluation of the coverage of these schemas over unseen documents.", "title": "A Database of Narrative Schemas"}, "147705f1ea26274879ba6436288697dc4bc9ba82": {"paper_id": "147705f1ea26274879ba6436288697dc4bc9ba82", "abstract": "This paper details the coreference resolution system submitted by Stanford at the CoNLL2011 shared task. Our system is a collection of deterministic coreference resolution models that incorporate lexical, syntactic, semantic, and discourse information. All these models use global document-level information by sharing mention attributes, such as gender and number, across mentions in the same cluster. We participated in both the open and closed tracks and submitted results using both predicted and gold mentions. Our system was ranked first in both tracks, with a score of 57.8 in the closed track and 58.3 in the open track.", "title": "Stanford's Multi-Pass Sieve Coreference Resolution System at the CoNLL-2011 Shared Task"}, "3a0e788268fafb23ab20da0e98bb578b06830f7d": {"paper_id": "3a0e788268fafb23ab20da0e98bb578b06830f7d", "abstract": "Computers understand very little of the meaning of human language. This profoundly limits our ability to give instructions to computers, the ability of computers to explain their actions to us, and the ability of computers to analyse and process text. Vector space models (VSMs) of semantics are beginning to address these limits. This paper surveys the use of VSMs for semantic processing of text. We organize the literature on VSMs according to the structure of the matrix in a VSM. There are currently three broad classes of VSMs, based on term\u2013document, word\u2013context, and pair\u2013pattern matrices, yielding three classes of applications. We survey a broad range of applications in these three categories and we take a detailed look at a specific open source project in each category. Our goal in this survey is to show the breadth of applications of VSMs for semantics, to provide a new perspective on VSMs for those who are already familiar with the area, and to provide pointers into the literature for those who are less familiar with the field.", "title": "From Frequency to Meaning: Vector Space Models of Semantics"}, "6803cbda3de9fdc0646748e7e30cff97a632a16f": {"paper_id": "6803cbda3de9fdc0646748e7e30cff97a632a16f", "abstract": "dde23 aims to make it as easy as possible to solve effectively delaydifferential equations (DDEs) with constant delays in Matlab. In this paper we discuss some of its features, including discontinuity tracking, iteration for short delays, and event location. We also develop some theoretical results that underlie the solver, including convergence, error estimation, and the effects of short delays on the evaluation of formulas and stability. Some examples illustrate the use of dde23 and show it to be a capable DDE solver that is exceptionally easy to use for a wide range of complex problems.", "title": "Solving DDEs in Matlab"}, "0a843b7c5bbfa3d2a384a8d3568e79e86fd50866": {"paper_id": "0a843b7c5bbfa3d2a384a8d3568e79e86fd50866", "abstract": "This paper describes mathematical and software developments for a suite of programs for solving ordinary differential equations in Matlab.", "title": "The MATLAB ODE Suite"}, "4894435e4e96b7ddcc42853992da4c694711c5fb": {"paper_id": "4894435e4e96b7ddcc42853992da4c694711c5fb", "abstract": "Recent advances in sensing technology have enabled a new generation of tabletop displays that can sense multiple points of input from several users simultaneously. However, apart from a few demonstration techniques [17], current user interfaces do not take advantage of this increased input bandwidth. We present a variety of multifinger and whole hand gestural interaction techniques for these displays that leverage and extend the types of actions that people perform when interacting on real physical tabletops. Apart from gestural input techniques, we also explore interaction and visualization techniques for supporting shared spaces, awareness, and privacy. These techniques are demonstrated within a prototype room furniture layout application, called RoomPlanner.", "title": "Multi-finger and whole hand gestural interaction techniques for multi-user tabletop displays"}, "656dc19ce6bda922d8d8cfeed11c6037d10e1c87": {"paper_id": "656dc19ce6bda922d8d8cfeed11c6037d10e1c87", "abstract": "real world can be overcome in VR, and physical tools can. be m a d e obsolete by more flexible, virtual alternatives'.. Physical tools can be hard to replace , however. At one time it seemed that paper might become obsolete, for example, and visionaries predicted the \"paperlless office\" would dominate within a few years. But the trouble is that people like paper. It is easier to read than a screen [5], it is cheap, universally accepted , tactile, and portable. According to some studies, paperwork in the office has increased by a l~actor of six since 1970, and is now growing at 20% annually [14]. Like electronic documents, paper has properties that people just cannot seem to give up, making it resilient in the face of computer-based alternatives [10]. Consequently, we have two desks: one for \"paper pushing\" and the other for \"pixel pushing.\" Although activities on the two desk.s are often related, the two are quite isolated from each other. Printers and scanners provide a way for documents to move back ;and forth between desk-tops, but this conversion process is inconvenient. Wang's Freestyle system [3], for example, was. a \"paper-less office\" system that attempted to coexist with partially paper-based processes. A key factor necessary for adoption of that system was to minimize the printing and scanning re-RoE.ALL o quired, because too many of these tasks would cause the process to revert entirely back to paper. Trade-offs between electronic and paper documents can make the choice of medium difficult, but imagine if we did not have to choose, and we had a space where documents could be both paper and electronic at the same time. Instead of putting the user in the virtual world of the computer , we could do the opposite: add the computer to the real world of the user and create a Computer Augmented Environment for paper (see guest editors' introduction.). Instead of replacing paper with computers, we could enhance paper with computation. The Xerox PaperWorks product [8] takes a step in this direction with its fax-based paper user interface (UI) to a storage and retrieval system. With this system, ordinary paper forms are enhanced to control a PC through a fax machine. These paper documents gain some properties of electronic documents, but fax machines are slow compared to computer screens. Response time is limited by the delay it takes to scan and print a \u2026", "title": "Interacting with Paper on the Digital Desk"}, "71a3fafcc01ece6131eba64e0805d00f7467861e": {"paper_id": "71a3fafcc01ece6131eba64e0805d00f7467861e", "abstract": "Two experiments were run to investigate two-handed input. The experimental tasks were representative of those found in CAD and office information systems.\nExperiment one involved the performance of a compound selection/positioning task. The two sub-tasks were performed by different hands using separate transducers. Without prompting, novice subjects adopted strategies that involved performing the two sub-tasks simultaneously. We interpret this as a demonstration that, in the appropriate context, users are capable of simultaneously providing continuous data from two hands without significant overhead. The results also show that the speed of performing the task was strongly correlated to the degree of parallelism employed.\nExperiment two involved the performance of a compound navigation/selection task. It compared a one-handed versus two-handed method for finding and selecting words in a document. The two-handed method significantly outperformed the commonly used one-handed method by a number of measures. Unlike experiment one, only two subjects adopted strategies that used both hands simultaneously. The benefits of the two-handed technique, therefore, are interpreted as being due to efficiency of hand motion. However, the two subjects who did use parallel strategies had the two fastest times of all subjects.", "title": "A study in two-handed input"}, "bc1b39230215db5b608dbd8f43c6da2bef9a8179": {"paper_id": "bc1b39230215db5b608dbd8f43c6da2bef9a8179", "abstract": "The advanced message queuing protocol (AMQP) working group's goal is to create an open standard for an interoperable enterprise-scale asynchronous messaging protocol. AMQP is finally addressing the lack of enterprise messaging interoperability standards. This relatively simple yet compellingly powerful enterprise messaging protocol is thus poised to open up a bright new era for enterprise messaging", "title": "Advanced Message Queuing Protocol"}, "be91946bedbf65d543a7eb9dd1e033e7aaf78c3c": {"paper_id": "be91946bedbf65d543a7eb9dd1e033e7aaf78c3c", "abstract": "Current methods for knowledge graph (KG) representation learning focus solely on the structure of the KG and do not exploit any kind of external information, such as visual and linguistic information corresponding to the KG entities. In this paper, we propose a multimodal translation-based approach that defines the energy of a KG triple as the sum of sub-energy functions that leverage both multimodal (visual and linguistic) and structural KG representations. Next, a ranking-based loss is minimized using a simple neural network architecture. Moreover, we introduce a new large-scale dataset for multimodal KG representation learning. We compared the performance of our approach to other baselines on two standard tasks, namely knowledge graph completion and triple classification, using our as well as the WN9-IMG dataset.1 The results demonstrate that our approach outperforms all baselines on both tasks and datasets.", "title": "A Multimodal Translation-Based Approach for Knowledge Graph Representation Learning"}, "04b52c8230c3f9f4f4032b06458069d81c8f07b2": {"paper_id": "04b52c8230c3f9f4f4032b06458069d81c8f07b2", "abstract": "We consider the problem of embedding entities and relationships of multirelational data in low-dimensional vector spaces. Our objective is to propose a canonical model which is easy to train, contains a reduced number of parameters and can scale up to very large databases. Hence, we propose TransE, a method which models relationships by interpreting them as translations operating on the low-dimensional embeddings of the entities. Despite its simplicity, this assumption proves to be powerful since extensive experiments show that TransE significantly outperforms state-of-the-art methods in link prediction on two knowledge bases. Besides, it can be successfully trained on a large scale data set with 1M entities, 25k relationships and more than 17M training samples.", "title": "Translating Embeddings for Modeling Multi-relational Data"}, "057ac29c84084a576da56247bdfd63bf17b5a891": {"paper_id": "057ac29c84084a576da56247bdfd63bf17b5a891", "abstract": "Many Knowledge Bases (KBs) are now readily available and encompass colossal quantities of information thanks to either a long-term funding effort (e.g. WordNet, OpenCyc) or a collaborative process (e.g. Freebase, DBpedia). However, each of them is based on a different rigid symbolic framework which makes it hard to use their data in other systems. It is unfortunate because such rich structured knowledge might lead to a huge leap forward in many other areas of AI like natural language processing (word-sense disambiguation, natural language understanding, ...), vision (scene classification, image semantic annotation, ...) or collaborative filtering. In this paper, we present a learning process based on an innovative neural network architecture designed to embed any of these symbolic representations into a more flexible continuous vector space in which the original knowledge is kept and enhanced. These learnt embeddings would allow data from any KB to be easily used in recent machine learning methods for prediction and information retrieval. We illustrate our method on WordNet and Freebase and also present a way to adapt it to knowledge extraction from raw text.", "title": "Learning Structured Embeddings of Knowledge Bases"}, "68a33a3afac65eb6e0fb3726c1f9c8b727f32a42": {"paper_id": "68a33a3afac65eb6e0fb3726c1f9c8b727f32a42", "abstract": "Relational learning is becoming increasingly important in many areas of application. Here, we present a novel approach to relational learning based on the factorization of a three-way tensor. We show that unlike other tensor approaches, our method is able to perform collective learning via the latent components of the model and provide an efficient algorithm to compute the factorization. We substantiate our theoretical considerations regarding the collective learning capabilities of our model by the means of experiments on both a new dataset and a dataset commonly used in entity resolution. Furthermore, we show on common benchmark datasets that our approach achieves better or on-par results, if compared to current state-of-the-art relational learning solutions, while it is significantly faster to compute.", "title": "A Three-Way Model for Collective Learning on Multi-Relational Data"}, "04cc04457e09e17897f9256c86b45b92d70a401f": {"paper_id": "04cc04457e09e17897f9256c86b45b92d70a401f", "abstract": "Many data such as social networks, movie preferences or knowledge bases are multi-relational, in that they describe multiple relations between entities. While there is a large body of work focused on modeling these data, modeling these multiple types of relations jointly remains challenging. Further, existing approaches tend to breakdown when the number of these types grows. In this paper, we propose a method for modeling large multi-relational datasets, with possibly thousands of relations. Our model is based on a bilinear structure, which captures various orders of interaction of the data, and also shares sparse latent factors across different relations. We illustrate the performance of our approach on standard tensor-factorization datasets where we attain, or outperform, state-of-the-art results. Finally, a NLP application demonstrates our scalability and the ability of our model to learn efficient and semantically meaningful verb representations.", "title": "A latent factor model for highly multi-relational data"}, "1976c9eeccc7115d18a04f1e7fb5145db6b96002": {"paper_id": "1976c9eeccc7115d18a04f1e7fb5145db6b96002", "abstract": "Freebase is a practical, scalable tuple database used to structure general human knowledge. The data in Freebase is collaboratively created, structured, and maintained. Freebase currently contains more than 125,000,000 tuples, more than 4000 types, and more than 7000 properties. Public read/write access to Freebase is allowed through an HTTP-based graph-query API using the Metaweb Query Language (MQL) as a data query and manipulation language. MQL provides an easy-to-use object-oriented interface to the tuple data in Freebase and is designed to facilitate the creation of collaborative, Web-based data-oriented applications.", "title": "Freebase: a collaboratively created graph database for structuring human knowledge"}, "2744288f090192987e980274999065ad2d6e45d6": {"paper_id": "2744288f090192987e980274999065ad2d6e45d6", "abstract": "Relational learning is concerned with predicting unknown values of a relation, given a database of entities and observed relations among entities. An example of relational learning is movie rating prediction, where entities could include users, movies, genres, and actors. Relations encode users' ratings of movies, movies' genres, and actors' roles in movies. A common prediction technique given one pairwise relation, for example a #users x #movies ratings matrix, is low-rank matrix factorization. In domains with multiple relations, represented as multiple matrices, we may improve predictive accuracy by exploiting information from one relation while predicting another. To this end, we propose a collective matrix factorization model: we simultaneously factor several matrices, sharing parameters among factors when an entity participates in multiple relations. Each relation can have a different value type and error distribution; so, we allow nonlinear relationships between the parameters and outputs, using Bregman divergences to measure error. We extend standard alternating projection algorithms to our model, and derive an efficient Newton update for the projection. Furthermore, we propose stochastic optimization methods to deal with large, sparse matrices. Our model generalizes several existing matrix factorization methods, and therefore yields new large-scale optimization algorithms for these problems. Our model can handle any pairwise relational schema and a wide variety of error models. We demonstrate its efficiency, as well as the benefit of sharing parameters among relations.", "title": "Relational learning via collective matrix factorization"}, "c829b63a3ae72a47e1953e1295826c7b2f93bf50": {"paper_id": "c829b63a3ae72a47e1953e1295826c7b2f93bf50", "abstract": "The recently introduced continuous Skip-gram model is an ef fici nt method for learning high-quality distributed vector representation s that capture a large number of precise syntactic and semantic word relationships. I n this paper we present several extensions that improve both the quality of the vect ors and the training speed. By subsampling of the frequent words we obtain signifi ca t speedup and also learn more regular word representations. We also descr ib a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their ind ifference to word order and their inability to represent idiomatic phrases. For exa mple, the meanings of \u201cCanada\u201d and \u201cAir\u201d cannot be easily combined to obtain \u201cAir C anada\u201d. Motivated by this example, we present a simple method for finding phrase s in t xt, and show that learning good vector representations for millions of p hrases is possible.", "title": "Distributed Representations of Words and Phrases and their Compositionality"}, "8f5450037cba1ba1f5c2f73fa4ffa66558eae5bd": {"paper_id": "8f5450037cba1ba1f5c2f73fa4ffa66558eae5bd", "abstract": "We consider the problem of learning probabilistic models fo r c mplex relational structures between various types of objects. A model can hel p us \u201cunderstand\u201d a dataset of relational facts in at least two ways, by finding in terpretable structure in the data, and by supporting predictions, or inferences ab out whether particular unobserved relations are likely to be true. Often there is a t radeoff between these two aims: cluster-based models yield more easily interpret abl representations, while factorization-based approaches have given better pr edictive performance on large data sets. We introduce the Bayesian Clustered Tensor Factorization (BCTF) model, which embeds a factorized representation of relatio ns in a nonparametric Bayesian clustering framework. Inference is fully Bayesia n but scales well to large data sets. The model simultaneously discovers interp retable clusters and yields predictive performance that matches or beats previo us probabilistic models for relational data.", "title": "Modelling Relational Data using Bayesian Clustered Tensor Factorization"}, "2b2c30dfd3968c5d9418bb2c14b2382d3ccc64b2": {"paper_id": "2b2c30dfd3968c5d9418bb2c14b2382d3ccc64b2", "abstract": "DBpedia is a community effort to extract structured information from Wikipedia and to make this information available on the Web. DBpedia allows you to ask sophisticated queries against datasets derived from Wikipedia and to link other datasets on the Web to Wikipedia data. We describe the extraction of the DBpedia datasets, and how the resulting information is published on the Web for humanand machineconsumption. We describe some emerging applications from the DBpedia community and show how website authors can facilitate DBpedia content within their sites. Finally, we present the current status of interlinking DBpedia with other open datasets on the Web and outline how DBpedia could serve as a nucleus for an emerging Web of open data.", "title": "DBpedia: A Nucleus for a Web of Open Data"}, "47291646a01c8786abd1b168cb78e6af575f9318": {"paper_id": "47291646a01c8786abd1b168cb78e6af575f9318", "abstract": "We present AutoExtend, a system to learn embeddings for synsets and lexemes. It is flexible in that it can take any word embeddings as input and does not need an additional training corpus. The synset/lexeme embeddings obtained live in the same vector space as the word embeddings. A sparse tensor formalization guarantees efficiency and parallelizability. We use WordNet as a lexical resource, but AutoExtend can be easily applied to other resources like Freebase. AutoExtend achieves state-of-the-art performance on word similarity and word sense disambiguation tasks.", "title": "AutoExtend: Extending Word Embeddings to Embeddings for Synsets and Lexemes"}, "1005645c05585c2042e3410daeed638b55e2474d": {"paper_id": "1005645c05585c2042e3410daeed638b55e2474d", "abstract": "Neural probabilistic language models (NPLMs) have been shown to be competitive with and occasionally superior to the widely-used n-gram language models. The main drawback of NPLMs is their extremely long training and testing times. Morin and Bengio have proposed a hierarchical language model built around a binary tree of words, which was two orders of magnitude faster than the nonhierarchical model it was based on. However, it performed considerably worse than its non-hierarchical counterpart in spite of using a word tree created using expert knowledge. We introduce a fast hierarchical language model along with a simple feature-based algorithm for automatic construction of word trees from the data. We then show that the resulting models can outperform non-hierarchical neural models as well as the best n-gram models.", "title": "A Scalable Hierarchical Distributed Language Model"}, "0825788b9b5a18e3dfea5b0af123b5e939a4f564": {"paper_id": "0825788b9b5a18e3dfea5b0af123b5e939a4f564", "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.", "title": "Glove: Global Vectors for Word Representation"}, "cd175f80821300931e81ed716b1e351b1a3aa11e": {"paper_id": "cd175f80821300931e81ed716b1e351b1a3aa11e", "abstract": "Most word representation methods assume that each word owns a single semantic vector. This is usually problematic because lexical ambiguity is ubiquitous, which is also the problem to be resolved by word sense disambiguation. In this paper, we present a unified model for joint word sense representation and disambiguation, which will assign distinct representations for each word sense.1 The basic idea is that both word sense representation (WSR) and word sense disambiguation (WSD) will benefit from each other: (1) highquality WSR will capture rich information about words and senses, which should be helpful for WSD, and (2) high-quality WSD will provide reliable disambiguated corpora for learning better sense representations. Experimental results show that, our model improves the performance of contextual word similarity compared to existing WSR methods, outperforms stateof-the-art supervised methods on domainspecific WSD, and achieves competitive performance on coarse-grained all-words WSD.", "title": "A Unified Model for Word Sense Representation and Disambiguation"}, "0cf0bfc09d0083c2a4afff79347f657523ae40b0": {"paper_id": "0cf0bfc09d0083c2a4afff79347f657523ae40b0", "abstract": "Open-text semantic parsers are designed to interpret any statement in natural language by inferring a corresponding meaning representation (MR \u2013 a formal representation of its sense). Unfortunately, large scale systems cannot be easily machine-learned due to a lack of directly supervised data. We propose a method that learns to assign MRs to a wide range of text (using a dictionary of more than 70,000 words mapped to more than 40,000 entities) thanks to a training scheme that combines learning from knowledge bases (e.g. WordNet) with learning from raw text. The model jointly learns representations of words, entities and MRs via a multi-task training process operating on these diverse sources of data. Hence, the system ends up providing methods for knowledge acquisition and wordsense disambiguation within the context of semantic parsing in a single elegant framework. Experiments on these various tasks indicate the promise of the approach.", "title": "Joint Learning of Words and Meaning Representations for Open-Text Semantic Parsing"}, "052b1d8ce63b07fec3de9dbb583772d860b7c769": {"paper_id": "052b1d8ce63b07fec3de9dbb583772d860b7c769", "abstract": "We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal \u2018hidden\u2019 units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.", "title": "Learning representations by back-propagating errors"}, "9931b8ea6594b97c7dfca93936a2d95a38167046": {"paper_id": "9931b8ea6594b97c7dfca93936a2d95a38167046", "abstract": "Word sense disambiguation (WSD) systems based on supervised learning achieved the best performance in SensEval and SemEval workshops. However, there are few publicly available open source WSD systems. This limits the use of WSD in other applications, especially for researchers whose research interests are not in WSD. In this paper, we present IMS, a supervised English all-words WSD system. The flexible framework of IMS allows users to integrate different preprocessing tools, additional features, and different classifiers. By default, we use linear support vector machines as the classifier with multiple knowledge-based features. In our implementation, IMS achieves state-of-the-art results on several SensEval and SemEval tasks.", "title": "It Makes Sense: A Wide-Coverage Word Sense Disambiguation System for Free Text"}, "7b3e09b3b0ebdc3f390f865a3da4233e6bfb49e8": {"paper_id": "7b3e09b3b0ebdc3f390f865a3da4233e6bfb49e8", "abstract": "Recent work has shown success in learning word embeddings with neural network language models (NNLM). However, the majority of previous NNLMs represent each word with a single embedding, which fails to capture polysemy. In this paper, we address this problem by representing words with multiple and sense-specific embeddings, which are learned from bilingual parallel data. We evaluate our embeddings using the word similarity measurement and show that our approach is significantly better in capturing the sense-level word similarities. We further feed our embeddings as features in Chinese named entity recognition and obtain noticeable improvements against single embeddings.", "title": "Learning Sense-specific Word Embeddings By Exploiting Bilingual Resources"}, "4aa4069693bee00d1b0759ca3df35e59284e9845": {"paper_id": "4aa4069693bee00d1b0759ca3df35e59284e9845", "abstract": "Modern visual recognition systems are often limited in their ability to scale to large numbers of object categories. This limitation is in part due to the increasing difficulty of acquiring sufficient training data in the form of labeled images as the number of object categories grows. One remedy is to leverage data from other sources \u2013 such as text data \u2013 both to train visual models and to constrain their predictions. In this paper we present a new deep visual-semantic embedding model trained to identify visual objects using both labeled image data as well as semantic information gleaned from unannotated text. We demonstrate that this model matches state-of-the-art performance on the 1000-class ImageNet object recognition challenge while making more semantically reasonable errors, and also show that the semantic information can be exploited to make predictions about tens of thousands of image labels not observed during training. Semantic knowledge improves such zero-shot predictions by up to 65%, achieving hit rates of up to 10% across thousands of novel labels never seen by the visual model.", "title": "DeViSE: A Deep Visual-Semantic Embedding Model"}, "29d591806cdc6ef0d580e4a21f32e5ad9d09d148": {"paper_id": "29d591806cdc6ef0d580e4a21f32e5ad9d09d148", "abstract": "Image annotation datasets are becoming larger and larger, with tens of millions of images and tens of thousands of possible annotations. We propose a strongly performing method that scales to such datasets by simultaneously learning to optimize precision at k of the ranked list of annotations for a given image and learning a low-dimensional joint embedding space for both images and annotations. Our method both outperforms several baseline methods and, in comparison to them, is faster and consumes less memory. We also demonstrate how our method learns an interpretable model, where annotations with alternate spellings or even languages are close in the embedding space. Hence, even when our model does not predict the exact annotation given by a human labeler, it often predicts similar annotations, a fact that we try to quantify by measuring the newly introduced \u201csibling\u201d precision metric, where our method also obtains excellent results.", "title": "Large scale image annotation: learning\u00a0to\u00a0rank with\u00a0joint word-image embeddings"}, "0122e063ca5f0f9fb9d144d44d41421503252010": {"paper_id": "0122e063ca5f0f9fb9d144d44d41421503252010", "abstract": "Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestlysized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm.", "title": "Large Scale Distributed Deep Networks"}, "10eb7bfa7687f498268bdf74b2f60020a151bdc6": {"paper_id": "10eb7bfa7687f498268bdf74b2f60020a151bdc6", "abstract": "We present a new technique called \u201ct-SNE\u201d that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large datasets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of datasets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on almost all of the datasets.", "title": "Visualizing Data using t-SNE"}, "23694a80bf1b9b38215be3e23068dd75296bc90f": {"paper_id": "23694a80bf1b9b38215be3e23068dd75296bc90f", "abstract": "A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.", "title": "A Neural Probabilistic Language Model"}, "981fef7155742608b8b6673f4a9566158b76cd67": {"paper_id": "981fef7155742608b8b6673f4a9566158b76cd67", "abstract": null, "title": "ImageNet Large Scale Visual Recognition Challenge"}, "40e927fdee7517fb7513d03735754af80fb9c7b0": {"paper_id": "40e927fdee7517fb7513d03735754af80fb9c7b0", "abstract": "Multi-class classification becomes challenging at test time when the number of classes is very large and testing against every possible class can become computationally infeasible. This problem can be alleviated by imposing (or learning) a structure over the set of classes. We propose an algorithm for learning a treestructure of classifiers which, by optimizing the overall tree loss, provides superior accuracy to existing tree labeling methods. We also propose a method that learns to embed labels in a low dimensional space that is faster than non-embedding approaches and has superior accuracy to existing embedding approaches. Finally we combine the two ideas resulting in the label embedding tree that outperforms alternative methods including One-vs-Rest while being orders of magnitude faster.", "title": "Label Embedding Trees for Large Multi-Class Tasks"}, "709198f1a7d42fb87d46a8f5dc48e23e6564df1c": {"paper_id": "709198f1a7d42fb87d46a8f5dc48e23e6564df1c", "abstract": "Many computer vision approaches take for granted positive answers to questions such as \u201cAre semantic categories visually separable?\u201d and \u201cIs visual similarity correlated to semantic similarity?\u201d. In this paper, we study experimentally whether these assumptions hold and show parallels to questions investigated in cognitive science about the human visual system. The insights gained from our analysis enable building a novel distance function between images assessing whether they are from the same basic-level category. This function goes beyond direct visual distance as it also exploits semantic similarity measured through ImageNet. We demonstrate experimentally that it outperforms purely visual distances.", "title": "Visual and semantic similarity in ImageNet"}, "06554235c2c9361a14c0569206b58a355a63f01b": {"paper_id": "06554235c2c9361a14c0569206b58a355a63f01b", "abstract": "This work introduces a model that can recognize objects in images even if no training data is available for the objects. The only necessary knowledge about the unseen categories comes from unsupervised large text corpora. In our zero-shot framework distributional information in language can be seen as spanning a semantic basis for understanding what objects look like. Most previous zero-shot learning models can only differentiate between unseen classes. In contrast, our model can both obtain state of the art performance on classes that have thousands of training images and obtain reasonable performance on unseen classes. This is achieved by first using outlier detection in the semantic space and then two separate recognition models. Furthermore, our model does not require any manually defined semantic features for either words or images.", "title": "Zero-Shot Learning Through Cross-Modal Transfer"}, "774f67303ea4a3a94874f08cf9a9dacc69b40782": {"paper_id": "774f67303ea4a3a94874f08cf9a9dacc69b40782", "abstract": "Many object detection systems are constrained by the time required to convolve a target image with a bank of filters that code for different aspects of an object's appearance, such as the presence of component parts. We exploit locality-sensitive hashing to replace the dot-product kernel operator in the convolution with a fixed number of hash-table probes that effectively sample all of the filter responses in time independent of the size of the filter bank. To show the effectiveness of the technique, we apply it to evaluate 100,000 deformable-part models requiring over a million (part) filters on multiple scales of a target image in less than 20 seconds using a single multi-core processor with 20GB of RAM. This represents a speed-up of approximately 20,000 times - four orders of magnitude - when compared with performing the convolutions explicitly on the same hardware. While mean average precision over the full set of 100,000 object classes is around 0.16 due in large part to the challenges in gathering training data and collecting ground truth for so many classes, we achieve a mAP of at least 0.20 on a third of the classes and 0.30 or better on about 20% of the classes.", "title": "Fast, Accurate Detection of 100,000 Object Classes on a Single Machine"}, "061356704ec86334dbbc073985375fe13cd39088": {"paper_id": "061356704ec86334dbbc073985375fe13cd39088", "abstract": "In this work we investigate the effect of the convolutional n etwork depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achi eved by pushing the depth to 16\u201319 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first a nd he second places in the localisation and classification tracks respec tively. We also show that our representations generalise well to other datasets, whe re t y achieve the stateof-the-art results. Importantly, we have made our two bestp rforming ConvNet models publicly available to facilitate further research o n the use of deep visual representations in computer vision.", "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition"}, "5ca4abab527f6b0270e50548f0dea30638c9b86e": {"paper_id": "5ca4abab527f6b0270e50548f0dea30638c9b86e", "abstract": "Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed can nevertheless remain finite: for a special class of initial conditions on the weights, very deep networks incur only a finite, depth independent, delay in learning speed relative to shallow networks. We show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, while scaled random Gaussian initializations cannot. We further exhibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos. Deep learning methods have realized impressive performance in a range of applications, from visual object classification [1, 2, 3] to speech recognition [4] and natural language processing [5, 6]. These successes have been achieved despite the noted difficulty of training such deep architectures [7, 8, 9, 10, 11]. Indeed, many explanations for the difficulty of deep learning have been advanced in the literature, including the presence of many local minima, low curvature regions due to saturating nonlinearities, and exponential growth or decay of back-propagated gradients [12, 13, 14, 15]. Furthermore, many neural network simulations have observed 1 ar X iv :1 31 2. 61 20 v3 [ cs .N E ] 1 9 Fe b 20 14 strikingly nonlinear learning dynamics, including long plateaus of little apparent improvement followed by almost stage-like transitions to better performance. However, a quantitative, analytical understanding of the rich dynamics of deep learning remains elusive. For example, what determines the time scales over which deep learning unfolds? How does training speed retard with depth? Under what conditions will greedy unsupervised pretraining speed up learning? And how do the final learned internal representations depend on the statistical regularities inherent in the training data? Here we provide an exact analytical theory of learning in deep linear neural networks that quantitatively answers these questions for this restricted setting. Because of its linearity, the input-output map of a deep linear network can always be rewritten as a shallow network. In this sense, a linear network does not gain expressive power from depth, and hence will underfit and perform poorly on complex real world problems. But while it lacks this important aspect of practical deep learning systems, a deep linear network can nonetheless exhibit highly nonlinear learning dynamics, and these dynamics change with increasing depth. Indeed, the training error, as a function of the network weights, is non-convex, and gradient descent dynamics on this non-convex error surface exhibits a subtle interplay between different weights across multiple layers of the network. Hence deep linear networks provide an important starting point for understanding deep learning dynamics. To answer these questions, we derive and analyze a set of nonlinear coupled differential equations describing learning dynamics on weight space as a function of the statistical structure of the inputs and outputs. We find exact time-dependent solutions to these nonlinear equations, as well as find conserved quantities in the weight dynamics arising from symmetries in the error function. These solutions provide intuition into how a deep network successively builds up information about the statistical structure of the training data and embeds this information into its weights and internal representations. Moreover, we compare our analytical solutions of learning dynamics in deep linear networks to numerical simulations of learning dynamics in deep non-linear networks, and find that our analytical solutions provide a reasonable approximation. Our solutions also reflect nonlinear phenomena seen in simulations, including alternating plateaus and sharp periods of rapid improvement. Indeed, it has been shown previously [16] that this nonlinear learning dynamics in deep linear networks is sufficient to qualitatively capture aspects of the progressive, hierarchical differentiation of conceptual structure seen in infant development. Next, we apply these solutions to investigate the commonly used greedy layer-wise pretraining strategy for training deep networks [17, 18], and recover conditions under which such pretraining speeds learning. We show that these conditions are approximately satisfied for the MNIST dataset, and that unsupervised pretraining therefore confers an optimization advantage for deep linear networks applied to MNIST. Finally, we exhibit a new class of random orthogonal initial conditions on weights that, in linear networks, provide depth independent learning times, and we show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos. In this regime, synaptic gains are tuned so that linear amplification due to propagation of neural activity through weight matrices exactly balances dampening of activity due to saturating nonlinearities. In particular, we show that even in nonlinear networks, operating in this special regime, Jacobians that are involved in backpropagating error signals act like near isometries. 1 General learning dynamics of gradient descent W 21 W 32 x \u2208 R1 h \u2208 R2 y \u2208 R3 Figure 1: The three layer network analyzed in this section. We begin by analyzing learning in a three layer network (input, hidden, and output) with linear activation functions (Fig 1). We letNi be the number of neurons in layer i. The inputoutput map of the network is y = W W x. We wish to train the network to learn a particular input-output map from", "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks"}, "0ee1916a0cb2dc7d3add086b5f1092c3d4beb38a": {"paper_id": "0ee1916a0cb2dc7d3add086b5f1092c3d4beb38a", "abstract": "The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection. This paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension.", "title": "The Pascal Visual Object Classes (VOC) Challenge"}, "53698b91709112e5bb71eeeae94607db2aefc57c": {"paper_id": "53698b91709112e5bb71eeeae94607db2aefc57c", "abstract": "We investigate architectures of discriminatively trained deep Convolutional Networks (ConvNets) for action recognition in video. The challenge is to capture the complementary information on appearance from still frames and motion between frames. We also aim to incorporate into the network design aspects of the best performing hand-crafted features. Our contribution is three-fold. First, we propose a two-stream ConvNet architecture which incorporates spatial and temporal networks. Second, we demonstrate that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data. Finally, we show that multitask learning, applied to two different action classification datasets, can be used to increase the amount of training data and improve the performance on both. Our architecture is trained and evaluated on the standard video actions benchmarks of UCF-101 and HMDB-51, where it matches the state of the art. It also exceeds by a large margin previous attempts to use deep nets for video classification.", "title": "Two-Stream Convolutional Networks for Action Recognition in Videos"}, "80d800dfadbe2e6c7b2367d9229cc82912d55889": {"paper_id": "80d800dfadbe2e6c7b2367d9229cc82912d55889", "abstract": "I present a new way to parallelize the training of convolutional neural networks across multiple GPUs. The method scales significantly better than all alternatives when applied to modern convolutional neural", "title": "One weird trick for parallelizing convolutional neural networks"}, "659fc2a483a97dafb8fb110d08369652bbb759f9": {"paper_id": "659fc2a483a97dafb8fb110d08369652bbb759f9", "abstract": "The Fisher kernel (FK) is a generic framework which combines the benefits of generative and discriminative approaches. In the context of image classification the FK was shown to extend the popular bag-of-visual-words (BOV) by going beyond count statistics. However, in practice, this enriched representation has not yet shown its superiority over the BOV. In the first part we show that with several well-motivated modifications over the original framework we can boost the accuracy of the FK. On PASCAL VOC 2007 we increase the Average Precision (AP) from 47.9% to 58.3%. Similarly, we demonstrate state-of-the-art accuracy on CalTech 256. A major advantage is that these results are obtained using only SIFT descriptors and costless linear classifiers. Equipped with this representation, we can now explore image classification on a larger scale. In the second part, as an application, we compare two abundant resources of labeled images to learn classifiers: ImageNet and Flickr groups. In an evaluation involving hundreds of thousands of training images we show that classifiers learned on Flickr groups perform surprisingly well (although they were not intended for this purpose) and that they can complement classifiers learned on more carefully annotated datasets.", "title": "Improving the Fisher Kernel for Large-Scale Image Classification"}, "2dc9b005e936c9c303386caacc8d41cabdb1a0a1": {"paper_id": "2dc9b005e936c9c303386caacc8d41cabdb1a0a1", "abstract": "The latest generation of Convolutional Neural Networks (CNN) have achieved impressive results in challenging benchmarks on image recognition and object detection, significantly raising the interest of the community in these methods. Nevertheless, it is still unclear how different CNN methods compare with each other and with previous state-of-the-art shallow representations such as the Bag-of-Visual-Words and the Improved Fisher Vector. This paper conducts a rigorous evaluation of these new techniques, exploring different deep architectures and comparing them on a common ground, identifying and disclosing important implementation details. We identify several useful properties of CNN-based representations, including the fact that the dimensionality of the CNN output layer can be reduced significantly without having an adverse effect on performance. We also identify aspects of deep and shallow methods that can be successfully shared. In particular, we show that the data augmentation techniques commonly applied to CNN-based methods can also be applied to shallow methods, and result in an analogous performance boost. Source code and models to reproduce the experiments in the paper is made publicly available.", "title": "Return of the Devil in the Details: Delving Deep into Convolutional Nets"}, "ed9db7b20e019cdb1c7db8b7921221ee2d9f36e2": {"paper_id": "ed9db7b20e019cdb1c7db8b7921221ee2d9f36e2", "abstract": "Current computational approaches to learning visual object categories require thousands of training images, are slow, cannot learn in an incremental manner and cannot incorporate prior information into the learning process. In addition, no algorithm presented in the literature has been tested on more than a handful of object categories. We present an method for learning object categories from just a few training images. It is quick and it uses prior information in a principled way. We test it on a dataset composed of images of objects belonging to 101 widely varied categories. Our proposed method is based on making use of prior information, assembled from (unrelated) object categories which were previously learnt. A generative probabilistic model is used, which represents the shape and appearance of a constellation of features belonging to the object. The parameters of the model are learnt incrementally in a a c Purchase Export Previous article Next article Check if you have access through your login credentials or your institution.", "title": "Learning Generative Visual Models from Few Training Examples: An Incremental Bayesian Approach Tested on 101 Object Categories"}, "398c296d0cc7f9d180f84969f8937e6d3a413796": {"paper_id": "398c296d0cc7f9d180f84969f8937e6d3a413796", "abstract": "Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible, wide and deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive MNIST handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks.", "title": "Multi-column deep neural networks for image classification"}, "54dd77bd7b904a6a69609c9f3af11b42f654ab5d": {"paper_id": "54dd77bd7b904a6a69609c9f3af11b42f654ab5d", "abstract": null, "title": "ImageNet: A large-scale hierarchical image database"}, "57820e6f974d198bf4bbdf26ae7e1063bac190c3": {"paper_id": "57820e6f974d198bf4bbdf26ae7e1063bac190c3", "abstract": null, "title": "The Semantic Web\" in Scientific American"}, "65227ddbbd12015ba8a45a81122b1fa540e79890": {"paper_id": "65227ddbbd12015ba8a45a81122b1fa540e79890", "abstract": "The importance of a Web page is an inherently subjective matter, which depends on the readers interests, knowledge and attitudes. But there is still much that can be said objectively about the relative importance of Web pages. This paper describes PageRank, a method for rating Web pages objectively and mechanically, e ectively measuring the human interest and attention devoted to them. We compare PageRank to an idealized random Web surfer. We show how to e ciently compute PageRank for large numbers of pages. And, we show how to apply PageRank to search and to user navigation.", "title": "The PageRank Citation Ranking: Bringing Order to the Web."}, "2a56a1a776beda725f8d1873eef6dc82396c90d1": {"paper_id": "2a56a1a776beda725f8d1873eef6dc82396c90d1", "abstract": "In this paper we study in what order a crawler should visit the URLs it has seen, in order to obtain more \u201cimportant\u201d pages first. Obtaining important pages rapidly can be very useful when a crawler cannot visit the entire Web in a reasonable amount of time. We define several importance metrics, ordering schemes, and performance evaluation measures for this problem. We also experimentally evaluate the ordering schemes on the Stanford University Web. Our results show that a crawler with a good ordering scheme can obtain important pages significantly faster than one without.", "title": "Efficient Crawling Through URL Ordering"}, "6653fd1fb4f9474f857de9a8c26e9ec979b580f1": {"paper_id": "6653fd1fb4f9474f857de9a8c26e9ec979b580f1", "abstract": "The war against cybercrime is a constant battle. While cyber criminals keep using the same basic attack techniques [M.v.j.], the amount and diversity of malware grows [M. Fossi]. This renders security defenses ineffective such that millions of computers are infected with malware in the form of computer viruses, internet worms and Trojan horses. These cybercrimes cost the society money [G. Lovet, M. Clement, C. Kanich] and are a threat to our privacy. Intrusion detection is a critical component when fighting cybercrime in the area of network security. Intrusion detection systems (IDS) look at characteristics of network packets or series of packets to determine whether the packets are malicious or not. The goal of this thesis is to show how better and more complex firewall rules can be created with the use of machine learning algorithms. These new firewall rules contribute to better computer intrusion detection systems. This thesis looks at existing machine learning methods such as random forests [L. Breiman] and neural networks [K. Hornik] and how firewall rules can be extracted from the resulting models. The models are trained and tested on a novel labeled network data set containing malicious and normal packets. This data set was created for this thesis and has not been used before. How the data set is created is also presented in this thesis. However, as machine learning techniques work well on networks with similar traffic such as SCADA networks, a special chapter in Appendix A is included to this thesis on SCADA networks.", "title": "Creating firewall rules with machine learning techniques"}, "47f94c0cd9876dea43d76a5be98de7646457c000": {"paper_id": "47f94c0cd9876dea43d76a5be98de7646457c000", "abstract": "High dimensionality of text can be a deterrent in applying co mplex learners such as Support Vector Machines to the task of text classification. Feature cluster ing is a powerful alternative to feature selection for reducing the dimensionality of text data. In t his paper we propose a new informationtheoretic divisive algorithm for feature/word clustering and apply it to text classification. Existing techniques for such \u201cdistributional clustering\u201d of words a re gglomerative in nature and result in (i) sub-optimal word clusters and (ii) high computational cost . In order to explicitly capture the optimality of word clusters in an information theoretic framewo rk, we first derive a global criterion for feature clustering. We then present a fast, divisive algori thm that monotonically decreases this objective function value. We show that our algorithm minimize s the \u201cwithin-cluster Jensen-Shannon divergence\u201d while simultaneously maximizing the \u201cbetween -cluster Jensen-Shannon divergence\u201d. In comparison to the previously proposed agglomerative str ategies our divisive algorithm is much faster and achieves comparable or higher classification acc ur ies. We further show that feature clustering is an effective technique for building smaller c lass models in hierarchical classification. We present detailed experimental results using Naive Bayes nd Support Vector Machines on the 20Newsgroups data set and a 3-level hierarchy of HTML docume nts collected from the Open Directory project ( www.dmoz.org).", "title": "A Divisive Information-Theoretic Feature Clustering Algorithm for Text Classification"}, "9ca182ba46b2c60b3b9ffaf8234312a933a8cc6f": {"paper_id": "9ca182ba46b2c60b3b9ffaf8234312a933a8cc6f", "abstract": "1. ABSTRACT Text categorization \u2013 the assignment of natural language texts to one or more predefined categories based on their content \u2013 is an important component in many information organization and management tasks. We compare the effectiveness of five different automatic learning algorithms for text categorization in terms of learning speed, realtime classification speed, and classification accuracy. We also examine training set size, and alternative document representations. Very accurate text classifiers can be learned automatically from training examples. Linear Support Vector Machines (SVMs) are particularly promising because they are very accurate, quick to train, and quick to evaluate. 1.1", "title": "Inductive Learning Algorithms and Representations for Text Categorization"}, "15e0daa3d2e1438159e96f6c6fd6c4dd3756052c": {"paper_id": "15e0daa3d2e1438159e96f6c6fd6c4dd3756052c", "abstract": "We define the relevant information in a signal x \u2208 X as being the information that this signal provides about another signal y \u2208 Y . Examples include the information that face images provide about the names of the people portrayed, or the information that speech sounds provide about the words spoken. Understanding the signal x requires more than just predicting y, it also requires specifying which features of X play a role in the prediction. We formalize this problem as that of finding a short code for X that preserves the maximum information about Y . That is, we squeeze the information that X provides about Y through a \u2018bottleneck\u2019 formed by a limited set of codewords X\u0303. This constrained optimization problem can be seen as a generalization of rate distortion theory in which the distortion measure d(x, x\u0303) emerges from the joint statistics of X and Y . This approach yields an exact set of self consistent equations for the coding rules X \u2192 X\u0303 and X\u0303 \u2192 Y . Solutions to these equations can be found by a convergent re\u2013estimation method that generalizes the Blahut\u2013Arimoto algorithm. Our variational principle provides a surprisingly rich framework for discussing a variety of problems in signal processing and learning, as will be described in detail elsewhere.", "title": "The information bottleneck method"}, "0b47b6ffe714303973f40851d975c042ff4fcde1": {"paper_id": "0b47b6ffe714303973f40851d975c042ff4fcde1", "abstract": "We describe and experimentally evaluate a method for automatically clustering words according to their distribution in particular syntactic contexts. Deterministic annealing is used to find lowest distortion sets of clusters. As the annealing parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical \u201csoft\u201d clustering of the data. Clusters are used as the basis for class models of word coocurrence, and the models evaluated with respect to held-out test data.", "title": "Distributional Clustering of English Words"}, "8884ef78bba4ceea3272ed2d25cafde8fc981c62": {"paper_id": "8884ef78bba4ceea3272ed2d25cafde8fc981c62", "abstract": "This paper explores the use of hierarchical structure for classifying a large, heterogeneous collection of web content. The hierarchical structure is initially used to train different second-level classifiers. In the hierarchical case, a model is learned to distinguish a second-level category from other categories within the same top level. In the flat non-hierarchical case, a model distinguishes a second-level category from all other second-level categories. Scoring rules can further take advantage of the hierarchy by considering only second-level categories that exceed a threshold at the top level.\nWe use support vector machine (SVM) classifiers, which have been shown to be efficient and effective for classification, but not previously explored in the context of hierarchical classification. We found small advantages in accuracy for hierarchical models over flat models. For the hierarchical approach, we found the same accuracy using a sequential Boolean decision rule and a multiplicative decision rule. Since the sequential approach is much more efficient, requiring only 14%-16% of the comparisons used in the other approaches, we find it to be a good choice for classifying text into large hierarchical structures.", "title": "Hierarchical classification of Web content"}, "2375f6d71ce85a9ff457825e192c36045e994bdd": {"paper_id": "2375f6d71ce85a9ff457825e192c36045e994bdd", "abstract": null, "title": "Multilayer feedforward networks are universal approximators"}, "ee7f0bc85b339d781c2e0c7e6db8e339b6b9fec2": {"paper_id": "ee7f0bc85b339d781c2e0c7e6db8e339b6b9fec2", "abstract": "K.M. Hornik, M. Stinchcombe, and H. White (Univ. of California at San Diego, Dept. of Economics Discussion Paper, June 1988; to appear in Neural Networks) showed that multilayer feedforward networks with as few as one hidden layer, no squashing at the output layer, and arbitrary sigmoid activation function at the hidden layer are universal approximators: they are capable of arbitrarily accurate approximation to arbitrary mappings, provided sufficiently many hidden units are available. The present authors obtain identical conclusions but do not require the hidden-unit activation to be sigmoid. Instead, it can be a rather general nonlinear function. Thus, multilayer feedforward networks possess universal approximation capabilities by virtue of the presence of intermediate layers with sufficiently many parallel processors; the properties of the intermediate-layer activation function are not so crucial. In particular, sigmoid activation functions are not necessary for universal approximation.<<ETX>>", "title": "Universal approximation using feedforward networks with non-sigmoid hidden layer activation functions"}, "60b7c281f3a677274b7126c67b7f4059c631b1ea": {"paper_id": "60b7c281f3a677274b7126c67b7f4059c631b1ea", "abstract": "The authors show that a multiple-input, single-output, single-hidden-layer feedforward network with (known) hardwired connections from input to hidden layer, monotone squashing at the hidden layer and no squashing at the output embeds as a special case a so-called Fourier network, which yields a Fourier series approximation properties of Fourier series representations. In particular, approximation to any desired accuracy of any square integrable function can be achieved by such a network, using sufficiently many hidden units. In this sense, such networks do not make avoidable mistakes.<<ETX>>", "title": "There exists a neural network that does not make avoidable mistakes"}, "3f85020032ae335baf57aaf65c4831b67e4030c9": {"paper_id": "3f85020032ae335baf57aaf65c4831b67e4030c9", "abstract": "We present the first marker-less approach for temporally coherent 3D performance capture of a human with general clothing from monocular video. Our approach reconstructs articulated human skeleton motion as well as medium-scale non-rigid surface deformations in general scenes. Human performance capture is a challenging problem due to the large range of articulation, potentially fast motion, and considerable non-rigid deformations, even from multi-view data. Reconstruction from monocular video alone is drastically more challenging, since strong occlusions and the inherent depth ambiguity lead to a highly ill-posed reconstruction problem. We tackle these challenges by a novel approach that employs sparse 2D and 3D human pose detections from a convolutional neural network using a batch-based pose estimation strategy. Joint recovery of per-batch motion allows us to resolve the ambiguities of the monocular reconstruction problem based on a low-dimensional trajectory subspace. In addition, we propose refinement of the surface geometry based on fully automatically extracted silhouettes to enable medium-scale non-rigid alignment. We demonstrate state-of-the-art performance capture results that enable exciting applications such as video editing and free viewpoint video, previously infeasible from monocular video. Our qualitative and quantitative evaluation demonstrates that our approach significantly outperforms previous monocular methods in terms of accuracy, robustness, and scene complexity that can be handled.", "title": "MonoPerfCap: Human Performance Capture From Monocular Video"}, "1f8c88e2f5d33d73d2857fb1727b1845ad3a32be": {"paper_id": "1f8c88e2f5d33d73d2857fb1727b1845ad3a32be", "abstract": "Music perception is highly intertwined with both emotions and context. Not surprisingly, many of the users\u2019 information seeking actions aim at retrieving music songs based on these perceptual dimensions \u2013 moods and themes, expressing how people feel about music or which situations they associate it with. In order to successfully support music retrieval along these dimensions, powerful methods are needed. Still, most existing approaches aiming at inferring some of the songs\u2019 latent characteristics focus on identifying musical genres. In this paper we aim at bridging this gap between users\u2019 information needs and indexed music features by developing algorithms for classifying music songs by moods and themes. We extend existing approaches by also considering the songs\u2019 thematic dimensions and by using social data from the Last.fm music portal, as support for the classification tasks. Our methods exploit both audio features and collaborative user annotations, fusing them to improve overall performance. Evaluation performed against the AllMusic.com ground truth shows that both kinds of information are complementary and should be merged for enhanced classification accuracy.", "title": "Music Mood and Theme Classification - a Hybrid Approach"}, "38dd11deaadd13d2c21904ae2bdf9789f6cfbbbb": {"paper_id": "38dd11deaadd13d2c21904ae2bdf9789f6cfbbbb", "abstract": "User studies focusing upon real-life music information needs, uses and seeking behaviours are still very scarce in the music information retrieval (MIR) and music digital library (MDL) fields. We are conducting a multigroup survey in an attempt to acquire information that can help eradicate false assumptions in designing MIR systems. Our goal is to provide an empirical basis for MIR/MDL system development. In this paper, we present our preliminary findings and analyses based on the 427 user responses we have received to date. Two major themes have been uncovered thus far that could have a significant influence the future development of successful MIR/MDL systems. First, people display \u201cpublic information-seeking\u201d behaviours by making use of collective knowledge and/or opinions of others about music such as reviews, ratings, recommendations, etc. in their music information-seeking. Second, respondents expressed needs for contextual metadata in addition to traditional bibliographic metadata.", "title": "Survey Of Music Information Needs, Uses, And Seeking Behaviours: Preliminary Findings"}, "8118675b02698d82ae0a9bca863f413385630103": {"paper_id": "8118675b02698d82ae0a9bca863f413385630103", "abstract": "In this paper we investigate social tags as a novel highvolume source of semantic metadata for music, using techniques from the fields of information retrieval and multivariate data analysis. We show that, despite the ad hoc and informal language of tagging, tags define a low-dimensional semantic space that is extremely well-behaved at the track level, in particular being highly organised by artist and musical genre. We introduce the use of Correspondence Analysis to visualise this semantic space, and show how it can be applied to create a browse-by-mood interface for a psychologically-motivated two-dimensional subspace rep resenting musical emotion.", "title": "A Semantic Space for Music Derived from Social Tags"}, "7e7a9899a839eda239ebb001bc6b45002fc1a2b7": {"paper_id": "7e7a9899a839eda239ebb001bc6b45002fc1a2b7", "abstract": "This paper reviews the state-of-the-art in automatic genre classification of music collections through three main paradigms: expert systems, unsupervised classification, and supervised classification. The paper discusses the importance of music genres with their definitions and hierarchies. It also presents techniques to extract meaningful information from audio data to characterize musical excerpts. The paper also presents the results of new emerging research fields and techniques that investigate the proximity of music genres", "title": "Automatic genre classification of music content: a survey"}, "07f1bf314056d39c24e995dab1e0a44a5cae3df0": {"paper_id": "07f1bf314056d39c24e995dab1e0a44a5cae3df0", "abstract": "Pairwise coupling is a popular multi-class classification method that combines together all pairwise comparisons for each pair of classes. This paper presents two approaches for obtaining class probabilities. Both methods can be reduced to linear systems and are easy to implement. We show conceptually and experimentally that the proposed approaches are more stable than two existing popular methods: voting and [3].", "title": "Probability Estimates for Multi-class Classification by Pairwise Coupling"}, "f1161cd952f7c52266d0761ab4973c1c134b4962": {"paper_id": "f1161cd952f7c52266d0761ab4973c1c134b4962", "abstract": "Algorithms which construct classifiers from sample data -such as neural networks, radial basis functions, and decision trees -have attracted growing attention for their wide applicability. Researchers in the fields of Statistics, Artificial Intelligence, Machine Learning, Data Mining, and Pattern Recognition are continually introducing (or rediscovering) induction methods, and often publishing implementing code. It is natural for practitioners and potential users to wonder, \"Which classification technique is best?\", or more realistically, \"What subset of methods tend to work well for a given type of dataset?\". This book provides perhaps the best current answer to that question.", "title": "Machine Learning, Neural and Statistical Classification"}, "cbcd9f32b526397f88d18163875d04255e72137f": {"paper_id": "cbcd9f32b526397f88d18163875d04255e72137f", "abstract": null, "title": "Gradient-based learning applied to document recognition"}, "2a7442d3e699515bf6c29a876cc714ae77faf6bd": {"paper_id": "2a7442d3e699515bf6c29a876cc714ae77faf6bd", "abstract": "We discuss a strategy for polychotomous classification that involves estimating class probabilities for each pair of classes, and then coupling the estimates together. The coupling model is similar to the Bradley-Terry method for paired comparisons. We study the nature of the class probability estimates that arise, and examine the performance of the procedure in simulated datasets. The classifiers used include linear discriminants and nearest neighbors: application to support vector machines is also briefly described.", "title": "Classification by Pairwise Coupling"}, "384bb3944abe9441dcd2cede5e7cd7353e9ee5f7": {"paper_id": "384bb3944abe9441dcd2cede5e7cd7353e9ee5f7", "abstract": null, "title": "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods"}, "62a134740314b4469c83c8921ae2e1beea22b8f5": {"paper_id": "62a134740314b4469c83c8921ae2e1beea22b8f5", "abstract": "We propose in this correspondence a new method to perform two-class clustering of 2-D data in a quick and automatic way by preserving certain features of the input data. The method is analytical, deterministic, unsupervised, automatic, and noniterative. The computation time is of order n if the data size is n, and hence much faster than any other method which requires the computation of an n-by-n dissimilarity matrix. Furthermore, the proposed method does not have the trouble of guessing initial values. This new approach is thus more suitable for fast automatic hierarchical clustering or any other fields requiring fast automatic two-class clustering of 2-D data. The method can be extended to cluster data in higher dimensional space. A 3-D example is included.", "title": "A Database for Handwritten Text Recognition Research"}, "6683426ca06560523fc7461152d4dd3b84a07854": {"paper_id": "6683426ca06560523fc7461152d4dd3b84a07854", "abstract": "Social tags are user-generated keywords associated with some resource on the Web. In the case of music, social tags have become an important component of \u201cWeb 2.0\u201d recommender systems, allowing users to generate playlists based on use-dependent terms such as chill or jogging that have been applied to particular songs. In this paper, we propose a method for predicting these social tags directly from MP3 files. Using a set of 360 classifiers trained using the online ensemble learning algorithm FilterBoost, we map audio features onto social tags collected from the Web. The resulting automatic tags (or autotags) furnish information about music that is otherwise untagged or poorly tagged, allowing for insertion of previously unheard music into a social recommender. This avoids the \u201ccold-start problem\u201d common in such systems. Autotags can also be used to smooth the tag space from which similarities and recommendations are made by providing a set of comparable baseline tags for all tracks in a recommender system. Because the words we learn are the same as those used by people who label their music collections, it is easy to integrate our predictions into existing similarity and prediction methods based on web data.", "title": "Autotagger: A Model for Predicting Social Tags from Acoustic Features on Large Music Databases"}, "6cf8ec34a008031b018c8a3a4640a87f476d0925": {"paper_id": "6cf8ec34a008031b018c8a3a4640a87f476d0925", "abstract": "We introduce a new interactive system: a game that is fun and can be used to create valuable output. When people play the game they help determine the contents of images by providing meaningful labels for them. If the game is played as much as popular online games, we estimate that most images on the Web can be labeled in a few months. Having proper labels associated with each image on the Web would allow for more accurate image search, improve the accessibility of sites (by providing descriptions of images to visually impaired individuals), and help users block inappropriate images. Our system makes a significant contribution because of its valuable output and because of the way it addresses the image-labeling problem. Rather than using computer vision techniques, which don't work well enough, we encourage people to do the work by taking advantage of their desire to be entertained.", "title": "Labeling images with a computer game"}, "0cf464e413e1e051b535d4d42de4ecd49853053e": {"paper_id": "0cf464e413e1e051b535d4d42de4ecd49853053e", "abstract": "We present a computer audition system that can both annotate novel audio tracks with semantically meaningful words and retrieve relevant tracks from a database of unlabeled audio content given a text-based query. We consider the related tasks of content-based audio annotation and retrieval as one supervised multiclass, multilabel problem in which we model the joint probability of acoustic features and words. We collect a data set of 1700 human-generated annotations that describe 500 Western popular music tracks. For each word in a vocabulary, we use this data to train a Gaussian mixture model (GMM) over an audio feature space. We estimate the parameters of the model using the weighted mixture hierarchies expectation maximization algorithm. This algorithm is more scalable to large data sets and produces better density estimates than standard parameter estimation techniques. The quality of the music annotations produced by our system is comparable with the performance of humans on the same task. Our ldquoquery-by-textrdquo system can retrieve appropriate songs for a large number of musically relevant words. We also show that our audition system is general by learning a model that can annotate and retrieve sound effects.", "title": "Semantic Annotation and Retrieval of Music and Sound Effects"}, "95dcdc9f2de1f30a4f0c79c378f98f11aa618f40": {"paper_id": "95dcdc9f2de1f30a4f0c79c378f98f11aa618f40", "abstract": "Content-based music genre classification is a fundamental component of music information retrieval systems and has been gaining importance and enjoying a growing amount of attention with the emergence of digital music on the Internet. Currently little work has been done on automatic music genre classification, and in addition, the reported classification accuracies are relatively low. This paper proposes a new feature extraction method for music genre classification, DWCHs. DWCHs stands for Daubechies Wavelet Coefficient Histograms. DWCHs capture the local and global information of music signals simultaneously by computing histograms on their Daubechies wavelet coefficients. Effectiveness of this new feature and of previously studied features are compared using various machine learning classification algorithms, including Support Vector Machines and Linear Discriminant Analysis. It is demonstrated that the use of DWCHs significantly improves the accuracy of music genre classification.", "title": "A comparative study on content-based music genre classification"}, "300ce829f75184b656540857acaf6c9ed3c68bea": {"paper_id": "300ce829f75184b656540857acaf6c9ed3c68bea", "abstract": "Social tags are user-generated keywords associated with some resource on the Web. In the case of music, social tags have become an important component of \u201cWeb2.0\u201d recommender systems, allowing users to generate playlists based on use-dependent terms such as chill or jogging that have been applied to particular songs. In this paper, we propose a method for predicting these social tags directly from MP3 files. Using a set of boosted classifiers, we map audio features onto social tags collected from the Web. The resulting automatic tags (or autotags) furnish information about music that is otherwise untagged or poorly tagged, allowing for insertion of previously unheard music into a social recommender. This avoids the \u201dcold-start problem\u201d common in such systems. Autotags can also be used to smooth the tag space from which similarities and recommendations are made by providing a set of comparable baseline tags for all tracks in a recommender system.", "title": "Automatic Generation of Social Tags for Music Recommendation"}, "196523c04f9e845cc6dde43f50e40d38909ffafd": {"paper_id": "196523c04f9e845cc6dde43f50e40d38909ffafd", "abstract": "Automated collaborative filtering (ACF) systems predict a person's affinity for items or information by connecting that person's recorded interests with the recorded interests of a community of people and sharing ratings between like-minded persons. However, current recommender systems are black boxes, providing no transparency into the working of the recommendation. Explanations provide that transparency, exposing the reasoning and data behind a recommendation. In this paper, we address explanation interfaces for ACF systems - how they should be implemented and why they should be implemented. To explore how, we present a model for explanations based on the user's conceptual model of the recommendation process. We then present experimental results demonstrating what components of an explanation are the most compelling. To address why, we present experimental evidence that shows that providing explanations can improve the acceptance of ACF systems. We also describe some initial explorations into measuring how explanations can improve the filtering performance of users.", "title": "Explaining collaborative filtering recommendations"}}